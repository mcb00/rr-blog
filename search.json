[
  {
    "objectID": "gradient-boosting-series.html",
    "href": "gradient-boosting-series.html",
    "title": "Gradient Boosting",
    "section": "",
    "text": "Ahh, gradient boosting. In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. This series of posts strives to give a comprehensive understanding of gradient boosting by providing intuitive mathematical explanations, from-scratch implementations of key algorithms, and examples of how to apply modern gradient boosting libraries to solve practical data science problems.\nI recommend reading through the series in order, since concepts tend to build on earlier ideas.\n\n\n\n\n  \n\n\n\n\nHow to Build a Gradient Boosting Machine from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nUnderstand the intuition behind the gradient boosting machine (GBM) and learn how to implement it from scratch.\n\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\nGet Down with Gradient Descent\n\n\n\n\n\n\n\ngradient boosting\n\n\n\n\nGet down with the intuition for gradient descent via a fresh analogy, develop the mathematical formulation of the algorithm, and implement it from scratch to train a linear regression model.\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow Gradient Boosting Does Gradient Descent\n\n\n\n\n\n\n\ngradient boosting\n\n\n\n\nUnderstand how gradient boosting does gradient descent in function space to minimize any differentiable loss function in the service of creating a good model.\n\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow to Implement a Gradient Boosting Machine that Works with Any Loss Function\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nSummarize Friedman’s seminal GBM paper and implement the generic gradient boosting algorithm to train models with any differentiable loss function.\n\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\nConsider the Decision Tree\n\n\n\n\n\n\n\ngradient boosting\n\n\n\n\nUnderstand the core strengths and weaknesses of the decision tree, and see how ensembling makes trees shine.\n\n\n\n\n\n\nDec 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\nDecision Tree from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nA detailed walkthrough of my from-scratch decision tree implementation in python.\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost Explained\n\n\n\n\n\n\n\ngradient boosting\n\n\nxgboost\n\n\n\n\nIn-depth explanation and mathematical derivation of the XGBoost algorithm\n\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nxgboost\n\n\nfrom scratch\n\n\n\n\nA walkthrough of my from-scratch python implementation of XGBoost.\n\n\n\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost for Regression in Python\n\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\n\nA step-bystep tutorial on regression with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\n\nSep 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGradient Boosting Multi-Class Classification from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nHow to implement multi-class classification for gradient boosting from scratch in python\n\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost for Binary and Multi-Class Classification in Python\n\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\n\nA step-bystep tutorial on binary and multi-class classification with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\n\nNov 28, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "The Ultimate Guide to XGBoost Parameter Tuning\n\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\n\nMy approach for efficiently tuning XGBoost parameters with optuna in python\n\n\n\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost for Binary and Multi-Class Classification in Python\n\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\n\nA step-bystep tutorial on binary and multi-class classification with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\n\nNov 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGradient Boosting Multi-Class Classification from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nHow to implement multi-class classification for gradient boosting from scratch in python\n\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost for Regression in Python\n\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\n\nA step-bystep tutorial on regression with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\n\nSep 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBlogging with Quarto and Jupyter: The Complete Guide\n\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\nblogging\n\n\n\n\nStep-by-step tutorial and best practices for creating a python blog with quarto and jupyter\n\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRandom Realizations Resurrected\n\n\n\n\n\n\n\nblogging\n\n\n\n\nThe world’s favorite data science blog is back.\n\n\n\n\n\n\nAug 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nxgboost\n\n\nfrom scratch\n\n\n\n\nA walkthrough of my from-scratch python implementation of XGBoost.\n\n\n\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\nXGBoost Explained\n\n\n\n\n\n\n\ngradient boosting\n\n\nxgboost\n\n\n\n\nIn-depth explanation and mathematical derivation of the XGBoost algorithm\n\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nDecision Tree from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nA detailed walkthrough of my from-scratch decision tree implementation in python.\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nConsider the Decision Tree\n\n\n\n\n\n\n\ngradient boosting\n\n\n\n\nUnderstand the core strengths and weaknesses of the decision tree, and see how ensembling makes trees shine.\n\n\n\n\n\n\nDec 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow to Implement a Gradient Boosting Machine that Works with Any Loss Function\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nSummarize Friedman’s seminal GBM paper and implement the generic gradient boosting algorithm to train models with any differentiable loss function.\n\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHello PySpark!\n\n\n\n\n\n\n\npython\n\n\nPySpark\n\n\ntutorial\n\n\n\n\nGet up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale.\n\n\n\n\n\n\nJun 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow Gradient Boosting Does Gradient Descent\n\n\n\n\n\n\n\ngradient boosting\n\n\n\n\nUnderstand how gradient boosting does gradient descent in function space to minimize any differentiable loss function in the service of creating a good model.\n\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGet Down with Gradient Descent\n\n\n\n\n\n\n\ngradient boosting\n\n\n\n\nGet down with the intuition for gradient descent via a fresh analogy, develop the mathematical formulation of the algorithm, and implement it from scratch to train a linear regression model.\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow to Build a Gradient Boosting Machine from Scratch\n\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\n\nUnderstand the intuition behind the gradient boosting machine (GBM) and learn how to implement it from scratch.\n\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\nThe 80/20 Pandas Tutorial\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\ntutorial\n\n\n\n\nAn opinionated pandas tutorial on my preferred methods to accomplish the most essential data transformation tasks in a way that will make veteran R and tidyverse users smile.\n\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n  \n\n\n\n\nHello World! And Why I’m Inspired to Start a Blog\n\n\n\n\n\n\n\nblogging\n\n\n\n\nA reflection on what inspired me to start a blog and three reasons I think it could be a good idea.\n\n\n\n\n\n\nNov 22, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/xgboost-explained/index.html",
    "href": "posts/xgboost-explained/index.html",
    "title": "XGBoost Explained",
    "section": "",
    "text": "Tree branches on a chilly day in Johnson City\nAhh, XGBoost, what an absolutely stellar implementation of gradient boosting. Once Tianqi Chen and Carlos Guestrin of the University of Washington published the XGBoost paper and shared the open source code in the mid 2010’s, the algorithm quickly gained adoption in the ML community, appearing in over half of winning Kagle submissions in 2015. Nowadays it’s certainly among the most popular gradient boosting libraries, along with LightGBM and CatBoost, although the highly scientific indicator of GitHub stars per year indicates that it is in fact the most beloved gradient boosting package of all. Since it was the first of the modern popular boosting frameworks, and since benchmarking indicates that no other boosting algorithm outperforms it, we can comfortably focus our attention on understanding XGBoost.\nThe XGBoost authors identify two key aspects of a machine learning system: (1) a flexible statistical model and (2) a scalable learning system to fit that model using data. XGBoost improves on both of these aspects, providing a more flexible and feature-rich statistical model and building a truly scalable system to fit it. In this post we’re going to focus on the statistical modeling innovations, outlining the key differences from the classic gradient boosting machine and divinginto the mathematical derivation of the XGBoost learning algorithm. If you’re not already familiar with gradient boosting, go back and read the earlier posts in the series before jumping in here.\nBuckle up, dear reader. Today we understand how XGBoost works, no hand waving required."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#xgboost-is-a-gradient-boosting-machine",
    "href": "posts/xgboost-explained/index.html#xgboost-is-a-gradient-boosting-machine",
    "title": "XGBoost Explained",
    "section": "XGBoost is a Gradient Boosting Machine",
    "text": "XGBoost is a Gradient Boosting Machine\nAt a high level, XGBoost is an iteratively constructed composite model, just like the classic gradient boosting machine we discussed back in the GBM post . The final model takes the form\n\\[\\hat{y}_i = F(\\mathbf{x}_i) = b + \\eta \\sum_{k=1}^K f_k(\\mathbf{x}_i) \\]\nwhere \\(b\\) is the base prediction, \\(\\eta\\) is the learning rate hyperparameter that helps control overfitting by reducing the contributions of each booster, and each of the \\(K\\) boosters \\(f_k\\) is a decision tree. To help us connect the dots between theory and code, whenever we encounter new hyperparameters, I’ll point out their names from the XGBoost Parameter Documentation. So, \\(b\\) can be set by base_score, and \\(\\eta\\) is set by either eta or learning_rate.\nXGBoost introduces two key statistical learning improvements over the classic gradient boosting model. First, it reimagines the gradient descent algorithm used for training, and second it uses a custom-built decision tree with extra functionality as its booster. We’ll dive into each of these key innovations in the following sections."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#descent-algorithm-innovations",
    "href": "posts/xgboost-explained/index.html#descent-algorithm-innovations",
    "title": "XGBoost Explained",
    "section": "Descent Algorithm Innovations",
    "text": "Descent Algorithm Innovations\n\nRegularized Objective Function\nIn the post on GBM with any loss function, we looked at loss functions of the form \\(\\sum_i l(y_i,\\hat{y}_i)\\) which compute some distance between targets \\(y_i\\) and predictions \\(\\hat{y}_i\\) and sum them up over the training dataset. XGBoost introduces regularization into the objective function so that the objective takes the form\n\\[ L = \\sum_i l(y_i,\\hat{y}_i) + \\sum_k \\Omega(f_k) \\]\nwhere \\(l\\) is some twice-differentiable loss function. \\(\\Omega\\) is a regularization that penalizes the complexity of each tree booster, taking the form\n\\[ \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2 \\]\nwhere \\(T\\) is the number of leaf nodes and \\(||w||^2\\) is the squared sum of the leaf prediction values. This introduces two new hyperparameters: \\(\\gamma\\) which penalizes the number of leaf nodes and \\(\\lambda\\) which is the L-2 regularization parameter for leaf predicted values. These are set by gamma and reg_lambbda in the XGBoost parametrization. Together, these provide powerful new controls to reduce overfitting due to overly complex tree boosters. Note that \\(\\gamma=\\lambda=0\\) reduces the objective back to an unregularized loss function as used in the classic GBM.\n\n\nAn Aside on Newton’s Method\nAs we’ll see soon, XGBoost uses Newton’s Method to minimize its objective function, so let’s start with a quick refresher.\nNewton’s method is an iterative procedure for minimizing a function \\(s(x)\\). At each step we have some input \\(x_t\\), and our goal is to find a nudge value \\(u\\) such that\n\\[ s(x_t + u) \\le s(x_t)\\]\nTo find a good nudge value \\(u\\), we generate a local quadratic approximation of the function in the neighborhood of the input \\(x_t\\), and then we find the input value that would bring us to the minimum of the quadratic approximation.\n\n\n\nSchematic of Newton’s method\n\n\nThe figure shows a single Newton step where we start at \\(x_t\\), find the local quadratic approximation, and then jump a distance \\(u\\) along the \\(x\\)-axis to land at the minimum of the quadratic. If we iterate in this way, we are likely to land close to the minimum of \\(s(x)\\).\nSo how do we compute the quadratic approximation? We use the second order Taylor series expansion of \\(s(x)\\) near the point \\(x_t\\).\n\\[ s(x_t + u) \\approx  s(x_t) + s'(x_t)u + \\frac{1}{2} s''(x_t) u^2 \\]\nTo find the nudge value \\(u\\) that minimizes the quadratic approximation, we can take the derivative with respect to \\(u\\), set it to zero, and solve for \\(u\\).\n\\[ 0 = \\frac{d}{du}  \\left ( s(x_t) + s'(x_t)u + \\frac{1}{2} s''(x_t) u^2 \\right ) = s'(x_t) + s''(x_t) u \\]\n\\[\\rightarrow u^* = -\\frac{s'(x_t)}{s''(x_t)} \\]\nAnd as long as \\(s''(x_t)&gt;0\\) (i.e., the parabola is pointing up), \\(s(x_t + u^*) \\le s(x_t)\\).\n\n\nTree Boosting with Newton’s Method\nThis lands us at the heart of XGBoost, which uses Newton’s method, rather than gradient descent, to guide each round of boosting. This explanation will correspond very closely to section 2.2 of the XGBoost paper, but here I’ll explicitly spell out some of the intermediate steps which are omitted from their derivation, and you’ll get some additional commentary from me along the way.\n\nNewton Descent in Tree Space\nSuppose we’ve done \\(t-1\\) boosting rounds, and we want to add the \\(t\\)-th booster to our composite model. Our current model’s prediction for instance \\(i\\) is \\(\\hat{y}_i^{(t-1)}\\). If we add a new tree booster \\(f_t\\) to our model, the objective function would give\n\\[ L^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t) \\]\nWe need to choose \\(f_t\\) so that it decreases the loss, i.e. we want\n\\[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) \\le l(y_i, \\hat{y}_i^{(t-1)})\\]\nDoes that sound familiar? In the previous section we used Newton’s method to find a value of \\(u\\) that would make \\(s(x_t + u) \\le s(x_t)\\). Let’s try the same thing with our loss function. To be explicit, the parallels are: \\(s(\\cdot) \\rightarrow l(y_i, \\cdot)\\), \\(x_t \\rightarrow \\hat{y}_i^{(t-1)}\\), and \\(u \\rightarrow f_t(\\mathbf{x}_i)\\).\nLet’s start by finding the second order Taylor series approximation for the loss around the point \\(\\hat{y}_i^{(t-1)}\\).\n\\[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) \\approx l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t(\\mathbf{x}_i)^2 \\]\nwhere\n\\[ g_i = \\frac{\\partial}{\\partial \\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)})\\]\nand\n\\[ h_i = \\frac{\\partial}{\\partial^2 \\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)})\\]\nare the first and second order partial derivatives of the loss with respect to the current predictions. The XGBoost paper calls these the gradients and hessians, respectively. Remember that when we specify an actual loss function to use, we would also specify the functional form of the gradients and hessians, so that they are directly computable.\nNow we can go back and substitute our quadratic approximation in for the loss function to get an approximation of the objective function in the neighborhood of \\(\\hat{y}_i^{(t-1)}\\)..\n\\[ L^{(t)} \\approx \\sum_{i=1}^n [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t(\\mathbf{x}_i)^2] + \\Omega(f_t) \\]\nSince \\(l(y_i,\\hat{y}_i^{(t-1)})\\) is constant regardless of our choice of \\(f_t\\), we can drop it and instead work with the modified objective, which gives us Equation (3) from the paper.\n\\[ \\tilde{L}^{(t)} = \\sum_{i=1}^n [ g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t(\\mathbf{x}_i)^2] + \\Omega(f_t) \\]\nNow the authors are about to do something great. They’re about to show how to directly compute the optimal prediction values for the leaf nodes of \\(f_t\\). We’ll circle back in a moment about how we find a good structure for \\(f_t\\), i.e. good node splits, but we’re going to find the optimal predicted values for any tree structure having \\(T\\) terminal nodes. Let \\(I_j\\) denote the set of instances \\(i\\) that are in the \\(j\\)-th leaf node of \\(f_t\\). Then we can rewrite the objective.\n\\[ \\tilde{L}^{(t)} = \\sum_{j=1}^T \\left [ \\sum_{i \\in I_j} g_i f_t(\\mathbf{x}_i) + \\frac{1}{2}  \\sum_{i \\in I_j} h_i f_t(\\mathbf{x}_i)^2 \\right ] + \\Omega(f_t)\\]\nWe notice that for all instances in \\(I_j\\), the tree yields the same predicted value \\(f_t(\\mathbf{x}_i)=w_j\\). Substituting in \\(w_j\\) for the predicted values and expanding \\(\\Omega(f_t)\\) we get\n\\[ \\tilde{L}^{(t)} = \\sum_{j=1}^T \\left [ \\sum_{i \\in I_j} g_i w_j + \\frac{1}{2}  \\sum_{i \\in I_j} h_i w_j^2 \\right ] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2\\]\nRearranging terms we obtain Equation (4).\n\\[ \\tilde{L}^{(t)} = \\sum_{j=1}^T \\left [ w_j \\sum_{i \\in I_j} g_i + \\frac{1}{2}  w_j^2 \\left ( \\sum_{i \\in I_j} h_i + \\lambda \\right )  \\right ] + \\gamma T \\]\nFor each leaf node \\(j\\), our modified objective function is quadratic in \\(w_j\\). To find the optimal predicted values we take the derivative, set to zero, and solve for \\(w_j\\).\n\\[ 0 = \\frac{d}{dw_j} \\left [ w_j \\sum_{i \\in I_j} g_i + \\frac{1}{2}  w_j^2 \\left ( \\sum_{i \\in I_j} h_i + \\lambda \\right )  \\right ] = \\left ( \\sum_{i \\in I_j} h_i + \\lambda \\right ) w_j + \\sum_{i \\in I_j} g_i \\]\nThis yields Equation (5).\n\\[ w_j^* = - \\frac{\\sum_{i \\in I_j} g_i } {\\sum_{i \\in I_j} h_i + \\lambda } \\]\n\n\nSplit Finding\nNow that we know how to find the optimal predicted value for any leaf node, we need to identify a criterion for finding a good tree structure, which boils down to finding the best split for a given node. Back in the [decision tree from scratch](/decision-tree-from-scratch post, we derived a split evaluation metric based on the reduction in the objective function associated with a particular split.\nTo do that, first we need a way to compute the objective function given a particular tree structure. Substituting the optimal predicted values \\(w_j^*\\) into the objective function, we get Equation (6).\n\\[ \\tilde{L}^{(t)} = -\\frac{1}{2} \\sum_{j=1}^T \\frac{ (\\sum_{i \\in I_j} g_i )^2 } {\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T \\]\nWe can then evaluate potential splits by comparing the objective before making a split to the objective after making a split, where the split with the maximum reduction in objective (a.k.a. gain) is best.\nMore formally, let \\(I\\) be the set of \\(n\\) data instances in the current node, and let \\(I_L\\) and \\(I_R\\) be the instances that fall into the left and right child nodes of a proposed split. Let \\(L\\) be the total loss for all instances in the node, while \\(L_L\\) and \\(L_R\\) are the losses for the left and right child nodes. The total loss contributed by instances in node \\(I\\) prior to any split is\n\\[L_{\\text{before split}} = -\\frac{1}{2} \\frac{ (\\sum_{i \\in I} g_i )^2 } {\\sum_{i \\in I} h_i + \\lambda} + \\gamma \\]\nAnd the loss after splitting \\(I\\) into \\(I_L\\) and \\(I_R\\) is\n\\[L_{\\text{after split}} = L_L + L_R = -\\frac{1}{2}  \\frac{ (\\sum_{i \\in I_L} g_i )^2 } {\\sum_{i \\in I_L} h_i + \\lambda} -\\frac{1}{2}  \\frac{ (\\sum_{i \\in I_R} g_i )^2 } {\\sum_{i \\in I_R} h_i + \\lambda} + 2 \\gamma \\]\nThe gain from this split is then\n\\[ \\Delta L = L_{\\text{before split}} -  L_{\\text{after split}} = L - (L_L + L_R)\\] \\[\\Delta L = \\frac{1}{2} \\left [ \\frac{ (\\sum_{i \\in I_L} g_i )^2 } {\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{ (\\sum_{i \\in I_R} g_i )^2 } {\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{ (\\sum_{i \\in I} g_i )^2 } {\\sum_{i \\in I} h_i + \\lambda} \\right ] - \\gamma \\]\nwhich is Equation (7) from the paper. In practice it makes sense to accept a split only if the gain is positive, thus the \\(\\gamma\\) parameter sets the minimum gain required to make a further split. This is why \\(\\gamma\\) can be set with the parameter gamma or the more descriptivemin_loss_split."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#tree-booster-innovations",
    "href": "posts/xgboost-explained/index.html#tree-booster-innovations",
    "title": "XGBoost Explained",
    "section": "Tree Booster Innovations",
    "text": "Tree Booster Innovations\n\nMissing Values and Sparsity-Aware Split Finding\nThe XGBoost paper also introduces a modified algorithm for tree split finding which explicitly handles missing feature values. Recall that in order to find the best threshold value for a given feature, we can simply try all possible threshold values, recording the score for each. If some feature values are missing, the XGBoost split finding algorithm simply scores each threshold twice: once with missing value instances in the left node and once with them in the right node. The best split will then specify both the threshold value and to which node instances with missing values should be assigned. The paper calls this the sparsity aware split finding routine, which is defined as Algorithm 2.\n\n\nPreventing Further Splitting\nIn addition to min_loss_split discussed above, XGBoost offers another parameter for limiting further tree splitting called min_child_weight. This name is a little confusing to me because the word “weight” has various meanings. In the context of this parameter, “weight” refers to the sum of the hessians \\(\\sum h_i\\) over instances in the node. For squared error loss \\(h_i=1\\), so this is equivalent to the number of samples. Thus this parameter generalizes the notion of the minimum number of samples allowed in a terminal node.\n\n\nSampling\nXGBoost takes a cue from Random Forest and introduces both column and row subsampling. These sampling methods can prevent overfitting and reduce training time by limiting the amount of data to be processed during boosting.\nLike random forest, XGBoost implements column subsampling, which limits tree split finding to randomly selected subsets of features. XGBoost provides column sampling for each tree, for each depth level within a tree, and for each split point within a tree, controlled by colsample_bytree, colsample_bbylevel, and colsample_bbynode respectively.\nOne interesting distinction is that XGBoost implements row sampling without replacement using subbsample, whereas random forest uses bootstrapping. The choice to bootstrap rows in RF probably spurred from a desire to use as much data as possible while training on the smaller datasets of the 1990’s when RF was developed. With larger datasets and the ability to generate a large number of trees, XGBoost simply takes a subsample of rows for each tree."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#scalability",
    "href": "posts/xgboost-explained/index.html#scalability",
    "title": "XGBoost Explained",
    "section": "Scalability",
    "text": "Scalability\nEven though we’re focused on statistical learning, I figured I’d comment on why XGBoost is highly scalable. Basically it boils down to efficient, parallelizable, and distributable methods for growing trees. You’ll notice there is a tree_method parameter which allows you to choose between the greedy exact algorithm (like the one we discussed in the decision tree from scratch post) and the approximate algorithm, which offers various scalability-related functionality, notably including the ability to consider only a small number of candidate split points instead of trying all possible splits. The algorithm also uses clever tricks like pre-sorting data for split finding and caching frequently needed values.\n\nWhy XGBoost is so Successful\nAs I mentioned in the intro, XGBoost is simply a very good implementation of the gradient boosting tree model. Therefore it inherits all the benefits of decision trees and tree ensembles, while making even further improvements over the classic gradient boosting machine. These improvements boil down to\n\nmore ways to control overfitting\nelegant handling of custom objectives\nscalability\n\nFirst, XGBoost introduces two new tree regularization hyperparameters \\(\\gamma\\) and \\(\\lambda\\) which are baked directly into its objective function. Combining these with the additional column and row sampling functionality provides a variety of ways to reduce overfitting.\nSecond, the XGBoost formulation provides a much more elegant way to train models on custom objective functions. Recall that for custom objectives, the classic GBM finds tree structure by fitting a squared error decision tree to the gradients of the loss function and then sets each leaf’s predicted value by running a numerical optimization routine to find the optimal predicted value.\nThe XGBoost formulation improves on this two-stage approach by unifying the generation of tree structure and predicted values. Both the split scoring metric and the predicted values are directly computable from the instance gradient and hessian values, which are connected directly back to the overall training objective. This also removes the need for additional numerical optimizations, which contributes to speed, stability, and scalability.\nFinally, speaking of scalability, XGBoost emerged at a time when industrial dataset size was exploding. Many use cases require scalable ML systems, and all use cases benefit from faster training and higher model development velocity."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#wrapping-up",
    "href": "posts/xgboost-explained/index.html#wrapping-up",
    "title": "XGBoost Explained",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, there you go, those are the salient ideas behind XGBoost, the gold standard in gradient boosting model implementations. Hopefully now we all understand the mathematical basis for the algorithm and appreciate the key improvements it makes over the classic GBM. If you want to go even deeper, you can join us for the next post where we’ll roll up our sleeves and implement XGBoost entirely from scratch."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#references",
    "href": "posts/xgboost-explained/index.html#references",
    "title": "XGBoost Explained",
    "section": "References",
    "text": "References\nThe XGBoost paper"
  },
  {
    "objectID": "posts/xgboost-explained/index.html#exercise",
    "href": "posts/xgboost-explained/index.html#exercise",
    "title": "XGBoost Explained",
    "section": "Exercise",
    "text": "Exercise\nProove that the XGBoost Newton Descent generalizes the classic GBM gradient descent. Hint: show that XGBoost with a squared error objective and no regularization reduces to the classic GBM."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "",
    "text": "SF buzzes silently in the distance\nAhh, gradient boosting. In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. Like its cousin random forest, gradient boosting is an ensemble technique that generates a single strong model by combining many simple models, usually decision trees. These tree ensemble methods perform very well on tabular data prediction problems and are therefore widely used in industrial applications and machine learning competitions.\nThere are several noteworthy variants of gradient boosting out there in the wild including XGBoost, NGBoost, LightGBM, and of course the classic gradient boosting machine (GBM). While XGBoost and LightGBM tend to have a marginal performance edge on the classic GBM, they are all based on a similar, very clever, idea about how to ensemble decision trees. Let’s avail ourselves of the intuition behind that clever idea, and then we’ll be able to build our very own GBM from scratch."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#toy-data",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#toy-data",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Toy Data",
    "text": "Toy Data\nWe begin our boosting adventure with a deceptively simple toy dataset having one feature \\(x\\) and target \\(y\\).\n\n\n\n\n\nNotice that \\(y\\) increases with \\(x\\) for a while, then flattens out. This is a pattern that happens all the time in real data, and it’s one that linear models epically fail to capture. Let’s build a gradient boosting machine to model it."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#intuition",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#intuition",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Intuition",
    "text": "Intuition\nSuppose we have a crappy model \\(F_0(x)\\) that uses features \\(x\\) to predict target \\(y\\). A crappy but reasonable choice of \\(F_0(x)\\) would be a model that always predicts the mean of \\(y\\).\n\\[F_0(x) = \\bar{y}\\]\nThat would look like this.\n\n\n\n\n\n\\(F_0(x)\\) by itself is not a great model, so its residuals \\(y - F_0(x)\\) are still pretty big and they still exhibit meaningful structure that we should try to capture with our model.\n\n\n\n\n\nWell what if I had another crappy model \\(h_1(x)\\) that could predict the residuals \\(y - F_0(x)\\)?\n\n\n\nModel\nFeatures\nTarget\n\n\n\n\n\\(h_1(x)\\)\n\\(x\\)\n\\[y-F_0(x)\\]\n\n\n\nIt’s worth noting that the crappiness of this new model is essential; in fact in this boosting context, it’s usually called a weak learner. To get a model that’s only slightly better than nothing, let’s use a very simple decision tree with a single split, a.k.a. a stump. This model basically divides our feature \\(x\\) into two regions and predicts the mean value of \\(y\\) for all of the \\(x\\)’s in each region. It might look like this.\n\n\n\n\n\nWe could make a composite model by adding the predictions of the base model \\(F_0(x)\\) to the predictions of the supplemental model \\(h_1(x)\\) (which will pick up some of the slack left by \\(F_0(x)\\)). We’d get a new model \\(F_1(x)\\):\n\\[F_1(x) = F_0(x) + h_1(x)\\]\nwhich is better at predicting \\(y\\) than the original model \\(F_0(x)\\) alone.\n\n\n\n\n\nWhy stop there? Our composite model \\(F_1(x)\\) might still be kind of crappy, and so its residuals \\(y - F_1(x)\\) might still be pretty big and structurey. Let’s add another model \\(h_2(x)\\) to predict those residuals.\n\n\n\nModel\nFeatures\nTarget\n\n\n\n\n\\(h_2(x)\\)\n\\(x\\)\n\\[y-F_1(x)\\]\n\n\n\nThe new composite model is\n\\[F_2(x) = F_1(x) + h_2(x).\\]\n\n\n\n\n\nIf we keep doing this, at each stage we’ll train a new model \\(h_m(x)\\) on the previous composite model’s residuals \\(y-F_{m-1}(x)\\), and we’ll get a new composite model\n\\[F_m(x) = F_{m-1}(x) + h_m(x).\\]\nIf we add \\(M\\) crappy models constructed in this way to our original crappy model \\(F_0(x)\\), we might actually end up with a pretty good model \\(F_M(x)\\) that looks like\n\\[\nF_M(x) = F_0(x) + \\sum_{m = 1}^{M} h_m(x)\n\\]\nHere’s how our model would evolve up to \\(M=6\\).\n\n\n\n\n\nVoila! That, friends, is boosting!"
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#learning-rate",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#learning-rate",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Learning Rate",
    "text": "Learning Rate\nLet’s talk about overfitting. In real life, if we just add our new weak learner \\(h_m(x)\\) directly to our existing composite model \\(F_{m-1}(x)\\), then we’re likely to end up overfitting on our training data. That’s because if we add enough of these weak learners, they’re going to chase down y so closely that all the remaining residuals are pretty much zero, and we will have successfully memorized the training data. To prevent that, we’ll scale them down a bit by a parameter \\(\\eta\\) called the learning rate.\nWith the learning rate \\(\\eta\\), the update step will then look like\n\\[F_{m}(x) = F_{m-1}(x) + \\eta h_m(x),\\]\nand our composite model will look like\n\\[F_M(x) = F_0(x) + \\eta \\sum_{m = 1}^{M} h_m(x)\\]\nNote that since the learning rate can be factored out of the sum, it looks kinda like we could just build our models without it and slap it on at the end when we sum up the weak learners to make the final composite model. But that won’t work, since at each stage we train the next weak learner on the residuals from the current composite model, and the current composite model depends on the learning rate."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#implementation",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#implementation",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Implementation",
    "text": "Implementation\nOk, we’re ready to implement this thing from “scratch”. Well, sort of. To quote Carl Sagan,\n\n\nIf you wish to make an apple pie from scratch, you must first invent the universe.\n\nWe will not be inventing a universe that contains the Earth, apple trees, computers, python, numpy, and sklearn. To keep the “scratch” implementation clean, we’ll allow ourselves the luxury of numpy and an off-the-shelf sklearn decision tree which we’ll use as our weak learner.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# model hyperparameters\nlearning_rate = 0.3\nn_trees = 10\nmax_depth = 1\n\n# Training\nF0 = y.mean() \nFm = F0\ntrees = []\nfor _ in range(n_trees):\n    tree = DecisionTreeRegressor(max_depth=max_depth)\n    tree.fit(x, y - Fm)\n    Fm += learning_rate * tree.predict(x)\n    trees.append(tree)\n\n# Prediction\ny_hat = F0 + learning_rate * np.sum([t.predict(x) for t in trees], axis=0)\n\nWe first define our hyperparameters: - learning_rate is (\\(\\eta\\)) - n_trees is the number of weak learner trees to add (\\(M\\)) - max_depth controls the depth of the trees; here we set to 1 for stumps\nWe define our base model predictions F0 to simply predict the mean value of y. Fm corresponds to the current composite model \\(F_m(x)\\) as we iteratively add weak learners, so we’ll initialize it with F0. trees is an empty list that we’ll use to hold our weak learners.\nNext we iteratively add n_trees weak learners to our composite model. At each iteration, we create a new decision tree and train it on x to predict the current residuals y - Fm. We update Fm with the newly trained learner’s predictions scaled by the learning rate, and we append the new weak learner \\(h_m(x)\\) in the trees list. We generate final predictions y_hat on the training data by summing up the predictions from each weak learner, scaling by the learning rate, and adding to the base model (a.k.a. the mean of y).\n\n\n\n\n\nNice! Our GBM fits that nonlinear data pretty well.\nNow that we have a working implementation, let’s go ahead and implement it as a class with fit and predict methods like we’re used to having in sklearn.\n\nclass GradientBoostingFromScratch():\n    \n    def __init__(self, n_trees, learning_rate, max_depth=1):\n        self.n_trees=n_trees; self.learning_rate=learning_rate; self.max_depth=max_depth;\n        \n    def fit(self, x, y):\n        self.trees = []\n        self.F0 = y.mean()\n        Fm = self.F0 \n        for _ in range(self.n_trees):\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(x, y - Fm)\n            Fm += self.learning_rate * tree.predict(x)\n            self.trees.append(tree)\n            \n    def predict(self, x):\n        return self.F0 + self.learning_rate * np.sum([tree.predict(x) for tree in self.trees], axis=0)\n\nLet’s compare the performance of our implementation with the sklearn GradientBoostingRegressor.\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\nsklearn_gbm = GradientBoostingRegressor(n_estimators =25, learning_rate=0.3, max_depth=1)\nsklearn_gbm.fit(x,y)\n\nscratch_gbm = GradientBoostingFromScratch(n_trees=25, learning_rate=0.3, max_depth=1)\nscratch_gbm.fit(x,y)\n\nmean_squared_error(y, sklearn_gbm.predict(x)), mean_squared_error(y, scratch_gbm.predict(x))\n\n(0.08622324648703916, 0.0862232464870392)\n\n\nHeck yeah! Our homemade GBM is consistent with the sklearn implementation!"
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#wrapping-up",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#wrapping-up",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAlright, there you have it, the intuition behind basic gradient boosting and a from scratch implementation of the gradient boosting machine. I tried to keep this explanation as simple as possible while giving a complete intuition for the basic GBM. But it turns out that the rabbit hole goes pretty deep on these gradient boosting algorithms. We can actually wave our magic generalization wand over some custom loss functions and end up with algorithms that can do gradient descent in function space (whatever that means). We’ll get into what that means and why it’s so baller in future posts. For now, go forth and boost!"
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "",
    "text": "A whiteboard session at Playa Pelada\nIn the last two posts, we learned the basics of gradient boosting machines and the gradient descent algorithm. But we still haven’t explicitly addressed what puts the “gradient” in gradient boosting. It turns out that gradient boosting models are using a sort of gradient descent to minimize their loss function; according to Friedman’s classic paper, they’re doing gradient descent in “function space”. If you’re like me, and this is your first encounter with this idea, then the phrase “gradient descent in function space” is going to sound a little, ahem, mysterious. No worries, friends; we’re about to make sense of it all.\nUnderstanding the underlying mechanics of gradient boosting as a form of gradient descent will empower us to train our models with custom loss functions. This opens up many interesting possibilities including doing not only regression and classification, but also predicting quantiles, prediction intervals, and even the conditional probability distribution of the response variable."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#generalized-intuition-for-gradient-boosting",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#generalized-intuition-for-gradient-boosting",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Generalized intuition for gradient boosting",
    "text": "Generalized intuition for gradient boosting\nIn my earlier post on [building a gradient boosting model from scratch](/gradient-boosting-machine-from-scratch, we established the intuition for how gradient boosting works in a regression problem. In this post we’re going to generalize the ideas we encountered in the regression context, so check out the earlier post if you’re not already familiar with gradient boosting for regression. In the following sections we’ll build up the intuition for gradient boosting in general terms, and then we’ll be able to state the gradient boosting algorithm in a form that can fit models to customized loss functions.\n\nThe loss function\nYou recall that we measure how well a model fits data by using a loss function that yields small values when a model fits well. “Training” essentially means finding the model that minimizes our loss function. A loss function takes the correct target values and the predicted target values, and it returns a scalar loss score. For example, in the last post on gradient descent we used a mean squared error (MSE) loss\n\\[L(\\mathbf{y}, \\hat{\\mathbf{y}}) =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nwhere we express the correct targets and predicted values as the vector arguments \\(\\mathbf{y}=[y_1,y_2,\\dots,y_n]\\) and \\(\\hat{\\mathbf{y}}=[\\hat{y}_1,\\hat{y}_2,\\dots,\\hat{y}_n]\\) respectively.\n\n\nWhich way to nudge a prediction to get a better model\nNow, let’s say we have a model \\(F(\\mathbf{X})=\\mathbf{\\hat{y}}\\) that we want to improve. One approach is that we could figure out whether each prediction \\(\\hat{y}_i\\) needed to be higher or lower to get a better loss score. We could then nudge each prediction in the right direction, thereby decreasing our model’s loss score.\nTo figure out whether we should increase or decrease a particular prediction \\(\\hat{y}_i\\) (and by how much), we can compute the partial derivative of the loss function with respect to that prediction. Recall the partial derivative just tells us the rate of change in a function when we change one of its arguments. Since we want to make the loss \\(L(\\mathbf{y},\\mathbf{\\hat{y}})\\) decrease, we can use the negative partial derivative of the loss function with respect to a given prediction to help us choose the right nudge for that prediction.\n\\[ \\text{nudge for } \\hat{y}_i = -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\]\nSometimes it can get a little intense when there are partial derivatives flying around, but it doesn’t have to be that way. Remember that in practice \\(-\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is just an expression that evaluates to a number like 2.7 or -0.5, and here it’s telling us how to nudge \\(\\hat{y}_i\\) to decrease our loss score.\nThe intuition is that if \\(\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is negative, then increasing the prediction \\(\\hat{y}_i\\) will make the loss decrease. We then notice that the negative of the partial derivative tells us whether to increase or decrease \\(\\hat{y}_i\\). For example, if \\(-\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is positive, then increasing the prediction \\(\\hat{y}_i\\) will make the loss decrease; whereas if \\(-\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is negative, then decreasing the prediction \\(\\hat{y}_i\\) will make the loss decrease.\nSince we’ll want to find the right nudge for each of the \\(\\hat{y}_i\\)’s, we can use the negative gradient of the loss function \\(L(\\mathbf{y},\\mathbf{\\hat{y}})\\) with respect to the vector argument \\(\\hat{\\mathbf{y}}\\) to get the vector of all the partial derivatives. Let’s call this vector of desired nudge values \\(\\mathbf{r}\\).\n\\[\\mathbf{r} = -\\nabla_{\\hat{\\mathbf{y}}} L(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\left [ -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_1}, -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_2}, \\cdots, -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_n}\\right ]\\]\n\n\nNudging predictions in the right direction\nGreat, now that we know we should nudge each prediction in the direction of the negative partial derivative of the loss with respect to that prediction, we need to figure out how to do the actual nudging. Remember that we already have an initial model \\(F(\\mathbf{X})=\\mathbf{\\hat{y}}\\).\nAt this point we might be tempted to simply add the vector of nudge values to our predictions to get better predictions.\n\\[\\text{we might be tempted to try } \\mathbf{\\hat{y}}_{\\text{new}} = \\mathbf{\\hat{y}} + \\mathbf{r}\\]\nSure, based on our reasoning in the previous section, plugging the vector of nudged predictions into the loss function would yield a lower loss score.\n\\[ L(\\mathbf{y},\\mathbf{\\hat{y}} + \\mathbf{r}) \\le L(\\mathbf{y},\\mathbf{\\hat{y}})\\]\nThe problem is that this will only work for in-sample data, because we only know the nudge values for the cases which are present in the training dataset. In order for our model to generalize to unseen test data, we need a way to get the nudge values for new observations of the independent variables. So how can we do that?\nWell what if we fit another model \\(h(\\mathbf{X})\\) that used our same features \\(\\mathbf{X}\\) to predict our desired nudge values \\(\\mathbf{r}\\), and then we added that new model to our original model \\(F(\\mathbf{X})\\). For a given prediction the nudge model \\(h(\\mathbf{X})\\) would essentially return an approximation of the desired nudge, so adding it would push the prediction in the right direction to decrease the loss function. Furthermore, the nudge model can return predictions of the nudges for out-of-sample cases which are not present in the training dataset. Since both the initial model \\(F(\\mathbf{X})\\) and the nudge model \\(h(\\mathbf{X})\\) are functions of our features \\(\\mathbf{X}\\), we can add the two functions to get an updated model that can generalize beyond the training data.\n\\[F_{\\text{new}} (\\mathbf{X}) = F(\\mathbf{X}) + h(\\mathbf{X})\\]"
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#a-generalized-gradient-boosting-algorithm",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#a-generalized-gradient-boosting-algorithm",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "A generalized gradient boosting algorithm",
    "text": "A generalized gradient boosting algorithm\nOk, let’s put these pieces of intuition together to create a more general gradient boosting algorithm recipe.\nWe begin with training data \\((\\mathbf{y}, \\mathbf{X})\\) where \\(\\mathbf{y}\\) is a length-\\(n\\) vector of target values, and \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix with \\(n\\) observations of \\(p\\) features. We also have a differentiable loss function \\(L(\\mathbf{y}, \\mathbf{\\hat{y}})\\), a “learning rate” hyperparameter \\(\\eta\\), and a fixed number of model iterations \\(M\\).\nWe create an initial model \\(F_0(\\mathbf{X})\\) that predicts a constant value. We choose the constant value that would give the best loss score.\n\\[F_0(\\mathbf{X}) = \\underset{c}{\\operatorname{argmin}} L(\\mathbf{y}, c)\\]\nThen we iteratively update the initial model with \\(M\\) nudge models.\nFor \\(m\\) in 0 to \\(M-1\\):\n\nCompute current composite model predictions \\(\\mathbf{\\hat{y}}_{m} = F_{m}(\\mathbf{X})\\).\nCompute the desired nudge values given by the negative gradient of the loss function with respect to each prediction \\(\\mathbf{r}_m = - \\nabla_{\\mathbf{\\hat{y}}_m} L (\\mathbf{y}, \\mathbf{\\hat{y}}_m)\\).\nFit a weak model (e.g. shallow decision tree) \\(h_{m}(\\mathbf{X})\\) that predicts the nudge values \\(\\mathbf{r}_{m}\\) using features \\(\\mathbf{X}\\).\nUpdate the composite model.\n\n\\[F_{m+1}(\\mathbf{X}) = F_{m}(\\mathbf{X}) + \\eta h_{m}(\\mathbf{X})\\]\nAfter \\(M\\) iterations, we are left with the final composite model \\(F_M(\\mathbf{X})\\)."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#wait-in-what-sense-is-this-doing-gradient-descent",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#wait-in-what-sense-is-this-doing-gradient-descent",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Wait, in what sense is this doing gradient descent?",
    "text": "Wait, in what sense is this doing gradient descent?\nIn my previous post, we learned how to use gradient descent to iteratively update model parameters to find a model that minimizes the loss function. We could write the update rule as\n\\[ \\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_{t} + \\eta ( - \\nabla_{\\mathbf{\\theta}} L(\\mathbf{y}, \\mathbf{\\hat{y}}_{\\mathbf{\\theta}_{t}}) ) \\]\nwhere the predictions \\(\\mathbf{\\hat{y}}\\) depend on the model parameters \\(\\mathbf{\\theta}\\), and we’re trying to find the value of the parameter vector \\(\\mathbf{\\theta}\\) that minimizes the loss function \\(L(\\mathbf{y}, \\mathbf{\\hat{y}}_{\\mathbf{\\theta}_{t}})\\), so we nudge the vector \\(\\mathbf{\\theta}_t\\) by the negative gradient of \\(L(\\mathbf{y}, \\mathbf{\\hat{y}}_{\\mathbf{\\theta}_{t}})\\) with respect to \\(\\mathbf{\\theta}_t\\). Compare that with the boosting model update rule we obtained in the previous section.\n\\[F_{m+1}(\\mathbf{X}) = F_{m}(\\mathbf{X}) + \\eta h_{m}(\\mathbf{X})\\]\nwhere \\(h_{m}(\\mathbf{X}) \\approx - \\nabla_{\\mathbf{\\hat{y}}_m} L (\\mathbf{y}, \\mathbf{\\hat{y}}_m)\\).\nIf we replace \\(F(\\mathbf{X})\\) with its prediction vector \\(\\mathbf{\\hat{y}}\\), and we replace the nudge model \\(h(\\mathbf{X})\\) with the negative gradient of the loss function (which it approximates), the likeness to the parameter gradient descent update rule becomes more obvious.\n\\[\\mathbf{\\hat{y}}_{m+1} \\approx \\mathbf{\\hat{y}}_m + \\eta (- \\nabla_{\\mathbf{\\hat{y}}_m} L (\\mathbf{y}, \\mathbf{\\hat{y}}_m))\\]\nIndeed, gradient boosting is performing gradient descent to obtain a good model by minimizing a loss function. But there are a couple of key differences between gradient boosting and the parameter gradient descent that we discussed in the previous post."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-versus-parameter-gradient-descent",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-versus-parameter-gradient-descent",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Gradient boosting versus parameter gradient descent",
    "text": "Gradient boosting versus parameter gradient descent\nThe generic gradient boosting algorithm outlined above implies two key differences from parameter gradient descent.\n\nInstead of nudging parameters, we nudge each individual prediction, thus instead of taking the gradient of loss with respect to the parameters, we take the gradient with respect to the predictions.\nInstead of directly adding the negative gradient to our current parameter values, we create a functional approximation of the negative gradient and add that to our model. Our functional approximation is just a crappy model that tries to use the model features to predict the negative gradient of the loss with respect to our current model predictions.\n\nThe true genius of the gradient boosting algorithm is in chasing the negative gradient of the loss with crappy models, rather than using it to directly update our predictions. If we just directly added the negative gradient of the loss to our predictions, and plugged them into the loss function we could get a lower loss score, but our updated model would be useless since it couldn’t make predictions on new out-of-sample data. Instead we train a crappy model to predict the negative gradient of the loss with respect to the current model predictions, thus we can iteratively update our composite model by adding these crappy models to it."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-is-gradient-descent-in-function-space-a.k.a.-prediction-space",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-is-gradient-descent-in-function-space-a.k.a.-prediction-space",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Gradient boosting is gradient descent in function space, a.k.a. prediction space",
    "text": "Gradient boosting is gradient descent in function space, a.k.a. prediction space\nLet’s address the statement in Friedman’s classic paper that gradient boosting is doing gradient descent in function space. Again we’ll use parameter gradient descent as a basis for comparison.\nIn parameter gradient descent, we have a vector of parameter values which, when plugged into the loss function, return some loss score. At each step of gradient descent, we compute the negative gradient of the loss function with respect to each parameter; that tells us which way to nudge each parameter value to achieve a lower loss score. We then add this vector of parameter nudge values to our previous parameter vector to get the new parameter vector. We could view this sequence of successive parameter vectors as a trajectory passing through parameter space, the space spanned by all possible parameter values. Therefore parameter gradient descent operates in parameter space.\nIn contrast, when we do gradient boosting, at each step we have a model, a.k.a. a function, that maps feature values to predictions. Given our training dataset, this model yields predictions which can be plugged into our loss function to get a loss score. At each boosting iteration, we compute the negative gradient of the loss with respect to each of the predictions; that tells us which way to nudge each prediction to achieve a lower loss score. We then create a function (a crappy model) that takes feature values and returns an approximation of the corresponding prediction’s nudge value. We then add this crappy model (a function) to our current composite model (also a function) to get the new composite model (you guessed it; also a function). And so by analogy with parameter vectors in parameter space, we can view this sequence of successive model functions as a trajectory passing through function space, the space spanned by all possible functions that map feature values to predictions. Therefore, gradient boosting does gradient descent in function space.\nIf this talk about function space still feels a little abstract, you could just use the same substitution trick we used above and swap the model \\(F(\\mathbf{X})\\) for its predictions \\(\\mathbf{\\hat{y}}\\) which is just a vector of numbers. The target values for our nudge models are given by the negative gradient of the loss with respect to this prediction vector. From here, we can see that each time we add a new nudge model to our composite model, we get a new prediction vector. We can view this sequence of successive prediction vectors as a trajectory passing through prediction space, the space spanned by all possible prediction vector values. Therefore we can also say that gradient boosting does gradient descent in prediction space."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#so-why-did-we-fit-the-crappy-models-to-the-residuals-in-our-regression-gbm",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#so-why-did-we-fit-the-crappy-models-to-the-residuals-in-our-regression-gbm",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "So why did we fit the crappy models to the residuals in our regression GBM?",
    "text": "So why did we fit the crappy models to the residuals in our regression GBM?\nIn my first post on [gradient boosting machines](/gradient-boosting-machine-from-scratch, in the interest of simplicity I left one key aspect of the problem unaddressed, that is, what loss function were we using to train that GBM? It turns out that because of the way we built our GBM, without knowing it we were actually using a mean squared error (MSE) loss function.\n\\[L(\\mathbf{y}, \\hat{\\mathbf{y}}) =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nIf the GBM was using gradient descent to find a \\(\\hat{\\mathbf{y}}\\) vector that minimized this loss function, then at each iteration it would have to nudge the current \\(\\hat{\\mathbf{y}}\\) by the negative gradient of the loss function with respect to \\(\\hat{\\mathbf{y}}\\), i.e. \\(-\\nabla_{\\hat{\\mathbf{y}}} L(\\mathbf{y}, \\hat{\\mathbf{y}})\\). Since our loss function takes a length \\(n\\) vector of predictions \\(\\hat{\\mathbf{y}}\\) as input, the gradient will be a length-\\(n\\) vector of partial derivatives with respect to each of the predictions \\(\\hat{y}_i\\). Let’s start by taking the negative partial derivative with respect to a particular prediction \\(\\hat{y}_j\\).\n\\[\n\\begin{array}{rcl}\n-\\frac{\\partial}{\\partial \\hat{y}_j} L(\\mathbf{y}, \\mathbf{\\hat{y}})\n    & = & -\\frac{\\partial}{\\partial \\hat{y}_j} \\left ( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\right ) \\\\\n    & = & -\\frac{\\partial}{\\partial \\hat{y}_j} \\left ( \\frac{1}{n} (y_j - \\hat{y}_j)^2 \\right ) \\\\\n    & = & -\\frac{1}{n} (2)(y_j - \\hat{y}_j) \\frac{\\partial}{\\partial \\hat{y}_j} (y_j - \\hat{y}_j) \\\\\n    & = & \\frac{2}{n} (y_j - \\hat{y}_j) \\\\\n\\end{array}\n\\]\nIt turns out that the negative partial derivative of the MSE loss function with respect to a particular prediction \\(\\hat{y}_i\\) is proportional to the residual \\(y_i - \\hat{y}_i\\)! This is a pretty intuitive result, because if we nudge a prediction by it’s residual, we’ll end up with the correct target value.\nWe can go ahead and write the nudge vector as\n\\[\\mathbf{r} = -\\nabla_{\\hat{\\mathbf{y}}} L(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{2}{n}(\\mathbf{y} - \\hat{\\mathbf{y}})\\]\nwhich is proportional to the residual vector \\(\\mathbf{y} - \\hat{\\mathbf{y}}\\). This means that when we use the mean squared error loss function, our nudge values are given by the current model residuals, and therefore each new crappy model targets the previous model’s residuals.\nAnd this result brings us full circle, back to our original intuition from the first GBM post about chasing residuals with crappy models. Now we see that intuitive idea is just a special case of the more general and, dare I say, even more beautiful idea of chasing the negative gradient of the loss function with crappy models."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#key-takeaways",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#key-takeaways",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nWe covered a lot of conceptual ground in this post, so let’s recap the key ideas.\n\nGradient boosting can use gradient descent to minimize any differentiable loss function in service of creating a good final model.\nThere are two key differences between gradient boosting and parameter gradient descent:\n\nIn gradient boosting, we nudge prediction values rather than parameter values, so to find the desired nudge values, we take the negative gradient of the loss function with respect to the predictions.\nIn gradient boosting, we nudge our predictions by adding a crappy model that approximates the nudge values, rather than adding the nudge values directly to the predictions.\n\nGradient boosting does gradient descent in function space. But since the model predictions are just numeric vectors, and since we take the gradient of the loss function with respect to the prediction vector, it’s also valid and probably easier to think of gradient boosting as gradient descent in prediction space.\nWe saw that iteratively fitting crappy models to the previous model residuals, as we did in the regression GBM from scratch post, is just a special case of fitting crappy models to the negative gradient of the loss function (in this case the mean squared error loss)."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#wrapping-up",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#wrapping-up",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nPhew, there it is, how gradient boosting models do gradient descent in function space. Understanding how the general form of gradient boosting works opens up the possibility for us to use any differentiable loss function for model training. That is pretty exciting because it means that we can get a lot of mileage out of this one class of learning algorithms. Stay tuned for more on some of the awesome things we can do with these ideas in future posts!\nThere are a couple of resources I found to be super helpful while researching the content in this post. Definitely check them out if you want to read more about gradient boosting and gradient descent.\nHow to explain gradient boosting by Terence Parr and Jeremy Howard\nUnderstanding Gradient Boosting as Gradient Descent by Nicolas Hug"
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "",
    "text": "Ahh, blogging. I think we can all agree it’s probably one of the greatest forms of written communication to have ever existed.\nWhats that you say? You’d like to set up your own blog? And you say you want to use a dead simple, data science friendly tech stack? And you wouldn’t be caught dead handing over your painstakingly crafted content to Medium? No worries, friend, I know exactly what you need.\nEnter Quarto.\nIn this post we’ll set up a blog using a lightweight tech stack consisting of a terminal running quarto, git, and jupyter, and we’ll use Github Pages to host our website for free. Optionally, for a few dollars a year, we can even host our website at our own custom domain.\nA quick note on how to use this post. Quarto’s documentation on blogging provides a nice high-level overview of the blogging workflow, and I refer to it and many other bits of Quarto documentation here. At the time of writing, the handful of other blog posts about setting up quarto blogs are aimed at the RStudio user. This post exists to provide a jupyter and python-centric path for you to follow through the entire setup of your new quarto blog, and to impart my opinionated recommendations about best practices.\nLet’s get into it!"
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#what-is-quarto",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#what-is-quarto",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto is a way to render plain text source files containing markdown and code in python, R, and other languages into published formats like websites, books, slides, journal articles, etc. There is clearly a lot that we can do with it, but Today, we’ll use it to make a nice looking blog out of some jupyter notebook files.\nQuarto follows the familiar convention of using a project directory to house all material for a given project. The directory will include source files like jupyter notebooks or Rmarkdown files, as well as configuration files that control how output files are rendered. We can then use the quarto command line utility to perform actions like previewing and rendering within the project directory."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#instantiate-your-blog",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#instantiate-your-blog",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Instantiate your blog",
    "text": "Instantiate your blog\n\nCreate a new Quarto project\nAfter installing quarto fire up a new terminal and check that the install was successful by running\nquarto --version\nNow think of a name for your blog’s project directory; this will also be the name of its git repository. The name will have no effect on your website’s name or URL, so don’t think too hard. The quarto documentation calls it myblog, so we’ll one-up them and call ours pirate-ninja-blog. Run the following command to create it in the current directory.\nquarto create-project pirate-ninja-blog --type website:blog\nThat command creates a directory called pirate-ninja-blog containing everything you need to render your new blog. You can preview your website by running\nquarto preview pirate-ninja-blog\nYour local website will open in a new browser window. As you edit various aspects of your blog, the preview will update with your changes. This preview feature is so simple and so great.\n\n\n\nPreviewing your blog with quarto preview command\n\n\n\n\nSet up a git repo\nChange into your project directory and we’ll start setting up your git repo.\ncd pirate-ninja-blog\ninitialize a new git repo.\ngit init -b main\nThe _site/ directory is where quarto puts the rendered output files, so you’ll want to ignore it in git. I also like to just ignore any hidden files too, so add the following to your .gitignore file.\n\n\n.gitignore\n\n/.quarto/\n/_site/\n.*\n\nFor now we’ll just stage the .gitignore file for the initial commit. Eventually you’ll want to commit the other files in your project too, either now or later as you edit them.\ngit add .gitignore \ngit commit -m \"Initial commit.\"\nThen follow GitHub’s instructions to add the local repo to GitHub using git. Basically just create a new blank repo on GitHub’s website, copy the remote repository url, then add the remote repo url to your local git repo.\ngit remote add origin &lt;REMOTE_URL&gt;\nThen you’ll be able to push any commits you make to your remote repository on GitHub by saying git push."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#understand-the-components-of-a-quarto-blog",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#understand-the-components-of-a-quarto-blog",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Understand the components of a Quarto blog",
    "text": "Understand the components of a Quarto blog\n\nContents of the quarto project directory\nLet’s have a quick look at what quarto put inside of the project directory.\n_quarto.yml\nabout.qmd\nindex.qmd\nprofile.jpg\nposts\nstyles.css\n_site\n\nQuarto uses yaml files to specify configurations. The _quarto.yml file specifies project-wide configurations.\nQuarto’s markdown file type uses extension qmd``. Each qmd file will correspond to a page in our website.index.qmdis the homepage andabout.qmd` is the About page.\nprofile.jpg is an image that is included on the about page.\nstyles.css defines css styles for the website.\nposts is a directory where we can put qmd and other documents which will be rendered into blog posts.\nposts/_metadata.yml contains configurations that apply to all documents in the posts directory.\n_site is a directory that contains the rendered website. Whereas all the other files and directories constitute the source code for our blog, _site is the rendered output, i.e. the website itself.\n\nLet’s take a closer look at these components and start to make the blog yours.\n\n\nProject-wide Configurations\nThe _quarto.yml file controls project-wide configurations, website options, and HTML document options. Options in this file are specified in yaml in a key/value structure with three top level keys: project, website, and format. The quarto website options documentation has the full list of options that you can set here. It will be very helpful to take a look at some example _quarto.yml files in the wild, such as the one from quarto.org or even the one from this blog.\nUnder the website key, go ahead and set the title and description for your blog.\nwebsite:\n  title: \"Pirate Ninja Blog\"\n  description: \"A blog about pirates, ninjas, and other things\"\nYou can also customize your navbar which is visible at the top of all pages on your site. Also go ahead and set your github and twitter urls for the icons in the navbar.\nUnder the format key, you can also try changing the HTML theme to one of the other 25 built-in themes.\n\n\nThe About Page\nThe about.qmd file defines an About page for the blog. Go ahead and fill in your details in the about.qmd file; you can also replace the profile.jpg file with your own image. Have a look at the quarto documentation on About pages to explore more functionality. Notably, you can change the template option to change the page layout.\n\n\nThe Homepage\nThe index.qmd file defines the landing page for your website. It is a listing page which shows links to all the pages in the posts directory. For now we don’t need to change anything here.\n\n\nThe posts/ directory\nThe posts directory contains all your blog posts. There aren’t really requirements for subdirectory structure inside the posts directory, but it’s a best practice to create a new subdirectory for each new blog post. This just helps keep auxillary files like images or conda environment files organized. Out of the box, the posts directory looks like this.\nposts\n├── _metadata.yml\n├── post-with-code\n│   ├── image.jpg\n│   └── index.qmd\n└── welcome\n    ├── index.qmd\n    └── thumbnail.jpg\nThere are two reasons we want to be deliberate about how we organize and name things in the posts directory. First, the vast majority of our blog’s content will live here, so we don’t want it to be a big confusing mess. Second, the directory sstructure and file naming will be reflected in the URLs to our blog posts; if you prefer tidy-looking URLs, and I know you do, then you want to use tidy directory and file names in the posts directory.\nYou can check how the URLs look by navigating to one of the pre-populated posts in the site preview in your browser. For instance, the welcome post’s URL would be\nhttps://example.com/posts/welcome/\nWhen quarto renders the qmd file at posts/welcome/index.qmd it creates an output document in the website at posts/welcome/index.html. In fact the full URL to the post is,\nhttps://example.com/posts/welcome/index.html\nbut the browser knows if you give it a URL with a path ending in a /, then it should look for the index.html file inside that directory.\nSo I think the best practice here is to name your new post subdirectory with the title of the post in all lower case with dashes for spaces, e.g. post-with-code. Then to force all output pages to be called index.html, you can set the output-file key in the posts/_metadata.yml file like this.\n\n\nposts/_metadata.yml\n\noutput-file: index.html\n\nNote that alternative naming conventions are possible; notably you might want to prefix each post name with the date in yyyy-mm-dd format, so the post subdirectories sort temporally and look nice in a list. That’s the convention used in Quarto’s own blog at quarto.org, As long as you keep everything for a given post inside its subdirectory, you should be good to go with nice-looking URLs."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#authoring-posts-with-jupyter",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#authoring-posts-with-jupyter",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Authoring posts with jupyter",
    "text": "Authoring posts with jupyter\n\nCreating a new post\nIt turns out that quarto will render not only .qmd files, but also .ipynb files in the posts directory. So let’s create a new blog post from a notebook.\nI think it’s a best practice to write draft posts in their own git branches, that way if you need to deploy some kind of hotfix to main while you’re drafting a post, you won’t have to deploy a half-written post livin on the main branch. To start a new post, create a new development branch, change into the posts directory, create a new subdirectory with your preferred naming convention, change into that new directory, and fire up jupyter.\ngit checkout -b new-post\ncd posts\nmkdir new-post\ncd new-post\njupyter notebook\nNow create a new notebook from the jupyter UI. In order for quarto to recognize the document, the first cell of the notebook must be a raw text cell (press r in command mode to change a cell to raw text), and it must contain the document’s yaml front matter. You can use the following as a frontmatter template.\n---\ntitle: New Post\ndate: 2023-07-12\ndescription: A nice new post\ncategories: [nonsense, code]\n---\nNow to preview your post, open a new terminal, change into your blog’s project directory and run the quarto preview command. You’ll see a link to the new post in the listing on the homepage. I usually like to have the preview open in a browser while I’m editing the jupyter notebook, just to make sure things look the way I want in the rendered output. From here you can keep editing the notebook, and the preview will update in the browser dynamically.\n\n\nMarkdown and code cells\nFrom here you can put text in markdown cells and you can write code in code cells. Let’s add a markdown cell with some markdown formatting.\n## A nice heading\n\nHere is some lovely text and an equation.\n\n$$ a^2 + b^2 = c^2 $$\n\nHere's a list.\n\n- a link to an [external website](https://quarto.org).\n- a link to [another post in this blog](/posts/welcome/index.qmd).\nThis markdown will be rendered into the HTML page for the post. The last line in the above cell demonstrates the best practice for using relative urls to link to other resources within your website. Instead of providing the full url in the parentheses, just give the path to the qmd or ipynb file that you want to link to. Note that paths need to start with the / at the root of the quarto project, since without it, quarto will try to resolve paths relative to the location of the current document instead of the root of the project.\nThen create a code cell with some code. Try something like this.\nprint('Hello, Quarto!')\nBy default, both code and cell output will be rendered into the HTML output. So far our jupyter notebook looks like this.\n\n\n\nView of a new post being written in jupyter notebook\n\n\nBack in the browser window running your blog preview, you can see the rendered page of the new post.\n\n\n\nView of the preview of the rendered post\n\n\n\n\nFigures\nLet’s add a figure to our post. Add a new code cell with the following code.\n# | fig-cap: This is my lovely line plot\n# | fig-alt: A line plot extending up and to the right\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = 2 * x + 1\nplt.plot(x, y);\nNotice a couple of important details. First I placed a semicolon at the end of the last line. That supresses the [&lt;matplotlib.lines.Line2D at 0x1111d00a0&gt;] text output, which would otherwise show up in your blog post too.\nSecond, I added a couple of special comments at the top of the cell. Quarto allows you to specify numerous code execution options, designated by the # | prefix, to control the behavior and appearance of the code and output at a cell level. I set two keys here, fig-cap and fig-alt which respectively set the figure caption text and the image alt tag text. The fig-alt key is particularly important to set on all your figures because it provides the non-visual description for screenreader users reading your post. The alt tag should be a simple description of what the plot is and possibly what it shows or means. Be a friend of the blind and visually impaired community and set fig-alt on all of your figures.\n\n\nVersion control\nAs you edit your new post, go ahead and commit your changes on your development branch. Once you’ve finished your new post, you can merge it into main like this.\ngit checkout main\ngit merge new-post\nThen you can push to GitHub by running git push. You should also be sure to run a final quarto preview to check that everything looks good before publishing to the web."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#publishing-your-blog-to-the-web",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#publishing-your-blog-to-the-web",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Publishing your blog to the web",
    "text": "Publishing your blog to the web\n\nHosting with GitHub Pages\nIt’s likely that the easiest (read best) option for you is to host your blog on GitHub Pages. This is because GitHub pages is free, and since you already have your blog’s source code checked into a remote repository at GitHub, it’s very easy to set up. Quarto’s documentation on publishing to GitHub Pages outlines three ways to publish your website, but I recommend their option 2, using the quarto publish command. Once you set up your gh-pages branch as described in the documentation, you simply run quarto publish at the command line and your updates are deployed to your website.\n\n\nSetting up your domain name\nBy default, if you choose to host with GitHub Pages, your website will be published to a url in the form https://username.github.io/reponame/. You can certainly do this; for example Jake VanderPlas’s awesome blog Pythonic Perambulations lives at http://jakevdp.github.io.\nBut, like me, you might want to get your own custom domain by buying, or really renting, one from a registrar. I use Namecheap. If you decide to go for a custom domain, refer to GitHub’s documentation on custom domains. You’ll also need to point your domain registrar to the IP address where GitHub Pages is hosting your website. For an example of how to do this at Namecheap, see Namecheap’s documentation about GitHub Pages\nWhether you decide to use the standard github.io domain or your own custom domain, be sure to set the site-url key in your _quarto.yml file to ensure other quarto functionality works correctly. For example\n\n\n_quarto.yml\n\nwebsite:\n  site-url: https://example.com/\n\nEdit: I found that after upgrading to quarto 1.3, using quarto publish to publish from the gh-pages branch obliterates the CNAME file that is created when you set a custom domain in your repository settings &gt; Pages &gt; Custom Domain. That breaks the mapping from your custom domain to your published website. See this disscussion thread for details. The fix is to manually create a CNAME file in the root of your project, and include it in the rendered website using the resources option under the project key in _quarto.yml. The CNAME file should just contain your custom domain, excluding any https://.\n\n\nCNAME\n\nexample.com\n\nWith the CNAME file in the root of your quarto project, you can then include it in the rendered output.\n\n\n_quarto.yml\n\nproject:\n  resources:\n    - CNAME"
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#keep-in-touch-with-your-readers",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#keep-in-touch-with-your-readers",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Keep in touch with your readers",
    "text": "Keep in touch with your readers\n\nRSS Feed\nThe RSS feed is handy for syndicating your posts to feed readers, other websites, and to your email subscribers. As described in quarto’s documentation on RSS feeds, you can automatically generate an RSS feed for your blog by first setting the value of site-url under the website key in _quarto.yml, and then setting feed: true under the listing key in the frontmatter of index.qmd. This will generate an RSS feed in the root of your website called index.xml. Once you have an RSS feed, go ahead and submit it to Python-Bloggers to have your work syndicated to a wider audience and to strengthen our little community of independent data science blogs.\n\n\nEmail Subscriptions\nThe idea here is to have a form field on your website where readers can input their email address to be added to your mailing list. Quarto’s documentation on subscriptions describes how to set up a subscribe box on your blog using MailChimp, so we won’t repeat it here. Once you have some subscribers, you can send them updates whenever you write a new post. You could do this manually or, in my case, set up an automation through MailChimp which uses your RSS feed to send out email updates to the list about new posts.\n\n\nComments\nQuarto has build-in support for three different comment systems: hypothesis, utterances, and giscus. The good news is that these are all free to use, easy to set up, and AFAIK do not engage in any sketchy tracking activities. The bad news is that none of them are ideal because they all require the user to create an account and login to leave a comment. We want to encourage readers to comment, so we don’t want them to have to create accounts or deal with passwords or pick all the squares with bicycles or any such nonsense, just to leave a little comment. To that end, I’ve actually been working on self-hosted login-free comments for this blog using isso, but it’s a bit more involved than these built-in solutions, so we’ll have to discuss it at length in a future post.\nIf you prefer an easy, out-of-the-box solution, I can recommend utterances, which uses GitHub issues to store comments for each post. I used utterances for comments on the first jekyll-based incarnation of this blog; you can still see the utterances comments on posts before this one. Go check out the Quarto documentation on comments to see how to set up utterances in your project.\n\n\nAnalytics\nAs a data enthusiast, you’ll likely enjoy collecting some data about page views and visitors to your site. You might be tempted to use Google Analytics to do this; indeed quarto makes it very easy to just add a line to your _quarto.yml file to set it up. Unfortunately, in this case, going with the easy and free solution means supporting Google’s dubious corporate surveillance activities. Be a conscientious internet citizen and avoid using Google Analytics on your blog. Fortunately, there are numerous privacy-friendly alternatives to Google Analytics. For this blog I’m self-hosting umami analytics, which might warrant its own post in the future."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#more-humbly-suggested-best-practices",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#more-humbly-suggested-best-practices",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "More humbly suggested best practices",
    "text": "More humbly suggested best practices\n\nUsing conda environments for reproducibility\nAs you know, it’s a good practice to use an environment manager to keep track of packages, their versions, and other dependencies for software in a data science project. The same applies to blog posts; especially if you’re using unusual or bleeding-edge packages in a post. This will help us out a lot when we have to go back and re-run a notebook a couple years later to regenerate the output. Here we’ll use conda as our environment manager.\nTo be clear, I don’t bother doing this if I’m just using fairly stable functionality in standard packages like pandas, numpy, and matplotlib, but we’ll do it here for illustration. From a terminal sitting inside our post subdirectory at posts/new-post, create a new conda environment with the packages you’re using in the post.\nconda create -p ./venv jupyter numpy matplotlib\nNote the -p flag which tells conda to save the environment to ./venv in the current working directory. This will save all the installed packages here in the post directory instead of in your system-wide location for conda environments. Note also that you’ll want to avoid checking anything in the venv directory into source control, so add venv to the .gitignore file at the root of the quarto project to ignore all venv directories throughout your quarto project.\nNow whenever you work on this post, you’ll navigate to the post subdirectory with a terminal and activate the conda environment.\nconda activate ./venv\nThen you can fire up your jupyter notebook from the command line, and it will use the active conda environment.\nSince we don’t want to check the venv directory with all its installed libraries into source control, we need to create an environment.yml file from which the environment can later be reproduced. With the local conda environment active, run the following.\nconda env export --from-history &gt; environment.yml\nThe --from-history flag tells conda to skip adding a bunch of system specific stuff that will gunk up your environment yaml file and make it harder to use for cross-platform reproducibility. This environment.yml file is the only environment management artifact that you need to check into git.\nLater if you need to recreate the environment from the environment.yml file, you can use the following command.\nconda env create -f environment.yml -p ./venv`\n\n\nImage file best practices\nLet’s talk about image file sizes. The key idea is that we want images to have just enough resolution to look good; any more than that and we’re just draging around larger-than-necessary files and wasting bandwidth and slowing down page load times.\nYou can read all about choosing optimal image sizes, but the TLDR is that images should be just large enough (in pixels) to fill the containers they occupy on the page. In our quarto blog, the two most common kinds of images are inline images we put in the body of posts and image thumbnails that show up as the associated image for a post, e.g. in the listing on our homepage. The inline image container seems to be about 800 pixels wide in my browser and the thumbnails are smaller, so adding some margin of error, I decided to go for 1000x750 for inline images and 500x375 for the thumbnails.\nI use a command line tool called Image Magick to resize image files. Go ahead and install image magick with homebrew, and let’s add some images to our new post.\nFor this example I’ll use a nice shot of the London Underground from Wikipedia. Save your image as image.jpg. Then use image magick to create two new resized images for inline and thumbnail use.\nconvert image.jpg -resize 1000x1000 main.jpg \nconvert image.jpg -resize 500x500 thumbnail.jpg \nThese commands do not change the aspect ratio of the image; they just reduce the size so that the image fits within the size specified.\nNow move both of your new images into the post subdirectory at posts/new-post/. To specify the thumbnail image, set the image key in the post’s front matter. Be sure to also add an alt tag description of the image using the image-alt key to keep it accessible for screen reader users. Our post’s frontmatter now looks like this.\n---\ntitle: New Post\ndate: 2023-07-12\ndescription: A nice new post\ncategories: [nonsense, code]\nimage: thumbnail.jpg\nimage-alt: \"A London Underground train emerging from a tunnel\"\n---\nTo include an image within the body of a post, use markdown in the post to include the image. I added a markdown cell just under the front matter containing the following.\n![A London Underground train emerging from a tunnel](main.jpg \"\")\nIn your preview browser window, you can see we have the thumbnail for our new post on the homepage listing.\n\n\n\nA screenshot of the homepage showing the new post’s thumbnail image\n\n\nAnd we also have the inline image appearing in the body of the post.\n\n\n\nA screenshot of the new post showing the image included in the body of the post\n\n\nYou can take a look at the source code for this blog to see some examples of including images in posts."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#seo",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#seo",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "SEO",
    "text": "SEO\nSEO is a huge topic, but here we’ll just focus on a few fundamental technical aspects that we want to be sure to get right. This boils down to registering with the top search engines by market share and ensuring that we’re providing them with the information they need to properly index our pages.\nI checked the top search engines by global market share and as of 2023 it looks like Google has about 85%, Bing has about 8%, and the others have 2% or less each. So let’s focus on setting our site up to work well with Google search and Bing to get over 90% coverage.\n\nGoogle Search Console and Bing Webmaster Tools\nGoogle Search Console is a tool for web admins to help analyze search traffic and identify any technical issues that might prevent pages from appearing or ranking well in search. Go ahead and set up an account and register your blog in search console. You can refer to Google’s documentation on search console to guide you through setup and configuration.\nOnce you get set up on GSC, you can also create an account for Bing Webmaster Tools. Do this after setting up GSC because there is an option to import your information from your GSC account.\nOnce you’re set up with GSC and BWT, you’ll get email alerts anytime they crawl your site and detect any indexing problems. When that happens, track down the issues and fix them so your pages can appear in organic searches.\n\n\nSitemap\nA sitemap is an xml document that lists all the pages on your website. It’s a map for the search engine bots that crawl the web looking for new pages to index. Quarto will automatically generate a sitemap called sitemap.xml in the root of your website, as long as you’ve filled out the site-url key in _quarto.yml. You can submit your website for indexing by providing your sitemap in Google Search Console and Bing Webmaster Tools."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#wrapping-up",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#wrapping-up",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nBoy howdy, that was a lot, but at this point you should have a fully functioning blog, built with a minimalist, data-science-friendly tech stack consisting of quarto, jupyter, and GitHub. If you do create a blog using quarto, drop a link to it in the comments, and we can all check it out and celebrate your creation!"
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html",
    "href": "posts/consider-the-decision-tree/index.html",
    "title": "Consider the Decision Tree",
    "section": "",
    "text": "A California cypress tree abides in silence on Alameda Beach.\nAh, the decision tree. It’s an underrated and often overlooked hero of modern statistical learning. Trees aren’t particularly powerful learning algorithms on their own, but when utilized as building blocks in larger ensemble models like random forest and gradient boosted trees, they can achieve state of the art performance in many practical applications. Since we’ve been focusing on gradient boosting ensembles lately, let’s take a moment to consider the humble decision tree itself. This post gives a high-level intuition for how trees work, an opinionated list of their key strengths and weaknesses, and some perspective on why ensembling makes them truly shine.\nOnward!"
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#classification-and-regression-trees",
    "href": "posts/consider-the-decision-tree/index.html#classification-and-regression-trees",
    "title": "Consider the Decision Tree",
    "section": "Classification and Regression Trees",
    "text": "Classification and Regression Trees\nA Decision tree is a type of statistical model that takes features or covariates as input and yields a prediction as output. The idea of the decision tree as a statistical learning tool traces back to a monograph published in 1984 by Breiman, Freidman, Olshen, and Stone called “Classification and Regression Trees” (a.k.a. CART). As the name suggests, trees come in two main varieties: classification trees which predict discrete class labels (e.g. DecisionTreeClassifier) and regression trees which predict numeric values (e.g. DecisionTreeRegressor).\nAs I mentioned earlier, tree models are not very powerful learners on their own. You might find that an individual tree model is useful for creating a simple and highly interpretable model in specific situations, but in general, trees tend to shine most as building blocks in more complex algorithms. These composite models are called ensembles, and the most important tree ensembles are random forest and gradient boosted trees. While random forest uses either regression or classification trees depending on the type of target, gradient boosting can use regression trees to solve both classification and regression tasks."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#regression-tree-in-action",
    "href": "posts/consider-the-decision-tree/index.html#regression-tree-in-action",
    "title": "Consider the Decision Tree",
    "section": "Regression Tree in Action",
    "text": "Regression Tree in Action\nLet’s have a closer look at regression trees by training one on the diabetes dataset from scikit learn. According to the documentation:\n\nTen baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n\nFirst we load the data. To make our lives easier, we’ll just use two features: average blood pressure (bp) and the first blood serum measurement (s1) to predict the target. I’ll rescale the features to make the values easier for me to read, but it won’t affect our tree–more on that later.\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor_palette = \"viridis\"\n\n\nfrom sklearn.datasets import load_diabetes\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\nX = 100 * X[['bp', 's1']]\n\n\n\n\n\n\nLet’s grow a tree to predict the target given values of blood pressure and blood serum.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(max_depth=2)\ntree.fit(X,y);\n\n\n\n\n\n\nTo make predictions using our fitted tree, we start at the root node (which is at the top), and we work our way down moving left if our feature is less than the split threshold and to the right if it’s greater than the split threshold. For example let’s predict the target for a new case with bp= 1 and s1 = 5. Since our blood pressure of 1 is less than 2.359, we move to the left child node. Here, since our serum of 5 is greater than the threshold at 0.875, we move to the right child node. This node has no further children, and thus we return its predicted value of 155.343.\n\ntree.predict(pd.DataFrame({'bp': 1, 's1': 5}, index=[0]))\n\narray([155.34313725])\n\n\nLet’s overlay these splits on our feature scatterplot to see how the tree has partitioned the feature space.\n\n\n\n\n\nThe tree has managed to carve out regions of feature space where the target values tend to be similar within each region, e.g. we have low target values in the bottom left partition and high target values in the far right region.\nLet’s take a look at the regression surface predicted by our tree. Since the tree predicts the exact same value for all instances in a given partition, the surface has only four distinct values.\n\n\n\n\n\nFabulous, now that we’ve seen a tree in action, let’s talk about trees’ key strengths and weaknesses."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#why-trees-are-awesome",
    "href": "posts/consider-the-decision-tree/index.html#why-trees-are-awesome",
    "title": "Consider the Decision Tree",
    "section": "Why trees are awesome",
    "text": "Why trees are awesome\nTrees are awesome because they are easy to use, and trees are easy to use because they are robust, require minimal data preprocessing, and can learn complex relationships without user intervention.\n\nFeature Scaling\nTrees owe their minimal data preprocessing requirements and their robustness to the fact that split finding is controlled by the sort order of the input feature values, rather than the values themselves. This means that trees are invariant to the scaling of input features, which in turn means that we don’t need to fuss around with carefully rescaling all the numeric features before fitting a tree. It also means that trees tend to work well even if features are highly skewed or contain outliers.\n\n\nCategoricals\nSince trees just split data based on numeric feature values, we can easily handle most categorical features by using integer encoding. For example we might encode a size feature with small = 1, medium = 2, and large = 3. This works particularly well with ordered categories, because partitioning is consistent with the category semantics. It can also work well even if the categories have no order, because with enough splits a tree can carve each category into its own partition.\n\n\nMissing Values\nIt’s worth calling out that different implementations of the decision tree handle missing feature values in different ways. Notably, scikit-learn handles them by throwing an error and telling you not to pull such shenanigans.\nValueError: Input contains NaN, infinity or a value too large for dtype('float32').\nOn the other hand, XGBoost supports an elegant way to make use of missing values, which we will discuss more in a later post.\n\n\nInteractions\nFeature interactions can also be learned automatically. An interaction means that the effect of one feature on the target differs depending on the value of another feature. For example, the effect of some drug may depend on whether or not the patient exercises. After a tree splits on exercise, it can naturally learn the correct drug effects for both exercisers and non-exercisers. This intuition extends to higher-order interactions as well, as long as the tree has enough splits to parse the relationships.\n\n\nFeature Selection\nBecause trees choose the best feature and threshold value at each split, they essentially perform automatic feature selection. This is great because even if we throw a lot of irrelevant features at a decision tree, it will simply tend not to use them for splits. Similarly, if two or more features are highly correlated or even redundant, the tree will simply choose one or the other when making each split; having both in the model will not cause catastrophic instability as it could in a linear model.\n\n\nFeature-Target Relationship\nFinally, it is possible for trees to discover complex nonlinear feature-target relationships without the need for user-specification of the relationships. This is because trees use local piecewise constant approximations without making any parametric assumptions. With enough splits, the tree can approximate arbitrary feature-target relationships."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#why-trees-are-not-so-awesome",
    "href": "posts/consider-the-decision-tree/index.html#why-trees-are-not-so-awesome",
    "title": "Consider the Decision Tree",
    "section": "Why trees are not so awesome",
    "text": "Why trees are not so awesome\nThe main weakness of the decision tree is that, on its own, it tends to have poor predictive performance compared to other algorithms. The main reasons for this are the tendency to overfit and prediction quantization issues.\n\nOverfitting\nIf we grow a decision tree until each leaf has exactly one instance in it, we will have simply memorized the training data, and our model will not generalize well. Basically the only defense against overfitting is to reduce the number of leaf nodes in the tree, either by using hyperparameters to stop splitting earlier or by removing certain leaf nodes after growing a deep tree. The problem here is that some of the benefits of trees, like ability to approximate arbitrary target patterns and ability to learn interaction effects, depend on having enough splits for the task. We can sometimes find ourselves in a situation where we cannot learn these complex relationships without overfitting the tree.\n\n\nQuantization\nBecause regression trees use piecewise constant functions to approximate the target, prediction accuracy can deteriorate near split boundaries. For example, if the target is increasing with the feature, a tree might tend to overpredict the target on the left side of split boundaries and overpredict on the right side of split boundaries.\n\n\n\n\n\n\n\nExtrapolation\nBecause they are trained by partitioning the feature space in a training dataset, trees cannot intelligently extrapolate beyond the data on which they are trained. For example if we query a tree for predictions beyond the greatest feature value encountered in training, it will just return the prediction corresponding to the largest in-sample feature values.\n\n\n\n\n\n\n\nThe Dark Side of Convenience\nFinally, there is always a price to pay for convenience. While trees can work well even with a messy dataset containing outliers, redundant features, and thoughtlessly encoded categoricals, we will rarely achieve the best performance under these conditions. Taking the time to deal with outliers, removing redundant information, purposefully choosing appropriate categorical encodings, and building an understanding of the data will often lead to much better results."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#how-ensembling-makes-trees-shine",
    "href": "posts/consider-the-decision-tree/index.html#how-ensembling-makes-trees-shine",
    "title": "Consider the Decision Tree",
    "section": "How ensembling makes trees shine",
    "text": "How ensembling makes trees shine\nWe can go a long way toward addressing the issues of overfitting and prediction quantization by using trees as building blocks in larger algorithms called tree ensembles, the most popular examples being random forest and gradient boosted trees. A tree ensemble is a collection of different individual tree models whose predictions are averaged to generate an overall prediction.\nEnsembling helps address overfitting because even if each individual tree is overfitted, the average of their individual noisy predictions will tend to be more stable. Think of it in terms of the bias variance tradeoff, where bias refers to a model’s failure to capture certain patterns and variance refers to how different a model prediction would be if the model were trained on a different sample of training data. Since the ensemble is averaging over the predictions of all the individual models, training it on a different sample of training data would change the individual models predictions, but their overall average prediction will tend to remain stable. Thus, ensembling helps reduce the effects of overfitting by reducing model variance without increasing bias.\nEnsembling also helps address prediction quantization issues. While each individual tree’s predictions might express large jumps in the regression surface, averaging many different trees’ predictions together effectively generates a surface with more partitions and smaller jumps between them. This provides a smoother approximation of the feature-target relationship."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#wrapping-up",
    "href": "posts/consider-the-decision-tree/index.html#wrapping-up",
    "title": "Consider the Decision Tree",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, there you go, that’s my take on the high-level overview of the decision tree and its main strengths and weaknesses. As we’ve seen, ensembling allows us to keep the conveniences of the decision tree while mitigating its core weakness of relatively weak predictive power. This is why tree ensembles are so popular in practical applications. We glossed over pretty much all details of how trees actually do their magic, but fear not, next time we’re going to get rowdy and build one of these things from scratch."
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello World! And Why I’m Inspired to Start a Blog",
    "section": "",
    "text": "Matt raises his arms in joy at the world.!\nWell, I’ve been thinking about getting this blog started for months now. I guess a combination of inertia, up-front investment in blogging platform selection/setup, and spending a little too much time writing and rewriting the first content post has drawn out the period from initial inspiration to making the blog a reality. Needless to say, I’m pretty excited to finally get things going.\nBefore we dive headlong into the weeds of ML algorithms, statistical methods, and whatever I happen to be learning and teaching at the moment, I figured it would be good to articulate why I’ve felt inspired to get started blogging in the first place. Hopefully this will serve the dual purpose of clarifying my intentions and introducing a vastly underappreciated concept in data science that I hope to weave through the posts to come."
  },
  {
    "objectID": "posts/hello-world/index.html#learning",
    "href": "posts/hello-world/index.html#learning",
    "title": "Hello World! And Why I’m Inspired to Start a Blog",
    "section": "Learning",
    "text": "Learning\nThe initial inception about blogging probably originated from some comments about learning that Jeremy Howard makes in the Practical Deep Learning course from fastai. During one of the lectures, he mentions that it’s a great idea to start blogging. To paraphrase Jeremy:\n\nThe thing I really love about blogging is that it helps you learn; by writing things down, you synthesize your ideas.\n\nBeautiful. That definitely rings true for me. I tend to take notes and play around with code when learning new concepts anyway. One of my key hypotheses about this blogging experiment is that making the effort to transform those notes into blog posts will help me learn more effectively."
  },
  {
    "objectID": "posts/hello-world/index.html#teaching",
    "href": "posts/hello-world/index.html#teaching",
    "title": "Hello World! And Why I’m Inspired to Start a Blog",
    "section": "Teaching",
    "text": "Teaching\nAh, teaching. Yes, sometimes it’s that thing that takes time away from your research, forcing you to sit alone in a windowless room squinting at hand-written math on a fat stack of homework assignments. But sometimes it actually involves interacting with students, endeavoring to explain a concept, and watching them light up when they get it. The latter manifestation of teaching was one of my favorite things about grad school and academia in general. While I certainly still get to do some teaching as an industry data scientist, I could see myself returning to a more teaching-centric gig somewhere off in the future. Thus we have our second key hypothesis about the blogging experiment, that the writing will entertain my inclination to teach."
  },
  {
    "objectID": "posts/hello-world/index.html#contributing",
    "href": "posts/hello-world/index.html#contributing",
    "title": "Hello World! And Why I’m Inspired to Start a Blog",
    "section": "Contributing",
    "text": "Contributing\nWorking in the field of data science today is a bit like standing in front of a massive complimentary all-you-can-learn buffet. There is an abundance of free material out on the interwebs for learning pretty much anything in data science from hello world python tutorials to research papers on cutting-edge deep learning techniques. I’ve personally benefited from many a blog post that helped me unpack a new concept or get started using a new tool. And let’s not forget the gigantic cyber warehouse full of freely available open source software tools that volunteer developers have straight-up donated to humanity.\nI realize that up to now, I’ve simply been consuming all of this free goodness without giving anything substantive back in return. Well then, it’s time to start evening the score. Which brings us to key hypothesis number three, that through these blog posts, I might be able to create something helpful, thereby being of service to a community that has freely given so much to me."
  },
  {
    "objectID": "posts/hello-world/index.html#live-long-and-prosper-blog",
    "href": "posts/hello-world/index.html#live-long-and-prosper-blog",
    "title": "Hello World! And Why I’m Inspired to Start a Blog",
    "section": "Live Long and Prosper, Blog",
    "text": "Live Long and Prosper, Blog\nPhew, there it is, the original source of inspiration for this blogging experiment, and three reasons I think it might be a good idea. The astute reader will have noticed that these three assertions have been formulated as hypotheses which are to be tested in the laboratory of experience. And thus, we also have our first glimpse of the scientific method, an underrated concept that is going to help us put the science back in data science.\nWith that, blog, I christen thee, Random Realizations."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "",
    "text": "Cold water cascades over the rocks in Erwin, Tennessee.\nFriends, this is going to be an epic post! Today, we bring together all the ideas we’ve built up over the past few posts to nail down our understanding of the key ideas in Jerome Friedman’s seminal 2001 paper: “Greedy Function Approximation: A Gradient Boosting Machine.” In particular, we’ll summarize the highlights from the paper, and we’ll build an in-house python implementation of his generic gradient boosting algorithm which can train with any differentiable loss function. What’s more, we’ll go ahead and take our generic gradient boosting machine for a spin by training it with several of the most popular loss functions used in practice.\nAre you freaking stoked or what?\nSweet. Let’s do this."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedman-2001-tldr",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedman-2001-tldr",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Friedman 2001: TL;DR",
    "text": "Friedman 2001: TL;DR\nI’ve mentioned this paper a couple of times before, but as far as I can tell, this is the origin of gradient boosting; it is therefore, a seminal work worth reading. You know what, I think you might like to pick up the paper and read it yourself. Like many papers, there is a lot of scary looking math in the first few pages, but if you’ve been following along on this blog, you’ll find that it’s actually totally approachable. This is the kind of thing that cures imposter syndrome, so give it a shot. That said, here’s the TL;DR as I see it.\nThe first part of the paper introduces the idea of fitting models by doing gradient descent in function space, an ingenious idea we spent an entire post demystifying earlier. Friedman goes on to introduce the generic gradient boost algorithm, which works with any differentiable loss function, as well as specific variants for minimizing absolute error, Huber loss, and binary deviance. In terms of hyperparameters, he points out that the learning rate can be used to reduce overfitting, while increased tree depth can help capture more complex interactions among features. He even discusses feature importance and partial dependence methods for interpreting fitted gradient boosting models.\nFriedman concludes by musing about the advantages of gradient boosting with trees. He notes some key advantages afforded by the use of decision trees including no need to rescale input data, robustness against irrelevant input features, and elegant handling of missing feature values. He points out that gradient boosting manages to capitalize on the benefits of decision trees while minimizing their key weakness (crappy accuracy). I think this offers a great insight into why gradient boosting models have become so widespread and successful in practical ML applications."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedmans-generic-gradient-boosting-algorithm",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedmans-generic-gradient-boosting-algorithm",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Friedman’s Generic Gradient Boosting Algorithm",
    "text": "Friedman’s Generic Gradient Boosting Algorithm\nLet’s take a closer look at Friedman’s original gradient boost algorithm, Alg. 1 in Section 3 of the paper (translated into the notation we’ve been using so far).\nLike last time, we have training data \\((\\mathbf{y}, \\mathbf{X})\\) where \\(\\mathbf{y}\\) is a length-\\(n\\) vector of target values, and \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix with \\(n\\) observations of \\(p\\) features. We also have a differentiable loss function \\(L(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^n l(y_i, \\hat{y}_i)\\), a “learning rate” hyperparameter \\(\\eta\\), and a fixed number of model iterations \\(M\\).\nAlgorithm: gradient_boost\\((\\mathbf{X},\\mathbf{y},L,\\eta, M)\\) returns: model \\(F_M\\)\n\nLet base model \\(F_0(\\mathbf{x}) = c\\), where \\(c = \\text{argmin}_{c} \\sum_{i=1}^n l(y_i, c)\\)\nfor \\(m\\) = \\(0\\) to \\(M-1\\):\n     Let “pseudo-residual” vector \\(\\mathbf{r}_m = -\\nabla_{\\mathbf{\\hat{y}}_m} L(\\mathbf{y},\\mathbf{\\hat{y}}_m)\\)\n     Train decision tree regressor \\(h_m(\\mathbf{X})\\) to predict \\(\\mathbf{r}_m\\) (minimizing squared error)\n     foreach terminal leaf node \\(t \\in h_m\\):\n          Let \\(v = \\text{argmin}_v \\sum_{i \\in t} l(y_i, F_m(\\mathbf{x}_i) + v)\\)\n          Set terminal leaf node \\(t\\) to predict value \\(v\\)\n     \\(F_{m+1}(\\mathbf{X}) = F_{m}(\\mathbf{X}) + \\eta h_m(\\mathbf{X})\\)\nReturn composite model \\(F_M\\)\n\nBy now, most of this is already familiar to us. We begin by setting the base model \\(F_0\\) equal to the constant prediction value that minimizes the loss over all examples in the training dataset (line 1). Then we begin the boosting iterations (line 2), each time computing the negative gradients of the loss with respect to the current model predictions (known as the pseudo residuals) (line 3). We then fit our next decision tree regressor to predict the pseudo residuals (line 4).\nThen we encounter something new on lines 5-7. When we fit a vanilla decision tree regressor to predict pseudo residuals, we’re using mean squared error as the loss function to train the tree. As you might imagine, this works well when the global loss function is also squared error. But if we want to use a global loss other than squared error, there is an additional trick we can use to further increase the composite model’s accuracy. The idea is to continue using squared error to train each decision tree, keeping its structure and split conditions but altering the predicted value in each leaf to help minimize the global loss function. Instead of using the mean target value as the prediction for each node (as we would do when minimizing squared error), we use a numerical optimization method like line search to choose the constant value for that leaf that leads to the best overall loss. This is the same thing we did in line 1 of the algorithm to set the base prediction, but here we choose the optimal prediction for each terminal node of the newly trained decision tree."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#implementation",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#implementation",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Implementation",
    "text": "Implementation\nI did some (half-assed) searching on the interweb for an implementation of GBM that allows the user to provide a custom loss function, and you know what? I couldn’t find anything. If you find another implementation, post in the comments so we can learn from it too.\nSince we need to modify the values predicted by our decision trees’ terminal nodes, we’ll want to brush up on the scikit-learn decision tree structure before we get going. You can see explanations of all the necessary decision tree hacks in this notebook.\n\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor \nfrom scipy.optimize import minimize\n\nclass GradientBoostingMachine():\n    '''Gradient Boosting Machine supporting any user-supplied loss function.\n    \n    Parameters\n    ----------\n    n_trees : int\n        number of boosting rounds\n        \n    learning_rate : float\n        learning rate hyperparameter\n        \n    max_depth : int\n        maximum tree depth\n    '''\n    \n    def __init__(self, n_trees, learning_rate=0.1, max_depth=1):\n        self.n_trees=n_trees; \n        self.learning_rate=learning_rate\n        self.max_depth=max_depth;\n    \n    def fit(self, X, y, objective):\n        '''Fit the GBM using the specified loss function.\n        \n        Parameters\n        ----------\n        X : ndarray of size (number observations, number features)\n            design matrix\n            \n        y : ndarray of size (number observations,)\n            target values\n            \n        objective : loss function class instance\n            Class specifying the loss function for training.\n            Should implement two methods:\n                loss(labels: ndarray, predictions: ndarray) -&gt; float\n                negative_gradient(labels: ndarray, predictions: ndarray) -&gt; ndarray\n        '''\n        \n        self.trees = []\n        self.base_prediction = self._get_optimal_base_value(y, objective.loss)\n        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n        for _ in range(self.n_trees):\n            pseudo_residuals = objective.negative_gradient(y, current_predictions)\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, pseudo_residuals)\n            self._update_terminal_nodes(tree, X, y, current_predictions, objective.loss)\n            current_predictions += self.learning_rate * tree.predict(X)\n            self.trees.append(tree)\n     \n    def _get_optimal_base_value(self, y, loss):\n        '''Find the optimal initial prediction for the base model.'''\n        fun = lambda c: loss(y, c)\n        c0 = y.mean()\n        return minimize(fun=fun, x0=c0).x[0]\n        \n    def _update_terminal_nodes(self, tree, X, y, current_predictions, loss):\n        '''Update the tree's predictions according to the loss function.'''\n        # terminal node id's\n        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n        # compute leaf for each sample in ``X``.\n        leaf_node_for_each_sample = tree.apply(X)\n        for leaf in leaf_nodes:\n            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n            y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n            preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n            val = self._get_optimal_leaf_value(y_in_leaf, \n                                               preds_in_leaf,\n                                               loss)\n            tree.tree_.value[leaf, 0, 0] = val\n            \n    def _get_optimal_leaf_value(self, y, current_predictions, loss):\n        '''Find the optimal prediction value for a given leaf.'''\n        fun = lambda c: loss(y, current_predictions + c)\n        c0 = y.mean()\n        return minimize(fun=fun, x0=c0).x[0]\n          \n    def predict(self, X):\n        '''Generate predictions for the given input data.'''\n        return (self.base_prediction \n                + self.learning_rate \n                * np.sum([tree.predict(X) for tree in self.trees], axis=0))\n\nIn terms of design, we implement a class for the GBM with scikit-like fit and predict methods. Notice in the below implementation that the fit method is only 10 lines long, and corresponds very closely to Friedman’s gradient boost algorithm from above. Most of the complexity comes from the helper methods for updating the leaf values according to the specified loss function.\nWhen the user wants to call the fit method, they’ll need to supply the loss function they want to use for boosting. We’ll make the user implement their loss (a.k.a. objective) function as a class with two methods: (1) a loss method taking the labels and the predictions and returning the loss score and (2) a negative_gradient method taking the labels and the predictions and returning an array of negative gradients."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#testing-our-model",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#testing-our-model",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Testing our Model",
    "text": "Testing our Model\nLet’s test drive our custom-loss-ready GBM with a few different loss functions! We’ll compare it to the scikit-learn GBM to sanity check our implementation.\n\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n\nrng = np.random.default_rng()\n\n# test data\ndef make_test_data(n, noise_scale):\n    x = np.linspace(0, 10, 500).reshape(-1,1)\n    y = (np.where(x &lt; 5, x, 5) + rng.normal(0, noise_scale, size=x.shape)).ravel()\n    return x, y\n    \n# print model loss scores\ndef print_model_loss_scores(obj, y, preds, sk_preds):\n    print(f'From Scratch Loss = {obj.loss(y, pred):0.4}')\n    print(f'Scikit-Learn Loss = {obj.loss(y, sk_pred):0.4}')\n\n\nMean Squared Error\nMean Squared Error (a.k.a. Least Squares) loss produces estimates of the mean target value conditioned on the feature values. Here’s the implementation.\n\nx, y = make_test_data(500, 0.4)\n\n\n# from scratch GBM\nclass SquaredErrorLoss():\n    '''User-Defined Squared Error Loss'''\n    \n    def loss(self, y, preds):\n        return np.mean((y - preds)**2)\n    \n    def negative_gradient(self, y, preds):\n        return y - preds\n    \n\ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                              max_depth=1)\ngbm.fit(x, y, SquaredErrorLoss())\npred = gbm.predict(x)\n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingRegressor(n_estimators=10,\n                                   learning_rate=0.5,\n                                   max_depth=1,\n                                   loss='squared_error')\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict(x)\n\n\nprint_model_loss_scores(SquaredErrorLoss(), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.168\nScikit-Learn Loss = 0.168\n\n\n\n\n\n\n\n\n\nMean Absolute Error\nMean Absolute Error (a.k.a.Least Absolute Deviations) loss produces estimates of the median target value conditioned on the feature values. Here’s the implementation.\n\nx, y = make_test_data(500, 0.4)\n\n\n\n# from scratch GBM\nclass AbsoluteErrorLoss():\n    '''User-Defined Absolute Error Loss'''\n    \n    def loss(self, y, preds):\n        return np.mean(np.abs(y - preds))\n    \n    def negative_gradient(self, y, preds):\n        return np.sign(y - preds)\n\n\ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                              max_depth=1)\ngbm.fit(x, y, AbsoluteErrorLoss())\npred = gbm.predict(x)\n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingRegressor(n_estimators=10,\n                                   learning_rate=0.5,\n                                   max_depth=1,\n                                   loss='absolute_error')\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict(x)\n\n\nprint_model_loss_scores(AbsoluteErrorLoss(), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.3225\nScikit-Learn Loss = 0.3208\n\n\n\n\n\n\n\n\n\nQuantile Loss\nQuantile loss yields estimates of a given quantile of the target variable conditioned on the features. Here’s my implementation.\n\nx, y = make_test_data(500, 1)\n\n\n\n# from scratch GBM\nclass QuantileLoss():\n    '''Quantile Loss\n    \n    Parameters\n    ----------\n    alpha : float\n        quantile to be estimated, 0 &lt; alpha &lt; 1\n    '''\n    \n    def __init__(self, alpha):\n        if alpha &lt; 0 or alpha &gt;1:\n            raise ValueError('alpha must be between 0 and 1')\n        self.alpha = alpha\n        \n    def loss(self, y, preds):\n        e = y - preds\n        return np.mean(np.where(e &gt; 0, self.alpha * e, (self.alpha - 1) * e))\n    \n    def negative_gradient(self, y, preds):\n        e = y - preds \n        return np.where(e &gt; 0, self.alpha, self.alpha - 1)\n\ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                             max_depth=1)\ngbm.fit(x, y, QuantileLoss(alpha=0.9))\npred = gbm.predict(x)    \n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingRegressor(n_estimators=10,\n                                 learning_rate=0.5,\n                                 max_depth=1,\n                                 loss='quantile', alpha=0.9)\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict(x)\n\n\nprint_model_loss_scores(QuantileLoss(alpha=0.9), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.1853\nScikit-Learn Loss = 0.1856\n\n\n\n\n\n\n\n\n\nBinary Cross Entropy Loss\nThe previous losses are useful for regression problems, where the target is numeric. But we can also solve classification problems, simply by swapping in an appropriate loss function. Here we’ll implement binary cross entropy, a.k.a. binary deviance, a.k.a. negative binomial log likelihood (sometimes abusively called log loss). One thing to remember is that, as with logistic regression, our model is actually predicting the log odds ratio, not the probability of the positive class. Thus we use expit transformations (the inverse of logit) whenever probabilities are needed, e.g., when predicting the probability that an observation belongs to the positive class.\n\n# make categorical test data\n\ndef expit(t):\n    return np.exp(t) / (1 + np.exp(t))\n\nx = np.linspace(-3, 3, 500)\np = expit(x)\ny = rng.binomial(1, p, size=p.shape)\nx = x.reshape(-1,1)\n\n\n# from scratch GBM\nclass BinaryCrossEntropyLoss():\n    '''Binary Cross Entropy Loss\n    \n    Note that the predictions should be log odds ratios.\n    '''\n    \n    def __init__(self):\n        self.expit = lambda t: np.exp(t) / (1 + np.exp(t))\n    \n    def loss(self, y, preds):\n        p = self.expit(preds)\n        return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    def negative_gradient(self, y, preds):\n        p = self.expit(preds)\n        return y / p - (1 - y) / (1 - p)\n\n    \ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                              max_depth=1)\ngbm.fit(x, y, BinaryCrossEntropyLoss())\npred = expit(gbm.predict(x))\n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingClassifier(n_estimators=10,\n                                    learning_rate=0.5,\n                                    max_depth=1,\n                                    loss='log_loss')\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict_proba(x)[:, 1]\n\n\nprint_model_loss_scores(BinaryCrossEntropyLoss(), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.6379\nScikit-Learn Loss = 0.6403"
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#wrapping-up",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#wrapping-up",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWoohoo! We did it! We finally made it through Friedman’s paper in its entirety, and we implemented the generic gradient boosting algorithm which works with any differentiable loss function. If you made it this far, great job, gold star! By now you hopefully have a pretty solid grasp on gradient boosting, which is good, because soon we’re going to dive into the modern Newton descent gradient boosting frameworks like XGBoost. Onward!"
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#references",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#references",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "References",
    "text": "References\nFriedman’s 2001 paper: Greedy Function Approximation: A Gradient Boosting Machine"
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html",
    "href": "posts/get-down-with-gradient-descent/index.html",
    "title": "Get Down with Gradient Descent",
    "section": "",
    "text": "Ahh, gradient descent. It’s probably one of the most ubiquitous algorithms used in data science, but you’re unlikely to see it being celebrated in the limelight of the Kaggle podium. Rather than taking center stage, gradient descent operates under the hood, powering the training for a wide range of models including deep neural networks, gradient boosting trees, generalized linear models, and mixed effects models. Getting an intuition for the algorithm will reveal how model fitting actually works and help us to see the common thread connecting a wide range of seemingly unrelated models. In this post we’ll get the intuition for gradient descent with a fresh analogy, develop the mathematical formulation, and ground our understanding by using it to train ourselves a linear regression model."
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#intuition",
    "href": "posts/get-down-with-gradient-descent/index.html#intuition",
    "title": "Get Down with Gradient Descent",
    "section": "Intuition",
    "text": "Intuition\nBefore we dive into the intuition for gradient descent itself, let’s get a high-level view of why it’s useful in training or fiting a model. Training a model basically means finding the model parameter values that make the model fit a given dataset well. We measure how well a model fits data using a special function variously called a loss or cost or objective function. A loss function takes the dataset and the model as arguments and returns a number that tells us how well our model fits the data. Therefore training is an optimization problem in which we search for the model parameter values that result in the minimum value of the loss function. Enter gradient descent.\nGradient descent is a numerical optimization technique that helps us find the inputs that yield the minimum value of a function. Since most explanations of the gradient descent algorithm seem to use a story about hikers being lost in some foggy mountains, we’re going to try out a new analogy.\nLet’s say you’re at a concert. Remember those? They’re these things that used to happen where people played music and everyone danced and had a great time.\n\nNOTE: Chiming in here in 2023 from a sort-of-post COVID 19 world, happily I can report that concerts and live music are back!\n\nNow suppose at this concert there’s a dance floor which has become a bit sweltering from copious amounts of “getting down”. But the temperature isn’t quite uniform; maybe there’s a cool spot from a ceiling fan somewhere.\n\n\n\ndance floor\n\n\nLet’s get ourselves to that cool spot using the following procedure.\n\nFrom our current location, figure out which direction feels coolest.\nTake a step (or simply shimmy) in that direction.\nRepeat steps 1 and 2 until we reach the coolest spot on the dance floor.\n\nThe crux of this procedure is figuring out, at each step, which direction yields the greatest temperature reduction. Our skin is pretty sensitive to temperature, so we can just use awareness of body sensation to sense which direction feels coolest. Luckily, we have a mathematical equivalent to our skin’s ability to sense local variation in temperature.\n\nDetermine which way to go\nLet \\(f(x,y)\\) be the temperature on the dance floor at position \\((x,y)\\). The direction of fastest decrease in temperature is going to be given by some vector in our \\((x,y)\\) space, e.g.,\n[vector component in \\(x\\) direction, vector component in \\(y\\) direction]\nTurns out that the gradient of a function evaluated at a particular location yields a vector that points in the direction of fastest increase in the function, pretty similar to what we’re looking for. The gradient of \\(f(x,y)\\) is given by\n\\[ \\nabla f(x,y) = \\left [ \\frac{\\partial f(x,y)}{\\partial x}, \\frac{\\partial f(x,y)}{\\partial y} \\right ] \\]\nThe components of the gradient vector are the partial derivatives of our function \\(f(x,y)\\), evaluated at the point \\((x,y)\\). These partial derivatives just tell us the slope of \\(f(x,y)\\) in the \\(x\\) and \\(y\\) directions respectively. The intuition is that if \\(\\frac{\\partial f(x,y)}{\\partial x}\\) is a large positive number, then moving in the positive \\(x\\) direction will make \\(f(x,y)\\) increase a lot, whereas if \\(\\frac{\\partial f(x,y)}{\\partial x}\\) is a large negative number, then moving in the negative \\(x\\) direction will make \\(f(x,y)\\) increase a lot.\nIt’s not too hard to see that the direction of fastest decrease is actually just the exact opposite direction from that of fastest increase. Since we can point a vector in the opposite direction by negating its component values, our direction of fastest temperature decrease will be given by the negative gradient of the temperature field \\(-\\nabla f(x,y)\\).\n\n\n\ndance floor with hot and cold sides\n\n\n\n\nTake a step in the right direction\nNow that we have our direction vector, we’re ready to take a step toward the cool part of the dance floor. To do this, we’ll just add our direction vector to our current position. The update rule would look like this.\n\\[ [x_\\text{next}, y_\\text{next}] = [x_\\text{prev}, y_\\text{prev}] - \\nabla f (x_\\text{prev}, y_\\text{prev}) = [x_\\text{prev}, y_\\text{prev}] -  \\left [ \\frac{\\partial f (x_\\text{prev}, y_\\text{prev})}{\\partial x}, \\frac{\\partial f (x_\\text{prev}, y_\\text{prev})}{\\partial y} \\right ] \\]\nIf we iteratively apply this update rule, we’ll end up tracing a trajectory through the \\((x,y)\\) space on the dance floor and we’ll eventually end up at the coolest spot!\n\n\n\ndance floor with trajectory from hot side to cool side\n\n\nGreat success!"
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#general-formulation",
    "href": "posts/get-down-with-gradient-descent/index.html#general-formulation",
    "title": "Get Down with Gradient Descent",
    "section": "General Formulation",
    "text": "General Formulation\nLet’s generalize a bit to get to the form of gradient descent you’ll see in references like the wikipedia article.\nFirst we modify our update equation above to handle functions with more than two arguments. We’ll use a bold \\(\\mathbf{x}\\) to indicate a vector of inputs \\(\\mathbf{x} = [x_1,x_2,\\dots,x_p]\\). Our function \\(f(\\mathbf{x}): \\mathbb{R}^p \\mapsto \\mathbb{R}\\) maps a \\(p\\) dimensional input to a scalar output.\nSecond, instead of displacing our current location with the negative gradient vector itself, we’ll first rescale it with a learning rate parameter. This helps address any issues with units on inputs versus outputs. Imagine the input could range between 0 and 1, but the output ranged from 0 to 1,000. We would need to rescale the partial derivatives so the update step doesn’t send us way too far off in input space.\nFinally, we’ll index our updates with \\(t=0,1,\\dots\\). We’ll run for some prespecified number of iterations or we’ll stop the procedure once the change in \\(f(\\mathbf{x})\\) is sufficiently small from one iteration to the next. Our update equation will look like this.\n\\[\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f ( \\mathbf{x}_t) \\]\nIn pseudocode we could write it like this.\n# gradient descent\nx = initial_value_of_x \nfor t in range(n_iterations):  # or some other convergence condition\n    x -= learning_rate * gradient_of_f(x)\nNow let’s see how this algorithm gets used to train models."
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#training-a-linear-regression-model-with-gradient-descent",
    "href": "posts/get-down-with-gradient-descent/index.html#training-a-linear-regression-model-with-gradient-descent",
    "title": "Get Down with Gradient Descent",
    "section": "Training a Linear Regression Model with Gradient Descent",
    "text": "Training a Linear Regression Model with Gradient Descent\nTo get the intuition for how we use gradient descent to train models, let’s use it to train a linear regression model. Note that we wouldn’t actually use gradient descent to train a linear model in real life since there is an exact analytical solution for the best-fit parameter values.\nAnyway, in the simple linear regression problem we have numerical feature \\(x\\) and numerical target \\(y\\), and we want to find a model of the form\n\\[F(x) = \\alpha + \\beta x\\]\nThis model has two parameters, \\(\\alpha\\) and \\(\\beta\\). Here “training” means finding the parameter values that make \\(F(x)\\) fit our \\(y\\) data best. We measure how well, or really how poorly, our model fits the data by using a loss function that yields a small value when a model fits well. Ordinary least squares is so named because it uses mean squared error as its loss function.\n\\[L(y, F(x)) =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - F(x_i))^2  =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\alpha + \\beta x_i))^2 \\]\nThe loss function \\(L\\) takes four arguments: \\(x\\), \\(y\\), \\(\\alpha\\), and \\(\\beta\\). But since \\(x\\) and \\(y\\) are fixed given our dataset, we could write the loss as \\(L(\\alpha, \\beta | x, y)\\) to emphasize that \\(\\alpha\\) and \\(\\beta\\) are the only free parameters. So we’re looking for the following.\n\\[\\underset{\\alpha,\\beta}{\\operatorname{argmin}} ~ L(\\alpha,\\beta|x,y) \\]\nThat’s right, we’re looking for the values of \\(\\alpha\\) and \\(\\beta\\) that minimize scalar-valued function \\(L(\\alpha, \\beta)\\). Sounds familiar huh?\nTo solve this minimization problem with gradient descent, we can use the following update rule.\n\\[[\\alpha_{t+1}, \\beta_{t+1}] = [\\alpha_{t}, \\beta_{t}] - \\eta \\nabla L(\\alpha_t, \\beta_t | x, y) \\]\nTo get the gradient \\(\\nabla L(\\alpha,\\beta|x,y)\\), we need the partial derivatives of \\(L\\) with respect to \\(\\alpha\\) and \\(\\beta\\). Since \\(L\\) is just a big sum, it’s easy to calculate the derivatives.\n\\[ \\frac{\\partial L(\\alpha, \\beta)}{\\partial \\alpha} = \\frac{1}{n} \\sum_{i=1}^{n} -2 (y_i - (\\alpha + \\beta x_i)) \\] \\[ \\frac{\\partial L(\\alpha, \\beta)}{\\partial \\beta} = \\frac{1}{n} \\sum_{i=1}^{n} -2x_i (y_i - (\\alpha + \\beta x_i)) \\]\nGreat! We’ve got everything we need to implement gradient descent to train an ordinary least squares model. Everything except data that is.\n\nToy Data\nLet’s make a friendly little linear dataset where \\(\\alpha=-10\\) and \\(\\beta=2\\), i.e.\n\\[ y = -10 + 2x + \\text{noise}\\]\n\nimport numpy as np \n\nalpha_true = -10\nbeta_true = 2\n\nrng = np.random.default_rng(42)\nx = np.linspace(0, 10, 50)\ny = alpha_true + beta_true*x + rng.normal(0, 1, size=x.shape)\n\n\n\n\n\n\n\n\nImplementation\nOur implementation will use a function to compute the gradient of the loss function. Since we have two parameters, we’ll use length-2 arrays to hold their values and their partial derivatives. At each iteration, we update the parameter values by subtracting the rescaled partial derivatives.\n\n\n# linear regression using gradient descent \n\ndef gradient_of_loss(parameters, x, y):\n    alpha = parameters[0]\n    beta = parameters[1]\n    partial_alpha = np.mean(-2*(y - (alpha + beta*x)))\n    partial_beta = np.mean(-2*x*(y - (alpha + beta*x)))\n    return np.array([partial_alpha, partial_beta])\n\nlearning_rate = 0.02\nparameters = np.array([0.0, 0.0]) # initial values of alpha and beta\n\nfor _ in range(500):\n    partial_derivatives = gradient_of_loss(parameters, x, y)\n    parameters -= learning_rate * partial_derivatives\n    \nparameters\n\narray([-10.07049616,   2.03559051])\n\n\nWe can see the loss function decreasing throughout the 500 iterations.\n\n\n\n\n\nAnd we can visualize the loss function as a contour plot over \\((\\alpha,\\beta)\\) space. The blue points show the trajectory our gradient descent followed as it shimmied from the initial position to the coolest spot in \\((\\alpha, \\beta)\\) space where the loss function is nice and small.\n\n\n\n\n\nOur gradient descent settles in a spot pretty close to \\((-10, 2)\\) in \\((\\alpha,\\beta)\\) space, which gives us the final fitted model below."
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#wrapping-up",
    "href": "posts/get-down-with-gradient-descent/index.html#wrapping-up",
    "title": "Get Down with Gradient Descent",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere you have it, gradient descent explained with a fresh new analogy having nothing whatsoever to do with foggy mountains, plus an implemented example fitting a linear model. While we often see gradient descent used to train models by performing an optimization in parameter space, as in generalized linear models and neural networks, there are other ways to use this powerful technique to train models. In particular, we’ll soon see how our beloved gradient boosting tree models use gradient descent in prediction space, rather than parameter space. Stay tuned for that mind bender in a future post."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html",
    "href": "posts/8020-pandas-tutorial/index.html",
    "title": "The 80/20 Pandas Tutorial",
    "section": "",
    "text": "Ahh, pandas. In addition to being everyone’s favorite mostly vegetarian bear from south central China, it’s also the python library for working with tabular data, a.k.a. dataframes. When you dive into pandas, you’ll quickly find out that there is a lot going on; indeed there are hundreds of methods for operating on dataframes. But luckily for us, as with many areas of life, there is a Pareto Principle, or 80/20 rule, that will help us focus on the small set of methods that collectively solve the majority of our data transformation needs.\nIf you’re like me, then pandas is not your first data-handling tool; maybe you’ve been using SQL or R with data.table or dplyr. If so, that’s great because you already have a sense for the key operations we need when working with tabular data. In their book, R for Data Science, Garrett Grolemund and Hadley Wickham describe five essential operations for manipulating dataframes. I’ve found that these cover the majority of my data transformation tasks to prepare data for analysis, visualization, and modeling.\nI would add that we also need a way to build up more complex transformations by chaining these fundamental operations together sequentially.\nBefore we dive in, here’s the TLDR on the pandas methods that I prefer for accomplishing these tasks, along with their equivalents from SQL and dplyr in R."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#imports-and-data",
    "href": "posts/8020-pandas-tutorial/index.html#imports-and-data",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Imports and Data",
    "text": "Imports and Data\nWe’ll use the nycflights13 dataset which contains data on the roughly 300k flights that departed from New York City in 2013.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'https://www.openintro.org/book/statdata/nycflights.csv'\nstorage_options = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0'}\ndf = pd.read_csv(url, storage_options=storage_options)\n\nTypically if I’m only going to be using a single dataframe, I’ll use the name “df”. This is a pretty strong convention in pandas, e.g. you can see the name “df” being used all over the pandas documentation; therefore it makes your code easier for others to understand. If there will be more than one dataframe, I suggest prepending a meaningful name to the “df”, e.g. flights_df.\nLet’s have a look at the dataframe structure using the info() method.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32735 entries, 0 to 32734\nData columns (total 16 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   year       32735 non-null  int64 \n 1   month      32735 non-null  int64 \n 2   day        32735 non-null  int64 \n 3   dep_time   32735 non-null  int64 \n 4   dep_delay  32735 non-null  int64 \n 5   arr_time   32735 non-null  int64 \n 6   arr_delay  32735 non-null  int64 \n 7   carrier    32735 non-null  object\n 8   tailnum    32735 non-null  object\n 9   flight     32735 non-null  int64 \n 10  origin     32735 non-null  object\n 11  dest       32735 non-null  object\n 12  air_time   32735 non-null  int64 \n 13  distance   32735 non-null  int64 \n 14  hour       32735 non-null  int64 \n 15  minute     32735 non-null  int64 \ndtypes: int64(12), object(4)\nmemory usage: 4.0+ MB"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#select-rows-based-on-their-values-with-query",
    "href": "posts/8020-pandas-tutorial/index.html#select-rows-based-on-their-values-with-query",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Select rows based on their values with query()",
    "text": "Select rows based on their values with query()\nquery() lets you retain a subset of rows based on the values of the data; it’s like dplyr::filter() in R or WHERE in SQL. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even check if the column value matches any items in a list.\n\n# compare one column to a value\ndf.query('month == 6')\n\n# compare two column values\ndf.query('arr_delay &gt; dep_delay')\n\n# using arithmetic\ndf.query('arr_delay &gt; 0.5 * air_time')\n\n# using \"and\"\ndf.query('month == 6 and day == 1')\n\n# using \"or\"\ndf.query('origin == \"JFK\" or dest == \"JFK\"')\n\n# column value matching any item in a list\ndf.query('carrier in [\"AA\", \"UA\"]')\n\nYou may have noticed that it seems to be much more popular to filter pandas data frames using boolean indexing. Indeed when I ask my favorite search engine how to filter a pandas dataframe on its values, I find this tutorial, this blog post, various questions on Stack Overflow, and even the pandas documentation, all espousing boolean indexing. Here’s what it looks like.\n\n# canonical boolean indexing\ndf[(df['carrier'] == \"AA\") & (df['origin'] == \"JFK\")]\n\n# the equivalent use of query()\ndf.query('carrier == \"AA\" and origin == \"JFK\"')\n\nThere are a few reasons I prefer query() over boolean indexing.\n\nquery() does not require me to type the dataframe name again, whereas boolean indexing requires me to type it every time I wish to refer to a column.\nquery() makes the code easier to read and understand, especially when expressions get complex.\nquery() is more computationally efficient than boolean indexing.\nquery() can safely be used in dot chains, which we’ll see very soon."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#select-columns-by-name-with-filter",
    "href": "posts/8020-pandas-tutorial/index.html#select-columns-by-name-with-filter",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Select columns by name with filter()",
    "text": "Select columns by name with filter()\nfilter() lets you pick out a specific set of columns by name; it’s analogous to dplyr::select() in R or SELECT in SQL. You can either provide exactly the column names you want, or you can grab all columns whose names contain a given substring or which match a given regular expression. This isn’t a big deal when your dataframe has only a few columns, but is particularly useful when you have a dataframe with tens or hundreds of columns.\n\n# select a list of columns\ndf.filter(['origin', 'dest'])\n\n# select columns containing a particular substring\ndf.filter(like='time')\n\n# select columns matching a regular expression\ndf.filter(regex='e$')"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#sort-rows-with-sort_values",
    "href": "posts/8020-pandas-tutorial/index.html#sort-rows-with-sort_values",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Sort rows with sort_values()",
    "text": "Sort rows with sort_values()\nsort_values() changes the order of the rows based on the data values; it’s likedplyr::arrange() in R or ORDER BY in SQL. You can specify one or more columns on which to sort, where their order denotes the sorting priority. You can also specify whether to sort in ascending or descending order.\n\n# sort by a single column\ndf.sort_values('air_time')\n\n# sort by a single column in descending order\ndf.sort_values('air_time', ascending=False)\n\n# sort by carrier, then within carrier, sort by descending distance\ndf.sort_values(['carrier', 'distance'], ascending=[True, False])"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#add-new-columns-with-assign",
    "href": "posts/8020-pandas-tutorial/index.html#add-new-columns-with-assign",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Add new columns with assign()",
    "text": "Add new columns with assign()\nassign() adds new columns which can be functions of the existing columns; it’s like dplyr::mutate() from R.\n\n# add a new column based on other columns\ndf.assign(speed = lambda x: x.distance / x.air_time)\n\n# another new column based on existing columns\ndf.assign(gain = lambda x: x.dep_delay - x.arr_delay)\n\nIf you’re like me, this way of using assign() might seem a little strange at first. Let’s break it down. In the call to assign() the keyword argument speed tells pandas the name of our new column. The business to the right of the = is a inline lambda function that takes the dataframe we passed to assign() and returns the column we want to add.\nI like using x as the lambda argument because its easy to type and it evokes tabular data (think design matrix), which reminds me that it refers to the entire dataframe. We can then access the other columns in our dataframe using the dot like x.other_column.\nIt’s true that you can skip the whole lambda business and refer to the dataframe to which you are assigning directly inside the assign. That might look like this.\ndf.assign(speed = flights.distance / flights.air_time)\nI prefer using a lambda for the following reasons.\n\nUsing the lambda will save you from typing the name every time you want to refer to a column.\nThe lambda makes your code more portable. Since you refer to the dataframe as a generic x, you can reuse this same assignment code on a dataframe with a different name.\nMost importantly, the lambda will allow you to harness the power of dot chaining."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#chain-transformations-together-with-the-dot-chain",
    "href": "posts/8020-pandas-tutorial/index.html#chain-transformations-together-with-the-dot-chain",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Chain transformations together with the dot chain",
    "text": "Chain transformations together with the dot chain\nOne of the awesome things about pandas is that the object.method() paradigm lets us easily build up complex dataframe transformations from a sequence of method calls. In R, this is effectively accomplished by the pipe %&gt;% operator. For example, suppose we want to look at high-speed flights from JFK to Honolulu, which would require us to query for JFK to Honolulu flights, assign a speed column, and maybe sort on that new speed column.\nWe can say:\n\n# neatly chain method calls together\n(\n    df\n    .query('origin == \"JFK\"')\n    .query('dest == \"HNL\"')\n    .assign(speed = lambda x: x.distance / x.air_time)\n    .sort_values(by='speed', ascending=False)\n    .query('speed &gt; 8.0')\n)\n\nWe compose the dot chain by wrapping the entire expression in parentheses and indenting each line within. The first line is the name of the dataframe on which we are operating. Each subsequent line has a single method call.\nThere are a few great things about writing the code this way:\n\nReadability - It’s easy to scan down the left margin of the code to see what’s happening. The first line gives us our noun (the dataframe) and each subsequent line starts with a verb. You could read this as “take df then query the rows where origin is JFK, then query for rows where destination is HNL, then assign a new column called speed, then sort the dataframe by speed, then query only for the rows where speed is greater than 8.0.\nFlexibility - It’s easy to comment out individual lines and re-run the cell. It’s also easy to reorder operations, since only one thing happens on each line.\nNeatness - We have not polluted our workspace with any intermediate variables, nor have we wasted any mental energy thinking of names for any temporary variables.\n\nBy default, dot chains do not modify the original dataframe; they just output a temporary result that we can inspect directly in the output. If you want to store the result, or pass it along to another function (e.g. for plotting), you can simply assign the entire dot chain to a variable.\n\n# store the output of the dot chain in a new dataframe\nhigh_speed_flights_df = (\n    df\n    .assign(speed = lambda x: x.distance / x.air_time)\n    .query('speed &gt; 8.0')\n)"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#collapsing-rows-into-grouped-summaries-with-groupby",
    "href": "posts/8020-pandas-tutorial/index.html#collapsing-rows-into-grouped-summaries-with-groupby",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Collapsing rows into grouped summaries with groupby()",
    "text": "Collapsing rows into grouped summaries with groupby()\ngroupby() combined with apply() gives us flexibility and control over our grouped summaries; it’s like dplyr::group_by() and dplyr::summarise() in R. This is the primary pattern I use for SQL-style groupby operations in pandas. Specifically it unlocks the following essential functionality you’re used to having in SQL.\n\nspecify the names of the aggregation columns we create\nspecify which aggregation function to use on which columns\ncompose more complex aggregations such as the proportion of rows meeting some condition\naggregate over arbitrary functions of multiple columns\n\nLet’s check out the departure delay stats for each carrier.\n\n# grouped summary with groupby and apply\n(\n    df\n    .groupby(['carrier'])\n    .apply(lambda d: pd.Series({\n        'n_flights': len(d),\n        'med_delay': d.dep_delay.median(),\n        'avg_delay': d.dep_delay.mean(),\n    }))\n    .head()\n)\n\n\n\n\n\n\n\n\nn_flights\nmed_delay\navg_delay\n\n\ncarrier\n\n\n\n\n\n\n\n9E\n1696.0\n-1.0\n17.285967\n\n\nAA\n3188.0\n-2.0\n9.142409\n\n\nAS\n66.0\n-4.5\n5.181818\n\n\nB6\n5376.0\n-1.0\n13.137091\n\n\nDL\n4751.0\n-2.0\n8.529573\n\n\n\n\n\n\n\nWhile you might be used to apply() acting over the rows or columns of a dataframe, here we’re calling apply on a grouped dataframe object, so it’s acting over the groups. According to the pandas documentation:\n\nThe function passed to apply must take a dataframe as its first argument and return a dataframe, a series or a scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method.\n\nWe need to supply apply() with a function that takes each chunk of the grouped dataframe and returns (in our case) a series object with one element for each new aggregation column. Notice that I use a lambda to specify the function we pass to apply()), and that I name its argument d, which reminds me that it’s a dataframe. My lambda returns a pandas series whose index entries specify the new aggregation column names, and whose values constitute the results of the aggregations for each group. Pandas will then stitch everything back together into a lovely dataframe.\nNotice how nice the code looks when we use this pattern. Each aggregation is specified on its own line, which makes it easy to see what aggregation columns we’re creating and allows us to comment, uncomment, and reorder the aggregations without breaking anything.\nHere are some more complex aggregations to illustrate some useful patterns.\n\n# more complex grouped summary\n(\n    df\n    .groupby(['carrier'])\n    .apply(lambda d: pd.Series({\n        'avg_gain': np.mean(d.dep_delay - d.arr_delay), \n        'pct_delay_gt_30': np.mean(d.dep_delay &gt; 30), \n        'pct_late_dep_early_arr': np.mean((d.dep_delay &gt; 0) & (d.arr_delay &lt; 0)), \n        'avg_arr_given_dep_delay_gt_0': d.query('dep_delay &gt; 0').arr_delay.mean(),\n        'cor_arr_delay_dep_delay': np.corrcoef(d.dep_delay, d.arr_delay)[0,1],\n    }))\n    .head()\n)\n\n\n\n\n\n\n\n\navg_gain\npct_delay_gt_30\npct_late_dep_early_arr\navg_arr_given_dep_delay_gt_0\ncor_arr_delay_dep_delay\n\n\ncarrier\n\n\n\n\n\n\n\n\n\n9E\n9.247642\n0.196934\n0.110259\n39.086111\n0.932485\n\n\nAA\n7.743726\n0.113237\n0.105395\n30.087165\n0.891013\n\n\nAS\n16.515152\n0.106061\n0.121212\n28.058824\n0.864565\n\n\nB6\n3.411458\n0.160528\n0.084449\n37.306866\n0.914180\n\n\nDL\n7.622816\n0.097874\n0.100821\n30.078029\n0.899327\n\n\n\n\n\n\n\nHere’s what’s happening.\n\nnp.mean(d.dep_delay - d.arr_delay) aggregates over the difference of two columns.\nnp.mean(d.dep_delay &gt; 30) computes the proportion of rows where the delay is greater than 30 minutes. Generating a boolean series based on some condition and then using mean() to find the proportion comes up all the time.\nnp.mean((d.dep_delay &gt; 0) & (d.arr_delay &lt; 0)) shows that we can compute proportions where conditions on multiple columns are met.\nd.query('dep_delay &gt; 0').arr_delay.mean() computes the average arrival delay on flights where the departure was delayed. Here we first filter each grouped dataframe down to the subset of rows where departure delay is greater than zero using query(), and then we take the mean of the remaining arrival delays.\nnp.corrcoef(d.dep_delay, d.arr_delay)[0,1] computes the correlation coefficient between departure and arrival delays. Remember we can use pretty much any reduction operation to collapse values down to a scalar.\n\nYou might have noticed that the canonical pandas approach for grouped summaries is to use agg(). That works well if you need to apply the same aggregation function on each column in the dataframe, e.g. taking the mean of every column. But because of the kind of data I work with these days, it’s much more common for me to use customized aggregations like those above, so the groupby() apply() idiom works best for me."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#wrapping-up",
    "href": "posts/8020-pandas-tutorial/index.html#wrapping-up",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere you have it, how to pull off the five most essential data transformation tasks using pandas in a style reminiscent of my beloved dplyr. Remember that part of the beauty of pandas is that since there are so many ways to do most tasks, you can develop your own style based on the kind of data you work with, what you like about other tools, how you see others using the tools, and of course your own taste and preferences.\nIf you found this post helpful or if you have your own preferred style for accomplishing any of these key transformations with pandas, do let me know about it in the comments."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "",
    "text": "Ahh, the dark art of hyperparameter tuning. It’s a key step in the machine learning workflow, and it’s an activity that can easily be overlooked or be overkill. Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils. Today I’ll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework. I’ll give you some intuition for how to think about the key parameters in XGBoost, and I’ll show you an efficient strategy for parameter tuning GBTs. I’ll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like. You can download a notebook with this tuning workflow from my data science templates repository. Finally we’ll wrap up with the kind of cautionary tale data scientists tell their colleagues around the campfire about when all this fancy hyperparameter tuning can backfire catastrophically—ignore at your own peril."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#xgboost-parameters",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#xgboost-parameters",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "XGBoost Parameters",
    "text": "XGBoost Parameters\nGradient boosting algorithms like XGBoost have two main types of hyperparameters: tree parameters which control the decision tree trained at each boosting round and boosting parameters which control the boosting procedure itself. Below I’ll highlight my favorite parameters, but you can see the full list in the documentation.\n\nTree Parameters\nIn theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, decision trees are typically the best choice. In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.\n\nTree construction algorithm\nThe tree construction algorithm boils down to split finding, and different algorithms have different ways of generating candidate splits to consider. In XGBoost we have the parameter:\n\ntree_method - select tree construction algorithm: exact, hist, approx, or the horrifying default—auto—which outsources your choice of tree construction algo to XGBoost and which you should never ever use. I’ve been burned by this hidden tree_method=auto default multiple times before learning my lesson. Why is the online model worse than the offline model? Why is this model suddenly taking so much longer to train? Avoid these debugging nightmares and set tree_method explicitly; the exact method tends to be slow and ironically less accurate, so I use either approx or hist.\n\n\n\nTree complexity parameters\nTree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be. I use these two parameters:\n\nmax_depth - maximum number of split levels allowed. Reasonable values are usually from 3-12.\nmin_child_weight - minimum allowable sum of hessian values over data in a node. When using the default squared error objective, this is the minimum number of samples allowed in a leaf node (see this explanation of why that’s true). For a squared error objective, values in [1, 200] usually work well.\n\nThese two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing min child weight makes trees less expressive and therefore is a powerful way to counter overfitting. Note that gamma (a.k.a. min_split_loss) also limits node splitting, but I usually don’t use it because min_child_weight seems to work well enough on its own.\n\n\nSampling parameters\nXGBoost can randomly sample rows and columns to be used for training each tree; you might think of this as bagging. We have a few parameters:\n\nsubsample - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.\ncolsample_bytree, colsample_bylevel, colsample_bynode - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split. Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.\n\n\n\nRegularization parameters\nIn XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero. I usually use:\n\nreg_lambda - L2 regularization of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. Valid values are in [0,\\(\\infty\\)), but good values typically fall in [0,10].\n\nThere is also an L1 regularization parameter called reg_alpha; feel free to use it instead. It seems that using one or the other is usually sufficient.\n\n\n\nBoosting Parameters and Early Stopping\nTrained gradient boosting models take the form:\n\\[ F(\\mathbf{x}) = b + \\eta \\sum_{k=1}^{K} f_k(\\mathbf{x}) \\]\nwhere \\(b\\) is the constant base predicted value, \\(f_k(\\cdot)\\) is the base learner for round \\(k\\), parameter \\(K\\) is the number of boosting rounds, and parameter \\(\\eta\\) is the learning rate. In XGBoost these parameters correspond with:\n\nnum_boost_round (\\(K\\)) - the number of boosting iterations\nlearning_rate (\\(\\eta\\)) - the scaling or “shrinkage” factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. Fun fact: the \\(\\eta\\) character is called “eta”, and learning_rate is aliased to eta in xgboost, so you can use parameter eta instead of learning_rate if you like.\n\nThese two parameters are very closely linked; the optimal value of one depends on the value of the other. To illustrate their relationship, we can train two different XGBoost models on the same training dataset, where one model has a lower learning rate than the other.\n\n\nCode\nimport xgboost as xgb \nfrom sklearn.datasets import make_regression \nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split \n\nX, y = make_regression(5000, random_state=0)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)\n\neta1, eta2 = 0.3, 0.15\nreg1 = xgb.XGBRegressor(n_estimators=1000, learning_rate=eta1, early_stopping_rounds=25)\nreg1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], \n        verbose=0);\nreg2 = xgb.XGBRegressor(n_estimators=1000, learning_rate=eta2, early_stopping_rounds=25)\nreg2.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], \n        verbose=0);\n\nfig, ax = plt.subplots()\n\nbest_round1, best_round2 = reg1.best_iteration, reg2.best_iteration\nobj1 = reg1.evals_result()['validation_1']['rmse']\nobj2 = reg2.evals_result()['validation_1']['rmse']\nbest_obj1 = obj1[best_round1]\nbest_obj2 = obj2[best_round1]\n\nplt.plot(reg1.evals_result()['validation_1']['rmse'], '-b', label=f'learning_rate={eta1}')\nplt.plot(reg2.evals_result()['validation_1']['rmse'], '-r', label=f'learning_rate={eta2}')\nax.annotate(f'learning_rate={eta1}\\nbest_iteration={best_round1}', \n            xy=(best_round1, best_obj1), \n            xytext=(best_round1, best_obj1+20),\n            horizontalalignment='right',\n            arrowprops=dict(facecolor='black', shrink=0.05))\nax.annotate(f'learning_rate={eta2}\\nbest_iteration={best_round2}', \n            xy=(best_round2, best_obj2), \n            xytext=(best_round2, best_obj2+20),\n            horizontalalignment='right',\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.legend()\nplt.ylabel('RMSE') \nplt.xlabel('boosting round') \nplt.title('Validation Scores by Boosting Round');\n\n\n\n\n\nHold-out validation score (RMSE) by boosting round for two XGBoost models differing only by learning rate.\n\n\n\n\nThe above figure shows root mean squared error measured on a held-out validation dataset for two different XGBoost models: one with a higher learning rate and one with a lower learning rate. The figure demonstrates two key properties of the boosting parameters:\n\nWhile training a model with a given learning rate, the evaluation score (computed on a hold-out set) tends to improve with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.\nAll else constant, a smaller learning rate leads to a model with more boosting rounds and better evaluation score.\n\nWe can leverage the first property to make our tuning more efficient by using XGBoost’s early_stopping_rounds: int argument, which terminates training after observing the specified number of boosting rounds without sufficient improvement to the evaluation metric. The models above were trained using early_stopping_rounds=50, which terminates training after 50 boosting rounds without improvement in RMSE on the validation data. For each model, the arrow indicates the boosting round with the best score.\nThe figure also exemplifies the second property, where the model with lower learning rate attains a better validation score but requires more boosting rounds to trigger early stopping. Note that smaller and smaller learning rates will provide diminishing improvements to the validation score."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#an-efficient-parameter-search-strategy-for-xgboost",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#an-efficient-parameter-search-strategy-for-xgboost",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "An Efficient Parameter Search Strategy for XGBoost",
    "text": "An Efficient Parameter Search Strategy for XGBoost\nEfficiency is the key to effective parameter tuning, because wasting less time means searching more parameter values and finding better models in a given amount of time. But as we just saw, there is a tradeoff between accuracy and training time via the learning rate. Given infinite time and compute resources, we would just choose an arbitrarily tiny learning rate and search through tree parameter values while using early stopping to choose the number of boosting rounds. The problem is that tiny learning rates require tons of boosting rounds, which will make our ideal search prohibitively slow when confronted with the reality of finite time and resources. So what can we do?\nMy approach is based on the claim that good tree parameters at one learning rate are also good tree parameters at other learning rates. The intuition is that given two models—one with good tree parameters and one with bad tree parameters—the model with good tree parameters will score better, regardless of the learning rate. Thus, tree parameters are “independent” of boosting parameters—See this notebook for justification of this claim.\nIndependence between tree parameters and boosting parameters suggests a two-stage procedure where we first find optimal tree parameters, then we maximize performance by pushing boosting parameters to the extreme. The procedure is:\n\nTune tree parameters. Fix the learning rate at a relatively high value (like 0.3ish) and enable early stopping so that each model trains within a few seconds. Use your favorite hyperparameter tuning technique to find the optimal tree parameters.\nTune boosting parameters. Using these optimal tree parameter values, fix the learning rate as low as you want and train your model, using early stopping to identify the optimal number of boosting rounds.\n\nWhy is this a good idea? Because by starting with a high learning rate and early stopping enabled, you can burn through hundreds of model training trials and find some really good tree parameters in a few minutes. Then, with the confidence that your tree parameters are actually quite good, you can set a really low learning rate and boost a few thousand rounds to get a model with the best of both tree parameter and boosting parameter worlds.\n\nYou can check out this notebook where I justify this approach by running two parameter searches—one with high learning rate and one with low learning rate—showing that they recover the same optimal tree parameters."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#tuning-xgboost-parameters-with-optuna",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#tuning-xgboost-parameters-with-optuna",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Tuning XGBoost Parameters with Optuna",
    "text": "Tuning XGBoost Parameters with Optuna\nOptuna is a model-agnostic python library for hyperparameter tuning. I like it because it has a flexible API that abstracts away the details of the search algorithm being used. That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more. Another massive benefit is that optuna provides a specific XGBoost integration which terminates training early on lousy parameter combinations.\nYou can install optuna with anaconda, e.g.\n$ conda install -c conda-forge optuna"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#example-tuning-the-bluebook-for-bulldozers-regression-model",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#example-tuning-the-bluebook-for-bulldozers-regression-model",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Example: Tuning the Bluebook for Bulldozers Regression Model",
    "text": "Example: Tuning the Bluebook for Bulldozers Regression Model\nTo illustrate the procedure, we’ll tune the parameters for the regression model we built back in the XGBoost for regression post. First we’ll load up the bulldozer data and prepare the features and target just like we did before.\n\n\nCode\nimport time \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport optuna \n\ndf = pd.read_csv('../xgboost-for-regression-in-python/Train.csv', parse_dates=['saledate']);\n\ndef encode_string_features(df):\n    out_df = df.copy()\n    for feature, feature_type in df.dtypes.items():\n        if feature_type == 'object':\n            out_df[feature] = out_df[feature].astype('category')\n    return out_df\n\ndf = encode_string_features(df)\n\ndf['saledate_days_since_epoch'] = (\n    df['saledate'] - pd.Timestamp(year=1970, month=1, day=1)\n    ).dt.days\n\ndf['logSalePrice'] = np.log1p(df['SalePrice'])\n\n\nfeatures = [\n    'SalesID',\n    'MachineID',\n    'ModelID',\n    'datasource',\n    'auctioneerID',\n    'YearMade',\n    'MachineHoursCurrentMeter',\n    'UsageBand',\n    'fiModelDesc',\n    'fiBaseModel',\n    'fiSecondaryDesc',\n    'fiModelSeries',\n    'fiModelDescriptor',\n    'ProductSize',\n    'fiProductClassDesc',\n    'state',\n    'ProductGroup',\n    'ProductGroupDesc',\n    'Drive_System',\n    'Enclosure',\n    'Forks',\n    'Pad_Type',\n    'Ride_Control',\n    'Stick',\n    'Transmission',\n    'Turbocharged',\n    'Blade_Extension',\n    'Blade_Width',\n    'Enclosure_Type',\n    'Engine_Horsepower',\n    'Hydraulics',\n    'Pushblock',\n    'Ripper',\n    'Scarifier',\n    'Tip_Control',\n    'Tire_Size',\n    'Coupler',\n    'Coupler_System',\n    'Grouser_Tracks',\n    'Hydraulics_Flow',\n    'Track_Type',\n    'Undercarriage_Pad_Width',\n    'Stick_Length',\n    'Thumb',\n    'Pattern_Changer',\n    'Grouser_Type',\n    'Backhoe_Mounting',\n    'Blade_Type',\n    'Travel_Controls',\n    'Differential_Type',\n    'Steering_Controls',\n    'saledate_days_since_epoch'\n ]\n\ntarget = 'logSalePrice'\n\n\nBut this time, since we’re going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes. We make four different xgboost.DMatrix datasets for this process: training, validation, training+validation, and test. Training and validation are for the parameter search, and training+validation and test are for the final model.\n\nn_valid = 12000\nn_test = 12000\n\nsorted_df = df.sort_values(by='saledate')\ntrain_df = sorted_df[:-(n_valid + n_test)] \nvalid_df = sorted_df[-(n_valid + n_test):-n_test] \ntest_df = sorted_df[-n_test:]\n\ndtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], \n                     enable_categorical=True)\ndvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], \n                     enable_categorical=True)\ndtest = xgb.DMatrix(data=test_df[features], label=test_df[target], \n                    enable_categorical=True)\ndtrainvalid = xgb.DMatrix(data=pd.concat([train_df, valid_df])[features], \n                          label=pd.concat([train_df, valid_df])[target], \n                          enable_categorical=True)"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#preliminaries-base-parameters-and-scoring-function",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#preliminaries-base-parameters-and-scoring-function",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Preliminaries: base parameters and scoring function",
    "text": "Preliminaries: base parameters and scoring function\nWe’ll go ahead and set a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping. We’ll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE.\n\nmetric = 'rmse'\nbase_params = {\n    'objective': 'reg:squarederror',\n    'eval_metric': metric,\n}\n\n\ndef score_model(model: xgb.core.Booster, dmat: xgb.core.DMatrix) -&gt; float:\n    y_true = dmat.get_label() \n    y_pred = model.predict(dmat) \n    return mean_squared_error(y_true, y_pred, squared=False)"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-1-tune-tree-parameters-with-optuna",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-1-tune-tree-parameters-with-optuna",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Stage 1: Tune Tree Parameters with Optuna",
    "text": "Stage 1: Tune Tree Parameters with Optuna\nNext we need to choose a fixed learning rate and tune the tree parameters. We want a learning rate that allows us to train within a few seconds, so we need to time model training. Start with a high learning rate (like 0.8) and work down until you find a rate that takes a few seconds. Below I end up landing at 0.3, which takes about 4 seconds to train on my little laptop.\n\nlearning_rate = 0.3\n\nparams = {\n    'tree_method': 'approx',\n    'learning_rate': learning_rate\n}\nparams.update(base_params)\ntic = time.time()\nmodel = xgb.train(params=params, dtrain=dtrain,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  num_boost_round=10000,\n                  early_stopping_rounds=50,\n                  verbose_eval=0)\nprint(f'{time.time() - tic:.1f} seconds')\n\n4.5 seconds\n\n\nThen we implement our optuna objective, a function taking an optuna study Trial object and returning the score we want to optimize. We use the suggest_categorical, suggest_float, and suggest_int methods of the Trial object to define the search space for each parameter. Note the use of the pruning callback function which we pass into the callback argument of the XGBoost train function; this is a must, since it allows optuna to terminate training on lousy models after a few boosting rounds. After training a model with the selected parameter values, we stash the optimal number of boosting rounds from early stopping into an optuna user attribute using the trial.user_attrs() method. Finally we return the score computed by our model_score function.\n\ndef objective(trial):\n    params = {\n        'tree_method': trial.suggest_categorical('tree_method', ['approx', 'hist']),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 250),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.1, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log=True),\n        'learning_rate': learning_rate,\n    }\n    num_boost_round = 10000\n    params.update(base_params)\n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, f'valid-{metric}')\n    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                      evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                      early_stopping_rounds=50,\n                      verbose_eval=0,\n                      callbacks=[pruning_callback])\n    trial.set_user_attr('best_iteration', model.best_iteration)\n    return model.best_score\n\nTo create a new optuna study and search through 50 parameter combinations, you could just run these two lines.\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nBut, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials—who knows how long 50 trials will take. I also want the results to be reproducible. So, to set the random seed and run the optimization for around 300 seconds (long enough to go make a nice cup of tea, stretch, and come back), I do something like this:\n\nsampler = optuna.samplers.TPESampler(seed=42)\nstudy = optuna.create_study(direction='minimize', sampler=sampler)\ntic = time.time()\nwhile time.time() - tic &lt; 300:\n    study.optimize(objective, n_trials=1)\n\n\n\nCode\nprint('Stage 1 ==============================')\nprint(f'best score = {study.best_trial.value}')\nprint('boosting params ---------------------------')\nprint(f'fixed learning rate: {learning_rate}')\nprint(f'best boosting round: {study.best_trial.user_attrs[\"best_iteration\"]}')\nprint('best tree params --------------------------')\nfor k, v in study.best_trial.params.items():\n    print(k, ':', v)\n\n\nStage 1 ==============================\nbest score = 0.23107522766919256\nboosting params ---------------------------\nfixed learning rate: 0.3\nbest boosting round: 23\nbest tree params --------------------------\ntree_method : approx\nmax_depth : 10\nmin_child_weight : 6\nsubsample : 0.9729188669457949\ncolsample_bynode : 0.8491983767203796\nreg_lambda : 0.008587261143813469\n\n\nIf we decide we want to tune the tree parameters a little more, we can just call study.optimize(...) again, adding as many trials as we want to the study. Once we’re happy with the tree parameters, we can proceed to stage 2."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-2-intensify-the-boosting-parameters",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-2-intensify-the-boosting-parameters",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Stage 2: Intensify the Boosting Parameters",
    "text": "Stage 2: Intensify the Boosting Parameters\nNow we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate; here I use 0.01, but you could go lower. The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you’ll need to max out the evaluation metric on the validation data.\n\nlow_learning_rate = 0.01\n\nparams = {}\nparams.update(base_params)\nparams.update(study.best_trial.params)\nparams['learning_rate'] = low_learning_rate\nmodel_stage2 = xgb.train(params=params, dtrain=dtrain, \n                         num_boost_round=10000,\n                         evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                         early_stopping_rounds=50,\n                         verbose_eval=0)\n\n\n\nCode\nprint('Stage 2 ==============================')\nprint(f'best score = {score_model(model_stage2, dvalid)}')\nprint('boosting params ---------------------------')\nprint(f'fixed learning rate: {params[\"learning_rate\"]}')\nprint(f'best boosting round: {model_stage2.best_iteration}')\n\n\nStage 2 ==============================\nbest score = 0.22172991931438446\nboosting params ---------------------------\nfixed learning rate: 0.01\nbest boosting round: 1446"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#train-and-evaluate-the-final-model",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#train-and-evaluate-the-final-model",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Train and Evaluate the Final Model",
    "text": "Train and Evaluate the Final Model\nNow we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2. Then we evaluate on the held out test data.\n\nmodel_final = xgb.train(params=params, dtrain=dtrainvalid, \n                        num_boost_round=model_stage2.best_iteration,\n                        verbose_eval=0)\n\n\n\nCode\nprint('Final Model ==========================')\nprint(f'test score = {score_model(model_final, dtest)}')\nprint('parameters ---------------------------')\nfor k, v in params.items():\n    print(k, ':', v)\nprint(f'num_boost_round: {model_stage2.best_iteration}')\n\n\nFinal Model ==========================\ntest score = 0.21621863543987274\nparameters ---------------------------\nobjective : reg:squarederror\neval_metric : rmse\ntree_method : approx\nmax_depth : 10\nmin_child_weight : 6\nsubsample : 0.9729188669457949\ncolsample_bynode : 0.8491983767203796\nreg_lambda : 0.008587261143813469\nlearning_rate : 0.01\nnum_boost_round: 1446\n\n\nBack in the regression post we got an RMSE of about 0.231 just using default parameter values, which put us in 5th place on the leaderboard for the Kagle dozers competition. Now with about 10 minutes of hyperparameter tuning, our RMSE is down to 0.216 which puts us in 1st place by a huge margin. 🙌"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#what-could-possibly-go-wrong",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#what-could-possibly-go-wrong",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "What could possibly go wrong?",
    "text": "What could possibly go wrong?\nHyperparameter tuning can easily be overlooked in the move-fast-and-break-everything hustle of building an ML product, but it can also easily become overkill or even downright harmful, depending on the application. There are three key questions to ask:\n\nHow much value is created by an incremental gain in model prediction accuracy?\nWhat is the cost of increasing model prediction accuracy?\nIs my model answering the right question?\n\nSometimes a small gain in model prediction performance translates into millions of dollars of impact. The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org’s KPIs, and get mad respect, bonuses, and promoted. But the reality is that often additional model accuracy doesn’t really change business KPIs by very much. Try to figure out the actual value of improved model accuracy and proceed accordingly.\nRemember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself. It can also lead us to larger and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.\nWorst of all and quite counterintuitively, it’s possible that improving a model’s prediction accuracy can compromise overall business KPIs. I’ve seen this with my own eyes at work; offline testing shows that hyperparameter tuning significantly improves a model’s prediction accuracy, but when the model goes into production, an AB test shows that the business KPIs are actually worse. What happened? In this case, the model’s prediction was being used indirectly to infer the relationship between one of the features and the prediction target to inform automatic business decisions. Answering questions about how changing an input will affect an output requires causal reasoning, and traditional ML models are not the right tool for the job. I’ll have a lot more to say about that soon; let this story foreshadow an epic new epoch on Random Realizations…."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#wrapping-up",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#wrapping-up",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna. If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!"
  },
  {
    "objectID": "posts/hello-pyspark/index.html",
    "href": "posts/hello-pyspark/index.html",
    "title": "Hello PySpark!",
    "section": "",
    "text": "A big day at Playa Guiones\nWell, you guessed it: it’s time for us to learn PySpark!\nI know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes?\nThat’s a totally fair question.\nSo what happens when we’re working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory? We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details.\nEnter PySpark.\nI think it’s fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it’s like pandas but scalable. It’s built on top of Apache Spark, a unified analytics engine for large-scale data processing. PySpark is essentially a way to access the functionality of spark via python code. While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice. PySpark also has great integration with SQL, and it has a companion machine learning library called MLlib that’s more or less a scalable scikit-learn (maybe we can cover it in a future post).\nSo, here’s the plan. First we’re going to get set up to run PySpark locally in a jupyter notebook on our laptop. This is my preferred environment for interactively playing with PySpark and learning the ropes. Then we’re going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas. Once we’re comfortable running PySpark on the laptop, it’s going to be much easier to jump onto a distributed cluster and run PySpark at scale.\nLet’s do this."
  },
  {
    "objectID": "posts/hello-pyspark/index.html#how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop",
    "href": "posts/hello-pyspark/index.html#how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop",
    "title": "Hello PySpark!",
    "section": "How to Run PySpark in a Jupyter Notebook on Your Laptop",
    "text": "How to Run PySpark in a Jupyter Notebook on Your Laptop\nOk, I’m going to walk us through how to get things installed on a Mac or Linux machine where we’re using homebrew and conda to manage virtual environments. If you have a different setup, your favorite search engine will help you get PySpark set up locally.\n\n\n\n\n\n\nNote\n\n\n\nIt’s possible for Homebrew and Anaconda to interfere with one another. The simple rule of thumb is that whenever you want to use the brew command, first deactivate your conda environment by running conda deactivate. See this Stack Overflow question for more details.\n\n\n\nInstall Spark\nInstall Spark with homebrew.\nbrew install apache-spark\nNext we need to set up a SPARK_HOME environment variable in the shell. Check where Spark is installed.\nbrew info apache-spark\nYou should see something like\n==&gt; apache-spark: stable 3.3.2 (bottled), HEAD\nEngine for large-scale data processing\nhttps://spark.apache.org/\n/opt/homebrew/Cellar/apache-spark/3.3.2 (1,453 files, 320.9MB) *\n...\nSet the SPARK_HOME environment variable to your spark installation path with /libexec appended to the end. To do this I added the following line to my .zshrc file.\nexport SPARK_HOME=/opt/homebrew/Cellar/apache-spark/3.3.2/libexec\nRestart your shell, and test the installation by starting the Spark shell.\nspark-shell\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n      /_/\n         \nUsing Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 19.0.2)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt; \nIf you get the scala&gt; prompt, then you’ve successfully installed Spark on your laptop!\n\n\nInstall PySpark\nUse conda to install the PySpark python package. As usual, it’s advisable to do this in a new virtual environment.\n$ conda install pyspark\nYou should be able to launch an interactive PySpark REPL by saying pyspark.\n$ pyspark\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n      /_/\n\nUsing Python version 3.8.3 (default, Jul  2 2020 11:26:31)\nSpark context Web UI available at http://192.168.100.47:4041\nSpark context available as 'sc' (master = local[*], app id = local-1624127229929).\nSparkSession available as 'spark'.\n&gt;&gt;&gt; \nThis time we get a familiar python &gt;&gt;&gt; prompt. This is an interactive shell where we can easily experiment with PySpark. Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we’ll get set up to run PySpark in a jupyter notebook.\n\n\n\n\n\n\nNote\n\n\n\nWhen I tried following this setup on a new Mac, I hit an error about being unable to find the Java Runtime. This stack overflow question lead me to the fix.\n\n\n\n\nThe Spark Session Object\nYou may have noticed that when we launched that PySpark interactive shell, it told us that something called SparkSession was available as 'spark'. So basically, what’s happening here is that when we launch the pyspark shell, it instantiates an object called spark which is an instance of class pyspark.sql.session.SparkSession. The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we’re going to be saying things like spark.this() and spark.that() to make stuff happen.\nThe PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically. However, when we’re using another interface to PySpark (like say a jupyter notebook running a python kernal), we’ll have to make a spark session object for ourselves.\n\n\nCreate a PySpark Session in a Jupyter Notebook\nThere are a few ways to run PySpark in jupyter which you can read about here.\nFor derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a jupyter notebook running on a regular python kernel. The method we’ll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session. So, first install the findspark package.\nconda install -c conda-forge findspark\nLaunch jupyter as usual.\njupyter notebook\nGo ahead and fire up a new notebook using a regular python 3 kernal. Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated. You can think of this as boilerplate code that we need to run in the first cell of a notebook where we’re going to use PySpark.\n\nimport pyspark\nimport findspark\nfrom pyspark.sql import SparkSession\n\nfindspark.init()\nspark = SparkSession.builder.appName('My Spark App').getOrCreate()\n\nFirst we’re running findspark’s init() method to find our Spark installation. If you run into errors here, make sure you got the SPARK_HOME environment variable correctly set in the install instructions above. Then we instantiate a spark session as spark. Once you run this, you’re ready to rock and roll with PySpark in your jupyter notebook.\n\n\n\n\n\n\nNote\n\n\n\nSpark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at http://localhost:4040/jobs/."
  },
  {
    "objectID": "posts/hello-pyspark/index.html#pyspark-concepts",
    "href": "posts/hello-pyspark/index.html#pyspark-concepts",
    "title": "Hello PySpark!",
    "section": "PySpark Concepts",
    "text": "PySpark Concepts\nPySpark provides two main abstractions for data: the RDD and the dataframe. RDD’s are just a distributed list of objects; we won’t go into details about them in this post. For us, the key object in PySpark is the dataframe.\nWhile PySpark dataframes expose much of the functionality you would expect from a library for tabular data manipulation, they behave a little differently from pandas dataframes, both syntactically and under-the-hood. There are a couple of key concepts that will help explain these idiosyncracies.\nImmutability - Pyspark RDD’s and dataframes are immutable. This means that if you change an object, e.g. by adding a column to a dataframe, PySpark returns a reference to a new dataframe; it does not modify the existing dataframe. This is kind of nice, because we don’t have to worry about that whole view versus copy nonsense that happens in pandas.\nLazy Evaluation - Lazy evaluation means that when we start manipulating a dataframe, PySpark won’t actually perform any of the computations until we explicitly ask for the result. This is nice because it potentially allows PySpark to do fancy optimizations before executing a sequence of operations. It’s also confusing at first, because PySpark will seem to blaze through complex operations and then take forever to print a few rows of the dataframe."
  },
  {
    "objectID": "posts/hello-pyspark/index.html#pyspark-dataframe-essentials",
    "href": "posts/hello-pyspark/index.html#pyspark-dataframe-essentials",
    "title": "Hello PySpark!",
    "section": "PySpark Dataframe Essentials",
    "text": "PySpark Dataframe Essentials\n\nCreating a PySpark dataframe with createDataFrame()\nThe first thing we’ll need is a way to make dataframes. createDataFrame() allows us to create PySpark dataframes from python objects like nested lists or pandas dataframes. Notice that createDataFrame() is a method of the spark session class, so we’ll call it from our spark session sparkby saying spark.createDataFrame().\n\n# create pyspark dataframe from nested  lists\nmy_df = spark.createDataFrame(\n    data=[\n        [2022, \"tiger\"],\n        [2023, \"rabbit\"],\n        [2024, \"dragon\"]\n    ],\n    schema=['year', 'animal']\n)\n\nLet’s read the seaborn tips dataset into a pandas dataframe and then use it to create a PySpark dataframe.\n\nimport pandas as pd\n\n# load tips dataset into a pandas dataframe\npandas_df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\n\n# create pyspark dataframe from a pandas dataframe\npyspark_df = spark.createDataFrame(pandas_df)\n\n\n\n\n\n\n\nNote\n\n\n\nIn real life when we’re running PySpark on a large-scale distributed system, we would not generally want to use python lists or pandas dataframes to load data into PySpark. Ideally we would want to read data directly from where it is stored on HDFS, e.g. by reading parquet files, or by querying directly from a hive database using spark sql.\n\n\n\n\nPeeking at a dataframe’s contents\nThe default print method for the PySpark dataframe will just give you the schema.\n\npyspark_df\n\nDataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]\n\n\nIf we want to peek at some of the data, we’ll need to use the show() method, which is analogous to the pandas head(). Remember that show() will cause PySpark to execute any operations that it’s been lazily waiting to evaluate, so sometimes it can take a while to run.\n\n# show the first few rows of the dataframe\npyspark_df.show(5)\n\n+----------+----+------+------+---+------+----+\n|total_bill| tip|   sex|smoker|day|  time|size|\n+----------+----+------+------+---+------+----+\n|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n+----------+----+------+------+---+------+----+\nonly showing top 5 rows\n\n\n\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]\n\n                                                                                \n\n\nWe thus encounter our first rude awakening. PySpark’s default representation of dataframes in the notebook isn’t as pretty as that of pandas. But no one ever said it would be pretty, they just said it would be scalable.\nYou can also use the printSchema() method for a nice vertical representation of the schema.\n\n# show the dataframe schema\npyspark_df.printSchema()\n\nroot\n |-- total_bill: double (nullable = true)\n |-- tip: double (nullable = true)\n |-- sex: string (nullable = true)\n |-- smoker: string (nullable = true)\n |-- day: string (nullable = true)\n |-- time: string (nullable = true)\n |-- size: long (nullable = true)\n\n\n\n\n\nSelect columns by name\nYou can select specific columns from a dataframe using the select() method. You can pass either a list of names, or pass names as arguments.\n\n# select some of the columns\npyspark_df.select('total_bill', 'tip')\n\n# select columns in a list\npyspark_df.select(['day', 'time', 'total_bill'])\n\n\n\nFilter rows based on column values\nAnalogous to the WHERE clause in SQL, and the query() method in pandas, PySpark provides a filter() method which returns only the rows that meet the specified conditions. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even do a SQL-like in to check if the column value matches any items in a list.\n\n## compare a column to a value\npyspark_df.filter('total_bill &gt; 20')\n\n# compare two columns with arithmetic\npyspark_df.filter('tip &gt; 0.15 * total_bill')\n\n# check equality with a string value\npyspark_df.filter('sex == \"Male\"')\n\n# check equality with any of several possible values\npyspark_df.filter('day in (\"Sat\", \"Sun\")')\n\n# use \"and\" \npyspark_df.filter('day == \"Fri\" and time == \"Lunch\"')\n\nIf you’re into boolean indexing with the brackets, PySpark does support that too, but I encourage you to use filter() instead. Check out my rant about why you shouldn’t use boolean indexing for the details. The TLDR is that filter() requires less typing, makes your code more readable and portable, and it allows you to chain method calls together using dot chains.\nHere’s the boolean indexing equivalent of the last example from above.\n\n# using boolean indexing\npyspark_df[(pyspark_df.day == 'Fri') & (pyspark_df.time == 'Lunch')]\n\nI know, it looks horrendous, but not as horrendous as the error message you’ll get if you forget the parentheses.\n\n\nAdd new columns to a dataframe\nYou can add new columns which are functions of the existing columns with the withColumn() method.\n\nimport pyspark.sql.functions as f\n\n# add a new column using col() to reference other columns\npyspark_df.withColumn('tip_percent', f.col('tip') / f.col('total_bill'))\n\nNotice that we’ve imported the pyspark.sql.functions module. This module contains lots of useful functions that we’ll be using all over the place, so it’s probably a good idea to go ahead and import it whenever you’re using PySpark. BTW, it seems like folks usually import this module as f or F. In this example we’re using the col() function, which allows us to refer to columns in our dataframe using string representations of the column names.\nYou could also achieve the same result using the dot to reference the other columns, but this requires us to type the dataframe name over and over again, which makes it harder to reuse this code on different dataframes or in dot chains.\n\n# add a new column using the dot to reference other columns (less recommended)\npyspark_df.withColumn('tip_percent', pyspark_df.tip / pyspark_df.total_bill)\n\nIf you want to apply numerical transformations like exponents or logs, use the built-in functions in the pyspark.sql.functions module.\n\n# log \npyspark_df.withColumn('log_bill', f.log(f.col('total_bill')))\n\n# exponent\npyspark_df.withColumn('bill_squared', f.pow(f.col('total_bill'), 2))\n\nYou can implement conditional assignment like SQL’s CASE WHEN construct using the when() function and the otherwise() method.\n\n# conditional assignment (like CASE WHEN)\npyspark_df.withColumn('is_male', f.when(f.col('sex') == 'Male', True).otherwise(False))\n\n# using multiple when conditions and values\npyspark_df.withColumn('bill_size', \n    f.when(f.col('total_bill') &lt; 10, 'small')\n    .when(f.col('total_bill') &lt; 20, 'medium')\n    .otherwise('large')\n)\n\nRemember that since PySpark dataframes are immutable, calling withColumns() on a dataframe returns a new dataframe. If you want to persist the result, you’ll need to make an assignment.\npyspark_df = pyspark_df.withColumns(...)\n\n\nGroup by and aggregate\nPySpark provides a groupBy() method similar to the pandas groupby(). Just like in pandas, we can call methods like count() and mean() on our grouped dataframe, and we also have a more flexible agg() method that allows us to specify column-aggregation mappings.\n\n\n# group by and count\npyspark_df.groupBy('time').count().show()\n\n+------+-----+\n|  time|count|\n+------+-----+\n|Dinner|  176|\n| Lunch|   68|\n+------+-----+\n\n\n\n\n\n# group by and specify column-aggregation mappings with agg()\npyspark_df.groupBy('time').agg({'total_bill': 'mean', 'tip': 'max'}).show()\n\n+------+--------+------------------+\n|  time|max(tip)|   avg(total_bill)|\n+------+--------+------------------+\n|Dinner|    10.0| 20.79715909090909|\n| Lunch|     6.7|17.168676470588235|\n+------+--------+------------------+\n\n\n\nIf you want to get fancier with your aggregations, it might just be easier to express them using hive syntax. Read on to find out how.\n\n\nRun Hive SQL on dataframes\nOne of the mind-blowing features of PySpark is that it allows you to write hive SQL queries on your dataframes. To take a PySpark dataframe into the SQL world, use the createOrReplaceTempView() method. This method takes one string argument which will be the dataframes name in the SQL world. Then you can use spark.sql() to run a query. The result is returned as a PySpark dataframe.\n\n\n# put pyspark dataframe in SQL world and query it\npyspark_df.createOrReplaceTempView('tips')\nspark.sql('select * from tips').show(5)\n\n+----------+----+------+------+---+------+----+\n|total_bill| tip|   sex|smoker|day|  time|size|\n+----------+----+------+------+---+------+----+\n|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n+----------+----+------+------+---+------+----+\nonly showing top 5 rows\n\n\n\nThis is awesome for a couple of reasons. First, it allows us to easily express any transformations in hive syntax. If you’re like me and you’ve already been using hive, this will dramatically reduce the PySpark learning curve, because when in doubt, you can always bump a dataframe into the SQL world and simply use hive to do what you need. Second, if you have a hive deployment, PySpark’s SQL world also has access to all of your hive tables. This means you can write queries involving both hive tables and your PySpark dataframes. It also means you can run hive commands, like inserting into a table, directly from PySpark.\nLet’s do some aggregations that might be a little trickier to do using the PySpark built-in functions.\n\n\n# run hive query and save result to dataframe\ntip_stats_by_time = spark.sql(\"\"\"\n    select\n        time\n        , count(*) as n \n        , avg(tip) as avg_tip\n        , percentile_approx(tip, 0.5) as med_tip\n        , avg(case when tip &gt; 3 then 1 else 0 end) as pct_tip_gt_3\n    from \n        tips\n    group by 1\n\"\"\")\n\ntip_stats_by_time.show()\n\n+------+---+------------------+-------+-------------------+\n|  time|  n|           avg_tip|med_tip|       pct_tip_gt_3|\n+------+---+------------------+-------+-------------------+\n|Dinner|176| 3.102670454545455|    3.0|0.44886363636363635|\n| Lunch| 68|2.7280882352941176|    2.2|0.27941176470588236|\n+------+---+------------------+-------+-------------------+"
  },
  {
    "objectID": "posts/hello-pyspark/index.html#visualization-with-pyspark",
    "href": "posts/hello-pyspark/index.html#visualization-with-pyspark",
    "title": "Hello PySpark!",
    "section": "Visualization with PySpark",
    "text": "Visualization with PySpark\nThere aren’t any tools for visualization included in PySpark. But that’s no problem, because we can just use the toPandas() method on a PySpark dataframe to pull data back into pandas. Once we have a pandas dataframe, we can happily build visualizations as usual. Of course, if your PySpark dataframe is huge, you wouldn’t want to use toPandas() directly, because PySpark will attempt to read the entire contents of its huge dataframe into memory. Instead, it’s best to use PySpark to generate aggregations of your data for plotting or to pull only a sample of your full data into pandas.\n\n# read aggregated pyspark dataframe into pandas for plotting\nplot_pdf = tip_stats_by_time.toPandas()\nplot_pdf.plot.bar(x='time', y=['avg_tip', 'med_tip']);"
  },
  {
    "objectID": "posts/hello-pyspark/index.html#wrapping-up",
    "href": "posts/hello-pyspark/index.html#wrapping-up",
    "title": "Hello PySpark!",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nSo that’s a wrap on our crash course in working with PySpark. You now have a good idea of what pyspark is and how to get started manipulating dataframes with it. Stay tuned for a future post on PySpark’s companion ML library MLlib. In the meantime, may no dataframe be too large for you ever again."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html",
    "href": "posts/xgboost-from-scratch/index.html",
    "title": "XGBoost from Scratch",
    "section": "",
    "text": "A weathered tree reaches toward the sea at Playa Mal País\nWell, dear reader, it’s that time again, time for us to do a seemingly unnecessary scratch build of a popular algorithm that most people would simply import from the library without a second thought. But readers of this blog are not most people. Of course you know that when we do scratch builds, it’s not for the hell of it, it’s for the purpose of demystification. To that end, today we are going to implement XGBoost from scratch in python, using only numpy and pandas.\nSpecifically we’re going to implement the core statistical learning algorithm of XGBoost, including most of the key hyperparameters and their functionality. Our implementation will also support user-defined custom objective functions, meaning that it can perform regression, classification, and whatever exotic learning tasks you can dream up, as long as you can write down a twice-differentiable objective function. We’ll refrain from implementing some simple features like column subsampling which will be left to you, gentle reader, as exercises. In terms of tree methods, we’re going to implement the exact tree-splitting algorithm, leaving the sparsity-aware method (used to handle missing feature values) and the approximate method (used for scalability) as exercises or maybe topics for future posts.\nAs always, if something is unclear, try backtracking through the previous posts on gradient boosting and decision trees to clarify your intuition. We’ve already built up all the statistical and computational background needed to make sense of this scratch build. Here are the most important prerequisite posts:\nGreat, let’s do this."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#the-xgboost-model-class",
    "href": "posts/xgboost-from-scratch/index.html#the-xgboost-model-class",
    "title": "XGBoost from Scratch",
    "section": "The XGBoost Model Class",
    "text": "The XGBoost Model Class\nWe begin with the user-facing API for our model, a class called XGBoostModel which will implement gradient boosting and prediction. To be more consistent with the XGBoost library, we’ll pass hyperparameters to our model in a parameter dictionary, so our init method is going to pull relevant parameters out of the dictionary and set them as object attributes. Note the use of python’s defaultdict so we don’t have to worry about handling key errors if we try to access a parameter that the user didn’t set in the dictionary.\n\nimport math\nimport numpy as np \nimport pandas as pd\nfrom collections import defaultdict\n\n\nclass XGBoostModel():\n    '''XGBoost from Scratch\n    '''\n    \n    def __init__(self, params, random_seed=None):\n        self.params = defaultdict(lambda: None, params)\n        self.subsample = self.params['subsample'] \\\n            if self.params['subsample'] else 1.0\n        self.learning_rate = self.params['learning_rate'] \\\n            if self.params['learning_rate'] else 0.3\n        self.base_prediction = self.params['base_score'] \\\n            if self.params['base_score'] else 0.5\n        self.max_depth = self.params['max_depth'] \\\n            if self.params['max_depth'] else 5\n        self.rng = np.random.default_rng(seed=random_seed)\n\nThe fit method, based on our classic GBM, takes a feature dataframe, a target vector, the objective function, and the number of boosting rounds as arguments. The user-supplied objective function should be an object with loss, gradient, and hessian methods, each of which takes a target vector and a prediction vector as input; the loss method should return a scalar loss score, the gradient method should return a vector of gradients, and the hessian method should return a vector of hessians.\nIn contrast to boosting in the classic GBM, instead of computing residuals between the current predictions and the target, we compute gradients and hessians of the loss function with respect to the current predictions, and instead of predicting residuals with a decision tree, we fit a special XGBoost tree booster (which we’ll implement in a moment) using the gradients and hessians. I’ve also added row subsampling by drawing a random subset of instance indices and passing them to the tree booster during each boosting round. The rest of the fit method is the same as the classic GBM, and the predict method is identical too.\n\ndef fit(self, X, y, objective, num_boost_round, verbose=False):\n    current_predictions = self.base_prediction * np.ones(shape=y.shape)\n    self.boosters = []\n    for i in range(num_boost_round):\n        gradients = objective.gradient(y, current_predictions)\n        hessians = objective.hessian(y, current_predictions)\n        sample_idxs = None if self.subsample == 1.0 \\\n            else self.rng.choice(len(y), \n                                 size=math.floor(self.subsample*len(y)), \n                                 replace=False)\n        booster = TreeBooster(X, gradients, hessians, \n                              self.params, self.max_depth, sample_idxs)\n        current_predictions += self.learning_rate * booster.predict(X)\n        self.boosters.append(booster)\n        if verbose: \n            print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n            \ndef predict(self, X):\n    return (self.base_prediction + self.learning_rate \n            * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n\nXGBoostModel.fit = fit\nXGBoostModel.predict = predict            \n\nAll we have to do now is implement the tree booster."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#the-xgboost-tree-booster",
    "href": "posts/xgboost-from-scratch/index.html#the-xgboost-tree-booster",
    "title": "XGBoost from Scratch",
    "section": "The XGBoost Tree Booster",
    "text": "The XGBoost Tree Booster\nThe XGBoost tree booster is a modified version of the decision tree that we built in the decision tree from scratch post. Like the decision tree, we recursively build a binary tree structure by finding the best split rule for each node in the tree. The main difference is the criterion for evaluating splits and the way that we define a leaf’s predicted value. Instead of being functions of the target values of the instances in each node, the criterion and predicted values are functions of the instance gradients and hessians. Thus we need only make a couple of modifications to our previous decision tree implementation to create the XGBoost tree booster.\n\nInitialization and Inserting Child Nodes\nMost of the init method is just parsing the parameter dictionary to assign parameters as object attributes. The one notable difference from our decision tree is in the way we define the node’s predicted value. We define self.value according to equation 5 of the XGBoost paper, a simple function of the gradient and hessian values of the instances in the current node. Of course the init also goes on to build the tree via the maybe insert child nodes method. This method is nearly identical to the one we implemented for our decision tree. So far so good.\n\nclass TreeBooster():\n \n    def __init__(self, X, g, h, params, max_depth, idxs=None):\n        self.params = params\n        self.max_depth = max_depth\n        assert self.max_depth &gt;= 0, 'max_depth must be nonnegative'\n        self.min_child_weight = params['min_child_weight'] \\\n            if params['min_child_weight'] else 1.0\n        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0\n        self.gamma = params['gamma'] if params['gamma'] else 0.0\n        self.colsample_bynode = params['colsample_bynode'] \\\n            if params['colsample_bynode'] else 1.0\n        if isinstance(g, pd.Series): g = g.values\n        if isinstance(h, pd.Series): h = h.values\n        if idxs is None: idxs = np.arange(len(g))\n        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n        self.best_score_so_far = 0.\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n\n    def _maybe_insert_child_nodes(self):\n        for i in range(self.c): self._find_better_split(i)\n        if self.is_leaf: return\n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = TreeBooster(self.X, self.g, self.h, self.params, \n                                self.max_depth - 1, self.idxs[left_idx])\n        self.right = TreeBooster(self.X, self.g, self.h, self.params, \n                                 self.max_depth - 1, self.idxs[right_idx])\n\n    @property\n    def is_leaf(self): return self.best_score_so_far == 0.\n\n    def _find_better_split(self, feature_idx):\n        pass\n\n\n\nSplit Finding\nSplit finding follows the exact same pattern that we used in the decision tree, except we keep track of gradient and hessian stats instead of target value stats, and of course we use the XGBoost gain criterion (equation 7 from the paper) for evaluating splits.\n\ndef _find_better_split(self, feature_idx):\n    x = self.X.values[self.idxs, feature_idx]\n    g, h = self.g[self.idxs], self.h[self.idxs]\n    sort_idx = np.argsort(x)\n    sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n    sum_g, sum_h = g.sum(), h.sum()\n    sum_g_right, sum_h_right = sum_g, sum_h\n    sum_g_left, sum_h_left = 0., 0.\n\n    for i in range(0, self.n - 1):\n        g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n        sum_g_left += g_i; sum_g_right -= g_i\n        sum_h_left += h_i; sum_h_right -= h_i\n        if sum_h_left &lt; self.min_child_weight or x_i == x_i_next:continue\n        if sum_h_right &lt; self.min_child_weight: break\n\n        gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n                        + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n                        - (sum_g**2 / (sum_h + self.reg_lambda))\n                        ) - self.gamma/2 # Eq(7) in the xgboost paper\n        if gain &gt; self.best_score_so_far: \n            self.split_feature_idx = feature_idx\n            self.best_score_so_far = gain\n            self.threshold = (x_i + x_i_next) / 2\n            \nTreeBooster._find_better_split = _find_better_split\n\n\n\nPrediction\nPrediction works exactly the same as in our decision tree, and the methods are nearly identical.\n\ndef predict(self, X):\n    return np.array([self._predict_row(row) for i, row in X.iterrows()])\n\ndef _predict_row(self, row):\n    if self.is_leaf: \n        return self.value\n    child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n        else self.right\n    return child._predict_row(row)\n\nTreeBooster.predict = predict \nTreeBooster._predict_row = _predict_row"
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#the-complete-xgboost-from-scratch-implementation",
    "href": "posts/xgboost-from-scratch/index.html#the-complete-xgboost-from-scratch-implementation",
    "title": "XGBoost from Scratch",
    "section": "The Complete XGBoost From Scratch Implementation",
    "text": "The Complete XGBoost From Scratch Implementation\nHere’s the entire implementation which produces a usable XGBoostModel class with fit and predict methods.\n\nclass XGBoostModel():\n    '''XGBoost from Scratch\n    '''\n    \n    def __init__(self, params, random_seed=None):\n        self.params = defaultdict(lambda: None, params)\n        self.subsample = self.params['subsample'] \\\n            if self.params['subsample'] else 1.0\n        self.learning_rate = self.params['learning_rate'] \\\n            if self.params['learning_rate'] else 0.3\n        self.base_prediction = self.params['base_score'] \\\n            if self.params['base_score'] else 0.5\n        self.max_depth = self.params['max_depth'] \\\n            if self.params['max_depth'] else 5\n        self.rng = np.random.default_rng(seed=random_seed)\n                \n    def fit(self, X, y, objective, num_boost_round, verbose=False):\n        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n        self.boosters = []\n        for i in range(num_boost_round):\n            gradients = objective.gradient(y, current_predictions)\n            hessians = objective.hessian(y, current_predictions)\n            sample_idxs = None if self.subsample == 1.0 \\\n                else self.rng.choice(len(y), \n                                     size=math.floor(self.subsample*len(y)), \n                                     replace=False)\n            booster = TreeBooster(X, gradients, hessians, \n                                  self.params, self.max_depth, sample_idxs)\n            current_predictions += self.learning_rate * booster.predict(X)\n            self.boosters.append(booster)\n            if verbose: \n                print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n            \n    def predict(self, X):\n        return (self.base_prediction + self.learning_rate \n                * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n    \nclass TreeBooster():\n \n    def __init__(self, X, g, h, params, max_depth, idxs=None):\n        self.params = params\n        self.max_depth = max_depth\n        assert self.max_depth &gt;= 0, 'max_depth must be nonnegative'\n        self.min_child_weight = params['min_child_weight'] \\\n            if params['min_child_weight'] else 1.0\n        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0\n        self.gamma = params['gamma'] if params['gamma'] else 0.0\n        self.colsample_bynode = params['colsample_bynode'] \\\n            if params['colsample_bynode'] else 1.0\n        if isinstance(g, pd.Series): g = g.values\n        if isinstance(h, pd.Series): h = h.values\n        if idxs is None: idxs = np.arange(len(g))\n        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n        self.best_score_so_far = 0.\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n\n    def _maybe_insert_child_nodes(self):\n        for i in range(self.c): self._find_better_split(i)\n        if self.is_leaf: return\n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = TreeBooster(self.X, self.g, self.h, self.params, \n                                self.max_depth - 1, self.idxs[left_idx])\n        self.right = TreeBooster(self.X, self.g, self.h, self.params, \n                                 self.max_depth - 1, self.idxs[right_idx])\n\n    @property\n    def is_leaf(self): return self.best_score_so_far == 0.\n    \n    def _find_better_split(self, feature_idx):\n        x = self.X.values[self.idxs, feature_idx]\n        g, h = self.g[self.idxs], self.h[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n        sum_g, sum_h = g.sum(), h.sum()\n        sum_g_right, sum_h_right = sum_g, sum_h\n        sum_g_left, sum_h_left = 0., 0.\n\n        for i in range(0, self.n - 1):\n            g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n            sum_g_left += g_i; sum_g_right -= g_i\n            sum_h_left += h_i; sum_h_right -= h_i\n            if sum_h_left &lt; self.min_child_weight or x_i == x_i_next:continue\n            if sum_h_right &lt; self.min_child_weight: break\n\n            gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n                            + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n                            - (sum_g**2 / (sum_h + self.reg_lambda))\n                            ) - self.gamma/2 # Eq(7) in the xgboost paper\n            if gain &gt; self.best_score_so_far: \n                self.split_feature_idx = feature_idx\n                self.best_score_so_far = gain\n                self.threshold = (x_i + x_i_next) / 2\n                \n    def predict(self, X):\n        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n\n    def _predict_row(self, row):\n        if self.is_leaf: \n            return self.value\n        child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n            else self.right\n        return child._predict_row(row)"
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#testing",
    "href": "posts/xgboost-from-scratch/index.html#testing",
    "title": "XGBoost from Scratch",
    "section": "Testing",
    "text": "Testing\nLet’s take this baby for a spin and benchmark its performance against the actual XGBoost library. We use the scikit learn California housing dataset for benchmarking.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n    \nX, y = fetch_california_housing(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    random_state=43)\n\nLet’s start with a nice friendly squared error objective function for training. We should probably have a future post all about how to define custom objective functions in XGBoost, but for now, here’s how I define squared error.\n\nclass SquaredErrorObjective():\n    def loss(self, y, pred): return np.mean((y - pred)**2)\n    def gradient(self, y, pred): return pred - y\n    def hessian(self, y, pred): return np.ones(len(y))\n\nHere I use a more or less arbitrary set of hyperparameters for training. Feel free to play around with tuning and trying other parameter combinations yourself.\n\nimport xgboost as xgb\n\nparams = {\n    'learning_rate': 0.1,\n    'max_depth': 5,\n    'subsample': 0.8,\n    'reg_lambda': 1.5,\n    'gamma': 0.0,\n    'min_child_weight': 25,\n    'base_score': 0.0,\n    'tree_method': 'exact',\n}\nnum_boost_round = 50\n\n# train the from-scratch XGBoost model\nmodel_scratch = XGBoostModel(params, random_seed=42)\nmodel_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)\n\n# train the library XGBoost model\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\nmodel_xgb = xgb.train(params, dtrain, num_boost_round)\n\nLet’s check the models’ performance on the held out test data to benchmark our implementation.\n\npred_scratch = model_scratch.predict(X_test)\npred_xgb = model_xgb.predict(dtest)\nprint(f'scratch score: {SquaredErrorObjective().loss(y_test, pred_scratch)}')\nprint(f'xgboost score: {SquaredErrorObjective().loss(y_test, pred_xgb)}')\n\nscratch score: 0.2434125759558149\nxgboost score: 0.24123239765807963\n\n\nWell, look at that! Our scratch-built SGBoost is looking pretty consistent with the library. Go us!"
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#wrapping-up",
    "href": "posts/xgboost-from-scratch/index.html#wrapping-up",
    "title": "XGBoost from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nI’d say this is a pretty good milestone for us here at Random Realizations. We’ve been hammering away at the various concepts around gradient boosting, leaving a trail of equations and scratch-built algos in our wake. Today we put all of that together to create a legit scratch build of XGBoost, something that would have been out of reach for me before we embarked on this journey together over a year ago. To anyone with the patience to read through this stuff, cheers to you! I hope you’re learning and enjoying this as much as I am."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#reader-exercises",
    "href": "posts/xgboost-from-scratch/index.html#reader-exercises",
    "title": "XGBoost from Scratch",
    "section": "Reader Exercises",
    "text": "Reader Exercises\nIf you want to take this a step further and deepen your understanding and coding abilities, let me recommend some exercises for you.\n\nImplement column subsampling. XGBoost itself provides column subsampling by tree, by level, and by node. Try implementing by tree first, then try adding by level or by node as well. These should be pretty straightforward to do.\nImplement sparsity aware split finding for missing feature values (Algorithm 2 in the XGBoost paper). This will be a little more involved, since you’ll need to refactor and modify several parts of the tree booster class."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html",
    "href": "posts/xgboost-for-classification-in-python/index.html",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "",
    "text": "Today we continue the saga on gradient boosting with a down-to-Earth tutorial on the essentials of solving classification problems with XGBoost. We’ll run through two examples: one for binary classification and another for multi-class classification. In both cases I’ll show you how to train XGBoost models using either the scikit-learn interface or the native xgboost training API. Once trained, we’ll evaluate the models with validation data then inspect them with feature importance and partial dependence plots. You can use the XGBoost classification notebook in my ds-templates repository to follow along with your own dataset."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#preparing-data-for-xgboost-classifier",
    "href": "posts/xgboost-for-classification-in-python/index.html#preparing-data-for-xgboost-classifier",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Preparing Data for XGBoost Classifier",
    "text": "Preparing Data for XGBoost Classifier\nOur dataset must satisfy two requirements to be used in an XGBoost classifier. First all feature data must be numeric—no strings and no datetimes; if you have non-numeric features, you need to transform your feature data. Second, the target must be integer encoded using \\(\\{0,1\\}\\) for binary targets and \\(\\{0,1,\\dots,K\\}\\) for multiclass targets. Note that if your data is encoded to positive integers (no 0 class) XGBoost will throw potentially cryptic errors. You can use the scikit-learn LabelEncoder (which we’ll do below) to generate a valid target encoding."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#xgboost-training-apis",
    "href": "posts/xgboost-for-classification-in-python/index.html#xgboost-training-apis",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "XGBoost Training APIs",
    "text": "XGBoost Training APIs\nThe xgboost python library offers two API’s for training classification models: the native train function and a wrapper class called XGBClassifier, which offers an API consistent with the scikit-learn universe. I’ll show you how to use both approaches in the examples below, but if you’re planning to use other utilities from scikit-learn, you might find the XGBClassifier approach to be more convenient, since the trained model object will generally play nice with sklearn functionality."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#binary-classification-example",
    "href": "posts/xgboost-for-classification-in-python/index.html#binary-classification-example",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Binary Classification Example",
    "text": "Binary Classification Example\n\nBreast Cancer Wisconsin Dataset\nWe’ll demonstrate binary classification in XGBoost using the breast cancer wisconsin data, one of scikit-learn’s built-in toy datasets. This is a tiny dataset with 569 observations of 30 features and a binary target representing whether samples are malignant or benign..\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom sklearn import datasets\nimport xgboost as xgb \n\ndbunch = datasets.load_breast_cancer(as_frame=True)\ndf = dbunch.frame\nfeatures = dbunch.feature_names \ntarget_names = dbunch.target_names \ntarget = 'target' \ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   mean radius              569 non-null    float64\n 1   mean texture             569 non-null    float64\n 2   mean perimeter           569 non-null    float64\n 3   mean area                569 non-null    float64\n 4   mean smoothness          569 non-null    float64\n 5   mean compactness         569 non-null    float64\n 6   mean concavity           569 non-null    float64\n 7   mean concave points      569 non-null    float64\n 8   mean symmetry            569 non-null    float64\n 9   mean fractal dimension   569 non-null    float64\n 10  radius error             569 non-null    float64\n 11  texture error            569 non-null    float64\n 12  perimeter error          569 non-null    float64\n 13  area error               569 non-null    float64\n 14  smoothness error         569 non-null    float64\n 15  compactness error        569 non-null    float64\n 16  concavity error          569 non-null    float64\n 17  concave points error     569 non-null    float64\n 18  symmetry error           569 non-null    float64\n 19  fractal dimension error  569 non-null    float64\n 20  worst radius             569 non-null    float64\n 21  worst texture            569 non-null    float64\n 22  worst perimeter          569 non-null    float64\n 23  worst area               569 non-null    float64\n 24  worst smoothness         569 non-null    float64\n 25  worst compactness        569 non-null    float64\n 26  worst concavity          569 non-null    float64\n 27  worst concave points     569 non-null    float64\n 28  worst symmetry           569 non-null    float64\n 29  worst fractal dimension  569 non-null    float64\n 30  target                   569 non-null    int64  \ndtypes: float64(30), int64(1)\nmemory usage: 137.9 KB\n\n\nIn this dataset, the features are all numeric, so no need to do preprocessing before passing to XGBoost. Below we’ll have a look at the target to ensure it’s encoded in \\(\\{0,1\\}\\) and to check the class balance.\n\nprint(df[target].unique())\nprint(target_names)\n\n[0 1]\n['malignant' 'benign']\n\n\n\ndf.target.value_counts().sort_index().plot.bar()\nplt.xlabel('target') \nplt.ylabel('count');\n\n\n\n\nclass counts for the breast cancer dataset\n\n\n\n\nNext We randomly split data into train and validation sets.\n\nfrom sklearn.model_selection import train_test_split\n\nn_valid = 50 \n\ntrain_df, valid_df = train_test_split(df, test_size=n_valid, random_state=42)\ntrain_df.shape, valid_df.shape\n\n((519, 31), (50, 31))\n\n\n\n\nTraining with the train function\nWe need to set a couple of model parameters, most notably objective, which should be set to binary:logistic for binary classification. I also prefer to explicitly set tree_method to something other than its default of auto; usually I’ll start with exact on small datasets or approx on larger ones. Note also that The train function expects to receive data as DMatrix objects, not pandas dataframes, so we need to create dense matrix objects as well.\n\nparams = {\n    'tree_method': 'exact',\n    'objective': 'binary:logistic',\n}\nnum_boost_round = 50\n\ndtrain = xgb.DMatrix(label=train_df[target], data=train_df[features])\ndvalid = xgb.DMatrix(label=valid_df[target], data=valid_df[features])\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=10)\n\n[0] train-logloss:0.46232   valid-logloss:0.49033\n[10]    train-logloss:0.04394   valid-logloss:0.13434\n[20]    train-logloss:0.01515   valid-logloss:0.12193\n[30]    train-logloss:0.00995   valid-logloss:0.11988\n[40]    train-logloss:0.00766   valid-logloss:0.12416\n[49]    train-logloss:0.00657   valid-logloss:0.12799\n\n\n\n\nTraining with XGBClassifier\nThe XGBClassifier takes dataframes or numpy arrays as input, so this time we don’t need to create those dense matrix objects.\n\nparams = {\n    'tree_method': 'exact',\n    'objective': 'binary:logistic',\n}\nnum_boost_round = 50\n\nclf = xgb.XGBClassifier(n_estimators=num_boost_round, **params)\nclf.fit(train_df[features], train_df[target], \n        eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n        verbose=10);\n\n[0] validation_0-logloss:0.46232    validation_1-logloss:0.49033\n[10]    validation_0-logloss:0.04394    validation_1-logloss:0.13434\n[20]    validation_0-logloss:0.01515    validation_1-logloss:0.12193\n[30]    validation_0-logloss:0.00995    validation_1-logloss:0.11988\n[40]    validation_0-logloss:0.00766    validation_1-logloss:0.12416\n[49]    validation_0-logloss:0.00657    validation_1-logloss:0.12799\n\n\n\n\nEvaluating the Model\nWe’ll use the sklearn.metrics module to evaluate model performance on the held-out validation set. Have a look at the scikit-learn metrics for classification for examples of other metrics to use.\nOne thing to watch out for when computing metrics is the difference between the actual labels (usually called y_true), the model’s predicted labels (usually called y_pred), and the models predicted probabilities (usually called y_score). If you’re using the XGBClassifier wrapper, you can get predicted labels with the predict method and predicted probabilities with the predict_proba method. Also note that whereas predict returns a vector of size (num data), predict_proba returns a vector of size (num data, num classes); thus for binary classification, we’ll take just the second column of the array which gives the probability of class 1.\n\ny_true = valid_df[target]\ny_pred = clf.predict(valid_df[features])\ny_score = clf.predict_proba(valid_df[features])[:,1]\n\nProbably the simplest classification metric is accuracy, the proportion of labels we predicted correctly.\n\nfrom sklearn import metrics \n\nmetrics.accuracy_score(y_true, y_pred)\n\n0.96\n\n\nWe can generate a classification report with several different metrics at once.\n\nprint(metrics.classification_report(y_true, y_pred, target_names=target_names))\n\n              precision    recall  f1-score   support\n\n   malignant       0.93      0.93      0.93        15\n      benign       0.97      0.97      0.97        35\n\n    accuracy                           0.96        50\n   macro avg       0.95      0.95      0.95        50\nweighted avg       0.96      0.96      0.96        50\n\n\n\nAnd we can compute the AUC, a popular classification metric based on the ROC curve, which depends on the predicted probability rather than the predicted labels.\n\nmetrics.roc_auc_score(y_true, y_score)\n\n0.9885714285714287\n\n\n\n\nFeature Importance\nBecause of the limitations of the built-in XGBoost feature importance metrics I recommend that you use either permutation feature importance or perhaps SHAP feature importance.\nHere we’ll compute the permutation feature importance, which tells us by how much the model’s performance changes when we scramble a particular feature’s values at prediction time. This reflects how much the model relies on each feature when making predictions.\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import make_scorer\n\nscorer = make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)\npermu_imp = permutation_importance(clf, valid_df[features], valid_df[target], \n                                   n_repeats=30, random_state=0, scoring=scorer)\n\n\nimportances_permutation = pd.Series(permu_imp['importances_mean'], index=features)\nimportances_permutation.sort_values(ascending=True)[-10:].plot.barh()\nplt.title('Permutation Importance on Out-of-Sample Set')\nplt.xlabel('change in log likelihood');\n\n\n\n\ntop 10 features by permutation importance on validation set"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#partial-dependence",
    "href": "posts/xgboost-for-classification-in-python/index.html#partial-dependence",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Partial Dependence",
    "text": "Partial Dependence\nA partial dependence plot (PDP) is a representation of the dependence between the model output and one or more feature variables. In binary classification, the model output is the probability of the so-called positive class, i.e. the class with encoded label 1, which corresponds to probability of “benign” in this example.. We can loosely interpret the partial dependence as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say “loosely” because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(clf, \n                                        valid_df[features], \n                                        ['worst area', 'area error', 'mean area']);\n\n\n\n\nPDP of target probability of benign vs three features"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#multi-class-classification-example",
    "href": "posts/xgboost-for-classification-in-python/index.html#multi-class-classification-example",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Multi-Class Classification Example",
    "text": "Multi-Class Classification Example\n\nForest Cover Type Dataset\nWe’ll illustrate multi-class classification using the scikit-learn forest cover type dataset, which has around 580k observations of 54 features and a target with 7 classes.\n\ndbunch = datasets.fetch_covtype(as_frame=True)\ndf = dbunch.frame\nfeatures = dbunch.feature_names \ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 581012 entries, 0 to 581011\nData columns (total 55 columns):\n #   Column                              Non-Null Count   Dtype  \n---  ------                              --------------   -----  \n 0   Elevation                           581012 non-null  float64\n 1   Aspect                              581012 non-null  float64\n 2   Slope                               581012 non-null  float64\n 3   Horizontal_Distance_To_Hydrology    581012 non-null  float64\n 4   Vertical_Distance_To_Hydrology      581012 non-null  float64\n 5   Horizontal_Distance_To_Roadways     581012 non-null  float64\n 6   Hillshade_9am                       581012 non-null  float64\n 7   Hillshade_Noon                      581012 non-null  float64\n 8   Hillshade_3pm                       581012 non-null  float64\n 9   Horizontal_Distance_To_Fire_Points  581012 non-null  float64\n 10  Wilderness_Area_0                   581012 non-null  float64\n 11  Wilderness_Area_1                   581012 non-null  float64\n 12  Wilderness_Area_2                   581012 non-null  float64\n 13  Wilderness_Area_3                   581012 non-null  float64\n 14  Soil_Type_0                         581012 non-null  float64\n 15  Soil_Type_1                         581012 non-null  float64\n 16  Soil_Type_2                         581012 non-null  float64\n 17  Soil_Type_3                         581012 non-null  float64\n 18  Soil_Type_4                         581012 non-null  float64\n 19  Soil_Type_5                         581012 non-null  float64\n 20  Soil_Type_6                         581012 non-null  float64\n 21  Soil_Type_7                         581012 non-null  float64\n 22  Soil_Type_8                         581012 non-null  float64\n 23  Soil_Type_9                         581012 non-null  float64\n 24  Soil_Type_10                        581012 non-null  float64\n 25  Soil_Type_11                        581012 non-null  float64\n 26  Soil_Type_12                        581012 non-null  float64\n 27  Soil_Type_13                        581012 non-null  float64\n 28  Soil_Type_14                        581012 non-null  float64\n 29  Soil_Type_15                        581012 non-null  float64\n 30  Soil_Type_16                        581012 non-null  float64\n 31  Soil_Type_17                        581012 non-null  float64\n 32  Soil_Type_18                        581012 non-null  float64\n 33  Soil_Type_19                        581012 non-null  float64\n 34  Soil_Type_20                        581012 non-null  float64\n 35  Soil_Type_21                        581012 non-null  float64\n 36  Soil_Type_22                        581012 non-null  float64\n 37  Soil_Type_23                        581012 non-null  float64\n 38  Soil_Type_24                        581012 non-null  float64\n 39  Soil_Type_25                        581012 non-null  float64\n 40  Soil_Type_26                        581012 non-null  float64\n 41  Soil_Type_27                        581012 non-null  float64\n 42  Soil_Type_28                        581012 non-null  float64\n 43  Soil_Type_29                        581012 non-null  float64\n 44  Soil_Type_30                        581012 non-null  float64\n 45  Soil_Type_31                        581012 non-null  float64\n 46  Soil_Type_32                        581012 non-null  float64\n 47  Soil_Type_33                        581012 non-null  float64\n 48  Soil_Type_34                        581012 non-null  float64\n 49  Soil_Type_35                        581012 non-null  float64\n 50  Soil_Type_36                        581012 non-null  float64\n 51  Soil_Type_37                        581012 non-null  float64\n 52  Soil_Type_38                        581012 non-null  float64\n 53  Soil_Type_39                        581012 non-null  float64\n 54  Cover_Type                          581012 non-null  int32  \ndtypes: float64(54), int32(1)\nmemory usage: 241.6 MB\n\n\nHere again the features are all numeric, so we don’t need to further preprocess them. Let’s have a look at the target.\n\ndf['Cover_Type'].value_counts().sort_index().plot.bar()\nplt.xlabel('cover type') \nplt.ylabel('count');\n\n\n\n\nclass counts for the forest cover type dataset\n\n\n\n\nFor multi-class classification, our target variable must take values in \\(\\{0,1,\\dots,K\\}\\). However, from the histogram of the cover type above, we see that it takes values in \\(\\{1,2,\\dots,7\\}\\). To fix this we can use the scikit-learn label encoder to create a valid target column.\n\nfrom sklearn.preprocessing import LabelEncoder \n\ntarget = 'encoded'\nenc = LabelEncoder()\ndf[target] = enc.fit_transform(df['Cover_Type'])\nprint(np.sort(df[target].unique()))\n\n[0 1 2 3 4 5 6]\n\n\nThen we can create training and validation sets.\n\nn_valid = 20000\n\ntrain_df, valid_df = train_test_split(df, test_size=n_valid, random_state=42)\ntrain_df.shape, valid_df.shape\n\n((561012, 56), (20000, 56))\n\n\n\n\nTraining with the train function\nIf you’re training with the train function, multi-class classification can be done with two objectives: multi:softmax and multi:softprob. Both use the same loss function—negative multinomial log likelihood—but the softmax option produces a trained Booster object whose predict method returns a 1d array of predicted labels, whereas the softprob option produces a trained Booster object whose predict method returns a 2d array of predicted probabilities. In either case, you also need to explicitly tell XGBoost how many classes the target has with the num_class parameter.\n\nparams = {\n    'tree_method': 'approx',\n    'objective': 'multi:softprob',\n    'num_class': df[target].nunique()\n}\nnum_boost_round = 10\n\ndtrain = xgb.DMatrix(label=train_df[target], data=train_df[features])\ndvalid = xgb.DMatrix(label=valid_df[target], data=valid_df[features])\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=2)\n\n[0] train-mlogloss:1.42032  valid-mlogloss:1.42366\n[2] train-mlogloss:1.00541  valid-mlogloss:1.00963\n[4] train-mlogloss:0.80557  valid-mlogloss:0.81109\n[6] train-mlogloss:0.69432  valid-mlogloss:0.70085\n[8] train-mlogloss:0.62653  valid-mlogloss:0.63350\n[9] train-mlogloss:0.60111  valid-mlogloss:0.60794\n\n\n\n\nTraining with XGBClassifier\nIn multi-class classification, I think the scikit-learn XGBClassifier wrapper is quite a bit more convenient than the native train function. You can set the objective parameter to multi:softprob, and XGBClassifier.fit will produce a model having both predict and predict_proba methods. Also there is no need to explicitly set the number of classes in the target and no need to create the DMatrix objects.\n\nparams = {\n    'tree_method': 'approx',\n    'objective': 'multi:softprob',\n}\nnum_boost_round = 10\n\nclf = xgb.XGBClassifier(n_estimators=num_boost_round, **params)\nclf.fit(train_df[features], train_df[target], \n        eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n        verbose=2);\n\n[0] validation_0-mlogloss:1.42032   validation_1-mlogloss:1.42366\n[2] validation_0-mlogloss:1.00541   validation_1-mlogloss:1.00963\n[4] validation_0-mlogloss:0.80557   validation_1-mlogloss:0.81109\n[6] validation_0-mlogloss:0.69432   validation_1-mlogloss:0.70085\n[8] validation_0-mlogloss:0.62653   validation_1-mlogloss:0.63350\n[9] validation_0-mlogloss:0.60111   validation_1-mlogloss:0.60794\n\n\n\n\nEvaluating the Model\nThis time, we’ll keep the entire 2d array of predicted probabilities in y_score.\n\ny_true = valid_df[target]\ny_pred = clf.predict(valid_df[features])\ny_score = clf.predict_proba(valid_df[features])\ny_true.shape, y_pred.shape, y_score.shape\n\n((20000,), (20000,), (20000, 7))\n\n\n\nmetrics.accuracy_score(y_true, y_pred)\n\n0.77425\n\n\n\nprint(metrics.classification_report(y_true, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.77      0.74      0.75      7365\n           1       0.78      0.84      0.81      9725\n           2       0.75      0.85      0.80      1207\n           3       0.82      0.78      0.80        85\n           4       0.93      0.26      0.40       317\n           5       0.76      0.31      0.44       627\n           6       0.88      0.68      0.77       674\n\n    accuracy                           0.77     20000\n   macro avg       0.81      0.64      0.68     20000\nweighted avg       0.78      0.77      0.77     20000\n\n\n\nSome binary classification metrics, like AUC, can be extended to the multi-class setting by computing the metric for each class, then averaging in some way to get an overall score. The details are controlled by the average and multi_class parameters, which are described in the documentation.\n\nmetrics.roc_auc_score(y_true, y_score, average='weighted', multi_class='ovr')\n\n0.9129422094408693\n\n\n\n\nFeature Importance\nWe can compute permutation feature importance with exactly the same code that we used for the binary classifier.\n\nscorer = make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)\npermu_imp = permutation_importance(clf, valid_df[features], valid_df[target], \n                                   n_repeats=30, random_state=0, scoring=scorer)\n\n\nimportances_permutation = pd.Series(permu_imp['importances_mean'], index=features)\nimportances_permutation.sort_values(ascending=True)[-10:].plot.barh()\nplt.title('Permutation Importance on Out-of-Sample Set')\nplt.xlabel('change in multivariate log likelihood');\n\n\n\n\ntop 10 features by permutation importance on validation set\n\n\n\n\n\n\nPartial Dependence\nRecall that partial dependence reflects how the expected model output changes with a particular feature. In the multi-class setting, the model has multiple outputs—one probability for each class—so we need to choose which class probability to show in the plots. We choose the target class with the target parameter; be sure to pass in the encoded value, e.g. we need to use the label encoder to transform a raw class label back into the encoded value. Here we’ll examine partial dependence for the probability of cover type 3.\n\nPartialDependenceDisplay.from_estimator(clf, \n                                        X=valid_df[features], \n                                        features=['Elevation', 'Horizontal_Distance_To_Roadways'], \n                                        target=enc.transform([3])[0]);\n\n\n\n\nPDP of target probability of cover type == 3 vs elevation and distance to roadway"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#wrapping-up",
    "href": "posts/xgboost-for-classification-in-python/index.html#wrapping-up",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, for me, those are really the minimal nuts and bolts one needs to get XGBoost models working on classification problems. If you dig this tutorial, or if you have additional insights into using XGBoost to solve classification problems, let me know about it down in the comments!"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#go-deeper",
    "href": "posts/xgboost-for-classification-in-python/index.html#go-deeper",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Go Deeper",
    "text": "Go Deeper\nIf you’re feeling like Alice, and you want to go tumbling down the rabbit hole, might I recommend checking out some of the following:\n\nXGBoost Explained - for a deep dive into the math\nXGBoost from Scratch - to see how to implement all those equations in code\nMulti-Class Gradient Boosting from Scratch - to fully grok the multi-class gradient boosting algorithm"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html",
    "href": "posts/xgboost-for-regression-in-python/index.html",
    "title": "XGBoost for Regression in Python",
    "section": "",
    "text": "In this post I’m going to show you my process for solving regression problems with XGBoost in python, using either the native xgboost API or the scikit-learn interface. This is a powerful methodology that can produce world class results in a short time with minimal thought or effort. While we’ll be working on an old Kagle competition for predicting the sale prices of bulldozers and other heavy machinery, you can use this flow to solve whatever tabular data regression problem you’re working on.\nThis post serves as the explanation and documentation for the XGBoost regression jupyter notebook from my ds-templates repo on GitHub, so go ahead and download the notebook and follow along with your own data.\nIf you’re not already comfortable with the ideas behind gradient boosting and XGBoost, you’ll find it helpful to read some of my previous posts to get up to speed. I’d start with this introduction to gradient boosting, and then read this explanation of how XGBoost works.\nLet’s get into it! 🚀"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#install-and-import-the-xgboost-library",
    "href": "posts/xgboost-for-regression-in-python/index.html#install-and-import-the-xgboost-library",
    "title": "XGBoost for Regression in Python",
    "section": "Install and import the xgboost library",
    "text": "Install and import the xgboost library\nIf you don’t already have it, go ahead and use conda to install the xgboost library, e.g.\n$ conda install -c conda-forge xgboost\nThen import it along with the usual suspects.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost as xgb"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#read-dataset-into-python",
    "href": "posts/xgboost-for-regression-in-python/index.html#read-dataset-into-python",
    "title": "XGBoost for Regression in Python",
    "section": "Read dataset into python",
    "text": "Read dataset into python\nIn this example we’ll work on the Kagle Bluebook for Bulldozers competition, which asks us to build a regression model to predict the sale price of heavy equipment. Amazingly, you can solve your own regression problem by swapping this data out with your organization’s data before proceeding with the tutorial.\nGo ahead and download the Train.zip file from Kagle and extract it into Train.csv. Then read the data into a pandas dataframe.\n\ndf = pd.read_csv('Train.csv', parse_dates=['saledate']);\n\nNotice I cheated a little bit, checking the columns ahead of time and telling pandas to treat the saledate column as a date. In general it will make life easier to read in any date-like columns as dates.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 401125 entries, 0 to 401124\nData columns (total 53 columns):\n #   Column                    Non-Null Count   Dtype         \n---  ------                    --------------   -----         \n 0   SalesID                   401125 non-null  int64         \n 1   SalePrice                 401125 non-null  int64         \n 2   MachineID                 401125 non-null  int64         \n 3   ModelID                   401125 non-null  int64         \n 4   datasource                401125 non-null  int64         \n 5   auctioneerID              380989 non-null  float64       \n 6   YearMade                  401125 non-null  int64         \n 7   MachineHoursCurrentMeter  142765 non-null  float64       \n 8   UsageBand                 69639 non-null   object        \n 9   saledate                  401125 non-null  datetime64[ns]\n 10  fiModelDesc               401125 non-null  object        \n 11  fiBaseModel               401125 non-null  object        \n 12  fiSecondaryDesc           263934 non-null  object        \n 13  fiModelSeries             56908 non-null   object        \n 14  fiModelDescriptor         71919 non-null   object        \n 15  ProductSize               190350 non-null  object        \n 16  fiProductClassDesc        401125 non-null  object        \n 17  state                     401125 non-null  object        \n 18  ProductGroup              401125 non-null  object        \n 19  ProductGroupDesc          401125 non-null  object        \n 20  Drive_System              104361 non-null  object        \n 21  Enclosure                 400800 non-null  object        \n 22  Forks                     192077 non-null  object        \n 23  Pad_Type                  79134 non-null   object        \n 24  Ride_Control              148606 non-null  object        \n 25  Stick                     79134 non-null   object        \n 26  Transmission              183230 non-null  object        \n 27  Turbocharged              79134 non-null   object        \n 28  Blade_Extension           25219 non-null   object        \n 29  Blade_Width               25219 non-null   object        \n 30  Enclosure_Type            25219 non-null   object        \n 31  Engine_Horsepower         25219 non-null   object        \n 32  Hydraulics                320570 non-null  object        \n 33  Pushblock                 25219 non-null   object        \n 34  Ripper                    104137 non-null  object        \n 35  Scarifier                 25230 non-null   object        \n 36  Tip_Control               25219 non-null   object        \n 37  Tire_Size                 94718 non-null   object        \n 38  Coupler                   213952 non-null  object        \n 39  Coupler_System            43458 non-null   object        \n 40  Grouser_Tracks            43362 non-null   object        \n 41  Hydraulics_Flow           43362 non-null   object        \n 42  Track_Type                99153 non-null   object        \n 43  Undercarriage_Pad_Width   99872 non-null   object        \n 44  Stick_Length              99218 non-null   object        \n 45  Thumb                     99288 non-null   object        \n 46  Pattern_Changer           99218 non-null   object        \n 47  Grouser_Type              99153 non-null   object        \n 48  Backhoe_Mounting          78672 non-null   object        \n 49  Blade_Type                79833 non-null   object        \n 50  Travel_Controls           79834 non-null   object        \n 51  Differential_Type         69411 non-null   object        \n 52  Steering_Controls         69369 non-null   object        \ndtypes: datetime64[ns](1), float64(2), int64(6), object(44)\nmemory usage: 162.2+ MB"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#prepare-raw-data-for-xgboost",
    "href": "posts/xgboost-for-regression-in-python/index.html#prepare-raw-data-for-xgboost",
    "title": "XGBoost for Regression in Python",
    "section": "Prepare raw data for XGBoost",
    "text": "Prepare raw data for XGBoost\nWhen faced with a new tabular dataset for modeling, we have two format considerations: data types and missingness. From the call to df.info() above, we can see we have both mixed types and missing values.\nWhen it comes to missing values, some models like the gradient booster or random forest in scikit-learn require purely non-missing inputs. One of the great strengths of XGBoost is that it relaxes this requirement, allowing us to pass in missing feature values, so we don’t have to worry about them.\nRegarding data types, all ML models for tabular data require inputs to be numeric, either integers or floats, so we’re going to have to deal with those object columns.\n\nEncode string features\nThe simplest way to encode string variables is to map each unique string value to an integer; this is called integer encoding.\nWe can easily accomplish this by using the categorical data type in pandas. The category type is a bit like the factor type in R; pandas stores the underlying data as integers, and it keeps a mapping from the integers back to the original string values. XGBoost is able to access the numeric data underlying the categorical features for model training and prediction. This is a nice way to encode string features because it’s easy to implement and it preserves the original category levels in the data frame. If you prefer to generate your own integer mappings, you can also do it with the scikit-learn OrdinalEncoder.\n\ndef encode_string_features(df):\n    out_df = df.copy()\n    for feature, feature_type in df.dtypes.items():\n        if feature_type == 'object':\n            out_df[feature] = out_df[feature].astype('category')\n    return out_df\n\ndf = encode_string_features(df)\n\n\n\nEncode date and timestamp features\nWhile dates feel sort of numeric, they are not quite numbers, so we need to transform them into numeric columns that XGBoost can understand. Unfortunately, encoding timestamps isn’t as straightforward as encoding strings, so we actually might need to engage in a little bit of feature engineering. A single date has many different attributes, e.g. days since epoch, year, quarter, month, day, day of year, day of week, is holiday, etc. Often a simple time index is the most useful information in a date column, so here we’ll just start by adding a feature that gives the number of days since some epoch date.\n\ndf['saledate_days_since_epoch'] = (\n    df['saledate'] - pd.Timestamp(year=1970, month=1, day=1)\n    ).dt.days\n\n\n\nTransform the target if necessary\nIn the interest of speed and efficiency, we didn’t bother doing any EDA with the feature data. Part of my justification for this is that trees are incredibly robust to outliers, colinearity, missingness, and other assorted nonsense in the feature data. However, they are not necessarily robust to nonsense in the target variable, so it’s worth having a look at it before proceeding any further.\n\ndf.SalePrice.hist(); plt.xlabel('SalePrice');\n\n\n\n\nOften when predicting prices it makes sense to use log price, especially when they span multiple orders of magnitude or have a strong right skew. These data look pretty friendly, lacking outliers and exhibiting only a mild positive skew; we could probably get away without doing any transformation. But checking the evaluation metric used to score the Kagle competition, we see they’re using root mean squared log error. That’s equivalent to using RMSE on log-transformed target data, so let’s go ahead and work with log prices.\n\ndf['logSalePrice'] = np.log1p(df['SalePrice'])\ndf.logSalePrice.hist(); plt.xlabel('logSalePrice');"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#train-and-evaluate-the-xgboost-regression-model",
    "href": "posts/xgboost-for-regression-in-python/index.html#train-and-evaluate-the-xgboost-regression-model",
    "title": "XGBoost for Regression in Python",
    "section": "Train and Evaluate the XGBoost regression model",
    "text": "Train and Evaluate the XGBoost regression model\nHaving prepared our dataset, we are now ready to train an XGBoost model. Let’s walk through the flow step-by-step.\n\nSplit the data into training and validation sets\nFirst we split the dataset into a training set and a validation set. Of course since we’re going to evaluate against the validation set a number of times as we iterate, it’s best practice to keep a separate test set reserved to check our final model to ensure it generalizes well. Assuming that final test set is hidden away, we can use the rest of the data for training and validation.\nThere are two main ways we might want to select the validation set. If there isn’t a temporal ordering of the observations, we might be able to randomly sample. In practice, it’s common that observations have a temporal ordering, and that models are trained on observations up to a certain time and used to predict on observations occuring after that time. Since this data is temporal, we don’t want to split randomly; instead we’ll split on observation date, reserving the latest observations for the validation set.\n\n# Temporal Validation Set\ndef train_test_split_temporal(df, datetime_column, n_test):\n    idx_sort = np.argsort(df[datetime_column])\n    idx_train, idx_test = idx_sort[:-n_valid], idx_sort[-n_valid:]\n    return df.iloc[idx_train, :], df.iloc[idx_test, :]\n\n\nn_valid = 12000\n\ntrain_df, valid_df = train_test_split_temporal(df, 'saledate', n_valid)\ntrain_df.shape, valid_df.shape\n\n((389125, 55), (12000, 55))\n\n\n\n\nSpecify target and feature columns\nNext we’ll put together a list of our features and define the target column. I like to have an actual list defined in the code so it’s easy to explicitly see everything we’re puting into the model and easier to add or remove features as we iterate. Just run something like list(df.columns) in a cel to get a copy-pasteable list of columns, then edit it down to the full list of features, i.e. remove the target, date columns, and other non-feature columns..\n\n#  list(df.columns)\n\n\nfeatures = [\n    'SalesID',\n    'MachineID',\n    'ModelID',\n    'datasource',\n    'auctioneerID',\n    'YearMade',\n    'MachineHoursCurrentMeter',\n    'UsageBand',\n    'fiModelDesc',\n    'fiBaseModel',\n    'fiSecondaryDesc',\n    'fiModelSeries',\n    'fiModelDescriptor',\n    'ProductSize',\n    'fiProductClassDesc',\n    'state',\n    'ProductGroup',\n    'ProductGroupDesc',\n    'Drive_System',\n    'Enclosure',\n    'Forks',\n    'Pad_Type',\n    'Ride_Control',\n    'Stick',\n    'Transmission',\n    'Turbocharged',\n    'Blade_Extension',\n    'Blade_Width',\n    'Enclosure_Type',\n    'Engine_Horsepower',\n    'Hydraulics',\n    'Pushblock',\n    'Ripper',\n    'Scarifier',\n    'Tip_Control',\n    'Tire_Size',\n    'Coupler',\n    'Coupler_System',\n    'Grouser_Tracks',\n    'Hydraulics_Flow',\n    'Track_Type',\n    'Undercarriage_Pad_Width',\n    'Stick_Length',\n    'Thumb',\n    'Pattern_Changer',\n    'Grouser_Type',\n    'Backhoe_Mounting',\n    'Blade_Type',\n    'Travel_Controls',\n    'Differential_Type',\n    'Steering_Controls',\n    'saledate_days_since_epoch'\n ]\n\ntarget = 'logSalePrice'\n\n\n\nCreate DMatrix data objects\nXGBoost uses a data type called dense matrix for efficient training and prediction, so next we need to create DMatrix objects for our training and validation datasets. Remember how we decided to encode our string columns by casting them as pandas categorical types? For this to work, we need to set the enable_categoricals argument to True.\n\ndtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], \n                     enable_categorical=True)\ndvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], \n                     enable_categorical=True)\n\n\n\nSet the XGBoost parameters\nXGBoost has numerous hyperparameters. Fortunately, just a handful of them tend to be the most influential; furthermore, the default values are not bad in most situations. I like to start out with a dictionary containing the default values for the parameters I’m most likely to adjust later, with one exception. I dislike the default value of auto for the tree_method parameter, which tells XGBoost to choose a tree method on it’s own. I’ve been burned by this ambiguity in the past, so now I prefer to set it to approx. For training there is one required boosting parameter called num_boost_round which I set to 50 as a starting point.\n\n# default values for important parameters\nparams = {\n    'tree_method': 'approx',\n    'learning_rate': 0.3,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'subsample': 1,\n    'colsample_bynode': 1,\n    'objective': 'reg:squarederror',\n}\nnum_boost_round = 50\n\n\n\nTrain the XGBoost model\nThe xgb.train() function takes our training dataset and parameters, and it returns a trained XGBoost model, which is an object of class xgb.core.Booster. Check out the documentation on the learning API to see all the training options. During training, I like to have XGBoost print out the evaluation metric on the train and validation set after every few boosting rounds and again at the end of training; that can be done by setting evals and verbose_eval.\n\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=10)\n\n[0] train-rmse:6.74240  valid-rmse:6.80825\n[10]    train-rmse:0.31071  valid-rmse:0.34532\n[20]    train-rmse:0.21950  valid-rmse:0.24364\n[30]    train-rmse:0.20878  valid-rmse:0.23669\n[40]    train-rmse:0.20164  valid-rmse:0.23254\n[49]    train-rmse:0.19705  valid-rmse:0.23125\n\n\n\n\nTrain the XGBoost model using the sklearn interface\nIf you prefer scikit-learn-like syntax, you can use the sklearn estimator interface to create and train XGBoost models. The XGBRegressor class, which is available in the xgboost library that we already imported, constructs an XGBRegressor object with fit and predict methods like you’re used to using in scikit-learn. The fit and predict methods take pandas dataframes, so you don’t need to create DMatrix data objects yourself; however, since these methods still have to transform input data into DMatrix objects internally, training and prediction seem to be slower via the sklearn interface.\n\n# scikit-learn interface\nreg = xgb.XGBRegressor(n_estimators=num_boost_round, enable_categorical=True, **params)\nreg.fit(train_df[features], train_df[target], \n        eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n        verbose=10);\n\n[0] validation_0-rmse:6.74240   validation_1-rmse:6.80825\n[10]    validation_0-rmse:0.31071   validation_1-rmse:0.34532\n[20]    validation_0-rmse:0.21950   validation_1-rmse:0.24364\n[30]    validation_0-rmse:0.20878   validation_1-rmse:0.23669\n[40]    validation_0-rmse:0.20164   validation_1-rmse:0.23254\n[49]    validation_0-rmse:0.19705   validation_1-rmse:0.23125\n\n\nSince not all features of XGBoost are available through the scikit-learn estimator interface, you might want to get the native xgb.core.Booster object back out of the sklearn wrapper.\n\nbooster = reg.get_booster()\n\n\n\nEvaluate the XGBoost model and check for overfitting\nWe get the model evaluation metrics on the training and validation sets printed to stdout when we use the evals argument to the training API. Typically I just look at those printed metrics, but sometimes it’s helpful to retain them in a variable for further inspection via, e.g. plotting. To do that we need to train again, passing an empty dictionary to the evals_result argument. In the objective curves, I’m looking for signs of overfitting, which could include validation scores staying the same or getting worse over later iterations or huge gaps between training and validation scores.\n\nevals_result = {}\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=10,\n                  evals_result=evals_result)\n\n[0] train-rmse:6.74240  valid-rmse:6.80825\n[10]    train-rmse:0.31071  valid-rmse:0.34532\n[20]    train-rmse:0.21950  valid-rmse:0.24364\n[30]    train-rmse:0.20878  valid-rmse:0.23669\n[40]    train-rmse:0.20164  valid-rmse:0.23254\n[49]    train-rmse:0.19705  valid-rmse:0.23125\n\n\n\npd.DataFrame({\n    'train': evals_result['train']['rmse'],\n    'valid': evals_result['valid']['rmse']\n}).plot(); plt.xlabel('boosting round'); plt.ylabel('objective');\n\n\n\n\nThese objective curves look pretty good–no obvious signs of trouble.\n\n\n\n\nWhile we could just look at the validation RMSE in the printed output from model training, let’s go ahead and compute it by hand, just to be sure.\n\nfrom sklearn.metrics import mean_squared_error\n\n# squared=False returns RMSE\nmean_squared_error(y_true=dvalid.get_label(), \n                   y_pred=model.predict(dvalid), \n                   squared=False)\n\n0.23124987\n\n\nSo, how good is that RMSLE of 0.231? Well, checking the Kagle leaderboard for this competition, we would have come in around 5th out of 474. That’s not bad for 10 minutes of work doing the bare minimum necessary to transform the raw data into a format consumable by XGBoost and then training a model using default hyperparameter values. To improve our model from here we would want to explore some feature engineering and some hyperparameter tuning, which we’ll save for another post.\n\nWait, why was that so easy? Since XGBoost made it’s big Kagle debut in the 2014 Higgs Boson competition, presumably no one in this 2013 competition was using it yet. A second potential reason is that we’re using a different validation set from that used for the final leaderboard (which is long closed), but our score is likely still a decent approximation for how we would have done in the competition."
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#xgboost-model-interpretation",
    "href": "posts/xgboost-for-regression-in-python/index.html#xgboost-model-interpretation",
    "title": "XGBoost for Regression in Python",
    "section": "XGBoost Model Interpretation",
    "text": "XGBoost Model Interpretation\nNext let’s have a look at how to apply a couple of the most common model interpretation techniques, feature importance and partial dependence, to XGBoost.\n\nRemember we have two trained models floating around: one called model of class xgb.core.Booster which is compatible with xgboost library utilities and another called reg of class XGBRegressor which is compatible with scikit-learn utilities. We need to be sure to use the model that’s compatible with whatever utility we’re using.\n\n\nWhile these interpretation tools are still very common, there’s a newer, more comprehensive, and self-consistent model interpretation framework called SHAP that’s worth checking out.\n\n\nFeature Importance for XGBoost\nWhile XGBoost automatically computes feature importance by three different metrics during training, you should only use them with great care and skepticism. The three metrics are\n\nweight: the number of splits that use the feature\ngain: the average gain in the objective function from splits which use the feature\ncover: the average number of training samples affected by splits that use the feature\n\nThe first problem with these metrics is that they are computed using only the training dataset, which means they don’t reflect how useful a feature is when predicting on out-of-sample data. If your model is overfit on some nonsense feature, it will still have a high importance. Secondly, I think they are difficult to interpret; all three are specific to decision trees and reflect domain-irrelevant idiosyncrasies like whether a feature is used nearer the root or the leaves of a tree. Anyway let’s see what these metrics have to say about our features.\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(5, 8))\nxgb.plot_importance(model, importance_type='weight', title='importance_type=weight', \n                    max_num_features=10, show_values=False, ax=ax1, )\nxgb.plot_importance(model, importance_type='cover', title='importance_type=cover', \n                    max_num_features=10, show_values=False, ax=ax2)\nxgb.plot_importance(model, importance_type='gain', title='importance_type=gain', \n                    max_num_features=10, show_values=False, ax=ax3)\nplt.tight_layout()\n\n\n\n\ntop 10 features according to each built-in XGBoost feature importance metric\n\n\n\n\nWow, notice that the top 10 features by weight and by cover are completely different. This should forever cause you to feel skeptical whenever you see a feature importance plot.\nLuckily, there is a better way. IMHO, permutation feature importance is better aligned with our intuition about what feature importance should mean. It tells us by how much the model performance decreases when the values of a particular feature are randomly shuffled during prediction. This effectively breaks the relationship between the feature and the target, thus revealing how much the model relies on that feature for prediction. It also has the benefit that it can be computed using either training data or out-of-sample data.\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import make_scorer\n\n# make a scorer for RMSE\nscorer = make_scorer(mean_squared_error, squared=False)\npermu_imp = permutation_importance(reg, valid_df[features], valid_df[target], \n                                   n_repeats=30, random_state=0, scoring=scorer)\n\n\nimportances_permutation = pd.Series(-1 * permu_imp['importances_mean'], index=features)\nimportances_permutation.sort_values(ascending=True)[-10:].plot.barh()\nplt.title('Permutation Importance on Out-of-Sample Set')\nplt.xlabel('drop in RMSE');\n\n\n\n\ntop 10 features by permutation importance on validation set\n\n\n\n\nNow we can see which features the model relies on most for out-of-sample predictions. These are good candidate features to dig into with some EDA and conversations with any domain expert collaborators."
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#partial-dependence-plots-for-xgboost",
    "href": "posts/xgboost-for-regression-in-python/index.html#partial-dependence-plots-for-xgboost",
    "title": "XGBoost for Regression in Python",
    "section": "Partial Dependence Plots for XGBoost",
    "text": "Partial Dependence Plots for XGBoost\nA partial dependence plot (PDP) is a representation of the dependence between the target variable and one or more feature variables. We can loosely interpret it as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say “loosely” because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(reg, \n                                        valid_df[features].query('YearMade &gt;= 1960'), \n                                        ['YearMade']);\n\n\n\n\nPDP of target logSalePrice on feature YearMade\n\n\n\n\nIt looks like the log sale price tends to increase in a non-linear way with year made.\n\n\nCode\n# The PDPs for categorical features expect numeric data, not pandas categorical types,\n# so the sklearn API for partial dependence won't work directly with the dataframe we've been using.\n# The workaround is to create a new dataframe where categorical columns are encoded numerically,\n# retrain the XGBoost model using the sklearn interface, create the PDPs,\n# then add the category levels as the tick labels for the PDP.\n\ndef cat_pdp():\n    cat_feature = 'Enclosure'\n    modified_df = df.copy()\n    cat_codes = modified_df[cat_feature].cat.codes\n    cat_labels = list(modified_df[cat_feature].cat.categories)\n    cat_labels = ['NaN'] + cat_labels if -1 in cat_codes.unique() else cat_labels\n    modified_df[cat_feature] = cat_codes\n\n    n_valid = 12000\n    train_df, valid_df = train_test_split_temporal(modified_df, 'saledate', n_valid)\n    train_df.shape, valid_df.shape\n\n    # scikit-learn interface\n    reg = xgb.XGBRegressor(n_estimators=num_boost_round, enable_categorical=True, **params)\n    reg.fit(train_df[features], train_df[target], \n            eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n            verbose=0);\n    PartialDependenceDisplay.from_estimator(reg, valid_df[features], [cat_feature], categorical_features=[cat_feature])\n    plt.xticks(ticks=cat_codes.unique(), labels=cat_labels)\ncat_pdp()\n\n\n\n\n\nPDP of target logSalePrice on categorical feature Enclosure\n\n\n\n\nYou can imagine how useful these model interpretation tools can be, both for understanding data and for improving your models."
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#wrapping-up",
    "href": "posts/xgboost-for-regression-in-python/index.html#wrapping-up",
    "title": "XGBoost for Regression in Python",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere you have it, a simple flow for solving regression problems with XGBoost in python. Remember you can use the XGBoost regression notebook from my ds-templates repo to make it easy to follow this flow on your own problems. If you found this helpful, or if you have additional ideas about solving regression problems with XGBoost, let me know down in the comments."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html",
    "href": "posts/decision-tree-from-scratch/index.html",
    "title": "Decision Tree from Scratch",
    "section": "",
    "text": "Yesterday we had a lovely discussion about the key strengths and weaknesses of decision trees and why tree ensembles are so great. But today, gentle reader, we abandon our philosophizing and get down to the business of implementing one of these decision trees from scratch.\nA note before we get started. This is going to be the most involved scratch-build that we’ve done at Random Realizations so far. It is not the kind of algorithm that I could just sit down and write all at once. We need to start with a basic frame and then add functionality step by step, testing all along the way to make sure things are working properly. Since I’m writing this in a jupyter notebook, I’ll try to give you a sense for how I actually put the algorithm together interactively in pieces, eventually landing on a fully-functional final product.\nShall we?"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#binary-tree-data-structure",
    "href": "posts/decision-tree-from-scratch/index.html#binary-tree-data-structure",
    "title": "Decision Tree from Scratch",
    "section": "Binary Tree Data Structure",
    "text": "Binary Tree Data Structure\nA decision tree takes a dataset with features and a target, partitions the feature space into chunks, and assigns a prediction value to each chunk. Since each partitioning step divides one chunk in two, and since the partitioning is done recursively, it’s natural to use a binary tree data structure to represent a decision tree.\nThe basic idea of the binary tree is that we define a class to represent nodes in the tree. If we want to add children to a given node, we simply assign them as attributes of the parent node. The child nodes we add are themselves instances of the same class, so we can add children to them in the same way.\nLet’s start out with a simple class for our decision tree. It takes a single value called max_depth as input, which will dictate how many layers of child nodes should be inserted below the root. This controls the depth of the tree. As long as max_depth is positive, the parent will instantiate two new instances of the binary tree node class, passing along max_depth decremented by one and attaching the two children to itself as attributes called left and right.\n\nimport math\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nclass DecisionTree():\n\n        def __init__(self, max_depth):\n            assert max_depth &gt;= 0, 'max_depth must be nonnegative'\n            self.max_depth = max_depth\n            if max_depth &gt; 0:\n                self.left = DecisionTree(max_depth=max_depth-1)\n                self.right = DecisionTree(max_depth=max_depth-1)\n\nLet’s make a new instance of our decision tree class, a tree with depth 2.\n\nt = DecisionTree(max_depth=2)\n\n\n\n\nBinary tree structure diagram\n\n\nWe can access individual nodes and check their value of max_depth.\n\nt.max_depth, t.left.max_depth, t.left.right.max_depth\n\n(2, 1, 0)\n\n\nOur full decision tree can expand on this idea where each node receives some input, modifies it, creates two child nodes, and passes the modified input along to them. Specifically, each node in our decision tree will receive a dataset, determine how best to split the dataset into two parts, create two child nodes, and pass one part of the data to the left child and the other part to the right child.\nAll we have to do now is add some additional functionality to our decision tree. First we’ll start by capturing all the inputs we need to grow a tree, which include the feature dataframe X, the target array y, max_depth to explicitly limit tree depth, min_samples_leaf to specify the minimum number of observations that are allowed in a leaf node, and an optional idxs which specifies the indices of data that the node should use. The indices argument is useful for users of our decision tree because it will allow them to implement row subsampling in ensemble methods like random forest. It will also be handy for internal use inside the decision tree when passing data along to child nodes; instead of passing copies of the two data subsets, we’ll just pass a reference to the full dataset and pass along a set of indices to identify that node’s instance subset.\nOnce we get our input, we’ll do a little bit of input validation and store things that we want to keep as object attributes. In case this is a leaf node, we’ll go ahead and compute its predicted value; since this is a regression tree, the prediction is just the mean of the target y. We’ll also go ahead and initialize a score metric which we’ll use to help us find the best split later; since lower scores are going to be better, we’ll initialize it to positive infinity. Finally, we’ll push the logic to add child nodes into a method called _maybe_insert_child_nodes that we’ll define next.\n\n\n\n\n\n\nNote\n\n\n\na leading underscore in a method name indicates the method is for internal use and not part of the user-facing API of the class.\n\n\n\nclass DecisionTree():\n\n    def __init__(self, X, y, min_samples_leaf=5, max_depth=6, idxs=None):\n        assert max_depth &gt;= 0, 'max_depth must be nonnegative'\n        assert min_samples_leaf &gt; 0, 'min_samples_leaf must be positive'\n        self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth\n        if isinstance(y, pd.Series): y = y.values\n        if idxs is None: idxs = np.arange(len(y))\n        self.X, self.y, self.idxs = X, y, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = np.mean(y[idxs]) # node's prediction value\n        self.best_score_so_far = float('inf') # initial loss before split finding\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n            \n    def _maybe_insert_child_nodes(self):\n        pass\n\nNow in order to test our class, we’ll need some actual data. We can use the same scikit-learn diabetes data from the last post.\n\nfrom sklearn.datasets import load_diabetes\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\n\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=5)\n\nSo far, so good."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#inserting-child-nodes",
    "href": "posts/decision-tree-from-scratch/index.html#inserting-child-nodes",
    "title": "Decision Tree from Scratch",
    "section": "Inserting Child Nodes",
    "text": "Inserting Child Nodes\nOur node inserting function _maybe_insert_child_nodes needs to first find the best split; then if a valid split exists, it needs to insert the child nodes. To find the best valid split, we need to loop through the columns and search each one for the best valid split. Again we’ll push the logic of finding the best split into a function that we’ll define later. Next if no split was found, we need to bail by returning before trying to insert the child nodes. To check if this node is a leaf (i.e. it shouldn’t have child nodes), we define a property called is_leaf which will just check if the best score so far is still infinity, in which case no split was found and the node is a leaf.\nIf a valid split was found, then we need to insert the child nodes. We’ll assume that our split finding function assigned attributes called split_feature_idx and threshold to tell us the split feature’s index and the split threshold value. We then use these to compute the indices of the data to be passed to the child nodes; the left child gets instances where the split feature value is less than or equal to the threshold, and the right child node gets instances where the split feature value is greater than the threshold. Then we create two new decision trees, passing the corresponding data indices to each and assigning them to the left and right attributes of the current node.\n\n    def _maybe_insert_child_nodes(self):\n        for j in range(self.c): \n            self._find_better_split(j)\n        if self.is_leaf: # do not insert children\n            return \n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[left_idx])\n        self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[right_idx])\n\n    def _find_better_split(self, feature_idx):\n        pass\n    \n    @property\n    def is_leaf(self): return self.best_score_so_far == float('inf')\n\nTo test these new methods , we can assign them to our DecisionTree class and create a new class instance to make sure things are still working.\n\nDecisionTree._maybe_insert_child_nodes = _maybe_insert_child_nodes\nDecisionTree._find_better_split = _find_better_split\nDecisionTree.is_leaf = is_leaf\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=6)\n\nYep, we’re still looking good."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#split-finding",
    "href": "posts/decision-tree-from-scratch/index.html#split-finding",
    "title": "Decision Tree from Scratch",
    "section": "Split Finding",
    "text": "Split Finding\nNow we need to fill in the functionality of the split finding method. The overall strategy is to consider every possible way to split on the current feature, measuring the quality of each potential split with some scoring mechanism, and keeping track of the best split we’ve seen so far. We’ll come back to the issue of how to try all the possible splits in a moment, but let’s start by figuring out how to score a particular potential split.\nLike other machine learning models, trees are trained by attempting to minimize some loss function that measures how well the model predicts the target data. We’ll be training our regression tree to minimize squared error.\n\\[ L = \\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\]\nFor a given node, we can replace \\(\\hat{y}\\) with \\(\\bar{y}\\) because each node uses the sample mean of its target instances as its prediction. We can then rewrite the loss for a given node as\n\\[ L = \\sum_{i=1}^n(y_i - \\bar{y})^2 \\] \\[  = \\sum_{i=1}^n(y_i^2 -2y_i\\bar{y} + \\bar{y}^2)  \\] \\[  = \\sum_{i=1}^ny_i^2 -2\\bar{y}\\sum_{i=1}^ny_i + n\\bar{y}^2 \\] \\[  = \\sum_{i=1}^ny_i^2 - \\frac{1}{n} \\left ( \\sum_{i=1}^ny_i \\right )^2 \\]\nWe can then evaluate potential splits by comparing the loss after splitting to the loss before splitting, where the split with the greatest loss reduction is best. Let’s work out a simple expression for the loss reduction from a given split.\nLet \\(I\\) be the set of \\(n\\) data instances in the current node, and let \\(I_L\\) and \\(I_R\\) be the instances that fall into the left and right child nodes of a proposed split. Let \\(L\\) be the total loss for all instances in the node, while \\(L_L\\) and \\(L_R\\) are the losses for the left and right child nodes. The total loss contributed by instances in \\(I\\) prior to any split is\n\\[L_{\\text{before split}} = L =  \\sum_{i \\in I} y_i^2 - \\frac{1}{n} \\left ( \\sum_{i \\in I} y_i \\right )^2 \\]\nAnd the loss after splitting \\(I\\) into \\(I_L\\) and \\(I_R\\) is\n\\[L_{\\text{after split}} = L_L + L_R =  \\sum_{i \\in I_L} y_i^2 - \\frac{1}{n_L} \\left ( \\sum_{i \\in I_L} y_i \\right )^2 + \\sum_{i \\in I_R} y_i^2 - \\frac{1}{n_R} \\left ( \\sum_{i \\in I_R} y_i \\right )^2 \\]\nThe reduction in loss from this split is\n\\[ \\Delta L = L_{\\text{after split}} -  L_{\\text{before split}} = (L_L + L_R) - L \\] \\[  = \\sum_{i \\in I_L} y_i^2 - \\frac{1}{n_L} \\left ( \\sum_{i \\in I_L} y_i \\right )^2 + \\sum_{i \\in I_R} y_i^2 - \\frac{1}{n_R} \\left ( \\sum_{i \\in I_R} y_i \\right )^2 - \\left ( \\sum_{i \\in I} y_i^2 - \\frac{1}{n} \\left ( \\sum_{i \\in I} y_i \\right )^2 \\right ) \\]\nSince \\(I = I_L \\cup I_R\\) the \\(\\sum y^2\\) terms cancel and we can simplify.\n\\[ \\Delta L = - \\frac{1}{n_L} \\left ( \\sum_{i \\in I_L} y_i \\right )^2\n- \\frac{1}{n_R} \\left ( \\sum_{i \\in I_R} y_i \\right )^2\n+ \\frac{1}{n} \\left ( \\sum_{i \\in I} y_i \\right )^2  \\]\nThis is a really nice formulation of the split scoring metric from a computational complexity perspective. We can sort the data by the feature values then, starting with the smallest min_samples_leaf instances in the left node and the rest in the right node, we check the score. Then to check the next split, we simply move a single target value from the right node into the left node, updating the score by subtracting it from the right node’s partial sum and adding it to the left node’s partial sum. The third term is constant for all splits, so we only need to compute it once. If any split’s score is lower than the best score so far, then we update the best score so far, the split feature, and the threshold value. When we’re done we can be sure we found the best possible split. The time bottleneck is the sort, which puts us at an average time complexity of \\(O(n\\log n)\\).\n\n    def _find_better_split(self, feature_idx):\n        x = self.X.values[self.idxs,feature_idx]\n        y = self.y[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_y, sort_x = y[sort_idx], x[sort_idx]\n        sum_y, n = y.sum(), len(y)\n        sum_y_right, n_right = sum_y, n\n        sum_y_left, n_left = 0., 0\n    \n        for i in range(0, self.n - self.min_samples_leaf):\n            y_i, x_i, x_i_next = sort_y[i], sort_x[i], sort_x[i + 1]\n            sum_y_left += y_i; sum_y_right -= y_i\n            n_left += 1; n_right -= 1\n            if  n_left &lt; self.min_samples_leaf or x_i == x_i_next:\n                continue\n            score = - sum_y_left**2 / n_left - sum_y_right**2 / n_right + sum_y**2 / n\n            if score &lt; self.best_score_so_far:\n                self.best_score_so_far = score\n                self.split_feature_idx = feature_idx\n                self.threshold = (x_i + x_i_next) / 2\n\nAgain, we assign the split finding method to our class and instantiate a new tree to make sure things are still working.\n\nDecisionTree._find_better_split = _find_better_split\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=6)\nX.columns[t.split_feature_idx], t.threshold\n\n('s5', -0.0037611760063045703)\n\n\nNice! Looks like the tree started with a split on the s5 feature."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#inspecting-the-tree",
    "href": "posts/decision-tree-from-scratch/index.html#inspecting-the-tree",
    "title": "Decision Tree from Scratch",
    "section": "Inspecting the Tree",
    "text": "Inspecting the Tree\nWhile we’re developing something complex like a decision tree class, we need a good way to inspect the object to help with testing and debugging. Let’s write a quick string representation method to make it easier to check what’s going on with a particular node.\n\n    def __repr__(self):\n        s = f'n: {self.n}'\n        s += f'; value:{self.value:0.2f}'\n        if not self.is_leaf:\n            split_feature_name = self.X.columns[self.split_feature_idx]\n            s += f'; split: {split_feature_name} &lt;= {self.threshold:0.3f}'\n        return s\n\nWe can assign the string representation method to the class and print a few nodes.\n\nDecisionTree.__repr__ = __repr__\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=2)\nprint(t)\nprint(t.left)\nprint(t.left.left)\n\nn: 442; value:152.13; split: s5 &lt;= -0.004\nn: 218; value:109.99; split: bmi &lt;= 0.006\nn: 171; value:96.31"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#prediction",
    "href": "posts/decision-tree-from-scratch/index.html#prediction",
    "title": "Decision Tree from Scratch",
    "section": "Prediction",
    "text": "Prediction\nWe need a public predict method that takes a feature dataframe and returns an array of predictions. We’ll need to look up the predicted value for one instance at a time and stitch them together in an array. We can do that by iterating over the feature dataframe rows with a list comprehension that calls a _predict_row method to grab the prediction for each row. The row predict method needs to return the current node’s predicted value if it’s a leaf, or if not, it needs to identify the appropriate child node based on its split and ask it for a prediction.\n\n    def predict(self, X):\n        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n    \n    def _predict_row(self, row):\n        if self.is_leaf: \n            return self.value\n        child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n                else self.right\n        return child._predict_row(row)\n\nLet’s assign the predict methods and make predictions on a few rows.\n\nDecisionTree.predict = predict\nDecisionTree._predict_row = _predict_row\nt.predict(X.iloc[:3, :])\n\narray([225.87962963,  96.30994152, 225.87962963])"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#the-complete-decision-tree-implementation",
    "href": "posts/decision-tree-from-scratch/index.html#the-complete-decision-tree-implementation",
    "title": "Decision Tree from Scratch",
    "section": "The Complete Decision Tree Implementation",
    "text": "The Complete Decision Tree Implementation\nHere’s the implementation, all in one place.\n\nclass DecisionTree():\n\n    def __init__(self, X, y, min_samples_leaf=5, max_depth=6, idxs=None):\n        assert max_depth &gt;= 0, 'max_depth must be nonnegative'\n        assert min_samples_leaf &gt; 0, 'min_samples_leaf must be positive'\n        self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth\n        if isinstance(y, pd.Series): y = y.values\n        if idxs is None: idxs = np.arange(len(y))\n        self.X, self.y, self.idxs = X, y, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = np.mean(y[idxs]) # node's prediction value\n        self.best_score_so_far = float('inf') # initial loss before split finding\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n            \n    def _maybe_insert_child_nodes(self):\n        for j in range(self.c): \n            self._find_better_split(j)\n        if self.is_leaf: # do not insert children\n            return \n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[left_idx])\n        self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[right_idx])\n    \n    @property\n    def is_leaf(self): return self.best_score_so_far == float('inf')\n    \n    def _find_better_split(self, feature_idx):\n        x = self.X.values[self.idxs,feature_idx]\n        y = self.y[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_y, sort_x = y[sort_idx], x[sort_idx]\n        sum_y, n = y.sum(), len(y)\n        sum_y_right, n_right = sum_y, n\n        sum_y_left, n_left = 0., 0\n    \n        for i in range(0, self.n - self.min_samples_leaf):\n            y_i, x_i, x_i_next = sort_y[i], sort_x[i], sort_x[i + 1]\n            sum_y_left += y_i; sum_y_right -= y_i\n            n_left += 1; n_right -= 1\n            if  n_left &lt; self.min_samples_leaf or x_i == x_i_next:\n                continue\n            score = - sum_y_left**2 / n_left - sum_y_right**2 / n_right + sum_y**2 / n\n            if score &lt; self.best_score_so_far:\n                self.best_score_so_far = score\n                self.split_feature_idx = feature_idx\n                self.threshold = (x_i + x_i_next) / 2\n                \n    def __repr__(self):\n        s = f'n: {self.n}'\n        s += f'; value:{self.value:0.2f}'\n        if not self.is_leaf:\n            split_feature_name = self.X.columns[self.split_feature_idx]\n            s += f'; split: {split_feature_name} &lt;= {self.threshold:0.3f}'\n        return s\n    \n    def predict(self, X):\n        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n    \n    def _predict_row(self, row):\n        if self.is_leaf: \n            return self.value\n        child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n                else self.right\n        return child._predict_row(row)"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#from-scratch-versus-scikit-learn",
    "href": "posts/decision-tree-from-scratch/index.html#from-scratch-versus-scikit-learn",
    "title": "Decision Tree from Scratch",
    "section": "From Scratch versus Scikit-Learn",
    "text": "From Scratch versus Scikit-Learn\nAs usual, we’ll test our homegrown handiwork by comparing it to the existing implementation in scikit-learn. First let’s train both models on the California Housing dataset which gives us 20k instances and 8 features to predict median house price by district.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\nX, y = fetch_california_housing(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmax_depth = 8\nmin_samples_leaf = 16\n\ntree = DecisionTree(X_train, y_train, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\npred = tree.predict(X_test)\n\nsk_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\nsk_tree.fit(X_train, y_train)\nsk_pred = sk_tree.predict(X_test)\n\nprint(f'from scratch MSE: {mean_squared_error(y_test, pred):0.4f}')\nprint(f'scikit-learn MSE: {mean_squared_error(y_test, sk_pred):0.4f}')\n\nfrom scratch MSE: 0.3988\nscikit-learn MSE: 0.3988\n\n\nWe get similar accuracy on a held-out test dataset.\nLet’s benchmark the two implementations on training time.\n\n%%time\nsk_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\nsk_tree.fit(X_train, y_train);\n\nCPU times: user 45.3 ms, sys: 555 µs, total: 45.8 ms\nWall time: 45.3 ms\n\n\nDecisionTreeRegressor(max_depth=8, min_samples_leaf=16)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=8, min_samples_leaf=16)\n\n\n\n%%time\ntree = DecisionTree(X_train, y_train, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n\nCPU times: user 624 ms, sys: 1.65 ms, total: 625 ms\nWall time: 625 ms\n\n\nWow, the scikit-learn implementation absolutely smoked us, training an order of magnitude faster. This is to be expected, since they implement split finding in cython, which generates compiled C code that can run much faster than our native python code. Maybe we can take a look at how to optimize python code with cython here on the blog one of these days."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#wrapping-up",
    "href": "posts/decision-tree-from-scratch/index.html#wrapping-up",
    "title": "Decision Tree from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHoly cow, we just implemented a decision tree using nothing but numpy. I hope you enjoyed the scratch build as much as I did, and I hope you got a little bit better at coding (I certainly did). That was actually way harder than I expected, but looking back at the finished product, it doesn’t seem so bad right? I almost thought we were going to get away with not implementing our own decision tree, but it turns out that this will be super helpful for us when it comes time to implement XGBoost from scratch."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#references",
    "href": "posts/decision-tree-from-scratch/index.html#references",
    "title": "Decision Tree from Scratch",
    "section": "References",
    "text": "References\nThis implementation is inspired and partially adapted from Jeremy Howard’s live coding of a Random Forest as part of the fastai ML course."
  },
  {
    "objectID": "posts/random-realizations-resurrected/index.html",
    "href": "posts/random-realizations-resurrected/index.html",
    "title": "Random Realizations Resurrected",
    "section": "",
    "text": "Christ the Redeemer towers into a vast blue Brazilian sky.!\n\n\nWell it’s been over a year since I posted anything here. You see, a lot has been going on here at the Random Realizations Remote Global Headquarters that has distracted from producing the high-quality data science content that you’re used to. Mostly I went on hiatus from work and started traveling, which turns out to be it’s own full time job. I had aspirations of writing more after leaving work, but of course, after leaving, I couldn’t be bothered to sit down at my laptop and type stuff about data science to yall. After all, life is bigger than that.\nWhen I finally felt like opening up my laptop, I was confronted with an email from the maintainers of fastpages, the open source content management system (CMS) I originally used to create this blog, notifying me that the project was being deprecated and that I would need to migrate my content to some other platform.\nBoo.\nThat didn’t sound like much fun, so I spent another few months ignoring the blog. But eventually, dear reader, I decided it was time to roll up my sleeves and get this blog thriving once again.\nOk so fastpages was going to be deprecated, and I needed to find a new CMS. My requirements were pretty simple: I wanted to write the blog posts with jupyter notebook, and I wanted to host the site on my own domain. Helpfully, the former maintainers of fastpages recommended an alternative CMS called Quarto which I had never heard of. Apparently I had been living under a rock because Quarto appears to be all the rage. Quarto’s website says it’s an open-source scientific and technical publishing system. I think it’s fair to think of it as a way to render plain text or source code from languages like python, R, and julia into a variety of different published formats like websites, books, or journal articles. It was developed by the good folks over at RStudio, and the project has a pretty active following over on github, so I think it’s less likely to suddenly disappear like fastpages.\nSo anyway, I’ve been migrating my content over into this new quarto universe.\nYou mayofficially consider this blog resurrected from the dead, because this is the first new post published after the migration. The site has a bit of a new look and feel, so I hope you like it. Do let me know in the comments if you find anything amiss with the new website. Otherwise we’ll just assume it’s fabulous.\nI’m working on a post about how to create a blog with quarto using jupyter and python, so you can too!\nSee you in more posts real soon! Love, Matt."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "",
    "text": "Tell me dear reader, who among us, while gazing in wonder at the improbably verdant aloe vera clinging to the windswept rock at Cape Point near the southern tip of Africa, hasn’t wondered: how the heck do gradient boosting trees implement multi-class classification? Today, we’ll unravel this mystery by reviewing the theory and implementing the algorithm for ourselves in python. Specifically, we’ll review the multi-class gradient boosting model originally described in Friedman’s classic Greedy Function Approximation paper, and we’ll implement components of the algorithm as we go along. Once we have all the pieces, we’ll write a python class for multi-class gradient boosting with a similar API to the scikit-learn GradientBoostingClassifier.\nIf you need a refresher on gradient boosting before diving in here, then start with my original gradient boosting from scratch post, which is the first installment in my ongoing series on gradient boosting."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-multi-class-gradient-boosting-classification-algorithm",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-multi-class-gradient-boosting-classification-algorithm",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "The multi-class gradient boosting classification algorithm",
    "text": "The multi-class gradient boosting classification algorithm\nFriedman describes the algorithm for training a multi-class classification gradient boosting model in Algorithm 6 of the classic Greedy Function Approximation paper. If you want a step-by-step walkthrough of the ideas in the paper, have a look at my post on the generalized gradient boosting algorithm. In high-level terms, the algorithm for multi-class gradient boosting is:\n\nSet the initial model predictions.\nRepeat the following for each boosting round.\n     Repeat the following for each class.\n          Compute the pseudo residuals.\n          Train a regression tree to predict the pseudo residuals.\n          Adjust the tree’s predicted values to optimize the objective function.\n          Add the new tree to the current composite model.\n\nLet’s take a look at the details for each of these steps."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#target-variable-encoding",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#target-variable-encoding",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Target variable encoding",
    "text": "Target variable encoding\nFollowing the convention in scikit-learn, when training a multi-class classifier, the target variable in the training dataset should be integer encoded so that the \\(K\\) distinct classes are mapped to the integers \\(0,1,\\dots,K-1\\). In the code for model training, however, it’s going to be more convenient to work with a one hot encoded representation of the target. Therefore we’ll start by writing an internal method to transform the target variable from integer encoding to one hot encoding. Remember that eventually we’ll write a class for our multi-class gradient boosting model, so I’ll write this function like a class method with a leading argument called self.\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import OneHotEncoder\n\ndef _one_hot_encode_labels(self, y):\n    if isinstance(y, pd.Series): y = y.values\n    ohe = OneHotEncoder()\n    y_ohe = ohe.fit_transform(y.reshape(-1, 1)).toarray()\n    return y_ohe\nThis code takes the integer-encoded target variable, makes sure it’s a numpy array, then uses cikit-learn’s one hot encoder to encode it as a 2D array with observations along the first axis and classes along the second axis. I tend to think of the one hot encoded output as a matrix with \\(n\\) rows (the number of observations in the training data) and \\(K\\) columns (the number of classes), although it’s technically not a matrix but rather a 2D array."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#model-predictions-in-raw-space-and-probability-space",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#model-predictions-in-raw-space-and-probability-space",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Model predictions in raw space and probability space",
    "text": "Model predictions in raw space and probability space\nIn amulti-class classification problem with \\(K\\) classes, the model prediction for a particular observation returns a list of \\(K\\) probabilities, one for each class. Essentially the model prediction is a conditional probability mass function for the discrete target variable, conditioned on the feature values.\nSo, we need a way to ensure that the model output is a valid probability mass function, i.e. each probability is in (0, 1) and the \\(K\\) class probabilities sum to 1. Analogous to logistic regression, we can accomplish this by using the model to first make a raw prediction which can be any real number, then using something like the inverse logit function to transform the raw model prediction into a number between 0 and 1 that can be interpreted as a probability. Again analogous to logistic regression, in the multi-class setting we use \\(K\\) different models, one for each class, to generate the raw predictions, then we transform the raw model predictions into probabilities using the softmax function,, which takes a length-\\(K\\) vector of real numbers as input and returns a probability mass function over \\(K\\) discrete classes.\nLet \\(\\{F_1(\\mathbf{x}),\\dots,F_K(\\mathbf{x})\\}=\\{F_k(\\mathbf{x})\\}_1^K\\) be the list of \\(K\\) raw model outputs, and let \\(\\{p_1(\\mathbf{x}),\\dots,p_K(\\mathbf{x})\\}=\\{p_k(\\mathbf{x})\\}_1^K\\) be the corresponding probability mass function over the \\(K\\) classes, then the softmax function is defined as\n\\[ p_k(\\mathbf{x}) = \\text{softmax}_k(\\{F_k(\\mathbf{x})\\}_1^K)\n    = \\frac{e^{F_k(\\mathbf{x})}}{\\sum_{l=1}^K e^{F_l(\\mathbf{x})}}\\]\nLet's implement an internal softmax method that transforms the raw predictions into probabilities.\ndef _softmax(self, raw_predictions):\n    numerator = np.exp(raw_predictions) \n    denominator = np.sum(np.exp(raw_predictions), axis=1).reshape(-1, 1)\n    return numerator / denominator"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#initial-model-predictions",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#initial-model-predictions",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Initial model predictions",
    "text": "Initial model predictions\nWe’re now ready to implement model training, starting with line 1 of the algorithm which sets the initial model predictions. In our code, we’ll keep the raw model predictions \\(\\{F_k(\\mathbf{x})\\}_1^K\\) for the \\(n\\) observations in the training dataset in a size \\(n \\times K\\) array called raw_predictions, and we’ll keep the corresponding probabilities \\(\\{p_k(\\mathbf{x})\\}_1^K\\) in another \\(n \\times K\\) array called probabilities. Perhaps the simplest reasonable initialization is to set the probabilities to \\(1/K\\), i.e. \\(p_k(\\mathbf{x})=1/K\\), which implies \\(F_k(\\mathbf{x})=0\\).\nWe’ll go ahead and create that one hot encoded representation of the target, then use it to set the right size for the model prediction arrays.\ny_ohe = self._one_hot_encode_labels(y)\nraw_predictions = np.zeros(shape=y_ohe.shape)\nprobabilities = self._softmax(raw_predictions)"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#boosting",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#boosting",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Boosting",
    "text": "Boosting\nLine 2 of the algorithm kicks off a loop to iteratively perform boosting rounds. Within each round, line 3 specifies that we iterate through each of the \\(K\\) classes, adding a new booster model for each class at each boosting round. We’ll keep all the boosters in a list called boosters, where each element is itself a list which we’ll call class_trees that contains the \\(K\\) trees we trained in a given boosting round. For each round and each class, we compute the pseudo residuals (negative gradients), train a decision tree to predict them, update the tree’s predicted values to optimize the overall objective function, then update the current raw and probability predictions before storing the new tree in that round’s list of class trees.\nself.boosters = []\nfor m in range(self.n_estimators):\n    class_trees = []\n    for k in range(self.n_classes):\n        negative_gradients = self._negative_gradients(y_ohe[:, k], probabilities[:, k])\n        hessians = self._hessians(probabilities[:, k])\n        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n        tree.fit(X, negative_gradients);\n        self._update_terminal_nodes(tree, X, negative_gradients, hessians)\n        raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n        probabilities = self._softmax(raw_predictions)\n        class_trees.append(tree)\n    self.boosters.append(class_trees)\nNext we’ll dive into the details of the pseudo residual computation and the adjustment to the tree booster predicted values."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#pseudo-residuals",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#pseudo-residuals",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Pseudo Residuals",
    "text": "Pseudo Residuals\nFor each observation in the training dataset, the pseudo residual is the negative gradient of the objective function with respect to the corresponding model prediction. The objective function for multi-class classification is the Multinomial Negative Log Likelihood. For a single observation, the objective is\n\\[ J(\\{ y_k, p_k(\\mathbf{x}) \\}_1^K) = -\\sum_{k=1}^K y_k \\log p_k(\\mathbf{x}) \\]\nWe can rewrite the objective in terms of our raw model output \\(F\\) like this.\n\\[ J(\\{ y_k, F_k(\\mathbf{x}) \\}_1^K) = -\\sum_{k=1}^K y_k \\log \\frac{e^{F_k(\\mathbf{x})}}{\\sum_{l=1}^K e^{F_l(\\mathbf{x})}}\\]\nThe negative gradient of the objective with respect to raw model prediction \\(F_k(\\mathbf{x}_i)\\) for training example \\(i\\) is given by\n\\[ r_{ik} = -J'(F_k(\\mathbf{x}_i)) = -\\left[ \\frac{\\partial J(\\{ y_{il}, F_l(\\mathbf{x_i})\\}_{l=1}^K)}{\\partial F_k(\\mathbf{x}_i) } \\right]\n=y_{ik} - p_{k}(\\mathbf{x}_i)\\]\nYou can take a look at the derivation if you’re curious how to work it out yourself. Note that this formula has a nice intuition. When \\(y_{ik}=1\\), if predicted probability \\(p_k(\\mathbf{x}_i)\\) is terrible and close to 0, then the pseudo residual will be positive, and the next boosting round will try to increase the predicted probability. Otherwise if the predicted probability is already good and close to 1, the pseudo residual will be close to 0 and the next boosting round won’t change the predicted probability very much.\nWe can easily implement an internal method to compute the negative gradients over the training dataset as follows.\ndef _negative_gradients(self, y_ohe, probabilities):\n    return y_ohe - probabilities"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#adjusting-the-trees-predicted-values",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#adjusting-the-trees-predicted-values",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Adjusting the trees’ predicted values",
    "text": "Adjusting the trees’ predicted values\nAfter training a regression tree to predict the pseudo residuals, we need to adjust the predicted values in its terminal nodes to optimize the overall objective function. In the Greedy Function Approximation paper, Friedman actually specifies finding the optimal value using a numerical optimization routine like line search. We could express that like\n\\[ v = \\text{argmin}_v \\sum_{i \\in t} J(y_{i}, F(\\mathbf{x}_i) + v) \\]\nwhere \\(t\\) is the set of samples falling into this terminal node.\nIn the scikit-learn implementation of gradient boosting classification, the authors instead use the approach from FHT00 which uses a single Newton descent step to approximate the optimal predicted value for each terminal node. See code and comments for the function _update_terminal_regions in the scikit-learn gradient boosting module. The updated value is computed like\n\\[ v = -\\frac{\\sum_{i \\in t} J'(F(\\mathbf{x}_i))}{\\sum_{i \\in t} J''(F(\\mathbf{x}_i))} \\]\nWe already found the first derivative of the objective, so we just need to calculate the second derivative.\n\\[ J''(F_k(\\mathbf{x}_i)) =\n\\left[ \\frac{\\partial J(\\{ y_{il}, F_l(\\mathbf{x_i})\\}_{l=1}^K)}{\\partial ^2 F_k(\\mathbf{x}_i) } \\right]\n= p_k(\\mathbf{x}_i) (1 - p_k(\\mathbf{x}_i))\n\\]\nHere’s the internal method to compute the second derivative .\ndef _hessians(self, probabilities): \n    return probabilities * (1 - probabilities)\nThen we can implement the internal method for updating the tree predicted values. I give more details about how to manually set scikit-learn’s decision tree predicted values in the post on gradient boosting with any loss function.\ndef _update_terminal_nodes(self, tree, X, negative_gradients, hessians):\n    '''Update the terminal node predicted values'''\n    # terminal node id's\n    leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n    # compute leaf for each sample in ``X``.\n    leaf_node_for_each_sample = tree.apply(X)\n    for leaf in leaf_nodes:\n        samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n        negative_gradients_in_leaf = negative_gradients.take(samples_in_this_leaf, axis=0)\n        hessians_in_leaf = hessians.take(samples_in_this_leaf, axis=0)\n        val = np.sum(negative_gradients_in_leaf) / np.sum(hessians_in_leaf)\n        tree.tree_.value[leaf, 0, 0] = val"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#prediction",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#prediction",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Prediction",
    "text": "Prediction\nAt inference time, the user supplies an X with multiple observations of the feature variables, and our model needs to issue a prediction for each observation. We’ll start by implementing the predict_proba method, which takes X as input and returns a length-\\(K\\) probability mass function for each observation in X. To do this, we’ll initialize the raw predictions with zeros, just as we did in training, and then for each class, we’ll loop through all the boosters, collecting their predictions on X, scaling by the learning rate, and summing them up. Finally, we use the softmax to transform raw predictions into the probabilities.\ndef predict_proba(self, X):\n    '''Generate probability predictions for the given input data.'''\n    raw_predictions =  np.zeros(shape=(X.shape[0], self.n_classes))\n    for k in range(self.n_classes):\n        for booster in self.boosters:\n            raw_predictions[:, k] +=self.learning_rate * booster[k].predict(X)\n    probabilities = self._softmax(raw_predictions)\n    return probabilities\nThen to get the predicted labels, we can use the predict_proba method to generate probabilities, simply returning the integer-encoded class label of the largest probability for each observation in X.\ndef predict(self, X):\n    '''Generate predicted labels (as integer-encoded array)'''\n    probabilities = self.predict_proba(X)\n    return np.argmax(probabilities, axis=1)"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-complete-multi-class-gradient-boosting-classification-model-implementation",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-complete-multi-class-gradient-boosting-classification-model-implementation",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "The complete multi-class gradient boosting classification model implementation",
    "text": "The complete multi-class gradient boosting classification model implementation\nNow we’re ready to implement a multi-class classification gradient boosting model class with public fit, predict_proba, and predict methods. We combine the components above into a fit method for model training, and we add the two prediction methods to complete the model’s functionality.\n\nimport numpy as np\nimport pandas as pd \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.preprocessing import OneHotEncoder\n\nclass GradientBoostingClassifierFromScratch():\n    '''Gradient Boosting Classifier from Scratch.\n    \n    Parameters\n    ----------\n    n_estimators : int\n        number of boosting rounds\n        \n    learning_rate : float\n        learning rate hyperparameter\n        \n    max_depth : int\n        maximum tree depth\n    '''\n    \n    def __init__(self, n_estimators, learning_rate=0.1, max_depth=1):\n        self.n_estimators=n_estimators; \n        self.learning_rate=learning_rate\n        self.max_depth=max_depth;\n    \n    def fit(self, X, y):\n        '''Fit the GBM\n        \n        Parameters\n        ----------\n        X : ndarray of size (number observations, number features)\n            design matrix\n            \n        y : ndarray of size (number observations,)\n            integer-encoded target labels in {0,1,...,k-1}\n        '''\n        \n        self.n_classes = pd.Series(y).nunique()\n        y_ohe = self._one_hot_encode_labels(y)\n\n        raw_predictions = np.zeros(shape=y_ohe.shape)\n        probabilities = self._softmax(raw_predictions)\n        self.boosters = []\n        for m in range(self.n_estimators):\n            class_trees = []\n            for k in range(self.n_classes):\n                negative_gradients = self._negative_gradients(y_ohe[:, k], probabilities[:, k])\n                hessians = self._hessians(probabilities[:, k])\n                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n                tree.fit(X, negative_gradients);\n                self._update_terminal_nodes(tree, X, negative_gradients, hessians)\n                raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n                probabilities = self._softmax(raw_predictions)\n                class_trees.append(tree)\n            self.boosters.append(class_trees)\n    \n    def _one_hot_encode_labels(self, y):\n        if isinstance(y, pd.Series): y = y.values\n        ohe = OneHotEncoder()\n        y_ohe = ohe.fit_transform(y.reshape(-1, 1)).toarray()\n        return y_ohe\n        \n    def _negative_gradients(self, y_ohe, probabilities):\n        return y_ohe - probabilities\n    \n    def _hessians(self, probabilities): \n        return probabilities * (1 - probabilities)\n\n    def _softmax(self, raw_predictions):\n        numerator = np.exp(raw_predictions) \n        denominator = np.sum(np.exp(raw_predictions), axis=1).reshape(-1, 1)\n        return numerator / denominator\n        \n    def _update_terminal_nodes(self, tree, X, negative_gradients, hessians):\n        '''Update the terminal node predicted values'''\n        # terminal node id's\n        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n        # compute leaf for each sample in ``X``.\n        leaf_node_for_each_sample = tree.apply(X)\n        for leaf in leaf_nodes:\n            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n            negative_gradients_in_leaf = negative_gradients.take(samples_in_this_leaf, axis=0)\n            hessians_in_leaf = hessians.take(samples_in_this_leaf, axis=0)\n            val = np.sum(negative_gradients_in_leaf) / np.sum(hessians_in_leaf)\n            tree.tree_.value[leaf, 0, 0] = val\n          \n    def predict_proba(self, X):\n        '''Generate probability predictions for the given input data.'''\n        raw_predictions =  np.zeros(shape=(X.shape[0], self.n_classes))\n        for k in range(self.n_classes):\n            for booster in self.boosters:\n                raw_predictions[:, k] +=self.learning_rate * booster[k].predict(X)\n        probabilities = self._softmax(raw_predictions)\n        return probabilities\n        \n    def predict(self, X):\n        '''Generate predicted labels (as 1-d array)'''\n        probabilities = self.predict_proba(X)\n        return np.argmax(probabilities, axis=1)"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#testing-our-implementation",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#testing-our-implementation",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Testing our implementation",
    "text": "Testing our implementation\nLet’s test our implementation alongside the scikit-learn GradientBoostingClassifier to ensure it works as expected.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX, y = make_classification(n_samples=10000, \n                           n_classes=5, \n                           n_features=20,\n                           n_informative=10,\n                           random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(n_estimators=10, \n                                 learning_rate=0.3, \n                                 max_depth=6)\ngbc.fit(X_train, y_train)\naccuracy_score(y_test, gbc.predict(X_test))\n\n0.7756\n\n\n\ngbcfs = GradientBoostingClassifierFromScratch(n_estimators=10, \n                                              learning_rate=0.3, \n                                              max_depth=6)\ngbcfs.fit(X_train, y_train)\naccuracy_score(y_test, gbcfs.predict(X_test))\n\n0.7768\n\n\nBeautiful. Our implementation is performing comparably to the sklearn gradient boosting classifier!"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#wrapping-up",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#wrapping-up",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, there you have it, another epic scratch build for the books. I think the most interesting thing about the multi-class gradient boosting algorithm is that it generates multi-dimensional predictions based on a single objective function by training multiple decision trees in each boosting round. That’s a very interesting extension of the classic gradient boosting machine! If you have questions about the implementation, or if you found this post helpful, please leave a comment below to tell me about it."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#references",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#references",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "References",
    "text": "References\n\nFriedman’s Greedy Function Approximation paper\nFriedman, Hastie, and Tibshirani 2000: paper on additive logistic regression"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "The Ultimate Guide to XGBoost Parameter Tuning\n\n\n\n\n\n\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n\n\nXGBoost for Binary and Multi-Class Classification in Python\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\n\n\n\n\n\n\nGradient Boosting Multi-Class Classification from Scratch\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\nXGBoost for Regression in Python\n\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\n\n\n\n\n\n\nBlogging with Quarto and Jupyter: The Complete Guide\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\nRandom Realizations Resurrected\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\n\n\n\n\n\n\nXGBoost from Scratch\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n\n\nXGBoost Explained\n\n\n\n\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n\n\nDecision Tree from Scratch\n\n\n\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\nConsider the Decision Tree\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n\n\n\n\n\n\nHow to Implement a Gradient Boosting Machine that Works with Any Loss Function\n\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\n\nHello PySpark!\n\n\n\n\n\n\n\n\n\nJun 22, 2021\n\n\n\n\n\n\n\n\nHow Gradient Boosting Does Gradient Descent\n\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n\n\nGet Down with Gradient Descent\n\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n\n\nHow to Build a Gradient Boosting Machine from Scratch\n\n\n\n\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n\n\nThe 80/20 Pandas Tutorial\n\n\n\n\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n\n\nHello World! And Why I’m Inspired to Start a Blog\n\n\n\n\n\n\n\n\n\nNov 22, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Random Realizations! This blog is a celebration of the fascinating world of data science. I hope you find the content useful, and I hope you enjoy reading along and learning with me!\nAnd me? Well, I’m Matt Bowers. I’m an ex data scientist at Uber where I solved problems using statistical modeling and machine learning. Over the years I’ve driven product decisions through controlled experimentation and user analytics, built ML products like ETA prediction on large-scale telematics data, and cracked long-standing ML problems in marketplace pricing. Before Uber I earned an MS in Applied Statistics and a PhD in Atmospheric Sciences at Purdue University where I studied the spatial-temporal correlation structure of tropical convection systems and developed statistical methods for climate research.\nBut enough about me. Let’s get down with some data science!"
  }
]