[
  {
    "objectID": "gradient-boosting-series.html",
    "href": "gradient-boosting-series.html",
    "title": "Gradient Boosting",
    "section": "",
    "text": "Ahh, gradient boosting. In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. This series of posts strives to give a comprehensive understanding of gradient boosting by providing intuitive mathematical explanations, from-scratch implementations of key algorithms, and examples of how to apply modern gradient boosting libraries to solve practical data science problems.\nI recommend reading through the series in order, since concepts tend to build on earlier ideas.\n\n\n\n\n\n\n\n\n\n\nHow to Build a Gradient Boosting Machine from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nUnderstand the intuition behind the gradient boosting machine (GBM) and learn how to implement it from scratch.\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nGet Down with Gradient Descent\n\n\n\n\n\n\ngradient boosting\n\n\n\nGet down with the intuition for gradient descent via a fresh analogy, develop the mathematical formulation of the algorithm, and implement it from scratch to train a linear regression model.\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow Gradient Boosting Does Gradient Descent\n\n\n\n\n\n\ngradient boosting\n\n\n\nUnderstand how gradient boosting does gradient descent in function space to minimize any differentiable loss function in the service of creating a good model.\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Implement a Gradient Boosting Machine that Works with Any Loss Function\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nSummarize Friedman‚Äôs seminal GBM paper and implement the generic gradient boosting algorithm to train models with any differentiable loss function.\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the Decision Tree\n\n\n\n\n\n\ngradient boosting\n\n\n\nUnderstand the core strengths and weaknesses of the decision tree, and see how ensembling makes trees shine.\n\n\n\n\n\nDec 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Tree from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nA detailed walkthrough of my from-scratch decision tree implementation in python.\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost Explained\n\n\n\n\n\n\ngradient boosting\n\n\nxgboost\n\n\n\nIn-depth explanation and mathematical derivation of the XGBoost algorithm\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nxgboost\n\n\nfrom scratch\n\n\n\nA walkthrough of my from-scratch python implementation of XGBoost.\n\n\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost for Regression in Python\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\nA step-bystep tutorial on regression with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\nSep 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting Multi-Class Classification from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nHow to implement multi-class classification for gradient boosting from scratch in python\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost for Binary and Multi-Class Classification in Python\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\nA step-bystep tutorial on binary and multi-class classification with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\nNov 28, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Logistic Regression with PyTorch\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\npytorch\n\n\n\nWe build a logistic regression model in PyTorch and modify the model to create a deep neural network.\n\n\n\n\n\nJul 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Modeling Primer\n\n\n\n\n\n\nbayesian\n\n\npython\n\n\n\nAn intuitive introduction to the Bayesian statistical workflow including modeling, inference, and interpretation of results.\n\n\n\n\n\nJun 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing After Tax Retirement Income: Roth vs.¬†Traditional 401(k)\n\n\n\n\n\n\npersonal finance\n\n\n\nQuantitative analysis of Roth vs.¬†traditional 401(k)‚Äîwhich is better for you?\n\n\n\n\n\nDec 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSHAP from Scratch\n\n\n\n\n\n\npython\n\n\nfrom scratch\n\n\n\nHow to compute SHAP values from scratch in python\n\n\n\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Ultimate Guide to XGBoost Parameter Tuning\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\nMy approach for efficiently tuning XGBoost parameters with optuna in python\n\n\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost for Binary and Multi-Class Classification in Python\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\nA step-bystep tutorial on binary and multi-class classification with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\nNov 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting Multi-Class Classification from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nHow to implement multi-class classification for gradient boosting from scratch in python\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost for Regression in Python\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\ngradient boosting\n\n\nxgboost\n\n\n\nA step-bystep tutorial on regression with XGBoost in python using sklearn and the xgboost library\n\n\n\n\n\nSep 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBlogging with Quarto and Jupyter: The Complete Guide\n\n\n\n\n\n\npython\n\n\ntutorial\n\n\nblogging\n\n\n\nStep-by-step tutorial and best practices for creating a python blog with quarto and jupyter\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Realizations Resurrected\n\n\n\n\n\n\nblogging\n\n\n\nThe world‚Äôs favorite data science blog is back.\n\n\n\n\n\nAug 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nxgboost\n\n\nfrom scratch\n\n\n\nA walkthrough of my from-scratch python implementation of XGBoost.\n\n\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost Explained\n\n\n\n\n\n\ngradient boosting\n\n\nxgboost\n\n\n\nIn-depth explanation and mathematical derivation of the XGBoost algorithm\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Tree from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nA detailed walkthrough of my from-scratch decision tree implementation in python.\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nConsider the Decision Tree\n\n\n\n\n\n\ngradient boosting\n\n\n\nUnderstand the core strengths and weaknesses of the decision tree, and see how ensembling makes trees shine.\n\n\n\n\n\nDec 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Implement a Gradient Boosting Machine that Works with Any Loss Function\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nSummarize Friedman‚Äôs seminal GBM paper and implement the generic gradient boosting algorithm to train models with any differentiable loss function.\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHello PySpark!\n\n\n\n\n\n\npython\n\n\nPySpark\n\n\ntutorial\n\n\n\nGet up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale.\n\n\n\n\n\nJun 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow Gradient Boosting Does Gradient Descent\n\n\n\n\n\n\ngradient boosting\n\n\n\nUnderstand how gradient boosting does gradient descent in function space to minimize any differentiable loss function in the service of creating a good model.\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGet Down with Gradient Descent\n\n\n\n\n\n\ngradient boosting\n\n\n\nGet down with the intuition for gradient descent via a fresh analogy, develop the mathematical formulation of the algorithm, and implement it from scratch to train a linear regression model.\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a Gradient Boosting Machine from Scratch\n\n\n\n\n\n\npython\n\n\ngradient boosting\n\n\nfrom scratch\n\n\n\nUnderstand the intuition behind the gradient boosting machine (GBM) and learn how to implement it from scratch.\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThe 80/20 Pandas Tutorial\n\n\n\n\n\n\npython\n\n\npandas\n\n\ntutorial\n\n\n\nAn opinionated pandas tutorial on my preferred methods to accomplish the most essential data transformation tasks in a way that will make veteran R and tidyverse users smile.\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nHello World! And Why I‚Äôm Inspired to Start a Blog\n\n\n\n\n\n\nblogging\n\n\n\nA reflection on what inspired me to start a blog and three reasons I think it could be a good idea.\n\n\n\n\n\nNov 22, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/xgboost-explained/index.html",
    "href": "posts/xgboost-explained/index.html",
    "title": "XGBoost Explained",
    "section": "",
    "text": "Tree branches on a chilly day in Johnson City\nAhh, XGBoost, what an absolutely stellar implementation of gradient boosting. Once Tianqi Chen and Carlos Guestrin of the University of Washington published the XGBoost paper and shared the open source code in the mid 2010‚Äôs, the algorithm quickly gained adoption in the ML community, appearing in over half of winning Kagle submissions in 2015. Nowadays it‚Äôs certainly among the most popular gradient boosting libraries, along with LightGBM and CatBoost, although the highly scientific indicator of GitHub stars per year indicates that it is in fact the most beloved gradient boosting package of all. Since it was the first of the modern popular boosting frameworks, and since benchmarking indicates that no other boosting algorithm outperforms it, we can comfortably focus our attention on understanding XGBoost.\nThe XGBoost authors identify two key aspects of a machine learning system: (1) a flexible statistical model and (2) a scalable learning system to fit that model using data. XGBoost improves on both of these aspects, providing a more flexible and feature-rich statistical model and building a truly scalable system to fit it. In this post we‚Äôre going to focus on the statistical modeling innovations, outlining the key differences from the classic gradient boosting machine and divinginto the mathematical derivation of the XGBoost learning algorithm. If you‚Äôre not already familiar with gradient boosting, go back and read the earlier posts in the series before jumping in here.\nBuckle up, dear reader. Today we understand how XGBoost works, no hand waving required."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#xgboost-is-a-gradient-boosting-machine",
    "href": "posts/xgboost-explained/index.html#xgboost-is-a-gradient-boosting-machine",
    "title": "XGBoost Explained",
    "section": "XGBoost is a Gradient Boosting Machine",
    "text": "XGBoost is a Gradient Boosting Machine\nAt a high level, XGBoost is an iteratively constructed composite model, just like the classic gradient boosting machine we discussed back in the GBM post . The final model takes the form\n\\[\\hat{y}_i = F(\\mathbf{x}_i) = b + \\eta \\sum_{k=1}^K f_k(\\mathbf{x}_i) \\]\nwhere \\(b\\) is the base prediction, \\(\\eta\\) is the learning rate hyperparameter that helps control overfitting by reducing the contributions of each booster, and each of the \\(K\\) boosters \\(f_k\\) is a decision tree. To help us connect the dots between theory and code, whenever we encounter new hyperparameters, I‚Äôll point out their names from the XGBoost Parameter Documentation. So, \\(b\\) can be set by base_score, and \\(\\eta\\) is set by either eta or learning_rate.\nXGBoost introduces two key statistical learning improvements over the classic gradient boosting model. First, it reimagines the gradient descent algorithm used for training, and second it uses a custom-built decision tree with extra functionality as its booster. We‚Äôll dive into each of these key innovations in the following sections."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#descent-algorithm-innovations",
    "href": "posts/xgboost-explained/index.html#descent-algorithm-innovations",
    "title": "XGBoost Explained",
    "section": "Descent Algorithm Innovations",
    "text": "Descent Algorithm Innovations\n\nRegularized Objective Function\nIn the post on GBM with any loss function, we looked at loss functions of the form \\(\\sum_i l(y_i,\\hat{y}_i)\\) which compute some distance between targets \\(y_i\\) and predictions \\(\\hat{y}_i\\) and sum them up over the training dataset. XGBoost introduces regularization into the objective function so that the objective takes the form\n\\[ L = \\sum_i l(y_i,\\hat{y}_i) + \\sum_k \\Omega(f_k) \\]\nwhere \\(l\\) is some twice-differentiable loss function. \\(\\Omega\\) is a regularization that penalizes the complexity of each tree booster, taking the form\n\\[ \\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda ||w||^2 \\]\nwhere \\(T\\) is the number of leaf nodes and \\(||w||^2\\) is the squared sum of the leaf prediction values. This introduces two new hyperparameters: \\(\\gamma\\) which penalizes the number of leaf nodes and \\(\\lambda\\) which is the L-2 regularization parameter for leaf predicted values. These are set by gamma and reg_lambbda in the XGBoost parametrization. Together, these provide powerful new controls to reduce overfitting due to overly complex tree boosters. Note that \\(\\gamma=\\lambda=0\\) reduces the objective back to an unregularized loss function as used in the classic GBM.\n\n\nAn Aside on Newton‚Äôs Method\nAs we‚Äôll see soon, XGBoost uses Newton‚Äôs Method to minimize its objective function, so let‚Äôs start with a quick refresher.\nNewton‚Äôs method is an iterative procedure for minimizing a function \\(s(x)\\). At each step we have some input \\(x_t\\), and our goal is to find a nudge value \\(u\\) such that\n\\[ s(x_t + u) \\le s(x_t)\\]\nTo find a good nudge value \\(u\\), we generate a local quadratic approximation of the function in the neighborhood of the input \\(x_t\\), and then we find the input value that would bring us to the minimum of the quadratic approximation.\n\n\n\nSchematic of Newton‚Äôs method\n\n\nThe figure shows a single Newton step where we start at \\(x_t\\), find the local quadratic approximation, and then jump a distance \\(u\\) along the \\(x\\)-axis to land at the minimum of the quadratic. If we iterate in this way, we are likely to land close to the minimum of \\(s(x)\\).\nSo how do we compute the quadratic approximation? We use the second order Taylor series expansion of \\(s(x)\\) near the point \\(x_t\\).\n\\[ s(x_t + u) \\approx  s(x_t) + s'(x_t)u + \\frac{1}{2} s''(x_t) u^2 \\]\nTo find the nudge value \\(u\\) that minimizes the quadratic approximation, we can take the derivative with respect to \\(u\\), set it to zero, and solve for \\(u\\).\n\\[ 0 = \\frac{d}{du}  \\left ( s(x_t) + s'(x_t)u + \\frac{1}{2} s''(x_t) u^2 \\right ) = s'(x_t) + s''(x_t) u \\]\n\\[\\rightarrow u^* = -\\frac{s'(x_t)}{s''(x_t)} \\]\nAnd as long as \\(s''(x_t)&gt;0\\) (i.e., the parabola is pointing up), \\(s(x_t + u^*) \\le s(x_t)\\).\n\n\nTree Boosting with Newton‚Äôs Method\nThis lands us at the heart of XGBoost, which uses Newton‚Äôs method, rather than gradient descent, to guide each round of boosting. This explanation will correspond very closely to section 2.2 of the XGBoost paper, but here I‚Äôll explicitly spell out some of the intermediate steps which are omitted from their derivation, and you‚Äôll get some additional commentary from me along the way.\n\nNewton Descent in Tree Space\nSuppose we‚Äôve done \\(t-1\\) boosting rounds, and we want to add the \\(t\\)-th booster to our composite model. Our current model‚Äôs prediction for instance \\(i\\) is \\(\\hat{y}_i^{(t-1)}\\). If we add a new tree booster \\(f_t\\) to our model, the objective function would give\n\\[ L^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) + \\Omega(f_t) \\]\nWe need to choose \\(f_t\\) so that it decreases the loss, i.e.¬†we want\n\\[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) \\le l(y_i, \\hat{y}_i^{(t-1)})\\]\nDoes that sound familiar? In the previous section we used Newton‚Äôs method to find a value of \\(u\\) that would make \\(s(x_t + u) \\le s(x_t)\\). Let‚Äôs try the same thing with our loss function. To be explicit, the parallels are: \\(s(\\cdot) \\rightarrow l(y_i, \\cdot)\\), \\(x_t \\rightarrow \\hat{y}_i^{(t-1)}\\), and \\(u \\rightarrow f_t(\\mathbf{x}_i)\\).\nLet‚Äôs start by finding the second order Taylor series approximation for the loss around the point \\(\\hat{y}_i^{(t-1)}\\).\n\\[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(\\mathbf{x}_i)) \\approx l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t(\\mathbf{x}_i)^2 \\]\nwhere\n\\[ g_i = \\frac{\\partial}{\\partial \\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)})\\]\nand\n\\[ h_i = \\frac{\\partial}{\\partial^2 \\hat{y}_i^{(t-1)}} l(y_i, \\hat{y}_i^{(t-1)})\\]\nare the first and second order partial derivatives of the loss with respect to the current predictions. The XGBoost paper calls these the gradients and hessians, respectively. Remember that when we specify an actual loss function to use, we would also specify the functional form of the gradients and hessians, so that they are directly computable.\nNow we can go back and substitute our quadratic approximation in for the loss function to get an approximation of the objective function in the neighborhood of \\(\\hat{y}_i^{(t-1)}\\)..\n\\[ L^{(t)} \\approx \\sum_{i=1}^n [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t(\\mathbf{x}_i)^2] + \\Omega(f_t) \\]\nSince \\(l(y_i,\\hat{y}_i^{(t-1)})\\) is constant regardless of our choice of \\(f_t\\), we can drop it and instead work with the modified objective, which gives us Equation (3) from the paper.\n\\[ \\tilde{L}^{(t)} = \\sum_{i=1}^n [ g_i f_t(\\mathbf{x}_i) + \\frac{1}{2} h_i f_t(\\mathbf{x}_i)^2] + \\Omega(f_t) \\]\nNow the authors are about to do something great. They‚Äôre about to show how to directly compute the optimal prediction values for the leaf nodes of \\(f_t\\). We‚Äôll circle back in a moment about how we find a good structure for \\(f_t\\), i.e.¬†good node splits, but we‚Äôre going to find the optimal predicted values for any tree structure having \\(T\\) terminal nodes. Let \\(I_j\\) denote the set of instances \\(i\\) that are in the \\(j\\)-th leaf node of \\(f_t\\). Then we can rewrite the objective.\n\\[ \\tilde{L}^{(t)} = \\sum_{j=1}^T \\left [ \\sum_{i \\in I_j} g_i f_t(\\mathbf{x}_i) + \\frac{1}{2}  \\sum_{i \\in I_j} h_i f_t(\\mathbf{x}_i)^2 \\right ] + \\Omega(f_t)\\]\nWe notice that for all instances in \\(I_j\\), the tree yields the same predicted value \\(f_t(\\mathbf{x}_i)=w_j\\). Substituting in \\(w_j\\) for the predicted values and expanding \\(\\Omega(f_t)\\) we get\n\\[ \\tilde{L}^{(t)} = \\sum_{j=1}^T \\left [ \\sum_{i \\in I_j} g_i w_j + \\frac{1}{2}  \\sum_{i \\in I_j} h_i w_j^2 \\right ] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2\\]\nRearranging terms we obtain Equation (4).\n\\[ \\tilde{L}^{(t)} = \\sum_{j=1}^T \\left [ w_j \\sum_{i \\in I_j} g_i + \\frac{1}{2}  w_j^2 \\left ( \\sum_{i \\in I_j} h_i + \\lambda \\right )  \\right ] + \\gamma T \\]\nFor each leaf node \\(j\\), our modified objective function is quadratic in \\(w_j\\). To find the optimal predicted values we take the derivative, set to zero, and solve for \\(w_j\\).\n\\[ 0 = \\frac{d}{dw_j} \\left [ w_j \\sum_{i \\in I_j} g_i + \\frac{1}{2}  w_j^2 \\left ( \\sum_{i \\in I_j} h_i + \\lambda \\right )  \\right ] = \\left ( \\sum_{i \\in I_j} h_i + \\lambda \\right ) w_j + \\sum_{i \\in I_j} g_i \\]\nThis yields Equation (5).\n\\[ w_j^* = - \\frac{\\sum_{i \\in I_j} g_i } {\\sum_{i \\in I_j} h_i + \\lambda } \\]\n\n\nSplit Finding\nNow that we know how to find the optimal predicted value for any leaf node, we need to identify a criterion for finding a good tree structure, which boils down to finding the best split for a given node. Back in the [decision tree from scratch](/decision-tree-from-scratch post, we derived a split evaluation metric based on the reduction in the objective function associated with a particular split.\nTo do that, first we need a way to compute the objective function given a particular tree structure. Substituting the optimal predicted values \\(w_j^*\\) into the objective function, we get Equation (6).\n\\[ \\tilde{L}^{(t)} = -\\frac{1}{2} \\sum_{j=1}^T \\frac{ (\\sum_{i \\in I_j} g_i )^2 } {\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T \\]\nWe can then evaluate potential splits by comparing the objective before making a split to the objective after making a split, where the split with the maximum reduction in objective (a.k.a. gain) is best.\nMore formally, let \\(I\\) be the set of \\(n\\) data instances in the current node, and let \\(I_L\\) and \\(I_R\\) be the instances that fall into the left and right child nodes of a proposed split. Let \\(L\\) be the total loss for all instances in the node, while \\(L_L\\) and \\(L_R\\) are the losses for the left and right child nodes. The total loss contributed by instances in node \\(I\\) prior to any split is\n\\[L_{\\text{before split}} = -\\frac{1}{2} \\frac{ (\\sum_{i \\in I} g_i )^2 } {\\sum_{i \\in I} h_i + \\lambda} + \\gamma \\]\nAnd the loss after splitting \\(I\\) into \\(I_L\\) and \\(I_R\\) is\n\\[L_{\\text{after split}} = L_L + L_R = -\\frac{1}{2}  \\frac{ (\\sum_{i \\in I_L} g_i )^2 } {\\sum_{i \\in I_L} h_i + \\lambda} -\\frac{1}{2}  \\frac{ (\\sum_{i \\in I_R} g_i )^2 } {\\sum_{i \\in I_R} h_i + \\lambda} + 2 \\gamma \\]\nThe gain from this split is then\n\\[ \\Delta L = L_{\\text{before split}} -  L_{\\text{after split}} = L - (L_L + L_R)\\] \\[\\Delta L = \\frac{1}{2} \\left [ \\frac{ (\\sum_{i \\in I_L} g_i )^2 } {\\sum_{i \\in I_L} h_i + \\lambda} + \\frac{ (\\sum_{i \\in I_R} g_i )^2 } {\\sum_{i \\in I_R} h_i + \\lambda} - \\frac{ (\\sum_{i \\in I} g_i )^2 } {\\sum_{i \\in I} h_i + \\lambda} \\right ] - \\gamma \\]\nwhich is Equation (7) from the paper. In practice it makes sense to accept a split only if the gain is positive, thus the \\(\\gamma\\) parameter sets the minimum gain required to make a further split. This is why \\(\\gamma\\) can be set with the parameter gamma or the more descriptivemin_loss_split."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#tree-booster-innovations",
    "href": "posts/xgboost-explained/index.html#tree-booster-innovations",
    "title": "XGBoost Explained",
    "section": "Tree Booster Innovations",
    "text": "Tree Booster Innovations\n\nMissing Values and Sparsity-Aware Split Finding\nThe XGBoost paper also introduces a modified algorithm for tree split finding which explicitly handles missing feature values. Recall that in order to find the best threshold value for a given feature, we can simply try all possible threshold values, recording the score for each. If some feature values are missing, the XGBoost split finding algorithm simply scores each threshold twice: once with missing value instances in the left node and once with them in the right node. The best split will then specify both the threshold value and to which node instances with missing values should be assigned. The paper calls this the sparsity aware split finding routine, which is defined as Algorithm 2.\n\n\nPreventing Further Splitting\nIn addition to min_loss_split discussed above, XGBoost offers another parameter for limiting further tree splitting called min_child_weight. This name is a little confusing to me because the word ‚Äúweight‚Äù has various meanings. In the context of this parameter, ‚Äúweight‚Äù refers to the sum of the hessians \\(\\sum h_i\\) over instances in the node. For squared error loss \\(h_i=1\\), so this is equivalent to the number of samples. Thus this parameter generalizes the notion of the minimum number of samples allowed in a terminal node.\n\n\nSampling\nXGBoost takes a cue from Random Forest and introduces both column and row subsampling. These sampling methods can prevent overfitting and reduce training time by limiting the amount of data to be processed during boosting.\nLike random forest, XGBoost implements column subsampling, which limits tree split finding to randomly selected subsets of features. XGBoost provides column sampling for each tree, for each depth level within a tree, and for each split point within a tree, controlled by colsample_bytree, colsample_bbylevel, and colsample_bbynode respectively.\nOne interesting distinction is that XGBoost implements row sampling without replacement using subbsample, whereas random forest uses bootstrapping. The choice to bootstrap rows in RF probably spurred from a desire to use as much data as possible while training on the smaller datasets of the 1990‚Äôs when RF was developed. With larger datasets and the ability to generate a large number of trees, XGBoost simply takes a subsample of rows for each tree."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#scalability",
    "href": "posts/xgboost-explained/index.html#scalability",
    "title": "XGBoost Explained",
    "section": "Scalability",
    "text": "Scalability\nEven though we‚Äôre focused on statistical learning, I figured I‚Äôd comment on why XGBoost is highly scalable. Basically it boils down to efficient, parallelizable, and distributable methods for growing trees. You‚Äôll notice there is a tree_method parameter which allows you to choose between the greedy exact algorithm (like the one we discussed in the decision tree from scratch post) and the approximate algorithm, which offers various scalability-related functionality, notably including the ability to consider only a small number of candidate split points instead of trying all possible splits. The algorithm also uses clever tricks like pre-sorting data for split finding and caching frequently needed values.\n\nWhy XGBoost is so Successful\nAs I mentioned in the intro, XGBoost is simply a very good implementation of the gradient boosting tree model. Therefore it inherits all the benefits of decision trees and tree ensembles, while making even further improvements over the classic gradient boosting machine. These improvements boil down to\n\nmore ways to control overfitting\nelegant handling of custom objectives\nscalability\n\nFirst, XGBoost introduces two new tree regularization hyperparameters \\(\\gamma\\) and \\(\\lambda\\) which are baked directly into its objective function. Combining these with the additional column and row sampling functionality provides a variety of ways to reduce overfitting.\nSecond, the XGBoost formulation provides a much more elegant way to train models on custom objective functions. Recall that for custom objectives, the classic GBM finds tree structure by fitting a squared error decision tree to the gradients of the loss function and then sets each leaf‚Äôs predicted value by running a numerical optimization routine to find the optimal predicted value.\nThe XGBoost formulation improves on this two-stage approach by unifying the generation of tree structure and predicted values. Both the split scoring metric and the predicted values are directly computable from the instance gradient and hessian values, which are connected directly back to the overall training objective. This also removes the need for additional numerical optimizations, which contributes to speed, stability, and scalability.\nFinally, speaking of scalability, XGBoost emerged at a time when industrial dataset size was exploding. Many use cases require scalable ML systems, and all use cases benefit from faster training and higher model development velocity."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#wrapping-up",
    "href": "posts/xgboost-explained/index.html#wrapping-up",
    "title": "XGBoost Explained",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, there you go, those are the salient ideas behind XGBoost, the gold standard in gradient boosting model implementations. Hopefully now we all understand the mathematical basis for the algorithm and appreciate the key improvements it makes over the classic GBM. If you want to go even deeper, you can join us for the next post where we‚Äôll roll up our sleeves and implement XGBoost entirely from scratch."
  },
  {
    "objectID": "posts/xgboost-explained/index.html#references",
    "href": "posts/xgboost-explained/index.html#references",
    "title": "XGBoost Explained",
    "section": "References",
    "text": "References\nThe XGBoost paper"
  },
  {
    "objectID": "posts/xgboost-explained/index.html#exercise",
    "href": "posts/xgboost-explained/index.html#exercise",
    "title": "XGBoost Explained",
    "section": "Exercise",
    "text": "Exercise\nProove that the XGBoost Newton Descent generalizes the classic GBM gradient descent. Hint: show that XGBoost with a squared error objective and no regularization reduces to the classic GBM."
  },
  {
    "objectID": "posts/bayesian-modeling-primer/index.html",
    "href": "posts/bayesian-modeling-primer/index.html",
    "title": "Bayesian Modeling Primer",
    "section": "",
    "text": "Well, dear reader, I know I haven‚Äôt been posting very much lately. That‚Äôs because I‚Äôve been busy moving to a new city and working a new DS gig and learning some new things, including Bayesian modeling. In particular I‚Äôve been reading Richard McElreath‚Äôs excellent book Statistical Rethinking, which I recommend to you as well. As a dedicated reader of this blog, I‚Äôm sure you‚Äôre perfectly capable of digesting a 600 page statistics textbook on your own, but just for fun, today I present to you my Bayesian statistics crash course.\nMy primary goal is to illuminate the major steps in the Bayesian workflow, that way you have a mental framework where you can store and contextualize new pieces of information as you learn. My secondary goal is to give you an intuitive understanding of Bayesian modeling from two interconnected perspectives: a mathematical formulation based primarily in probability theory and a probabilistic programming approach based on writing code to generate random data. Each perspective supports the other, and they are both necessary to grasp the full picture. I will attempt to weave these two perspectives throughout the description of the workflow, which is motivated by a toy example we‚Äôll use throughout the post.\nLet‚Äôs do this! ‚û°Ô∏è"
  },
  {
    "objectID": "posts/bayesian-modeling-primer/index.html#the-rock-paper-scissors-pro",
    "href": "posts/bayesian-modeling-primer/index.html#the-rock-paper-scissors-pro",
    "title": "Bayesian Modeling Primer",
    "section": "ü™®üìÑ‚úÇÔ∏è The Rock Paper Scissors Pro",
    "text": "ü™®üìÑ‚úÇÔ∏è The Rock Paper Scissors Pro\nI spent a summer as an intern at RAND Corporation during my PhD. It was a fascinating place full of fascinating characters. One of the researchers, Fritz R, liked to take each cohort of interns out for drinks at some point in the summer. After picking up our first round himself, Fritz offered to buy a second drink for any of the interns who could beat him in a rock paper scissors (RPS) match, warning us that he was ‚Äúpretty good at it.‚Äù\nLet‚Äôs fact check his claim. We‚Äôd like to know something about his actual RPS win rate, but that is unobservable. We can‚Äôt observe it directly, but we could observe some match outcomes and make an inference about what his actual win rate might plausibly be.\nLet‚Äôs say that after facing off with the 10 interns, Fritz racks up the following match outcomes.\n\nobserved_outcomes = [1, 1, 0, 1, 0, 0, 1, 1, 1, 1]\n\nHe won 7 out of 10 matches‚Äînot bad. But is his performance the result of skill or simply a lucky round? We‚Äôre going to address this question using Bayesian statistical analysis."
  },
  {
    "objectID": "posts/bayesian-modeling-primer/index.html#the-bayesian-workflow-in-3-steps",
    "href": "posts/bayesian-modeling-primer/index.html#the-bayesian-workflow-in-3-steps",
    "title": "Bayesian Modeling Primer",
    "section": "üõ†Ô∏è The Bayesian Workflow in 3 Steps",
    "text": "üõ†Ô∏è The Bayesian Workflow in 3 Steps\nI consider the Bayesian workflow to have 3 major steps:\n\nModeling - specify the data generating process as a generative model\nInference - use the model, the observed data, and some inference algorithm to infer the values of unknown model parameters\nInterpretation - summarize and interpret the inferred model parameters to answer your analysis questions\n\n\n\n\nThe Bayesian Workflow: modeling, inference, interpretation."
  },
  {
    "objectID": "posts/bayesian-modeling-primer/index.html#step-1.-modeling",
    "href": "posts/bayesian-modeling-primer/index.html#step-1.-modeling",
    "title": "Bayesian Modeling Primer",
    "section": "‚öôÔ∏è Step 1. Modeling",
    "text": "‚öôÔ∏è Step 1. Modeling\n\nModeling the Data Generating Process\nIn this step, we‚Äôre going to build a generative model, i.e.¬†a model that can simulate data similar to our observed data. If you‚Äôre coming from ML, the key mental shift is to think about modeling the data generating process (DGP), rather than curve-fitting the data itself. Practically this means our model is a set of random variables which relate to one another in some way and from which we can draw realizations‚Ä¶ random realizations, that is. You can invent a DGP as follows:\n\nIdentify the key variables in the system.\nDefine each variable as a draw from some probability distribution, or in terms of the other variables.\nUse unknown parameters as needed in the probability distributions or in the functional relationships among the key variables.\n\nIn our RPS example, there is one key variable‚ÄîFritz‚Äôs match outcome. We can define the match outcome variable as a random draw from some distribution, e.g.¬†a Bernoulli distribution. The Bernoulli distribution has one parameter‚Äîthe success probability‚Äîwhich corresponds here to Fritz‚Äôs actual true win rate. Given some true win rate, we can simulate match outcomes by drawing realizations from the Bernoulli distribution.\n\\[ y_i \\sim \\text{Bernoulli}(\\theta) \\]\nwhere \\(y_i = 0\\) if Fritz loses to intern \\(i\\) and \\(y_i = 1\\) if he wins, and \\(i=1,\\dots,N\\) where \\(N=10\\). In this DGP, the parameter \\(\\theta\\) corresponds to Fritz‚Äôs true win rate.\nThis is a good start, but we can‚Äôt simulate data from this model yet because \\(\\theta\\) has no particular value. So, what value should we use?\n\n\nProbability as Relative Plausibility\nOne of the key ideas in Bayesian modeling is that we can represent the relative plausibility of potential values of any unobserved variable using a probability distribution. Highly plausible values get higher probability, and less plausible values get lower probability.\nIt is this view of probability as a measure of relative plausibility that distinguishes Bayesian statistics from Frequentist statistics, which views probability as the relative frequency of events.\nWe don‚Äôt know the true value of Fritz‚Äôs RPS win rate, but even before collecting any data, we might have some contextual knowledge about how the world works which can provide some prior information about the relative plausibility of its possible values. For me it‚Äôs easiest to think in terms of how surprising a given true value would be. I wouldn‚Äôt be surprised at all if his win rate was near 0.5, but I would be shocked if it was 0.9 or 0.1, hence 0.5 has higher relative plausibility than 0.9 or 0.1.\nLet‚Äôs represent the prior relative plausibility of values of Fritz‚Äôs RPS win rate with a probability distribution. Below are a few different probability distributions defined over the possible values \\(0 \\le \\theta \\le 1\\).\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, bernoulli\n\n# set colors for later\nprior_color = \"C0\"\npost_color = \"C1\"\n\n# prior beta parameters\nparameters = [\n    (4, 4),\n    (7, 7),\n    (10, 10)\n]\n\n# plot the prior\nx = np.linspace(0, 1, 100)\nplt.figure()\nfor i, (alpha, beta_val) in enumerate(parameters):\n    y = beta.pdf(x, alpha, beta_val)\n    label = f'Beta($\\\\alpha$={alpha}, $\\\\beta$={beta_val})'\n    plt.plot(x, y, label=label)\n\nplt.xlabel(\"win rate (theta)\")\nplt.ylabel(\"Probability Density\")\nplt.title('Relative Plausibility of RPS Win Rate')\nplt.legend()\n\n\n\n\n\n\n\n\n\nEach of these PDFs has a mode at \\(\\theta=0.5\\) and decreases toward 0 and 1. They‚Äôre all aligned with the relative plausibilities we discussed earlier.\nYou can check the relative plausibility between two possible values of \\(\\theta\\) implied by a given pdf by taking the ratio of the height of the pdf at one value of \\(\\theta\\) versus the height at another value of \\(\\theta\\).\nFor example, let‚Äôs compare \\(\\theta=0.5\\) to \\(\\theta=0.7\\) for a Beta(10, 10) prior.\n\nbeta.pdf(0.5, 10, 10) / beta.pdf(0.7, 10, 10)\n\nnp.float64(4.802710683776413)\n\n\nThe Beta(10, 10) distribution implies that a 0.5 win rate is about 5 times more plausible than a 0.7 win rate, which sounds, ahem, plausible.\n\n\nPriors\nWe can include this prior information about the relative plausibility of values of \\(\\theta\\) in our model as follows.\n\\[\n\\theta \\sim \\text{Beta}(10, 10)\n\\] \\[\ny_i \\sim \\text{Bernoulli}(\\theta)\n\\]\nIn Bayesian parlance, we call the probability distribution that represents the relative plausibilities of an unobserved parameter its prior distribution, or simply its prior. Notice that with the addition of the prior for \\(\\theta\\), our model is now fully generative.\n\n\nImplementing the generative model\nLet‚Äôs implement the DGP using random variables from scipy.\n\n# Implementing the DGP as a generative model\n\ndef draw_from_prior(alpha_param, beta_param):\n    return beta.rvs(alpha_param, beta_param)\n\ndef simulate_one_outcome(theta, N):\n    y = bernoulli.rvs(theta, size=N)\n    return {\"theta\": theta, \"y\": y, \"sum_y\": np.sum(y)}\n\ndef simulate_outcomes(n_outcomes, alpha_param, beta_param, N):\n    return pd.DataFrame([\n        simulate_one_outcome(theta=draw_from_prior(alpha_param, beta_param), N=N)\n        for _ in range(n_outcomes)\n    ])\n\n# set DGP parameters\nalpha_param, beta_param = 10, 10\nN = 10\n\n# simulate outcomes from the generative model\noutcome_df = simulate_outcomes(1_000, alpha_param, beta_param, N)\noutcome_df.head()\n\n\n\n\n\n\n\n\ntheta\ny\nsum_y\n\n\n\n\n0\n0.451620\n[0, 0, 1, 1, 1, 0, 0, 1, 0, 0]\n4\n\n\n1\n0.460305\n[0, 1, 1, 0, 1, 1, 1, 0, 1, 1]\n7\n\n\n2\n0.555594\n[0, 0, 1, 1, 1, 1, 0, 1, 0, 0]\n5\n\n\n3\n0.518724\n[1, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n5\n\n\n4\n0.569247\n[1, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n8\n\n\n\n\n\n\n\nEach time you run this simulation, you first draw a new value of \\(\\theta\\) from its prior, then that value is used in the Bernoulli distribution to draw an array of binary match win/loss observations. To help us summarize the match observations in each simulated outcome, we also compute the sum of the match values, i.e.¬†the number of wins.\n\n\nPrior Predictive Check\nBut how do we know that the prior we chose is reasonable? There are two places we can look: (1) at the parameter itself and (2) at the downstream variables it influences. We already looked at the parameter itself by inspecting its pdf and thinking about the relative plausibilities it implies. To look at its impact on the downstream variables, we can simply run simulations from the model and inspect the outcome data it produces. If we see it‚Äôs generating lots of highly implausible outcomes, then we know something isn‚Äôt right. This process is called a prior predictive check, because we‚Äôre checking the simulated outcomes (a.k.a. predictions) implied by the prior. Let‚Äôs run our model simulation 1000 times and have a look at the distribution of the number of wins out of 10 matches that it predicts, i.e.¬†the sum of the y variable from each simulation.\n\n\nCode\noutcome_df.hist(\"sum_y\", bins=50, color=prior_color)\nplt.xlabel('sum(y)')\nplt.ylabel('count')\nplt.title('Prior Predictive Check');\n\n\n\n\n\n\n\n\n\nThe histogram shows most of the simulations yield between 3 and 7 wins, with very few outcomes less than 3 or greater than 7. That seems pretty reasonable.\nLet‚Äôs look at what the prior predictive check might look like when things aren‚Äôt quite right.\n\n\nCode\nsimulate_outcomes(1_000, alpha_param=0.5, beta_param=0.5, N=10).hist(\"sum_y\", bins=50, color=prior_color)\nplt.xlabel('sum(y)')\nplt.ylabel('count')\nplt.title('Prior Predictive Check');\n\n\n\n\n\n\n\n\n\nIn this simulation, many of the outcomes are close to 0 or 10 wins out of 10. From our prior knowledge about RPS, we know it would be possible but very unusual for someone to win either 0/10 or 10/10 matches. This tips us off that something isn‚Äôt right with our priors. At this point we would iterate on our priors until we find something reasonable like our Beta(10, 10).\nOnce we‚Äôve got our generative model and its priors nailed down, we‚Äôre ready to move from the modeling step to the inference step!"
  },
  {
    "objectID": "posts/bayesian-modeling-primer/index.html#step-2.-inference",
    "href": "posts/bayesian-modeling-primer/index.html#step-2.-inference",
    "title": "Bayesian Modeling Primer",
    "section": "üßÆ Step 2. Inference",
    "text": "üßÆ Step 2. Inference\n\nThe Goal of Bayesian Inference\nIn the inference step, we use observed outcome data to infer the plausible values of the unobserved parameters. Whereas simulation passes information forward from parameters to outcomes, inference passes it backwards from observed outcomes to parameters. It‚Äôs analogous to model fitting or training in machine learning; it‚Äôs the part where we use data to learn about the model parameters. The specific output of inference is the updated relative plausibility of the unknown model parameters. Whereas we represent the prior relative plausibilities with the prior distribution, we represent the posterior relative plausibilities (after incorporating information from the data) with the posterior distribution, or simply, the posterior. Like the prior, our model‚Äôs posterior distribution is a probability density defined over the possible values of \\(\\theta\\), where larger values indicate higher relative plausibility.\n\n\nAnalytical Formulation of Bayesian Inference\nLet‚Äôs nail down the mathematical formulation of Bayesian inference. We have data \\(y\\) and parameter(s) \\(\\theta\\). These have a joint probability density \\(p(\\theta, y)\\). This joint distribution of data and parameters is defined by our generative model of the system‚Äîsimulating data from our DGP is equivalent to drawing realizations from the joint distribution \\(p(\\theta, y)\\). Using the definition of conditional probability,, we can write the joint distribution as:\n\\[p(\\theta, y) = p(y|\\theta)p(\\theta) \\]\nwhere\n\n\\(p(\\theta, y)\\) is the joint distribution of parameter \\(\\theta\\) and data \\(y\\)\n\\(p(\\theta)\\) is the prior  distribution of the parameter \\(\\theta\\)\n\\(p(y|\\theta)\\) is the likelihood‚Äîthe conditional distribution of observed data \\(y\\) given parameter \\(\\theta\\).\n\nWhen we do inference we are interested in the relative plausibility of unknown parameter \\(\\theta\\) given data \\(y\\), which we quantify as the conditional distribution of \\(\\theta\\) given \\(y\\). Using Baye‚Äôs Rule, we can write the posterior as\n\\[ p(\\theta | y) = \\frac{ p(\\theta, y) } { p(y) } = \\frac { p(y|\\theta)p(\\theta)  } { p(y) }  \\]\nwhere\n\n\\(p(\\theta | y)\\) is the posterior distribution of the parameter \\(\\theta\\)\n\\(p(y)\\) is the marginal likelihood of the data \\(y\\) (to be explained soon)\n\nTechnically, the joint distribution and the posterior are functions of both parameter \\(\\theta\\) and data \\(y\\). But in practice when we compute the posterior, we‚Äôll have some actual observed data‚Äîsay \\(y_{\\text{obs}}\\)‚Äîso that \\(y\\) is actually fixed at \\(y=y_{\\text{obs}}\\). Substituting the fixed value \\(y_{\\text{obs}}\\) in the posterior, we get\n\\[ p(\\theta | y_{\\text{obs}}) = \\frac{ p(\\theta, y_{\\text{obs}}) } { p(y_{\\text{obs}}) } =  \\frac { p(y_{\\text{obs}}|\\theta)p(\\theta)  } {  \\int p(\\theta|y_{\\text{obs}})p(\\theta) d \\theta  }  \\]\nIf we view \\(y_{\\text{obs}}\\) as fixed, then the posterior can be interpreted as just the slice of the joint distribution where \\(y=y_{\\text{obs}}\\). To get a proper conditional probability distribution, we just need to divide the sliced joint density function by the area under \\(p(\\theta,y_{\\text{obs}})\\) along the \\(\\theta\\) axis. And guess what? That‚Äôs exactly what the marginal likelihood is doing; \\(p(y_{\\text{obs}}) = \\int p(\\theta|y_{\\text{obs}})p(\\theta) d \\theta\\) is just the area under the sliced joint density, and it‚Äôs there in the denominator to normalize the sliced joint density so that we get a proper conditional distribution for the posterior.\n\n\nComputing the Posterior using Grid Approximation\nLet‚Äôs compute the posterior using the formulas we cooked up in the previous section. Earlier when we wrote down our generative model, we already identified all the pieces we need:\n\nthe prior‚Äîsince \\(\\theta \\sim \\text{Beta}(10, 10)\\), \\(p(\\theta)\\) is the probability density function of a \\(\\text{Beta}(10,10)\\) random variable.\nthe likelihood‚Äîsince \\(y_i \\sim \\text{Bernoulli}(\\theta)\\), the likelihood is the probability mass function of a Bernoulli random variable with parameter \\(\\theta\\)‚Ä¶ well, almost.\n\nThe one remaining detail to iron out is that our observed data \\(y_{\\text{obs}}=[y_1,\\dots,y_N]\\) consists of \\(N=10\\) observations of the binary match outcomes. Our likelihood needs to reflect the conditional probability of the entire dataset given \\(\\theta\\), not just a single observation. We know from probability theory that the joint probability of two independent events is the product of their individual probabilities. Therefore, assuming independence among our observations, the joint likelihood of the full dataset is the product of the likelihood of each observation.\n\\[ p(y_{\\text{obs}}|\\theta) = p(y_1,\\dots,y_N|\\theta) =  \\prod_{i=1}^N p(y_i|\\theta) \\]\nLet‚Äôs implement the prior, the likelihood, the joint distribution, and the posterior in python and plot out the prior and the posterior distribution of the parameter \\(\\theta\\).\n\n# 0. Defining the observed data\n\ny_obs = np.array(observed_outcomes)\nsum_y_obs = np.sum(y_obs)\n\n# 1. Functions for Prior and Likelihood\n\ndef prior(theta):\n    return beta.pdf(theta, alpha_param, beta_param)\n\ndef likelihood(theta, y):\n    product_of_likelihoods = 1.0\n    for y_i in y:\n        product_of_likelihoods *= bernoulli.pmf(y_i, p=theta)\n    return product_of_likelihoods\n\n# 2. Function for Joint Density p(y, theta)\n\ndef joint_density(theta, y):\n    return likelihood(theta, y) * prior(theta)\n\n# 3. Computing the Posterior by \"Slicing\" and Normalizing\n\ndef posterior_from_joint_slice(theta_values, y_observed_data):\n    \n    # compute grid of p(theta, y_obs) over values of theta and fixed y_obs\n    unnormalized_posterior_values = np.array([\n        joint_density(theta, y_observed_data) for theta in theta_values\n    ])\n    \n    # numerical integration  to get marginal likelihood p(y_obs\n    delta_theta = theta_values[1] - theta_values[0] \n    marginal_likelihood_approx = np.sum(unnormalized_posterior_values * delta_theta)\n    \n    # p(theta | y_obs) = p(theta, y_obs) / p(y_obs)\n    normalized_posterior_values = unnormalized_posterior_values / marginal_likelihood_approx\n    \n    return normalized_posterior_values\n\n\n# Define a grid of theta values\ntheta_grid = np.linspace(0.001, 0.999, 500) \n\n# Calculate prior over the grid of theta values\nprior_values = np.array([prior(theta) for theta in theta_grid])\n\n# Calculate likelihood values over the grid of theta values and fixed y_obs\nlikelihood_values = np.array([likelihood(theta, y_obs) for theta in theta_grid])\n\n# Calculate the posterior over the grid of theta values and fixed y_obs\nposterior_values = posterior_from_joint_slice(theta_grid, y_obs)\n\n\n\nCode\n# Plotting\nplt.plot(theta_grid, prior_values, label=f'Prior p(theta) ~ Beta({alpha_param},{beta_param})', linestyle='--', color=prior_color)\nplt.plot(theta_grid, posterior_values, label=f'Posterior p(theta|y_obs)', color=post_color, linewidth=2)\nplt.title('Bayesian Inference: Prior, Likelihood, and Posterior')\nplt.xlabel(r'$\\theta$ (Probability of Match Win)')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True, linestyle=':', alpha=0.7)\nplt.xlim(0, 1)\nplt.ylim(bottom=0)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nFrom the figure we can see that while the prior is centered at \\(\\theta=0.5\\), the posterior is actually pulled slightly toward larger values of \\(\\theta\\) by the observed data, indicating increased relative plausibility on win rates greater than 0.5.\nIt‚Äôs nice to see that the math works and that we can successfully implement it in code, but grid approximation is a pedagogical endeavor. In practice, when models start to get complicated, we‚Äôll need a more flexible approach for finding the posterior.\n\n\nSampling from the Posterior\nIt turns out that we can do inference using our generative model and our observed data without having to compute the likelihood directly. But while the forward problem of simulating data from the model is quite straightforward, it‚Äôs less obvious how to approach the inverse problem of doing inference. There is no silver bullet here. In fact there are tons of different algorithms for doing inference on probabilistic models, but luckily, since virtually all the important inference algorithms use sampling-based approaches, e.g.¬†Hamiltonian Monte Carlo, Metropolis-Hastings, and Variational Inference, no matter which algorithm we use under the hood, we‚Äôll end up with the same kind of output at the end‚Äîsamples of the unknown parameters which have been drawn from the posterior.\nTo get some intuition for how we can do inference by sampling from the generative model,, we‚Äôre going to implement a very simple inference algorithm called rejection sampling. The key idea is based on the insight we discussed earlier that the posterior is essentially a slice through the joint distribution where the data is fixed to what we actually observed. Since simulating from the generative model is equivalent to drawing samples from the joint distribution, isolating simulation outcomes where the simmulated data is equal to the observed data is equivalent to sampling from the joint distribution along the slice, and hence equivalent to sampling from the posterior.\nThe algorithm for doing Bayesian inference via rejection sampling is as follows\n\nGenerate a sample \\((\\theta^*,y^*)\\) from the generative model.\nKeep the sample if \\(y^*=y_{\\text{obs}}\\), otherwise discard it.\nRepeat 1 and 2 until the desired number of retained samples is collected.\nThe retained samples of \\(\\theta\\) can be interpreted as samples from the posterior.\n\nLet‚Äôs do this in python. In our example, the order of the wins and losses over the \\(N=10\\) match observations doesn‚Äôt matter, so we‚Äôll focus on the number of wins. Since \\(\\sum_iy_i=7\\) in our observed data, we‚Äôll isolate the simulation outcomes where sum_y equals 7.\n\n\nCode\nfig, ax = plt.subplots()\noutcome_df.query('sum_y != @sum_y_obs').plot(x=\"sum_y\", y=\"theta\", kind=\"scatter\", color=\"black\", alpha=0.2, label=\"outcomes where $y^* \\\\ne y_{\\\\text{obs}}$\", ax=ax)\noutcome_df.query('sum_y == @sum_y_obs').plot(x=\"sum_y\", y=\"theta\", kind=\"scatter\", color=post_color, alpha=0.4, label=\"outcomes where $y^* = y_{\\\\text{obs}}$\", ax=ax)\nax.axvline(x=sum_y_obs-0.25, color=\"black\", linestyle='--')\nax.axvline(x=sum_y_obs+0.25, color=\"black\", linestyle='--')\nplt.title(\"Samples from the Generative Model\");\n\n\n\n\n\n\n\n\n\nBoom! By isolating the outcomes where \\(y=y_{\\text{obs}}\\), we effectively have samples from the posterior. Let‚Äôs draw a larger number of samples so we get an adequate sample from the posterior.\n\n# drawing a large number of samples and isolating outcomes where y = y_obs\nrejection_sampling_outcome_df = simulate_outcomes(10_000, alpha_param, beta_param, N).query('sum_y == @sum_y_obs')\n\n# posterior samples from rejection sampling\nposterior_samples = rejection_sampling_outcome_df[\"theta\"]\n\n\n\nCode\nposterior_samples.hist(bins=30, color=post_color)\nplt.xlabel(\"theta\")\nplt.title(\"Samples from the Posterior\");\n\n\n\n\n\n\n\n\n\nNow let‚Äôs put all the pieces together.\n\n\nCode\nfig, ax = plt.subplots()\nprior_samples = outcome_df[\"theta\"]\nprior_samples.hist(density=True, bins=30, alpha=0.7, color=prior_color, label=\"prior samples\", ax=ax)\nposterior_samples.hist(density=True, bins=30, alpha=0.7, color=post_color, label=\"posterior samples\", ax=ax)\nax.plot(theta_grid, prior_values, label=f'Prior p(theta) ~ Beta({alpha_param},{beta_param})', linestyle='--', color=prior_color)\nax.plot(theta_grid, posterior_values, label=f'Posterior p(theta|y_obs)', color=post_color, linewidth=2)\nplt.title('Bayesian Inference: Prior and Posterior')\nplt.xlabel(r'$\\theta$ (Probability of Match Win)')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True, linestyle=':', alpha=0.7)\nplt.xlim(0, 1)\nplt.ylim(bottom=0)\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\nIn this figure we have:\n\nthe functional form of the prior\nthe functional form of the posterior from grid approximation\nsamples from the prior drawn directly from the generative model\nand samples of the posterior obtained by applying rejection sampling to the generative model.\n\nGreat! Our sampling algorithms are generating samples from the prior and the posterior which are consistent with the functional forms we computed earlier!\nWhile we used rejection sampling here, regardless of what sampling algorithm we choose, we‚Äôll end up with the same thing after inference‚Äîa set of samples from the posterior distribution for each unknown parameter. Once we have those samples, we‚Äôre ready to move to the interpretation and analysis step."
  },
  {
    "objectID": "posts/bayesian-modeling-primer/index.html#step-3.-interpretation",
    "href": "posts/bayesian-modeling-primer/index.html#step-3.-interpretation",
    "title": "Bayesian Modeling Primer",
    "section": "üî¨ Step 3. Interpretation",
    "text": "üî¨ Step 3. Interpretation\nSo how do we get insight into our analysis questions from this posterior_samples array? Well we‚Äôve got samples from the posterior distribution of \\(\\theta\\) which represents our updated beliefs about the relative plausibility of different values of Fritz‚Äôs actual underlying RPS win rate. We can use them just like any other dataset to answer questions about his win rate.\nLet‚Äôs start with getting a point estimate of his true win rate. We can simply take the mean of the samples.\n\nprint(f'point estimate of theta: {np.mean(posterior_samples)}')\n\npoint estimate of theta: 0.5655783348185507\n\n\nTo get a confidence interval, often called a credible interval in the Bayesian context, we can just pull the quantiles of the posterior distribution. Note there are fancier ways to do this, e.g.¬†computing highest posterior density intervals (HPDIs), but conceptually we‚Äôre basically just looking at the quantiles of the sample.\n\nprint(f'89% credible interval of theta:: {np.quantile(posterior_samples, [0.055, 0.945])}')\n\n89% credible interval of theta:: [0.42005907 0.70367915]\n\n\nWhat‚Äôs the probability that Fritz‚Äôs win rate is actually really good, say greater than 75%? We can just check the samples directly for the proportion greater than 0.75.\n\nprint(f'P[theta &gt; 0.75]: {np.mean(posterior_samples &gt; 0.75)}')\n\nP[theta &gt; 0.75]: 0.011933174224343675\n\n\nThis means our analysis implies theres only a 1.5% chance that his true win rate is larger than 75%.\nNow that we‚Äôve taken a look at interpreting the posterior samples to get some insight into the unknown parameter \\(\\theta\\) representing Fritz‚Äôs actual RPS win rate, we can take it one step further and make some predictions.\n\nPosterior Predictive Distribution\nWe have one more character to meet in this cast of Bayesian players‚Äîthe posterior predictive distribution.\n\\(p(y_{\\text{new}}|y_{\\text{obs}})\\)\nIt represents the most plausible distribution of new data \\(y_{\\text{new}}\\) given that we observed data $y_{}, i.e.¬†based on the posterior rather than the prior. Mathematically it is obtained by integrating the product of the likelihood for the new data and the posterior distribution over the parameter space.\n\\[p(y_{\\text{new}}|y_{\\text{obs}}) = \\int p(y_{\\text{new}}|\\theta) p(\\theta|y_{\\text{obs}}) d\\theta\\]\nwhere\n\n\\(p(y_{\\text{new}}|\\theta)\\) is the likelihood of the new data, assuming a specific parameter value \\(\\theta\\). It‚Äôs the same likelihood function we used before, evaluated on \\(y_{\\text{new}}\\).\n\\(p(\\theta|y_{\\text{obs}})\\) is just the posterior distribution\nthe integral \\(\\int \\dots d \\theta\\) effectively ‚Äúaverages‚Äù the likelihood of the new data over all possible values of \\(\\theta\\), weighted by how plausible each value is according to the posterior.\n\nIn terms of the DGP, we can generate samples from the posterior predictive by\n\nDrawing a sample of \\(\\theta\\) from the posterior distribution.\nUsing this value of \\(\\theta\\) to generate a value of \\(y\\) from the generative model.\n\nLet‚Äôs implement this in python.\n\ndef simulate_posterior_predictive(posterior_samples, N):\n    return pd.DataFrame([\n        simulate_one_outcome(theta=theta, N=N)\n        for theta in posterior_samples\n    ])\n\nposterior_predictive_df = simulate_posterior_predictive(posterior_samples, N)\n\nHere again, we can inspect the distribution using any familiar techniques for working with samples of data. Here‚Äôs a histogram of the posterior predictive.\n\n\nCode\nposterior_predictive_df[\"sum_y\"].hist(density=True, bins=50, color=post_color)\nplt.title(\"Posterior Predictive Distribution\")\nplt.xlabel(\"sum(y)\")\nplt.ylabel(\"probability mass\");\n\n\n\n\n\n\n\n\n\nWe can use this for forecasting, e.g.¬†what‚Äôs the probability that Fritz wins at least 7 games in his next round?\n\nprint(f'Probability of winning &gt;= 7 in next round: {np.mean(posterior_predictive_df[\"sum_y\"] &gt;= 7)}')\n\nProbability of winning &gt;= 7 in next round: 0.32537788385043753\n\n\nHere‚Äôs where things can get interesting. In addition to forecasting the outcome itself, we can also compute the probabilities of events that depend on the outcome. For example, let‚Äôs say Fritz has only $50 left in his wallet, and he wants to know the probability that he can cover his bill after the next round. Let‚Äôs assume each drink costs $12. We can compute that probability as follows.\n\n# probability that Fritz's next round bill is less than or equal to $50\n\ncost_per_drink = 12.0\nposterior_predictive_df = (\n    posterior_predictive_df\n    .assign(losses = lambda x: N - x.sum_y)\n    .assign(bill = lambda x: cost_per_drink * x.losses)\n)\n\nprint(f'Probability next round bill &lt;= $50: {np.mean(posterior_predictive_df[\"bill\"] &lt;= 50)}')\n\nProbability next round bill &lt;= $50: 0.5369928400954654"
  },
  {
    "objectID": "posts/bayesian-modeling-primer/index.html#summary-of-the-bayesian-workflow",
    "href": "posts/bayesian-modeling-primer/index.html#summary-of-the-bayesian-workflow",
    "title": "Bayesian Modeling Primer",
    "section": "Summary of the Bayesian Workflow",
    "text": "Summary of the Bayesian Workflow\nWow, we covered a lot of ground today. Let‚Äôs summarize the key points.\nThe Bayesian Analysis Workflow has three major steps:\n\nModeling\n\nWe build a generative model that describes the relationships among key variables and unknown parameters to represent the data generating process.\nWe encode our prior knowledge about the relative plausibility of different parameter values in the prior distribution of the parameters.\nWe can use prior predictive checks to simulate outcome data from the generative model to sanity check our modeling assumptions.\n\nInference\n\nBased on our modeling assumptions, we can use observed data to infer the posterior distribution, which quantifies the relative plausibility of different values of the unknown parameters after observing data.\nWe can view simulations from the generative model as sampling from the joint distribution of data and parameters, and we can view the posterior as the result of conditioning the joint distribution on the data we actually observed.\nWe can do inference by using sampling-based inference algorithms like rejection sampling, which uses logic on top of our generative model to isolate samples from the posterior distribution.\n\nInterpretation\n\nAfter inference we can use data analysis tools to summarize the samples from the posterior distribution to compute point estimates, intervals, and probabilities of interest.\nIf we simulate data from our generative model using the posterior rather than the prior, we get samples from the posterior predictive distribution, which predicts future outcomes given observed data.\nAgain, we can analyze these posterior predictive samples to compute point estimates, intervals, or probabilities of future outcomes.\n\n\nPhew! Hopefully that‚Äôs a helpful introduction to the Bayesian workflow and the major ideas behind it. The techniques we looked at here are mostly for pedagogical purposes; if you want to apply Bayesian methods to practical problems, you‚Äôll want to use a probabilistic programming language like PyMC, pyro, or stan. Maybe we‚Äôll get into some of those tools in future posts. See you then!\n\nResources\n\nStatistical Rethinking - This page links to where you can obtain the book and also to a number of repos where folks have ported the R and Stan code examples to python libraries like PyMC and pyro.\n\n\n\nReader Exercises\nYou didn‚Äôt think you‚Äôd get away without homework did you? Here are a couple suggestions for exercises.\n\ncompute the posterior predictive distribution \\(p(y_{\\text{new}}|y_{\\text{obs}})\\) for the RPS example using grid approximation.\nSuppose that each RPS match was played as best out of three. Rewrite the generative model to generate both sub-match and match outcomes. Do inference with rejection sampling. Use your model to find the probability that Fritz wins his next match by winning two submatches in a row."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "",
    "text": "SF buzzes silently in the distance\nAhh, gradient boosting. In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. Like its cousin random forest, gradient boosting is an ensemble technique that generates a single strong model by combining many simple models, usually decision trees. These tree ensemble methods perform very well on tabular data prediction problems and are therefore widely used in industrial applications and machine learning competitions.\nThere are several noteworthy variants of gradient boosting out there in the wild including XGBoost, NGBoost, LightGBM, and of course the classic gradient boosting machine (GBM). While XGBoost and LightGBM tend to have a marginal performance edge on the classic GBM, they are all based on a similar, very clever, idea about how to ensemble decision trees. Let‚Äôs avail ourselves of the intuition behind that clever idea, and then we‚Äôll be able to build our very own GBM from scratch."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#toy-data",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#toy-data",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Toy Data",
    "text": "Toy Data\nWe begin our boosting adventure with a deceptively simple toy dataset having one feature \\(x\\) and target \\(y\\).\n\n\nCode\nimport numpy as np \n\nrng = np.random.default_rng()\nx = np.linspace(0, 10, 50)\ny = np.where(x &lt; 5, x, 5) + rng.normal(0, 0.4, size=x.shape)\nx = x.reshape(-1,1)\n\n\n\n\nCode\nplt.scatter(x,y, label='y')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend();\n\n\n\n\n\n\n\n\n\nNotice that \\(y\\) increases with \\(x\\) for a while, then flattens out. This is a pattern that happens all the time in real data, and it‚Äôs one that linear models epically fail to capture. Let‚Äôs build a gradient boosting machine to model it."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#intuition",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#intuition",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Intuition",
    "text": "Intuition\nSuppose we have a crappy model \\(F_0(x)\\) that uses features \\(x\\) to predict target \\(y\\). A crappy but reasonable choice of \\(F_0(x)\\) would be a model that always predicts the mean of \\(y\\).\n\\[F_0(x) = \\bar{y}\\]\nThat would look like this.\n\n\nCode\neta = 1\n\nF_0 = y.mean() * np.ones(shape=y.shape)\n\nh_1 = DecisionTreeRegressor(max_depth=1)\nh_1.fit(x, y - F_0)\nF_1 = F_0 + eta * h_1.predict(x)\n\nh_2 = DecisionTreeRegressor(max_depth=1)\nh_2.fit(x, y - F_1)\nF_2 = F_1 + eta * h_2.predict(x)\n\n\n\n\nCode\nplt.plot(x, y, 'o', label=r'$y$')\nplt.plot(x, F_0, '-r', label=r'$F_0(x)$')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.ylim([-1,7]);\n\n\n\n\n\n\n\n\n\n\\(F_0(x)\\) by itself is not a great model, so its residuals \\(y - F_0(x)\\) are still pretty big and they still exhibit meaningful structure that we should try to capture with our model.\n\n\nCode\nfig, ax = plt.subplots()\nax.plot(x, y - F_0, 'd', label=r'$y - F_0(x)$')\nax.vlines(x, 0, y - F_0, linewidth=0.5)\n# plt.plot(x, h_1.predict(x), '--r', label=r'$h_1(x)$')\nplt.xlabel('x')\nplt.ylabel('residuals')\nax.legend()\nax.set_ylim([-6, 6]);\n\n\n\n\n\n\n\n\n\nWell what if I had another crappy model \\(h_1(x)\\) that could predict the residuals \\(y - F_0(x)\\)?\n\n\n\nModel\nFeatures\nTarget\n\n\n\n\n\\(h_1(x)\\)\n\\(x\\)\n\\[y-F_0(x)\\]\n\n\n\nIt‚Äôs worth noting that the crappiness of this new model is essential; in fact in this boosting context, it‚Äôs usually called a weak learner. To get a model that‚Äôs only slightly better than nothing, let‚Äôs use a very simple decision tree with a single split, a.k.a. a stump. This model basically divides our feature \\(x\\) into two regions and predicts the mean value of \\(y\\) for all of the \\(x\\)‚Äôs in each region. It might look like this.\n\n\nCode\nfig, ax = plt.subplots()\nax.plot(x, y - F_0, 'd', label=r'$y - F_0(x)$')\nax.vlines(x, 0, y - F_0, linewidth=0.5)\nplt.plot(x, h_1.predict(x), '--r', label=r'$h_1(x)$')\nplt.xlabel('x')\nplt.ylabel('residuals')\nax.legend()\nax.set_ylim([-6, 6]);\n\n\n\n\n\n\n\n\n\nWe could make a composite model by adding the predictions of the base model \\(F_0(x)\\) to the predictions of the supplemental model \\(h_1(x)\\) (which will pick up some of the slack left by \\(F_0(x)\\)). We‚Äôd get a new model \\(F_1(x)\\):\n\\[F_1(x) = F_0(x) + h_1(x)\\]\nwhich is better at predicting \\(y\\) than the original model \\(F_0(x)\\) alone.\n\n\nCode\nplt.plot(x, y, 'o', label=r'$y$')\nplt.plot(x, F_1, '-r', label=r'$F_1(x)$')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.ylim([-1,7]);\n\n\n\n\n\n\n\n\n\nWhy stop there? Our composite model \\(F_1(x)\\) might still be kind of crappy, and so its residuals \\(y - F_1(x)\\) might still be pretty big and structurey. Let‚Äôs add another model \\(h_2(x)\\) to predict those residuals.\n\n\n\nModel\nFeatures\nTarget\n\n\n\n\n\\(h_2(x)\\)\n\\(x\\)\n\\[y-F_1(x)\\]\n\n\n\nThe new composite model is\n\\[F_2(x) = F_1(x) + h_2(x).\\]\n\n\nCode\n# m = 2\n\nfig, ax = plt.subplots(1,2, figsize=(10,4))\n\nax[0].plot(x, y - F_1, 'd', label=r'$y - F_1(x)$')\nax[0].vlines(x, 0, y - F_1, linewidth=0.5)\nax[0].plot(x, h_2.predict(x), '--r', label=r'$h_2(x)$')\nax[0].legend()\nax[0].set_ylim([-6, 6])\n\nax[1].plot(x, y, 'o', label=r'$y$')\nax[1].plot(x, F_2, '-r', label=r'$F_2(x)$')\nax[1].legend()\nax[1].set_ylim([-1,7]);\n\n\n\n\n\n\n\n\n\nIf we keep doing this, at each stage we‚Äôll train a new model \\(h_m(x)\\) on the previous composite model‚Äôs residuals \\(y-F_{m-1}(x)\\), and we‚Äôll get a new composite model\n\\[F_m(x) = F_{m-1}(x) + h_m(x).\\]\nIf we add \\(M\\) crappy models constructed in this way to our original crappy model \\(F_0(x)\\), we might actually end up with a pretty good model \\(F_M(x)\\) that looks like\n\\[\nF_M(x) = F_0(x) + \\sum_{m = 1}^{M} h_m(x)\n\\]\nHere‚Äôs how our model would evolve up to \\(M=6\\).\n\n\nCode\n# hyperparameters\nlearning_rate = 1\nn_trees = 6\nmax_depth = 1\n\n# Training\nF0 = y.mean() \n\nfig, ax = plt.subplots(7,2, figsize=(10,28))\n\nax[0,0].axis('off')\nax[0,1].plot(x, y, 'o', label=r'$y$')\nax[0,1].plot(x, F_0, '-r', label=r'$F_0(x)$')\nax[0,1].set_title('m = 0')\nax[0,1].set_ylabel('y')\nax[0,1].legend()\nax[0,1].set_ylim([-1,7]);\n\nFm = F0\ntrees = []\nfor i in range(n_trees):\n    tree = DecisionTreeRegressor(max_depth=max_depth)\n    tree.fit(x, y - Fm)\n\n    m = i + 1\n    \n    ax[m,0].plot(x, y - Fm, 'd', label=r'$y - F_{}(x)$'.format(m-1))\n    ax[m,0].vlines(x, 0, y - Fm, linewidth=0.5)\n    ax[m,0].plot(x, tree.predict(x), '--r', label=r'$h_{}(x)$'.format(m))\n    ax[m,0].legend()\n    ax[m,0].set_ylim([-6, 6])\n    ax[m,0].set_title('m = {}'.format(m))\n    ax[m,0].set_ylabel('residuals')\n\n    Fm += learning_rate * tree.predict(x)\n    trees.append(tree)\n    \n    ax[m,1].plot(x, y, 'o', label=r'$y$')\n    ax[m,1].plot(x, Fm, '-r', label=r'$F_{}(x)$'.format(m))\n    ax[m,1].legend()\n    ax[m,1].set_ylim([-1,7])\n    ax[m,1].set_title('m = {}'.format(m))\n    ax[m,1].set_ylabel('y')\n\nax[m,0].set_xlabel('x')\nax[m,1].set_xlabel('x');\n\n\n\n\n\n\n\n\n\nVoila! That, friends, is boosting!"
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#learning-rate",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#learning-rate",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Learning Rate",
    "text": "Learning Rate\nLet‚Äôs talk about overfitting. In real life, if we just add our new weak learner \\(h_m(x)\\) directly to our existing composite model \\(F_{m-1}(x)\\), then we‚Äôre likely to end up overfitting on our training data. That‚Äôs because if we add enough of these weak learners, they‚Äôre going to chase down y so closely that all the remaining residuals are pretty much zero, and we will have successfully memorized the training data. To prevent that, we‚Äôll scale them down a bit by a parameter \\(\\eta\\) called the learning rate.\nWith the learning rate \\(\\eta\\), the update step will then look like\n\\[F_{m}(x) = F_{m-1}(x) + \\eta h_m(x),\\]\nand our composite model will look like\n\\[F_M(x) = F_0(x) + \\eta \\sum_{m = 1}^{M} h_m(x)\\]\nNote that since the learning rate can be factored out of the sum, it looks kinda like we could just build our models without it and slap it on at the end when we sum up the weak learners to make the final composite model. But that won‚Äôt work, since at each stage we train the next weak learner on the residuals from the current composite model, and the current composite model depends on the learning rate."
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#implementation",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#implementation",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Implementation",
    "text": "Implementation\nOk, we‚Äôre ready to implement this thing from ‚Äúscratch‚Äù. Well, sort of. To quote Carl Sagan,\n\n\nIf you wish to make an apple pie from scratch, you must first invent the universe.\n\nWe will not be inventing a universe that contains the Earth, apple trees, computers, python, numpy, and sklearn. To keep the ‚Äúscratch‚Äù implementation clean, we‚Äôll allow ourselves the luxury of numpy and an off-the-shelf sklearn decision tree which we‚Äôll use as our weak learner.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# model hyperparameters\nlearning_rate = 0.3\nn_trees = 10\nmax_depth = 1\n\n# Training\nF0 = y.mean() \nFm = F0\ntrees = []\nfor _ in range(n_trees):\n    tree = DecisionTreeRegressor(max_depth=max_depth)\n    tree.fit(x, y - Fm)\n    Fm += learning_rate * tree.predict(x)\n    trees.append(tree)\n\n# Prediction\ny_hat = F0 + learning_rate * np.sum([t.predict(x) for t in trees], axis=0)\n\nWe first define our hyperparameters: - learning_rate is (\\(\\eta\\)) - n_trees is the number of weak learner trees to add (\\(M\\)) - max_depth controls the depth of the trees; here we set to 1 for stumps\nWe define our base model predictions F0 to simply predict the mean value of y. Fm corresponds to the current composite model \\(F_m(x)\\) as we iteratively add weak learners, so we‚Äôll initialize it with F0. trees is an empty list that we‚Äôll use to hold our weak learners.\nNext we iteratively add n_trees weak learners to our composite model. At each iteration, we create a new decision tree and train it on x to predict the current residuals y - Fm. We update Fm with the newly trained learner‚Äôs predictions scaled by the learning rate, and we append the new weak learner \\(h_m(x)\\) in the trees list. We generate final predictions y_hat on the training data by summing up the predictions from each weak learner, scaling by the learning rate, and adding to the base model (a.k.a. the mean of y).\n\n\nCode\nplt.plot(x,y,'o', label='y')\nplt.plot(x,y_hat,'-k', label='GBM')\nplt.legend()\nplt.xlabel('x');\n\n\n\n\n\n\n\n\n\nNice! Our GBM fits that nonlinear data pretty well.\nNow that we have a working implementation, let‚Äôs go ahead and implement it as a class with fit and predict methods like we‚Äôre used to having in sklearn.\n\nclass GradientBoostingFromScratch():\n    \n    def __init__(self, n_trees, learning_rate, max_depth=1):\n        self.n_trees=n_trees; self.learning_rate=learning_rate; self.max_depth=max_depth;\n        \n    def fit(self, x, y):\n        self.trees = []\n        self.F0 = y.mean()\n        Fm = self.F0 \n        for _ in range(self.n_trees):\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(x, y - Fm)\n            Fm += self.learning_rate * tree.predict(x)\n            self.trees.append(tree)\n            \n    def predict(self, x):\n        return self.F0 + self.learning_rate * np.sum([tree.predict(x) for tree in self.trees], axis=0)\n\nLet‚Äôs compare the performance of our implementation with the sklearn GradientBoostingRegressor.\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\nsklearn_gbm = GradientBoostingRegressor(n_estimators =25, learning_rate=0.3, max_depth=1)\nsklearn_gbm.fit(x,y)\n\nscratch_gbm = GradientBoostingFromScratch(n_trees=25, learning_rate=0.3, max_depth=1)\nscratch_gbm.fit(x,y)\n\nmean_squared_error(y, sklearn_gbm.predict(x)), mean_squared_error(y, scratch_gbm.predict(x))\n\n(0.08622324648703916, 0.0862232464870392)\n\n\nHeck yeah! Our homemade GBM is consistent with the sklearn implementation!"
  },
  {
    "objectID": "posts/gradient-boosting-machine-from-scratch/index.html#wrapping-up",
    "href": "posts/gradient-boosting-machine-from-scratch/index.html#wrapping-up",
    "title": "How to Build a Gradient Boosting Machine from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nAlright, there you have it, the intuition behind basic gradient boosting and a from scratch implementation of the gradient boosting machine. I tried to keep this explanation as simple as possible while giving a complete intuition for the basic GBM. But it turns out that the rabbit hole goes pretty deep on these gradient boosting algorithms. We can actually wave our magic generalization wand over some custom loss functions and end up with algorithms that can do gradient descent in function space (whatever that means). We‚Äôll get into what that means and why it‚Äôs so baller in future posts. For now, go forth and boost!"
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "",
    "text": "A whiteboard session at Playa Pelada\nIn the last two posts, we learned the basics of gradient boosting machines and the gradient descent algorithm. But we still haven‚Äôt explicitly addressed what puts the ‚Äúgradient‚Äù in gradient boosting. It turns out that gradient boosting models are using a sort of gradient descent to minimize their loss function; according to Friedman‚Äôs classic paper, they‚Äôre doing gradient descent in ‚Äúfunction space‚Äù. If you‚Äôre like me, and this is your first encounter with this idea, then the phrase ‚Äúgradient descent in function space‚Äù is going to sound a little, ahem, mysterious. No worries, friends; we‚Äôre about to make sense of it all.\nUnderstanding the underlying mechanics of gradient boosting as a form of gradient descent will empower us to train our models with custom loss functions. This opens up many interesting possibilities including doing not only regression and classification, but also predicting quantiles, prediction intervals, and even the conditional probability distribution of the response variable."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#generalized-intuition-for-gradient-boosting",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#generalized-intuition-for-gradient-boosting",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Generalized intuition for gradient boosting",
    "text": "Generalized intuition for gradient boosting\nIn my earlier post on building a gradient boosting model from scratch, we established the intuition for how gradient boosting works in a regression problem. In this post we‚Äôre going to generalize the ideas we encountered in the regression context, so check out the earlier post if you‚Äôre not already familiar with gradient boosting for regression. In the following sections we‚Äôll build up the intuition for gradient boosting in general terms, and then we‚Äôll be able to state the gradient boosting algorithm in a form that can fit models to customized loss functions.\n\nThe loss function\nYou recall that we measure how well a model fits data by using a loss function that yields small values when a model fits well. ‚ÄúTraining‚Äù essentially means finding the model that minimizes our loss function. A loss function takes the correct target values and the predicted target values, and it returns a scalar loss score. For example, in the last post on gradient descent we used a mean squared error (MSE) loss\n\\[L(\\mathbf{y}, \\hat{\\mathbf{y}}) =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nwhere we express the correct targets and predicted values as the vector arguments \\(\\mathbf{y}=[y_1,y_2,\\dots,y_n]\\) and \\(\\hat{\\mathbf{y}}=[\\hat{y}_1,\\hat{y}_2,\\dots,\\hat{y}_n]\\) respectively.\n\n\nWhich way to nudge a prediction to get a better model\nNow, let‚Äôs say we have a model \\(F(\\mathbf{X})=\\mathbf{\\hat{y}}\\) that we want to improve. One approach is that we could figure out whether each prediction \\(\\hat{y}_i\\) needed to be higher or lower to get a better loss score. We could then nudge each prediction in the right direction, thereby decreasing our model‚Äôs loss score.\nTo figure out whether we should increase or decrease a particular prediction \\(\\hat{y}_i\\) (and by how much), we can compute the partial derivative of the loss function with respect to that prediction. Recall the partial derivative just tells us the rate of change in a function when we change one of its arguments. Since we want to make the loss \\(L(\\mathbf{y},\\mathbf{\\hat{y}})\\) decrease, we can use the negative partial derivative of the loss function with respect to a given prediction to help us choose the right nudge for that prediction.\n\\[ \\text{nudge for } \\hat{y}_i = -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\]\nSometimes it can get a little intense when there are partial derivatives flying around, but it doesn‚Äôt have to be that way. Remember that in practice \\(-\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is just an expression that evaluates to a number like 2.7 or -0.5, and here it‚Äôs telling us how to nudge \\(\\hat{y}_i\\) to decrease our loss score.\nThe intuition is that if \\(\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is negative, then increasing the prediction \\(\\hat{y}_i\\) will make the loss decrease. We then notice that the negative of the partial derivative tells us whether to increase or decrease \\(\\hat{y}_i\\). For example, if \\(-\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is positive, then increasing the prediction \\(\\hat{y}_i\\) will make the loss decrease; whereas if \\(-\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_i}\\) is negative, then decreasing the prediction \\(\\hat{y}_i\\) will make the loss decrease.\nSince we‚Äôll want to find the right nudge for each of the \\(\\hat{y}_i\\)‚Äôs, we can use the negative gradient of the loss function \\(L(\\mathbf{y},\\mathbf{\\hat{y}})\\) with respect to the vector argument \\(\\hat{\\mathbf{y}}\\) to get the vector of all the partial derivatives. Let‚Äôs call this vector of desired nudge values \\(\\mathbf{r}\\).\n\\[\\mathbf{r} = -\\nabla_{\\hat{\\mathbf{y}}} L(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\left [ -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_1}, -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_2}, \\cdots, -\\frac{\\partial L(\\mathbf{y},\\mathbf{\\hat{y}})}{\\partial \\hat{y}_n}\\right ]\\]\n\n\nNudging predictions in the right direction\nGreat, now that we know we should nudge each prediction in the direction of the negative partial derivative of the loss with respect to that prediction, we need to figure out how to do the actual nudging. Remember that we already have an initial model \\(F(\\mathbf{X})=\\mathbf{\\hat{y}}\\).\nAt this point we might be tempted to simply add the vector of nudge values to our predictions to get better predictions.\n\\[\\text{we might be tempted to try } \\mathbf{\\hat{y}}_{\\text{new}} = \\mathbf{\\hat{y}} + \\mathbf{r}\\]\nSure, based on our reasoning in the previous section, plugging the vector of nudged predictions into the loss function would yield a lower loss score.\n\\[ L(\\mathbf{y},\\mathbf{\\hat{y}} + \\mathbf{r}) \\le L(\\mathbf{y},\\mathbf{\\hat{y}})\\]\nThe problem is that this will only work for in-sample data, because we only know the nudge values for the cases which are present in the training dataset. In order for our model to generalize to unseen test data, we need a way to get the nudge values for new observations of the independent variables. So how can we do that?\nWell what if we fit another model \\(h(\\mathbf{X})\\) that used our same features \\(\\mathbf{X}\\) to predict our desired nudge values \\(\\mathbf{r}\\), and then we added that new model to our original model \\(F(\\mathbf{X})\\). For a given prediction the nudge model \\(h(\\mathbf{X})\\) would essentially return an approximation of the desired nudge, so adding it would push the prediction in the right direction to decrease the loss function. Furthermore, the nudge model can return predictions of the nudges for out-of-sample cases which are not present in the training dataset. Since both the initial model \\(F(\\mathbf{X})\\) and the nudge model \\(h(\\mathbf{X})\\) are functions of our features \\(\\mathbf{X}\\), we can add the two functions to get an updated model that can generalize beyond the training data.\n\\[F_{\\text{new}} (\\mathbf{X}) = F(\\mathbf{X}) + h(\\mathbf{X})\\]"
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#a-generalized-gradient-boosting-algorithm",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#a-generalized-gradient-boosting-algorithm",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "A generalized gradient boosting algorithm",
    "text": "A generalized gradient boosting algorithm\nOk, let‚Äôs put these pieces of intuition together to create a more general gradient boosting algorithm recipe.\nWe begin with training data \\((\\mathbf{y}, \\mathbf{X})\\) where \\(\\mathbf{y}\\) is a length-\\(n\\) vector of target values, and \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix with \\(n\\) observations of \\(p\\) features. We also have a differentiable loss function \\(L(\\mathbf{y}, \\mathbf{\\hat{y}})\\), a ‚Äúlearning rate‚Äù hyperparameter \\(\\eta\\), and a fixed number of model iterations \\(M\\).\nWe create an initial model \\(F_0(\\mathbf{X})\\) that predicts a constant value. We choose the constant value that would give the best loss score.\n\\[F_0(\\mathbf{X}) = \\underset{c}{\\operatorname{argmin}} L(\\mathbf{y}, c)\\]\nThen we iteratively update the initial model with \\(M\\) nudge models.\nFor \\(m\\) in 0 to \\(M-1\\):\n\nCompute current composite model predictions \\(\\mathbf{\\hat{y}}_{m} = F_{m}(\\mathbf{X})\\).\nCompute the desired nudge values given by the negative gradient of the loss function with respect to each prediction \\(\\mathbf{r}_m = - \\nabla_{\\mathbf{\\hat{y}}_m} L (\\mathbf{y}, \\mathbf{\\hat{y}}_m)\\).\nFit a weak model (e.g.¬†shallow decision tree) \\(h_{m}(\\mathbf{X})\\) that predicts the nudge values \\(\\mathbf{r}_{m}\\) using features \\(\\mathbf{X}\\).\nUpdate the composite model.\n\n\\[F_{m+1}(\\mathbf{X}) = F_{m}(\\mathbf{X}) + \\eta h_{m}(\\mathbf{X})\\]\nAfter \\(M\\) iterations, we are left with the final composite model \\(F_M(\\mathbf{X})\\)."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#wait-in-what-sense-is-this-doing-gradient-descent",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#wait-in-what-sense-is-this-doing-gradient-descent",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Wait, in what sense is this doing gradient descent?",
    "text": "Wait, in what sense is this doing gradient descent?\nIn my previous post, we learned how to use gradient descent to iteratively update model parameters to find a model that minimizes the loss function. We could write the update rule as\n\\[ \\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_{t} + \\eta ( - \\nabla_{\\mathbf{\\theta}} L(\\mathbf{y}, \\mathbf{\\hat{y}}_{\\mathbf{\\theta}_{t}}) ) \\]\nwhere the predictions \\(\\mathbf{\\hat{y}}\\) depend on the model parameters \\(\\mathbf{\\theta}\\), and we‚Äôre trying to find the value of the parameter vector \\(\\mathbf{\\theta}\\) that minimizes the loss function \\(L(\\mathbf{y}, \\mathbf{\\hat{y}}_{\\mathbf{\\theta}_{t}})\\), so we nudge the vector \\(\\mathbf{\\theta}_t\\) by the negative gradient of \\(L(\\mathbf{y}, \\mathbf{\\hat{y}}_{\\mathbf{\\theta}_{t}})\\) with respect to \\(\\mathbf{\\theta}_t\\). Compare that with the boosting model update rule we obtained in the previous section.\n\\[F_{m+1}(\\mathbf{X}) = F_{m}(\\mathbf{X}) + \\eta h_{m}(\\mathbf{X})\\]\nwhere \\(h_{m}(\\mathbf{X}) \\approx - \\nabla_{\\mathbf{\\hat{y}}_m} L (\\mathbf{y}, \\mathbf{\\hat{y}}_m)\\).\nIf we replace \\(F(\\mathbf{X})\\) with its prediction vector \\(\\mathbf{\\hat{y}}\\), and we replace the nudge model \\(h(\\mathbf{X})\\) with the negative gradient of the loss function (which it approximates), the likeness to the parameter gradient descent update rule becomes more obvious.\n\\[\\mathbf{\\hat{y}}_{m+1} \\approx \\mathbf{\\hat{y}}_m + \\eta (- \\nabla_{\\mathbf{\\hat{y}}_m} L (\\mathbf{y}, \\mathbf{\\hat{y}}_m))\\]\nIndeed, gradient boosting is performing gradient descent to obtain a good model by minimizing a loss function. But there are a couple of key differences between gradient boosting and the parameter gradient descent that we discussed in the previous post."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-versus-parameter-gradient-descent",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-versus-parameter-gradient-descent",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Gradient boosting versus parameter gradient descent",
    "text": "Gradient boosting versus parameter gradient descent\nThe generic gradient boosting algorithm outlined above implies two key differences from parameter gradient descent.\n\nInstead of nudging parameters, we nudge each individual prediction, thus instead of taking the gradient of loss with respect to the parameters, we take the gradient with respect to the predictions.\nInstead of directly adding the negative gradient to our current parameter values, we create a functional approximation of the negative gradient and add that to our model. Our functional approximation is just a crappy model that tries to use the model features to predict the negative gradient of the loss with respect to our current model predictions.\n\nThe true genius of the gradient boosting algorithm is in chasing the negative gradient of the loss with crappy models, rather than using it to directly update our predictions. If we just directly added the negative gradient of the loss to our predictions, and plugged them into the loss function we could get a lower loss score, but our updated model would be useless since it couldn‚Äôt make predictions on new out-of-sample data. Instead we train a crappy model to predict the negative gradient of the loss with respect to the current model predictions, thus we can iteratively update our composite model by adding these crappy models to it."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-is-gradient-descent-in-function-space-a.k.a.-prediction-space",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#gradient-boosting-is-gradient-descent-in-function-space-a.k.a.-prediction-space",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Gradient boosting is gradient descent in function space, a.k.a. prediction space",
    "text": "Gradient boosting is gradient descent in function space, a.k.a. prediction space\nLet‚Äôs address the statement in Friedman‚Äôs classic paper that gradient boosting is doing gradient descent in function space. Again we‚Äôll use parameter gradient descent as a basis for comparison.\nIn parameter gradient descent, we have a vector of parameter values which, when plugged into the loss function, return some loss score. At each step of gradient descent, we compute the negative gradient of the loss function with respect to each parameter; that tells us which way to nudge each parameter value to achieve a lower loss score. We then add this vector of parameter nudge values to our previous parameter vector to get the new parameter vector. We could view this sequence of successive parameter vectors as a trajectory passing through parameter space, the space spanned by all possible parameter values. Therefore parameter gradient descent operates in parameter space.\nIn contrast, when we do gradient boosting, at each step we have a model, a.k.a. a function, that maps feature values to predictions. Given our training dataset, this model yields predictions which can be plugged into our loss function to get a loss score. At each boosting iteration, we compute the negative gradient of the loss with respect to each of the predictions; that tells us which way to nudge each prediction to achieve a lower loss score. We then create a function (a crappy model) that takes feature values and returns an approximation of the corresponding prediction‚Äôs nudge value. We then add this crappy model (a function) to our current composite model (also a function) to get the new composite model (you guessed it; also a function). And so by analogy with parameter vectors in parameter space, we can view this sequence of successive model functions as a trajectory passing through function space, the space spanned by all possible functions that map feature values to predictions. Therefore, gradient boosting does gradient descent in function space.\nIf this talk about function space still feels a little abstract, you could just use the same substitution trick we used above and swap the model \\(F(\\mathbf{X})\\) for its predictions \\(\\mathbf{\\hat{y}}\\) which is just a vector of numbers. The target values for our nudge models are given by the negative gradient of the loss with respect to this prediction vector. From here, we can see that each time we add a new nudge model to our composite model, we get a new prediction vector. We can view this sequence of successive prediction vectors as a trajectory passing through prediction space, the space spanned by all possible prediction vector values. Therefore we can also say that gradient boosting does gradient descent in prediction space."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#so-why-did-we-fit-the-crappy-models-to-the-residuals-in-our-regression-gbm",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#so-why-did-we-fit-the-crappy-models-to-the-residuals-in-our-regression-gbm",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "So why did we fit the crappy models to the residuals in our regression GBM?",
    "text": "So why did we fit the crappy models to the residuals in our regression GBM?\nIn my first post on [gradient boosting machines](/gradient-boosting-machine-from-scratch, in the interest of simplicity I left one key aspect of the problem unaddressed, that is, what loss function were we using to train that GBM? It turns out that because of the way we built our GBM, without knowing it we were actually using a mean squared error (MSE) loss function.\n\\[L(\\mathbf{y}, \\hat{\\mathbf{y}}) =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nIf the GBM was using gradient descent to find a \\(\\hat{\\mathbf{y}}\\) vector that minimized this loss function, then at each iteration it would have to nudge the current \\(\\hat{\\mathbf{y}}\\) by the negative gradient of the loss function with respect to \\(\\hat{\\mathbf{y}}\\), i.e.¬†\\(-\\nabla_{\\hat{\\mathbf{y}}} L(\\mathbf{y}, \\hat{\\mathbf{y}})\\). Since our loss function takes a length \\(n\\) vector of predictions \\(\\hat{\\mathbf{y}}\\) as input, the gradient will be a length-\\(n\\) vector of partial derivatives with respect to each of the predictions \\(\\hat{y}_i\\). Let‚Äôs start by taking the negative partial derivative with respect to a particular prediction \\(\\hat{y}_j\\).\n\\[\n\\begin{array}{rcl}\n-\\frac{\\partial}{\\partial \\hat{y}_j} L(\\mathbf{y}, \\mathbf{\\hat{y}})\n    & = & -\\frac{\\partial}{\\partial \\hat{y}_j} \\left ( \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\right ) \\\\\n    & = & -\\frac{\\partial}{\\partial \\hat{y}_j} \\left ( \\frac{1}{n} (y_j - \\hat{y}_j)^2 \\right ) \\\\\n    & = & -\\frac{1}{n} (2)(y_j - \\hat{y}_j) \\frac{\\partial}{\\partial \\hat{y}_j} (y_j - \\hat{y}_j) \\\\\n    & = & \\frac{2}{n} (y_j - \\hat{y}_j) \\\\\n\\end{array}\n\\]\nIt turns out that the negative partial derivative of the MSE loss function with respect to a particular prediction \\(\\hat{y}_i\\) is proportional to the residual \\(y_i - \\hat{y}_i\\)! This is a pretty intuitive result, because if we nudge a prediction by it‚Äôs residual, we‚Äôll end up with the correct target value.\nWe can go ahead and write the nudge vector as\n\\[\\mathbf{r} = -\\nabla_{\\hat{\\mathbf{y}}} L(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{2}{n}(\\mathbf{y} - \\hat{\\mathbf{y}})\\]\nwhich is proportional to the residual vector \\(\\mathbf{y} - \\hat{\\mathbf{y}}\\). This means that when we use the mean squared error loss function, our nudge values are given by the current model residuals, and therefore each new crappy model targets the previous model‚Äôs residuals.\nAnd this result brings us full circle, back to our original intuition from the first GBM post about chasing residuals with crappy models. Now we see that intuitive idea is just a special case of the more general and, dare I say, even more beautiful idea of chasing the negative gradient of the loss function with crappy models."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#key-takeaways",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#key-takeaways",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nWe covered a lot of conceptual ground in this post, so let‚Äôs recap the key ideas.\n\nGradient boosting can use gradient descent to minimize any differentiable loss function in service of creating a good final model.\nThere are two key differences between gradient boosting and parameter gradient descent:\n\nIn gradient boosting, we nudge prediction values rather than parameter values, so to find the desired nudge values, we take the negative gradient of the loss function with respect to the predictions.\nIn gradient boosting, we nudge our predictions by adding a crappy model that approximates the nudge values, rather than adding the nudge values directly to the predictions.\n\nGradient boosting does gradient descent in function space. But since the model predictions are just numeric vectors, and since we take the gradient of the loss function with respect to the prediction vector, it‚Äôs also valid and probably easier to think of gradient boosting as gradient descent in prediction space.\nWe saw that iteratively fitting crappy models to the previous model residuals, as we did in the regression GBM from scratch post, is just a special case of fitting crappy models to the negative gradient of the loss function (in this case the mean squared error loss)."
  },
  {
    "objectID": "posts/how-gradient-boosting-does-gradient-descent/index.html#wrapping-up",
    "href": "posts/how-gradient-boosting-does-gradient-descent/index.html#wrapping-up",
    "title": "How Gradient Boosting Does Gradient Descent",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nPhew, there it is, how gradient boosting models do gradient descent in function space. Understanding how the general form of gradient boosting works opens up the possibility for us to use any differentiable loss function for model training. That is pretty exciting because it means that we can get a lot of mileage out of this one class of learning algorithms. Stay tuned for more on some of the awesome things we can do with these ideas in future posts!\nThere are a couple of resources I found to be super helpful while researching the content in this post. Definitely check them out if you want to read more about gradient boosting and gradient descent.\nHow to explain gradient boosting by Terence Parr and Jeremy Howard\nUnderstanding Gradient Boosting as Gradient Descent by Nicolas Hug"
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "",
    "text": "Ahh, blogging. I think we can all agree it‚Äôs probably one of the greatest forms of written communication to have ever existed.\nWhats that you say? You‚Äôd like to set up your own blog? And you say you want to use a dead simple, data science friendly tech stack? And you wouldn‚Äôt be caught dead handing over your painstakingly crafted content to Medium? No worries, friend, I know exactly what you need.\nEnter Quarto.\nIn this post we‚Äôll set up a blog using a lightweight tech stack consisting of a terminal running quarto, git, and jupyter, and we‚Äôll use Github Pages to host our website for free. Optionally, for a few dollars a year, we can even host our website at our own custom domain.\nA quick note on how to use this post. Quarto‚Äôs documentation on blogging provides a nice high-level overview of the blogging workflow, and I refer to it and many other bits of Quarto documentation here. At the time of writing, the handful of other blog posts about setting up quarto blogs are aimed at the RStudio user. This post exists to provide a jupyter and python-centric path for you to follow through the entire setup of your new quarto blog, and to impart my opinionated recommendations about best practices.\nLet‚Äôs get into it!"
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#what-is-quarto",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#what-is-quarto",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto is a way to render plain text source files containing markdown and code in python, R, and other languages into published formats like websites, books, slides, journal articles, etc. There is clearly a lot that we can do with it, but Today, we‚Äôll use it to make a nice looking blog out of some jupyter notebook files.\nQuarto follows the familiar convention of using a project directory to house all material for a given project. The directory will include source files like jupyter notebooks or Rmarkdown files, as well as configuration files that control how output files are rendered. We can then use the quarto command line utility to perform actions like previewing and rendering within the project directory."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#instantiate-your-blog",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#instantiate-your-blog",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Instantiate your blog",
    "text": "Instantiate your blog\n\nCreate a new Quarto project\nAfter installing quarto fire up a new terminal and check that the install was successful by running\nquarto --version\nNow think of a name for your blog‚Äôs project directory; this will also be the name of its git repository. The name will have no effect on your website‚Äôs name or URL, so don‚Äôt think too hard. The quarto documentation calls it myblog, so we‚Äôll one-up them and call ours pirate-ninja-blog. Run the following command to create it in the current directory.\nquarto create-project pirate-ninja-blog --type website:blog\nThat command creates a directory called pirate-ninja-blog containing everything you need to render your new blog. You can preview your website by running\nquarto preview pirate-ninja-blog\nYour local website will open in a new browser window. As you edit various aspects of your blog, the preview will update with your changes. This preview feature is so simple and so great.\n\n\n\nPreviewing your blog with quarto preview command\n\n\n\n\nSet up a git repo\nChange into your project directory and we‚Äôll start setting up your git repo.\ncd pirate-ninja-blog\ninitialize a new git repo.\ngit init -b main\nThe _site/ directory is where quarto puts the rendered output files, so you‚Äôll want to ignore it in git. I also like to just ignore any hidden files too, so add the following to your .gitignore file.\n\n\n.gitignore\n\n/.quarto/\n/_site/\n.*\n\nFor now we‚Äôll just stage the .gitignore file for the initial commit. Eventually you‚Äôll want to commit the other files in your project too, either now or later as you edit them.\ngit add .gitignore \ngit commit -m \"Initial commit.\"\nThen follow GitHub‚Äôs instructions to add the local repo to GitHub using git. Basically just create a new blank repo on GitHub‚Äôs website, copy the remote repository url, then add the remote repo url to your local git repo.\ngit remote add origin &lt;REMOTE_URL&gt;\nThen you‚Äôll be able to push any commits you make to your remote repository on GitHub by saying git push."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#understand-the-components-of-a-quarto-blog",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#understand-the-components-of-a-quarto-blog",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Understand the components of a Quarto blog",
    "text": "Understand the components of a Quarto blog\n\nContents of the quarto project directory\nLet‚Äôs have a quick look at what quarto put inside of the project directory.\n_quarto.yml\nabout.qmd\nindex.qmd\nprofile.jpg\nposts\nstyles.css\n_site\n\nQuarto uses yaml files to specify configurations. The _quarto.yml file specifies project-wide configurations.\nQuarto‚Äôs markdown file type uses extension qmd``. Each qmd file will correspond to a page in our website.index.qmdis the homepage andabout.qmd` is the About page.\nprofile.jpg is an image that is included on the about page.\nstyles.css defines css styles for the website.\nposts is a directory where we can put qmd and other documents which will be rendered into blog posts.\nposts/_metadata.yml contains configurations that apply to all documents in the posts directory.\n_site is a directory that contains the rendered website. Whereas all the other files and directories constitute the source code for our blog, _site is the rendered output, i.e.¬†the website itself.\n\nLet‚Äôs take a closer look at these components and start to make the blog yours.\n\n\nProject-wide Configurations\nThe _quarto.yml file controls project-wide configurations, website options, and HTML document options. Options in this file are specified in yaml in a key/value structure with three top level keys: project, website, and format. The quarto website options documentation has the full list of options that you can set here. It will be very helpful to take a look at some example _quarto.yml files in the wild, such as the one from quarto.org or even the one from this blog.\nUnder the website key, go ahead and set the title and description for your blog.\nwebsite:\n  title: \"Pirate Ninja Blog\"\n  description: \"A blog about pirates, ninjas, and other things\"\nYou can also customize your navbar which is visible at the top of all pages on your site. Also go ahead and set your github and twitter urls for the icons in the navbar.\nUnder the format key, you can also try changing the HTML theme to one of the other 25 built-in themes.\n\n\nThe About Page\nThe about.qmd file defines an About page for the blog. Go ahead and fill in your details in the about.qmd file; you can also replace the profile.jpg file with your own image. Have a look at the quarto documentation on About pages to explore more functionality. Notably, you can change the template option to change the page layout.\n\n\nThe Homepage\nThe index.qmd file defines the landing page for your website. It is a listing page which shows links to all the pages in the posts directory. For now we don‚Äôt need to change anything here.\n\n\nThe posts/ directory\nThe posts directory contains all your blog posts. There aren‚Äôt really requirements for subdirectory structure inside the posts directory, but it‚Äôs a best practice to create a new subdirectory for each new blog post. This just helps keep auxillary files like images or conda environment files organized. Out of the box, the posts directory looks like this.\nposts\n‚îú‚îÄ‚îÄ _metadata.yml\n‚îú‚îÄ‚îÄ post-with-code\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ image.jpg\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ index.qmd\n‚îî‚îÄ‚îÄ welcome\n    ‚îú‚îÄ‚îÄ index.qmd\n    ‚îî‚îÄ‚îÄ thumbnail.jpg\nThere are two reasons we want to be deliberate about how we organize and name things in the posts directory. First, the vast majority of our blog‚Äôs content will live here, so we don‚Äôt want it to be a big confusing mess. Second, the directory sstructure and file naming will be reflected in the URLs to our blog posts; if you prefer tidy-looking URLs, and I know you do, then you want to use tidy directory and file names in the posts directory.\nYou can check how the URLs look by navigating to one of the pre-populated posts in the site preview in your browser. For instance, the welcome post‚Äôs URL would be\nhttps://example.com/posts/welcome/\nWhen quarto renders the qmd file at posts/welcome/index.qmd it creates an output document in the website at posts/welcome/index.html. In fact the full URL to the post is,\nhttps://example.com/posts/welcome/index.html\nbut the browser knows if you give it a URL with a path ending in a /, then it should look for the index.html file inside that directory.\nSo I think the best practice here is to name your new post subdirectory with the title of the post in all lower case with dashes for spaces, e.g.¬†post-with-code. Then to force all output pages to be called index.html, you can set the output-file key in the posts/_metadata.yml file like this.\n\n\nposts/_metadata.yml\n\noutput-file: index.html\n\nNote that alternative naming conventions are possible; notably you might want to prefix each post name with the date in yyyy-mm-dd format, so the post subdirectories sort temporally and look nice in a list. That‚Äôs the convention used in Quarto‚Äôs own blog at quarto.org, As long as you keep everything for a given post inside its subdirectory, you should be good to go with nice-looking URLs."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#authoring-posts-with-jupyter",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#authoring-posts-with-jupyter",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Authoring posts with jupyter",
    "text": "Authoring posts with jupyter\n\nCreating a new post\nIt turns out that quarto will render not only .qmd files, but also .ipynb files in the posts directory. So let‚Äôs create a new blog post from a notebook.\nI think it‚Äôs a best practice to write draft posts in their own git branches, that way if you need to deploy some kind of hotfix to main while you‚Äôre drafting a post, you won‚Äôt have to deploy a half-written post livin on the main branch. To start a new post, create a new development branch, change into the posts directory, create a new subdirectory with your preferred naming convention, change into that new directory, and fire up jupyter.\ngit checkout -b new-post\ncd posts\nmkdir new-post\ncd new-post\njupyter notebook\nNow create a new notebook from the jupyter UI. In order for quarto to recognize the document, the first cell of the notebook must be a raw text cell (press r in command mode to change a cell to raw text), and it must contain the document‚Äôs yaml front matter. You can use the following as a frontmatter template.\n---\ntitle: New Post\ndate: 2023-07-12\ndescription: A nice new post\ncategories: [nonsense, code]\n---\nNow to preview your post, open a new terminal, change into your blog‚Äôs project directory and run the quarto preview command. You‚Äôll see a link to the new post in the listing on the homepage. I usually like to have the preview open in a browser while I‚Äôm editing the jupyter notebook, just to make sure things look the way I want in the rendered output. From here you can keep editing the notebook, and the preview will update in the browser dynamically.\n\n\nMarkdown and code cells\nFrom here you can put text in markdown cells and you can write code in code cells. Let‚Äôs add a markdown cell with some markdown formatting.\n## A nice heading\n\nHere is some lovely text and an equation.\n\n$$ a^2 + b^2 = c^2 $$\n\nHere's a list.\n\n- a link to an [external website](https://quarto.org).\n- a link to [another post in this blog](/posts/welcome/index.qmd).\nThis markdown will be rendered into the HTML page for the post. The last line in the above cell demonstrates the best practice for using relative urls to link to other resources within your website. Instead of providing the full url in the parentheses, just give the path to the qmd or ipynb file that you want to link to. Note that paths need to start with the / at the root of the quarto project, since without it, quarto will try to resolve paths relative to the location of the current document instead of the root of the project.\nThen create a code cell with some code. Try something like this.\nprint('Hello, Quarto!')\nBy default, both code and cell output will be rendered into the HTML output. So far our jupyter notebook looks like this.\n\n\n\nView of a new post being written in jupyter notebook\n\n\nBack in the browser window running your blog preview, you can see the rendered page of the new post.\n\n\n\nView of the preview of the rendered post\n\n\n\n\nFigures\nLet‚Äôs add a figure to our post. Add a new code cell with the following code.\n# | fig-cap: This is my lovely line plot\n# | fig-alt: A line plot extending up and to the right\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.arange(10)\ny = 2 * x + 1\nplt.plot(x, y);\nNotice a couple of important details. First I placed a semicolon at the end of the last line. That supresses the [&lt;matplotlib.lines.Line2D at 0x1111d00a0&gt;] text output, which would otherwise show up in your blog post too.\nSecond, I added a couple of special comments at the top of the cell. Quarto allows you to specify numerous code execution options, designated by the # | prefix, to control the behavior and appearance of the code and output at a cell level. I set two keys here, fig-cap and fig-alt which respectively set the figure caption text and the image alt tag text. The fig-alt key is particularly important to set on all your figures because it provides the non-visual description for screenreader users reading your post. The alt tag should be a simple description of what the plot is and possibly what it shows or means. Be a friend of the blind and visually impaired community and set fig-alt on all of your figures.\n\n\nVersion control\nAs you edit your new post, go ahead and commit your changes on your development branch. Once you‚Äôve finished your new post, you can merge it into main like this.\ngit checkout main\ngit merge new-post\nThen you can push to GitHub by running git push. You should also be sure to run a final quarto preview to check that everything looks good before publishing to the web."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#publishing-your-blog-to-the-web",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#publishing-your-blog-to-the-web",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Publishing your blog to the web",
    "text": "Publishing your blog to the web\n\nHosting with GitHub Pages\nIt‚Äôs likely that the easiest (read best) option for you is to host your blog on GitHub Pages. This is because GitHub pages is free, and since you already have your blog‚Äôs source code checked into a remote repository at GitHub, it‚Äôs very easy to set up. Quarto‚Äôs documentation on publishing to GitHub Pages outlines three ways to publish your website, but I recommend their option 2, using the quarto publish command. Once you set up your gh-pages branch as described in the documentation, you simply run quarto publish at the command line and your updates are deployed to your website.\n\n\nSetting up your domain name\nBy default, if you choose to host with GitHub Pages, your website will be published to a url in the form https://username.github.io/reponame/. You can certainly do this; for example Jake VanderPlas‚Äôs awesome blog Pythonic Perambulations lives at http://jakevdp.github.io.\nBut, like me, you might want to get your own custom domain by buying, or really renting, one from a registrar. I use Namecheap. If you decide to go for a custom domain, refer to GitHub‚Äôs documentation on custom domains. You‚Äôll also need to point your domain registrar to the IP address where GitHub Pages is hosting your website. For an example of how to do this at Namecheap, see Namecheap‚Äôs documentation about GitHub Pages\nWhether you decide to use the standard github.io domain or your own custom domain, be sure to set the site-url key in your _quarto.yml file to ensure other quarto functionality works correctly. For example\n\n\n_quarto.yml\n\nwebsite:\n  site-url: https://example.com/\n\nEdit: I found that after upgrading to quarto 1.3, using quarto publish to publish from the gh-pages branch obliterates the CNAME file that is created when you set a custom domain in your repository settings &gt; Pages &gt; Custom Domain. That breaks the mapping from your custom domain to your published website. See this disscussion thread for details. The fix is to manually create a CNAME file in the root of your project, and include it in the rendered website using the resources option under the project key in _quarto.yml. The CNAME file should just contain your custom domain, excluding any https://.\n\n\nCNAME\n\nexample.com\n\nWith the CNAME file in the root of your quarto project, you can then include it in the rendered output.\n\n\n_quarto.yml\n\nproject:\n  resources:\n    - CNAME"
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#keep-in-touch-with-your-readers",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#keep-in-touch-with-your-readers",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Keep in touch with your readers",
    "text": "Keep in touch with your readers\n\nRSS Feed\nThe RSS feed is handy for syndicating your posts to feed readers, other websites, and to your email subscribers. As described in quarto‚Äôs documentation on RSS feeds, you can automatically generate an RSS feed for your blog by first setting the value of site-url under the website key in _quarto.yml, and then setting feed: true under the listing key in the frontmatter of index.qmd. This will generate an RSS feed in the root of your website called index.xml. Once you have an RSS feed, go ahead and submit it to Python-Bloggers to have your work syndicated to a wider audience and to strengthen our little community of independent data science blogs.\n\n\nEmail Subscriptions\nThe idea here is to have a form field on your website where readers can input their email address to be added to your mailing list. Quarto‚Äôs documentation on subscriptions describes how to set up a subscribe box on your blog using MailChimp, so we won‚Äôt repeat it here. Once you have some subscribers, you can send them updates whenever you write a new post. You could do this manually or, in my case, set up an automation through MailChimp which uses your RSS feed to send out email updates to the list about new posts.\n\n\nComments\nQuarto has build-in support for three different comment systems: hypothesis, utterances, and giscus. The good news is that these are all free to use, easy to set up, and AFAIK do not engage in any sketchy tracking activities. The bad news is that none of them are ideal because they all require the user to create an account and login to leave a comment. We want to encourage readers to comment, so we don‚Äôt want them to have to create accounts or deal with passwords or pick all the squares with bicycles or any such nonsense, just to leave a little comment. To that end, I‚Äôve actually been working on self-hosted login-free comments for this blog using isso, but it‚Äôs a bit more involved than these built-in solutions, so we‚Äôll have to discuss it at length in a future post.\nIf you prefer an easy, out-of-the-box solution, I can recommend utterances, which uses GitHub issues to store comments for each post. I used utterances for comments on the first jekyll-based incarnation of this blog; you can still see the utterances comments on posts before this one. Go check out the Quarto documentation on comments to see how to set up utterances in your project.\n\n\nAnalytics\nAs a data enthusiast, you‚Äôll likely enjoy collecting some data about page views and visitors to your site. You might be tempted to use Google Analytics to do this; indeed quarto makes it very easy to just add a line to your _quarto.yml file to set it up. Unfortunately, in this case, going with the easy and free solution means supporting Google‚Äôs dubious corporate surveillance activities. Be a conscientious internet citizen and avoid using Google Analytics on your blog. Fortunately, there are numerous privacy-friendly alternatives to Google Analytics. For this blog I‚Äôm self-hosting umami analytics, which might warrant its own post in the future."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#more-humbly-suggested-best-practices",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#more-humbly-suggested-best-practices",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "More humbly suggested best practices",
    "text": "More humbly suggested best practices\n\nUsing conda environments for reproducibility\nAs you know, it‚Äôs a good practice to use an environment manager to keep track of packages, their versions, and other dependencies for software in a data science project. The same applies to blog posts; especially if you‚Äôre using unusual or bleeding-edge packages in a post. This will help us out a lot when we have to go back and re-run a notebook a couple years later to regenerate the output. Here we‚Äôll use conda as our environment manager.\nTo be clear, I don‚Äôt bother doing this if I‚Äôm just using fairly stable functionality in standard packages like pandas, numpy, and matplotlib, but we‚Äôll do it here for illustration. From a terminal sitting inside our post subdirectory at posts/new-post, create a new conda environment with the packages you‚Äôre using in the post.\nconda create -p ./venv jupyter numpy matplotlib\nNote the -p flag which tells conda to save the environment to ./venv in the current working directory. This will save all the installed packages here in the post directory instead of in your system-wide location for conda environments. Note also that you‚Äôll want to avoid checking anything in the venv directory into source control, so add venv to the .gitignore file at the root of the quarto project to ignore all venv directories throughout your quarto project.\nNow whenever you work on this post, you‚Äôll navigate to the post subdirectory with a terminal and activate the conda environment.\nconda activate ./venv\nThen you can fire up your jupyter notebook from the command line, and it will use the active conda environment.\nSince we don‚Äôt want to check the venv directory with all its installed libraries into source control, we need to create an environment.yml file from which the environment can later be reproduced. With the local conda environment active, run the following.\nconda env export --from-history &gt; environment.yml\nThe --from-history flag tells conda to skip adding a bunch of system specific stuff that will gunk up your environment yaml file and make it harder to use for cross-platform reproducibility. This environment.yml file is the only environment management artifact that you need to check into git.\nLater if you need to recreate the environment from the environment.yml file, you can use the following command.\nconda env create -f environment.yml -p ./venv`\n\n\nImage file best practices\nLet‚Äôs talk about image file sizes. The key idea is that we want images to have just enough resolution to look good; any more than that and we‚Äôre just draging around larger-than-necessary files and wasting bandwidth and slowing down page load times.\nYou can read all about choosing optimal image sizes, but the TLDR is that images should be just large enough (in pixels) to fill the containers they occupy on the page. In our quarto blog, the two most common kinds of images are inline images we put in the body of posts and image thumbnails that show up as the associated image for a post, e.g.¬†in the listing on our homepage. The inline image container seems to be about 800 pixels wide in my browser and the thumbnails are smaller, so adding some margin of error, I decided to go for 1000x750 for inline images and 500x375 for the thumbnails.\nI use a command line tool called Image Magick to resize image files. Go ahead and install image magick with homebrew, and let‚Äôs add some images to our new post.\nFor this example I‚Äôll use a nice shot of the London Underground from Wikipedia. Save your image as image.jpg. Then use image magick to create two new resized images for inline and thumbnail use.\nconvert image.jpg -resize 1000x1000 main.jpg \nconvert image.jpg -resize 500x500 thumbnail.jpg \nThese commands do not change the aspect ratio of the image; they just reduce the size so that the image fits within the size specified.\nNow move both of your new images into the post subdirectory at posts/new-post/. To specify the thumbnail image, set the image key in the post‚Äôs front matter. Be sure to also add an alt tag description of the image using the image-alt key to keep it accessible for screen reader users. Our post‚Äôs frontmatter now looks like this.\n---\ntitle: New Post\ndate: 2023-07-12\ndescription: A nice new post\ncategories: [nonsense, code]\nimage: thumbnail.jpg\nimage-alt: \"A London Underground train emerging from a tunnel\"\n---\nTo include an image within the body of a post, use markdown in the post to include the image. I added a markdown cell just under the front matter containing the following.\n![A London Underground train emerging from a tunnel](main.jpg \"\")\nIn your preview browser window, you can see we have the thumbnail for our new post on the homepage listing.\n\n\n\nA screenshot of the homepage showing the new post‚Äôs thumbnail image\n\n\nAnd we also have the inline image appearing in the body of the post.\n\n\n\nA screenshot of the new post showing the image included in the body of the post\n\n\nYou can take a look at the source code for this blog to see some examples of including images in posts."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#seo",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#seo",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "SEO",
    "text": "SEO\nSEO is a huge topic, but here we‚Äôll just focus on a few fundamental technical aspects that we want to be sure to get right. This boils down to registering with the top search engines by market share and ensuring that we‚Äôre providing them with the information they need to properly index our pages.\nI checked the top search engines by global market share and as of 2023 it looks like Google has about 85%, Bing has about 8%, and the others have 2% or less each. So let‚Äôs focus on setting our site up to work well with Google search and Bing to get over 90% coverage.\n\nGoogle Search Console and Bing Webmaster Tools\nGoogle Search Console is a tool for web admins to help analyze search traffic and identify any technical issues that might prevent pages from appearing or ranking well in search. Go ahead and set up an account and register your blog in search console. You can refer to Google‚Äôs documentation on search console to guide you through setup and configuration.\nOnce you get set up on GSC, you can also create an account for Bing Webmaster Tools. Do this after setting up GSC because there is an option to import your information from your GSC account.\nOnce you‚Äôre set up with GSC and BWT, you‚Äôll get email alerts anytime they crawl your site and detect any indexing problems. When that happens, track down the issues and fix them so your pages can appear in organic searches.\n\n\nSitemap\nA sitemap is an xml document that lists all the pages on your website. It‚Äôs a map for the search engine bots that crawl the web looking for new pages to index. Quarto will automatically generate a sitemap called sitemap.xml in the root of your website, as long as you‚Äôve filled out the site-url key in _quarto.yml. You can submit your website for indexing by providing your sitemap in Google Search Console and Bing Webmaster Tools."
  },
  {
    "objectID": "posts/blogging-with-quarto-and-jupyter/index.html#wrapping-up",
    "href": "posts/blogging-with-quarto-and-jupyter/index.html#wrapping-up",
    "title": "Blogging with Quarto and Jupyter: The Complete Guide",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nBoy howdy, that was a lot, but at this point you should have a fully functioning blog, built with a minimalist, data-science-friendly tech stack consisting of quarto, jupyter, and GitHub. If you do create a blog using quarto, drop a link to it in the comments, and we can all check it out and celebrate your creation!"
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html",
    "href": "posts/consider-the-decision-tree/index.html",
    "title": "Consider the Decision Tree",
    "section": "",
    "text": "A California cypress tree abides in silence on Alameda Beach.\nAh, the decision tree. It‚Äôs an underrated and often overlooked hero of modern statistical learning. Trees aren‚Äôt particularly powerful learning algorithms on their own, but when utilized as building blocks in larger ensemble models like random forest and gradient boosted trees, they can achieve state of the art performance in many practical applications. Since we‚Äôve been focusing on gradient boosting ensembles lately, let‚Äôs take a moment to consider the humble decision tree itself. This post gives a high-level intuition for how trees work, an opinionated list of their key strengths and weaknesses, and some perspective on why ensembling makes them truly shine.\nOnward!"
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#classification-and-regression-trees",
    "href": "posts/consider-the-decision-tree/index.html#classification-and-regression-trees",
    "title": "Consider the Decision Tree",
    "section": "Classification and Regression Trees",
    "text": "Classification and Regression Trees\nA Decision tree is a type of statistical model that takes features or covariates as input and yields a prediction as output. The idea of the decision tree as a statistical learning tool traces back to a monograph published in 1984 by Breiman, Freidman, Olshen, and Stone called ‚ÄúClassification and Regression Trees‚Äù (a.k.a. CART). As the name suggests, trees come in two main varieties: classification trees which predict discrete class labels (e.g.¬†DecisionTreeClassifier) and regression trees which predict numeric values (e.g.¬†DecisionTreeRegressor).\nAs I mentioned earlier, tree models are not very powerful learners on their own. You might find that an individual tree model is useful for creating a simple and highly interpretable model in specific situations, but in general, trees tend to shine most as building blocks in more complex algorithms. These composite models are called ensembles, and the most important tree ensembles are random forest and gradient boosted trees. While random forest uses either regression or classification trees depending on the type of target, gradient boosting can use regression trees to solve both classification and regression tasks."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#regression-tree-in-action",
    "href": "posts/consider-the-decision-tree/index.html#regression-tree-in-action",
    "title": "Consider the Decision Tree",
    "section": "Regression Tree in Action",
    "text": "Regression Tree in Action\nLet‚Äôs have a closer look at regression trees by training one on the diabetes dataset from scikit learn. According to the documentation:\n\nTen baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n\nFirst we load the data. To make our lives easier, we‚Äôll just use two features: average blood pressure (bp) and the first blood serum measurement (s1) to predict the target. I‚Äôll rescale the features to make the values easier for me to read, but it won‚Äôt affect our tree‚Äìmore on that later.\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor_palette = \"viridis\"\n\n\nfrom sklearn.datasets import load_diabetes\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\nX = 100 * X[['bp', 's1']]\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs grow a tree to predict the target given values of blood pressure and blood serum.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(max_depth=2)\ntree.fit(X,y);\n\n\n\n\n\n\n\n\n\n\nTo make predictions using our fitted tree, we start at the root node (which is at the top), and we work our way down moving left if our feature is less than the split threshold and to the right if it‚Äôs greater than the split threshold. For example let‚Äôs predict the target for a new case with bp= 1 and s1 = 5. Since our blood pressure of 1 is less than 2.359, we move to the left child node. Here, since our serum of 5 is greater than the threshold at 0.875, we move to the right child node. This node has no further children, and thus we return its predicted value of 155.343.\n\ntree.predict(pd.DataFrame({'bp': 1, 's1': 5}, index=[0]))\n\narray([155.34313725])\n\n\nLet‚Äôs overlay these splits on our feature scatterplot to see how the tree has partitioned the feature space.\n\n\n\n\n\n\n\n\n\nThe tree has managed to carve out regions of feature space where the target values tend to be similar within each region, e.g.¬†we have low target values in the bottom left partition and high target values in the far right region.\nLet‚Äôs take a look at the regression surface predicted by our tree. Since the tree predicts the exact same value for all instances in a given partition, the surface has only four distinct values.\n\n\n\n\n\n\n\n\n\nFabulous, now that we‚Äôve seen a tree in action, let‚Äôs talk about trees‚Äô key strengths and weaknesses."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#why-trees-are-awesome",
    "href": "posts/consider-the-decision-tree/index.html#why-trees-are-awesome",
    "title": "Consider the Decision Tree",
    "section": "Why trees are awesome",
    "text": "Why trees are awesome\nTrees are awesome because they are easy to use, and trees are easy to use because they are robust, require minimal data preprocessing, and can learn complex relationships without user intervention.\n\nFeature Scaling\nTrees owe their minimal data preprocessing requirements and their robustness to the fact that split finding is controlled by the sort order of the input feature values, rather than the values themselves. This means that trees are invariant to the scaling of input features, which in turn means that we don‚Äôt need to fuss around with carefully rescaling all the numeric features before fitting a tree. It also means that trees tend to work well even if features are highly skewed or contain outliers.\n\n\nCategoricals\nSince trees just split data based on numeric feature values, we can easily handle most categorical features by using integer encoding. For example we might encode a size feature with small = 1, medium = 2, and large = 3. This works particularly well with ordered categories, because partitioning is consistent with the category semantics. It can also work well even if the categories have no order, because with enough splits a tree can carve each category into its own partition.\n\n\nMissing Values\nIt‚Äôs worth calling out that different implementations of the decision tree handle missing feature values in different ways. Notably, scikit-learn handles them by throwing an error and telling you not to pull such shenanigans.\nValueError: Input contains NaN, infinity or a value too large for dtype('float32').\nOn the other hand, XGBoost supports an elegant way to make use of missing values, which we will discuss more in a later post.\n\n\nInteractions\nFeature interactions can also be learned automatically. An interaction means that the effect of one feature on the target differs depending on the value of another feature. For example, the effect of some drug may depend on whether or not the patient exercises. After a tree splits on exercise, it can naturally learn the correct drug effects for both exercisers and non-exercisers. This intuition extends to higher-order interactions as well, as long as the tree has enough splits to parse the relationships.\n\n\nFeature Selection\nBecause trees choose the best feature and threshold value at each split, they essentially perform automatic feature selection. This is great because even if we throw a lot of irrelevant features at a decision tree, it will simply tend not to use them for splits. Similarly, if two or more features are highly correlated or even redundant, the tree will simply choose one or the other when making each split; having both in the model will not cause catastrophic instability as it could in a linear model.\n\n\nFeature-Target Relationship\nFinally, it is possible for trees to discover complex nonlinear feature-target relationships without the need for user-specification of the relationships. This is because trees use local piecewise constant approximations without making any parametric assumptions. With enough splits, the tree can approximate arbitrary feature-target relationships."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#why-trees-are-not-so-awesome",
    "href": "posts/consider-the-decision-tree/index.html#why-trees-are-not-so-awesome",
    "title": "Consider the Decision Tree",
    "section": "Why trees are not so awesome",
    "text": "Why trees are not so awesome\nThe main weakness of the decision tree is that, on its own, it tends to have poor predictive performance compared to other algorithms. The main reasons for this are the tendency to overfit and prediction quantization issues.\n\nOverfitting\nIf we grow a decision tree until each leaf has exactly one instance in it, we will have simply memorized the training data, and our model will not generalize well. Basically the only defense against overfitting is to reduce the number of leaf nodes in the tree, either by using hyperparameters to stop splitting earlier or by removing certain leaf nodes after growing a deep tree. The problem here is that some of the benefits of trees, like ability to approximate arbitrary target patterns and ability to learn interaction effects, depend on having enough splits for the task. We can sometimes find ourselves in a situation where we cannot learn these complex relationships without overfitting the tree.\n\n\nQuantization\nBecause regression trees use piecewise constant functions to approximate the target, prediction accuracy can deteriorate near split boundaries. For example, if the target is increasing with the feature, a tree might tend to overpredict the target on the left side of split boundaries and overpredict on the right side of split boundaries.\n\n\n\n\n\n\n\n\n\n\n\nExtrapolation\nBecause they are trained by partitioning the feature space in a training dataset, trees cannot intelligently extrapolate beyond the data on which they are trained. For example if we query a tree for predictions beyond the greatest feature value encountered in training, it will just return the prediction corresponding to the largest in-sample feature values.\n\n\n\n\n\n\n\n\n\n\n\nThe Dark Side of Convenience\nFinally, there is always a price to pay for convenience. While trees can work well even with a messy dataset containing outliers, redundant features, and thoughtlessly encoded categoricals, we will rarely achieve the best performance under these conditions. Taking the time to deal with outliers, removing redundant information, purposefully choosing appropriate categorical encodings, and building an understanding of the data will often lead to much better results."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#how-ensembling-makes-trees-shine",
    "href": "posts/consider-the-decision-tree/index.html#how-ensembling-makes-trees-shine",
    "title": "Consider the Decision Tree",
    "section": "How ensembling makes trees shine",
    "text": "How ensembling makes trees shine\nWe can go a long way toward addressing the issues of overfitting and prediction quantization by using trees as building blocks in larger algorithms called tree ensembles, the most popular examples being random forest and gradient boosted trees. A tree ensemble is a collection of different individual tree models whose predictions are averaged to generate an overall prediction.\nEnsembling helps address overfitting because even if each individual tree is overfitted, the average of their individual noisy predictions will tend to be more stable. Think of it in terms of the bias variance tradeoff, where bias refers to a model‚Äôs failure to capture certain patterns and variance refers to how different a model prediction would be if the model were trained on a different sample of training data. Since the ensemble is averaging over the predictions of all the individual models, training it on a different sample of training data would change the individual models predictions, but their overall average prediction will tend to remain stable. Thus, ensembling helps reduce the effects of overfitting by reducing model variance without increasing bias.\nEnsembling also helps address prediction quantization issues. While each individual tree‚Äôs predictions might express large jumps in the regression surface, averaging many different trees‚Äô predictions together effectively generates a surface with more partitions and smaller jumps between them. This provides a smoother approximation of the feature-target relationship."
  },
  {
    "objectID": "posts/consider-the-decision-tree/index.html#wrapping-up",
    "href": "posts/consider-the-decision-tree/index.html#wrapping-up",
    "title": "Consider the Decision Tree",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, there you go, that‚Äôs my take on the high-level overview of the decision tree and its main strengths and weaknesses. As we‚Äôve seen, ensembling allows us to keep the conveniences of the decision tree while mitigating its core weakness of relatively weak predictive power. This is why tree ensembles are so popular in practical applications. We glossed over pretty much all details of how trees actually do their magic, but fear not, next time we‚Äôre going to get rowdy and build one of these things from scratch."
  },
  {
    "objectID": "posts/logistic-regression-with-pytorch/index.html",
    "href": "posts/logistic-regression-with-pytorch/index.html",
    "title": "Logistic Regression with PyTorch",
    "section": "",
    "text": "Note from December 2025: Well dear reader, it looks like I wrote this post back in July and forgot to publish it, so here‚Äôs my early Christmas present to you. Enjoy!\nIn this post we‚Äôll bridge the gap between traditional ML and deep learning by showing that logistic regression is a special case of a neural network, and we‚Äôll compare the classic scikit-learn logistic regression to a neural network implementation that we‚Äôll build in PyTorch. Then we‚Äôll add some hidden layers to our PyTorch model to go from logistic regression to the multi-layer perceptron, a simple deep neural network that‚Äôs like the major scale of deep learning model architectures."
  },
  {
    "objectID": "posts/logistic-regression-with-pytorch/index.html#multiclass-logistic-regressiontraditional-ml-vs-neural-network",
    "href": "posts/logistic-regression-with-pytorch/index.html#multiclass-logistic-regressiontraditional-ml-vs-neural-network",
    "title": "Logistic Regression with PyTorch",
    "section": "Multiclass Logistic Regression‚ÄîTraditional ML vs Neural Network",
    "text": "Multiclass Logistic Regression‚ÄîTraditional ML vs Neural Network\nWe want to classify \\(N\\) instances, each a \\(D\\) dimensional input, into one of \\(K\\) discrete classes by predicting the probability mass function over the \\(K\\) classes. In matrix notation, the classical ML model is\n\\[ z =  X W^T + b \\] \\[ \\hat{p} = \\text{softmax}(z) = \\frac{\\exp (z)} {\\sum_{k=1}^K \\exp (z_k)}\\]\nwhere\n\n\\(X \\in \\mathbb{R}^{N \\times D}\\) is the \\(D\\) dimensional input data for each instance\n\\(W \\in \\mathbb{R}^{K \\times D}\\) is the coefficient matrix (\\(D\\) coefficients for each class)\n\\(b \\in \\mathbb{R}^K\\) is the intercept for each class\n\\(z \\in \\mathbb{R}^{N \\times K}\\) are the \\(K\\) raw logits or linear scores for each instance\n\\(\\text{softmax}(\\cdot):\\mathbb{R}^K\\rightarrow(0,1)^K\\) is applied to each instance to transform the logits in \\((-\\infty, \\infty)\\) to probabilities in \\((0,1)\\).\n\\(\\hat{p} \\in (0,1)^{N \\times K}\\) are the \\(K\\) class probabilities predicted for each instance\n\nIn neural network terms we can express the above formulation as a network with\n\nInput layer: \\(X\\)\nLinear layer: \\(z=XW^T+b\\)\nNon-linear activation: \\(\\text{softmax}(z)\\)\n\nIn both cases, model training is guided by a negative log likelihood loss function.\n\nFYI these formulations are also closely related to multi-class gradient boosting, which We talked about back in the gradient boosting for multi-class classification from scratch post. You can go back and reread that post for some additional intuition on how multi-class classification works.\n\nLet‚Äôs implement logistic regression as a traditional ML model and as a neural network."
  },
  {
    "objectID": "posts/logistic-regression-with-pytorch/index.html#mnist-data",
    "href": "posts/logistic-regression-with-pytorch/index.html#mnist-data",
    "title": "Logistic Regression with PyTorch",
    "section": "MNIST Data",
    "text": "MNIST Data\nWe‚Äôll train our logistic regression models to classify the handwritten digits in the classic MNIST dataset. Adapting this scikit-learn example, we‚Äôll load up the data, plot some of the digits, normalize the input images, and then fit a classical logistic regression model.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import check_random_state\n\n# Load data from https://www.openml.org/d/554\nX, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n\n# Shuffle the data\nrandom_state = check_random_state(0)\npermutation = random_state.permutation(X.shape[0])\nX = X[permutation]\ny = y[permutation]\nX = X.reshape((X.shape[0], -1))\n\n# train test split\ntrain_samples = 10_000\ntest_samples = 10_000\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=train_samples, test_size=test_samples\n)\n\n# Normalize image data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nprint(f\"Number of classes: {len(np.unique(y))}\")\nprint(f\"Shape of X: {X.shape}\")\n\nNumber of classes: 10\nShape of X: (70000, 784)\n\n\nWe have \\(K=10\\) classes corresponding to the digits 0-9. The image data in X is stored as a \\(N \\times D\\) array with \\(N=70000\\) images and each image having \\(D=784\\) pixels. In this raw form, the images are flattened out into a single dimension, which is ideal for modeling. To visualize them, we‚Äôll need to reshape each image from \\(1 \\times 784\\) to \\(28 \\times 28\\).\n\n\nCode\ndef plot_digits(X, n_rows, n_cols):\n    X = X.reshape(-1, 28, 28)\n    n_images = n_rows * n_cols\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=(0.7*n_cols, 0.7*n_rows))\n\n    # Flatten axs to iterate easily\n    axs = axs.flatten()\n\n    for i in range(n_images):\n        axs[i].imshow(X[i], cmap=\"gray\")\n        axs[i].axis(\"off\")  # turn off axes completely\n\n    # Remove all spacing between plots\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.tight_layout(pad=0)\n    # return fig, axs\n\nplot_digits(X, 10, 10); # post image\n\n\n\n\n\n\nSome instances from the MNIST hand-written digit dataset"
  },
  {
    "objectID": "posts/logistic-regression-with-pytorch/index.html#logistic-regression-with-scikit-learn",
    "href": "posts/logistic-regression-with-pytorch/index.html#logistic-regression-with-scikit-learn",
    "title": "Logistic Regression with PyTorch",
    "section": "Logistic Regression with scikit-learn",
    "text": "Logistic Regression with scikit-learn\nWe‚Äôll start with the traditional logistic regression model implementation from scikit-learn.\n\n# Classical Logistic Regression\n\nclf = LogisticRegression(penalty=None, solver=\"sag\", tol=0.1) \nclf.fit(X_train, y_train)\nprint(f\"Test Accuracy: {100 * clf.score(X_test, y_test):.2f}%\")\n\nTest Accuracy: 89.39%\n\n\nNow to visualize what this model is doing, let‚Äôs have a look at its coefficients.\n\nclf.coef_.shape\n\n(10, 784)\n\n\nRecall the coefficients \\(W\\) are in a \\(K \\times D\\) array, so each of the \\(K\\) rows contains the \\(D\\) coefficients for the corresponding class. Let‚Äôs reshape each of the rows into a 28 by 28 image and plot them.\n\n\nCode\ndef plot_class_weights(weights: np.ndarray, title: str = \"Classification Weights\"):\n    num_classes = weights.shape[0]\n    n_rows, n_cols = 2, (num_classes + 1) // 2\n\n    fig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 5))\n    scale = np.abs(weights).max()\n\n    # Set up shared color scale\n    cmap = plt.cm.viridis\n    norm = plt.Normalize(vmin=-scale, vmax=scale)\n\n    axs = axs.flatten()\n    for i in range(num_classes):\n        ax = axs[i]\n        im = ax.imshow(\n            weights[i].reshape(28, 28),\n            interpolation=\"nearest\",\n            cmap=cmap,\n            norm=norm\n        )\n        ax.set_xticks(())\n        ax.set_yticks(())\n        # ax.set_xlabel(f\"Class {i}\")\n        ax.set_xlabel(f\"Class {i}\", labelpad=5)\n        ax.xaxis.set_label_position('top')\n\n    for j in range(num_classes, len(axs)):\n        axs[j].axis('off')\n\n    # Add colorbar BELOW the entire figure\n    cbar_ax = fig.add_axes([0.2, -0.05, 0.6, 0.03])  # [left, bottom, width, height]\n    fig.colorbar(\n        plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n        cax=cbar_ax,\n        orientation=\"horizontal\"\n    ).set_label(\"value\")\n\n    fig.suptitle(title)\n    # plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Make space for suptitle\n    plt.show()\n\n\n\nplot_class_weights(clf.coef_, title=\"Scikit-learn Logistic Regression Coefficients\")\n\n\n\n\nTraditional logistic regression coefficients for each class\n\n\n\n\nIntuitively, for a given digit, we‚Äôd expect the coefficients to be positive on pixels where the digit is typically located, and we‚Äôd expect the coefficients to be zero or perhaps even negative on pixels where other digits tend to be located. We can see that‚Äôs more or less what this model is doing."
  },
  {
    "objectID": "posts/logistic-regression-with-pytorch/index.html#logistic-regression-in-pytorch",
    "href": "posts/logistic-regression-with-pytorch/index.html#logistic-regression-in-pytorch",
    "title": "Logistic Regression with PyTorch",
    "section": "Logistic Regression in PyTorch",
    "text": "Logistic Regression in PyTorch\nI recommend checking out the PyTorch Basic Tutorial to get started with the library‚Äôs API. Our basic flow for creating PyTorch models will look like\n\nCreate Dataset and DataLoader objects.\nBuild the model.\nTrain the model.\n\n\nCreate PyTorch Dataset and DataLoader Objects\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Convert to torch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.astype(np.int64), dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test.astype(np.int64), dtype=torch.long)\n\n# Wrap in TensorDataset\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n\nBuild the Model\n\n# Logistic Regression Model\nclass LogReg(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(784, 10)\n        self.activation = nn.LogSoftmax(dim=1) # returns log probabilities\n\n    def forward(self, x):\n        z = self.linear(x)\n        p = self.activation(z)\n        return p\n\nmodel = LogReg()\n\n\n\nTrain the Model\n\n# Hyperparameters\nnum_epochs = 5\nlearning_rate = 0.1\n\n# Loss\ncriterion = nn.NLLLoss() # expects log probabilities\n\n# Optimizer\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        images = images.view(-1, 28*28)  # Flatten\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}\")\n\nEpoch [1/5], Training Loss: 0.4151\nEpoch [2/5], Training Loss: 0.1858\nEpoch [3/5], Training Loss: 0.1102\nEpoch [4/5], Training Loss: 0.5600\nEpoch [5/5], Training Loss: 0.4414\n\n\n\n\nEvaluate the Model\n\n# Evaluate model\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.view(-1, 28*28)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nTest Accuracy: 90.71%\n\n\n\n\nPyTorch Model Weights\nLet‚Äôs take a look at the weights.\n\nweights = model.linear.weight.detach().numpy()\nweights.shape\n\n(10, 784)\n\n\n\nplot_class_weights(weights, title=\"PyTorch Logistic Regression Weights\")\n\n\n\n\nPyTorch logistic regression coefficients for each class\n\n\n\n\nNice! We can see that the weights from the neural network are qualitatively similar to the coefficients from the logistic regression model. Interestingly, the neural network weight patterns look a bit more noisy than the classical logistic regression coefficients, and yet, the models are performing similarly on the test data (89-90%). Likely there are a lot of logistic regression parameter solutions that yield similar performance, and these two models have found slightly different solutions in parameter space."
  },
  {
    "objectID": "posts/logistic-regression-with-pytorch/index.html#multilayer-perceptron-in-pytorch",
    "href": "posts/logistic-regression-with-pytorch/index.html#multilayer-perceptron-in-pytorch",
    "title": "Logistic Regression with PyTorch",
    "section": "Multilayer Perceptron in PyTorch",
    "text": "Multilayer Perceptron in PyTorch\nWell, logistic regression is great and all, but of course, the reason for messing around with PyTorch is so that we can start building more interesting neural network architectures. The next obvious step is to build out a multilayer perceptron (MLP). The MLP is a network with\n\nAn input layer\nOne or more hidden layers comprised of a linear transformation passed to a non-linear activation function\nAn output layer, e.g.¬†returning class probabilities\n\nWhen we add ‚Äúhidden‚Äù layers between the input and output layers, the network earns the modifier ‚Äúdeep‚Äù, meaning that MLP‚Äôs are deep networks. We‚Äôll build a model with two hidden layers that uses rectified linear unit activation functions (ReLU). The ReLU just replaces negative inputs with zero and passes positive inputs through unchanged‚Äîa very simple form of non-linearity.\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(784, 256),   # Input layer\n            nn.ReLU(),\n            nn.Linear(256, 128),   # Hidden layer\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.LogSoftmax(dim=1)       # output layer - log probabilities\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nmlp = MLP()\n\n\n# Hyperparameters\nnum_epochs = 20\nlearning_rate = 0.001\n\noptimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\ncriterion = nn.NLLLoss() # expects log probability\n\n# Training loop\nmlp.train()\nfor epoch in range(num_epochs):\n    for images, labels in train_loader:\n        images = images.view(-1, 28*28)  # Flatten\n        outputs = mlp(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}\")\n\nEpoch [1/20], Training Loss: 0.4061\nEpoch [2/20], Training Loss: 0.0821\nEpoch [3/20], Training Loss: 0.0214\nEpoch [4/20], Training Loss: 0.0585\nEpoch [5/20], Training Loss: 0.0027\nEpoch [6/20], Training Loss: 0.0086\nEpoch [7/20], Training Loss: 0.0005\nEpoch [8/20], Training Loss: 0.0018\nEpoch [9/20], Training Loss: 0.0016\nEpoch [10/20], Training Loss: 0.0809\nEpoch [11/20], Training Loss: 0.0024\nEpoch [12/20], Training Loss: 0.0000\nEpoch [13/20], Training Loss: 0.0002\nEpoch [14/20], Training Loss: 0.0001\nEpoch [15/20], Training Loss: 0.0001\nEpoch [16/20], Training Loss: 0.0004\nEpoch [17/20], Training Loss: 0.0015\nEpoch [18/20], Training Loss: 0.0011\nEpoch [19/20], Training Loss: 0.0004\nEpoch [20/20], Training Loss: 0.0000\n\n\n\n# Evaluate model\nmlp.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.view(-1, 28*28)\n        outputs = mlp(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n\nTest Accuracy: 95.83%"
  },
  {
    "objectID": "posts/logistic-regression-with-pytorch/index.html#wrapping-up",
    "href": "posts/logistic-regression-with-pytorch/index.html#wrapping-up",
    "title": "Logistic Regression with PyTorch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nNice! In this post we‚Äôve bridged the gap between classic ML and deep learning by showing that logistic regression is a special case of a neural network, building a logistic regression model in PyTorch, and then adding hidden layers to that model to obtain a multilayer perceptron."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html",
    "href": "posts/xgboost-from-scratch/index.html",
    "title": "XGBoost from Scratch",
    "section": "",
    "text": "A weathered tree reaches toward the sea at Playa Mal Pa√≠s\nWell, dear reader, it‚Äôs that time again, time for us to do a seemingly unnecessary scratch build of a popular algorithm that most people would simply import from the library without a second thought. But readers of this blog are not most people. Of course you know that when we do scratch builds, it‚Äôs not for the hell of it, it‚Äôs for the purpose of demystification. To that end, today we are going to implement XGBoost from scratch in python, using only numpy and pandas.\nSpecifically we‚Äôre going to implement the core statistical learning algorithm of XGBoost, including most of the key hyperparameters and their functionality. Our implementation will also support user-defined custom objective functions, meaning that it can perform regression, classification, and whatever exotic learning tasks you can dream up, as long as you can write down a twice-differentiable objective function. We‚Äôll refrain from implementing some simple features like column subsampling which will be left to you, gentle reader, as exercises. In terms of tree methods, we‚Äôre going to implement the exact tree-splitting algorithm, leaving the sparsity-aware method (used to handle missing feature values) and the approximate method (used for scalability) as exercises or maybe topics for future posts.\nAs always, if something is unclear, try backtracking through the previous posts on gradient boosting and decision trees to clarify your intuition. We‚Äôve already built up all the statistical and computational background needed to make sense of this scratch build. Here are the most important prerequisite posts:\nGreat, let‚Äôs do this."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#the-xgboost-model-class",
    "href": "posts/xgboost-from-scratch/index.html#the-xgboost-model-class",
    "title": "XGBoost from Scratch",
    "section": "The XGBoost Model Class",
    "text": "The XGBoost Model Class\nWe begin with the user-facing API for our model, a class called XGBoostModel which will implement gradient boosting and prediction. To be more consistent with the XGBoost library, we‚Äôll pass hyperparameters to our model in a parameter dictionary, so our init method is going to pull relevant parameters out of the dictionary and set them as object attributes. Note the use of python‚Äôs defaultdict so we don‚Äôt have to worry about handling key errors if we try to access a parameter that the user didn‚Äôt set in the dictionary.\n\nimport math\nimport numpy as np \nimport pandas as pd\nfrom collections import defaultdict\n\n\nclass XGBoostModel():\n    '''XGBoost from Scratch\n    '''\n    \n    def __init__(self, params, random_seed=None):\n        self.params = defaultdict(lambda: None, params)\n        self.subsample = self.params['subsample'] \\\n            if self.params['subsample'] else 1.0\n        self.learning_rate = self.params['learning_rate'] \\\n            if self.params['learning_rate'] else 0.3\n        self.base_prediction = self.params['base_score'] \\\n            if self.params['base_score'] else 0.5\n        self.max_depth = self.params['max_depth'] \\\n            if self.params['max_depth'] else 5\n        self.rng = np.random.default_rng(seed=random_seed)\n\nThe fit method, based on our classic GBM, takes a feature dataframe, a target vector, the objective function, and the number of boosting rounds as arguments. The user-supplied objective function should be an object with loss, gradient, and hessian methods, each of which takes a target vector and a prediction vector as input; the loss method should return a scalar loss score, the gradient method should return a vector of gradients, and the hessian method should return a vector of hessians.\nIn contrast to boosting in the classic GBM, instead of computing residuals between the current predictions and the target, we compute gradients and hessians of the loss function with respect to the current predictions, and instead of predicting residuals with a decision tree, we fit a special XGBoost tree booster (which we‚Äôll implement in a moment) using the gradients and hessians. I‚Äôve also added row subsampling by drawing a random subset of instance indices and passing them to the tree booster during each boosting round. The rest of the fit method is the same as the classic GBM, and the predict method is identical too.\n\ndef fit(self, X, y, objective, num_boost_round, verbose=False):\n    current_predictions = self.base_prediction * np.ones(shape=y.shape)\n    self.boosters = []\n    for i in range(num_boost_round):\n        gradients = objective.gradient(y, current_predictions)\n        hessians = objective.hessian(y, current_predictions)\n        sample_idxs = None if self.subsample == 1.0 \\\n            else self.rng.choice(len(y), \n                                 size=math.floor(self.subsample*len(y)), \n                                 replace=False)\n        booster = TreeBooster(X, gradients, hessians, \n                              self.params, self.max_depth, sample_idxs)\n        current_predictions += self.learning_rate * booster.predict(X)\n        self.boosters.append(booster)\n        if verbose: \n            print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n            \ndef predict(self, X):\n    return (self.base_prediction + self.learning_rate \n            * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n\nXGBoostModel.fit = fit\nXGBoostModel.predict = predict            \n\nAll we have to do now is implement the tree booster."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#the-xgboost-tree-booster",
    "href": "posts/xgboost-from-scratch/index.html#the-xgboost-tree-booster",
    "title": "XGBoost from Scratch",
    "section": "The XGBoost Tree Booster",
    "text": "The XGBoost Tree Booster\nThe XGBoost tree booster is a modified version of the decision tree that we built in the decision tree from scratch post. Like the decision tree, we recursively build a binary tree structure by finding the best split rule for each node in the tree. The main difference is the criterion for evaluating splits and the way that we define a leaf‚Äôs predicted value. Instead of being functions of the target values of the instances in each node, the criterion and predicted values are functions of the instance gradients and hessians. Thus we need only make a couple of modifications to our previous decision tree implementation to create the XGBoost tree booster.\n\nInitialization and Inserting Child Nodes\nMost of the init method is just parsing the parameter dictionary to assign parameters as object attributes. The one notable difference from our decision tree is in the way we define the node‚Äôs predicted value. We define self.value according to equation 5 of the XGBoost paper, a simple function of the gradient and hessian values of the instances in the current node. Of course the init also goes on to build the tree via the maybe insert child nodes method. This method is nearly identical to the one we implemented for our decision tree. So far so good.\n\nclass TreeBooster():\n \n    def __init__(self, X, g, h, params, max_depth, idxs=None):\n        self.params = params\n        self.max_depth = max_depth\n        assert self.max_depth &gt;= 0, 'max_depth must be nonnegative'\n        self.min_child_weight = params['min_child_weight'] \\\n            if params['min_child_weight'] else 1.0\n        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0\n        self.gamma = params['gamma'] if params['gamma'] else 0.0\n        self.colsample_bynode = params['colsample_bynode'] \\\n            if params['colsample_bynode'] else 1.0\n        if isinstance(g, pd.Series): g = g.values\n        if isinstance(h, pd.Series): h = h.values\n        if idxs is None: idxs = np.arange(len(g))\n        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n        self.best_score_so_far = 0.\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n\n    def _maybe_insert_child_nodes(self):\n        for i in range(self.c): self._find_better_split(i)\n        if self.is_leaf: return\n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = TreeBooster(self.X, self.g, self.h, self.params, \n                                self.max_depth - 1, self.idxs[left_idx])\n        self.right = TreeBooster(self.X, self.g, self.h, self.params, \n                                 self.max_depth - 1, self.idxs[right_idx])\n\n    @property\n    def is_leaf(self): return self.best_score_so_far == 0.\n\n    def _find_better_split(self, feature_idx):\n        pass\n\n\n\nSplit Finding\nSplit finding follows the exact same pattern that we used in the decision tree, except we keep track of gradient and hessian stats instead of target value stats, and of course we use the XGBoost gain criterion (equation 7 from the paper) for evaluating splits.\n\ndef _find_better_split(self, feature_idx):\n    x = self.X.values[self.idxs, feature_idx]\n    g, h = self.g[self.idxs], self.h[self.idxs]\n    sort_idx = np.argsort(x)\n    sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n    sum_g, sum_h = g.sum(), h.sum()\n    sum_g_right, sum_h_right = sum_g, sum_h\n    sum_g_left, sum_h_left = 0., 0.\n\n    for i in range(0, self.n - 1):\n        g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n        sum_g_left += g_i; sum_g_right -= g_i\n        sum_h_left += h_i; sum_h_right -= h_i\n        if sum_h_left &lt; self.min_child_weight or x_i == x_i_next:continue\n        if sum_h_right &lt; self.min_child_weight: break\n\n        gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n                        + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n                        - (sum_g**2 / (sum_h + self.reg_lambda))\n                        ) - self.gamma/2 # Eq(7) in the xgboost paper\n        if gain &gt; self.best_score_so_far: \n            self.split_feature_idx = feature_idx\n            self.best_score_so_far = gain\n            self.threshold = (x_i + x_i_next) / 2\n            \nTreeBooster._find_better_split = _find_better_split\n\n\n\nPrediction\nPrediction works exactly the same as in our decision tree, and the methods are nearly identical.\n\ndef predict(self, X):\n    return np.array([self._predict_row(row) for i, row in X.iterrows()])\n\ndef _predict_row(self, row):\n    if self.is_leaf: \n        return self.value\n    child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n        else self.right\n    return child._predict_row(row)\n\nTreeBooster.predict = predict \nTreeBooster._predict_row = _predict_row"
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#the-complete-xgboost-from-scratch-implementation",
    "href": "posts/xgboost-from-scratch/index.html#the-complete-xgboost-from-scratch-implementation",
    "title": "XGBoost from Scratch",
    "section": "The Complete XGBoost From Scratch Implementation",
    "text": "The Complete XGBoost From Scratch Implementation\nHere‚Äôs the entire implementation which produces a usable XGBoostModel class with fit and predict methods.\n\nclass XGBoostModel():\n    '''XGBoost from Scratch\n    '''\n    \n    def __init__(self, params, random_seed=None):\n        self.params = defaultdict(lambda: None, params)\n        self.subsample = self.params['subsample'] \\\n            if self.params['subsample'] else 1.0\n        self.learning_rate = self.params['learning_rate'] \\\n            if self.params['learning_rate'] else 0.3\n        self.base_prediction = self.params['base_score'] \\\n            if self.params['base_score'] else 0.5\n        self.max_depth = self.params['max_depth'] \\\n            if self.params['max_depth'] else 5\n        self.rng = np.random.default_rng(seed=random_seed)\n                \n    def fit(self, X, y, objective, num_boost_round, verbose=False):\n        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n        self.boosters = []\n        for i in range(num_boost_round):\n            gradients = objective.gradient(y, current_predictions)\n            hessians = objective.hessian(y, current_predictions)\n            sample_idxs = None if self.subsample == 1.0 \\\n                else self.rng.choice(len(y), \n                                     size=math.floor(self.subsample*len(y)), \n                                     replace=False)\n            booster = TreeBooster(X, gradients, hessians, \n                                  self.params, self.max_depth, sample_idxs)\n            current_predictions += self.learning_rate * booster.predict(X)\n            self.boosters.append(booster)\n            if verbose: \n                print(f'[{i}] train loss = {objective.loss(y, current_predictions)}')\n            \n    def predict(self, X):\n        return (self.base_prediction + self.learning_rate \n                * np.sum([booster.predict(X) for booster in self.boosters], axis=0))\n    \nclass TreeBooster():\n \n    def __init__(self, X, g, h, params, max_depth, idxs=None):\n        self.params = params\n        self.max_depth = max_depth\n        assert self.max_depth &gt;= 0, 'max_depth must be nonnegative'\n        self.min_child_weight = params['min_child_weight'] \\\n            if params['min_child_weight'] else 1.0\n        self.reg_lambda = params['reg_lambda'] if params['reg_lambda'] else 1.0\n        self.gamma = params['gamma'] if params['gamma'] else 0.0\n        self.colsample_bynode = params['colsample_bynode'] \\\n            if params['colsample_bynode'] else 1.0\n        if isinstance(g, pd.Series): g = g.values\n        if isinstance(h, pd.Series): h = h.values\n        if idxs is None: idxs = np.arange(len(g))\n        self.X, self.g, self.h, self.idxs = X, g, h, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = -g[idxs].sum() / (h[idxs].sum() + self.reg_lambda) # Eq (5)\n        self.best_score_so_far = 0.\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n\n    def _maybe_insert_child_nodes(self):\n        for i in range(self.c): self._find_better_split(i)\n        if self.is_leaf: return\n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = TreeBooster(self.X, self.g, self.h, self.params, \n                                self.max_depth - 1, self.idxs[left_idx])\n        self.right = TreeBooster(self.X, self.g, self.h, self.params, \n                                 self.max_depth - 1, self.idxs[right_idx])\n\n    @property\n    def is_leaf(self): return self.best_score_so_far == 0.\n    \n    def _find_better_split(self, feature_idx):\n        x = self.X.values[self.idxs, feature_idx]\n        g, h = self.g[self.idxs], self.h[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_g, sort_h, sort_x = g[sort_idx], h[sort_idx], x[sort_idx]\n        sum_g, sum_h = g.sum(), h.sum()\n        sum_g_right, sum_h_right = sum_g, sum_h\n        sum_g_left, sum_h_left = 0., 0.\n\n        for i in range(0, self.n - 1):\n            g_i, h_i, x_i, x_i_next = sort_g[i], sort_h[i], sort_x[i], sort_x[i + 1]\n            sum_g_left += g_i; sum_g_right -= g_i\n            sum_h_left += h_i; sum_h_right -= h_i\n            if sum_h_left &lt; self.min_child_weight or x_i == x_i_next:continue\n            if sum_h_right &lt; self.min_child_weight: break\n\n            gain = 0.5 * ((sum_g_left**2 / (sum_h_left + self.reg_lambda))\n                            + (sum_g_right**2 / (sum_h_right + self.reg_lambda))\n                            - (sum_g**2 / (sum_h + self.reg_lambda))\n                            ) - self.gamma/2 # Eq(7) in the xgboost paper\n            if gain &gt; self.best_score_so_far: \n                self.split_feature_idx = feature_idx\n                self.best_score_so_far = gain\n                self.threshold = (x_i + x_i_next) / 2\n                \n    def predict(self, X):\n        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n\n    def _predict_row(self, row):\n        if self.is_leaf: \n            return self.value\n        child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n            else self.right\n        return child._predict_row(row)"
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#testing",
    "href": "posts/xgboost-from-scratch/index.html#testing",
    "title": "XGBoost from Scratch",
    "section": "Testing",
    "text": "Testing\nLet‚Äôs take this baby for a spin and benchmark its performance against the actual XGBoost library. We use the scikit learn California housing dataset for benchmarking.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n    \nX, y = fetch_california_housing(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    random_state=43)\n\nLet‚Äôs start with a nice friendly squared error objective function for training. We should probably have a future post all about how to define custom objective functions in XGBoost, but for now, here‚Äôs how I define squared error.\n\nclass SquaredErrorObjective():\n    def loss(self, y, pred): return np.mean((y - pred)**2)\n    def gradient(self, y, pred): return pred - y\n    def hessian(self, y, pred): return np.ones(len(y))\n\nHere I use a more or less arbitrary set of hyperparameters for training. Feel free to play around with tuning and trying other parameter combinations yourself.\n\nimport xgboost as xgb\n\nparams = {\n    'learning_rate': 0.1,\n    'max_depth': 5,\n    'subsample': 0.8,\n    'reg_lambda': 1.5,\n    'gamma': 0.0,\n    'min_child_weight': 25,\n    'base_score': 0.0,\n    'tree_method': 'exact',\n}\nnum_boost_round = 50\n\n# train the from-scratch XGBoost model\nmodel_scratch = XGBoostModel(params, random_seed=42)\nmodel_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)\n\n# train the library XGBoost model\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\nmodel_xgb = xgb.train(params, dtrain, num_boost_round)\n\nLet‚Äôs check the models‚Äô performance on the held out test data to benchmark our implementation.\n\npred_scratch = model_scratch.predict(X_test)\npred_xgb = model_xgb.predict(dtest)\nprint(f'scratch score: {SquaredErrorObjective().loss(y_test, pred_scratch)}')\nprint(f'xgboost score: {SquaredErrorObjective().loss(y_test, pred_xgb)}')\n\nscratch score: 0.2434125759558149\nxgboost score: 0.24123239765807963\n\n\nWell, look at that! Our scratch-built SGBoost is looking pretty consistent with the library. Go us!"
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#wrapping-up",
    "href": "posts/xgboost-from-scratch/index.html#wrapping-up",
    "title": "XGBoost from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nI‚Äôd say this is a pretty good milestone for us here at Random Realizations. We‚Äôve been hammering away at the various concepts around gradient boosting, leaving a trail of equations and scratch-built algos in our wake. Today we put all of that together to create a legit scratch build of XGBoost, something that would have been out of reach for me before we embarked on this journey together over a year ago. To anyone with the patience to read through this stuff, cheers to you! I hope you‚Äôre learning and enjoying this as much as I am."
  },
  {
    "objectID": "posts/xgboost-from-scratch/index.html#reader-exercises",
    "href": "posts/xgboost-from-scratch/index.html#reader-exercises",
    "title": "XGBoost from Scratch",
    "section": "Reader Exercises",
    "text": "Reader Exercises\nIf you want to take this a step further and deepen your understanding and coding abilities, let me recommend some exercises for you.\n\nImplement column subsampling. XGBoost itself provides column subsampling by tree, by level, and by node. Try implementing by tree first, then try adding by level or by node as well. These should be pretty straightforward to do.\nImplement sparsity aware split finding for missing feature values (Algorithm 2 in the XGBoost paper). This will be a little more involved, since you‚Äôll need to refactor and modify several parts of the tree booster class."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "",
    "text": "Cold water cascades over the rocks in Erwin, Tennessee.\nFriends, this is going to be an epic post! Today, we bring together all the ideas we‚Äôve built up over the past few posts to nail down our understanding of the key ideas in Jerome Friedman‚Äôs seminal 2001 paper: ‚ÄúGreedy Function Approximation: A Gradient Boosting Machine.‚Äù In particular, we‚Äôll summarize the highlights from the paper, and we‚Äôll build an in-house python implementation of his generic gradient boosting algorithm which can train with any differentiable loss function. What‚Äôs more, we‚Äôll go ahead and take our generic gradient boosting machine for a spin by training it with several of the most popular loss functions used in practice.\nAre you freaking stoked or what?\nSweet. Let‚Äôs do this."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedman-2001-tldr",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedman-2001-tldr",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Friedman 2001: TL;DR",
    "text": "Friedman 2001: TL;DR\nI‚Äôve mentioned this paper a couple of times before, but as far as I can tell, this is the origin of gradient boosting; it is therefore, a seminal work worth reading. You know what, I think you might like to pick up the paper and read it yourself. Like many papers, there is a lot of scary looking math in the first few pages, but if you‚Äôve been following along on this blog, you‚Äôll find that it‚Äôs actually totally approachable. This is the kind of thing that cures imposter syndrome, so give it a shot. That said, here‚Äôs the TL;DR as I see it.\nThe first part of the paper introduces the idea of fitting models by doing gradient descent in function space, an ingenious idea we spent an entire post demystifying earlier. Friedman goes on to introduce the generic gradient boost algorithm, which works with any differentiable loss function, as well as specific variants for minimizing absolute error, Huber loss, and binary deviance. In terms of hyperparameters, he points out that the learning rate can be used to reduce overfitting, while increased tree depth can help capture more complex interactions among features. He even discusses feature importance and partial dependence methods for interpreting fitted gradient boosting models.\nFriedman concludes by musing about the advantages of gradient boosting with trees. He notes some key advantages afforded by the use of decision trees including no need to rescale input data, robustness against irrelevant input features, and elegant handling of missing feature values. He points out that gradient boosting manages to capitalize on the benefits of decision trees while minimizing their key weakness (crappy accuracy). I think this offers a great insight into why gradient boosting models have become so widespread and successful in practical ML applications."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedmans-generic-gradient-boosting-algorithm",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#friedmans-generic-gradient-boosting-algorithm",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Friedman‚Äôs Generic Gradient Boosting Algorithm",
    "text": "Friedman‚Äôs Generic Gradient Boosting Algorithm\nLet‚Äôs take a closer look at Friedman‚Äôs original gradient boost algorithm, Alg. 1 in Section 3 of the paper (translated into the notation we‚Äôve been using so far).\nLike last time, we have training data \\((\\mathbf{y}, \\mathbf{X})\\) where \\(\\mathbf{y}\\) is a length-\\(n\\) vector of target values, and \\(\\mathbf{X}\\) is an \\(n \\times p\\) matrix with \\(n\\) observations of \\(p\\) features. We also have a differentiable loss function \\(L(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^n l(y_i, \\hat{y}_i)\\), a ‚Äúlearning rate‚Äù hyperparameter \\(\\eta\\), and a fixed number of model iterations \\(M\\).\nAlgorithm: gradient_boost\\((\\mathbf{X},\\mathbf{y},L,\\eta, M)\\) returns: model \\(F_M\\)\n\nLet base model \\(F_0(\\mathbf{x}) = c\\), where \\(c = \\text{argmin}_{c} \\sum_{i=1}^n l(y_i, c)\\)\nfor \\(m\\) = \\(0\\) to \\(M-1\\):\n¬†¬†¬†¬† Let ‚Äúpseudo-residual‚Äù vector \\(\\mathbf{r}_m = -\\nabla_{\\mathbf{\\hat{y}}_m} L(\\mathbf{y},\\mathbf{\\hat{y}}_m)\\)\n¬†¬†¬†¬† Train decision tree regressor \\(h_m(\\mathbf{X})\\) to predict \\(\\mathbf{r}_m\\) (minimizing squared error)\n¬†¬†¬†¬† foreach terminal leaf node \\(t \\in h_m\\):\n¬†¬†¬†¬† ¬†¬†¬†¬† Let \\(v = \\text{argmin}_v \\sum_{i \\in t} l(y_i, F_m(\\mathbf{x}_i) + v)\\)\n¬†¬†¬†¬† ¬†¬†¬†¬† Set terminal leaf node \\(t\\) to predict value \\(v\\)\n¬†¬†¬†¬† \\(F_{m+1}(\\mathbf{X}) = F_{m}(\\mathbf{X}) + \\eta h_m(\\mathbf{X})\\)\nReturn composite model \\(F_M\\)\n\nBy now, most of this is already familiar to us. We begin by setting the base model \\(F_0\\) equal to the constant prediction value that minimizes the loss over all examples in the training dataset (line 1). Then we begin the boosting iterations (line 2), each time computing the negative gradients of the loss with respect to the current model predictions (known as the pseudo residuals) (line 3). We then fit our next decision tree regressor to predict the pseudo residuals (line 4).\nThen we encounter something new on lines 5-7. When we fit a vanilla decision tree regressor to predict pseudo residuals, we‚Äôre using mean squared error as the loss function to train the tree. As you might imagine, this works well when the global loss function is also squared error. But if we want to use a global loss other than squared error, there is an additional trick we can use to further increase the composite model‚Äôs accuracy. The idea is to continue using squared error to train each decision tree, keeping its structure and split conditions but altering the predicted value in each leaf to help minimize the global loss function. Instead of using the mean target value as the prediction for each node (as we would do when minimizing squared error), we use a numerical optimization method like line search to choose the constant value for that leaf that leads to the best overall loss. This is the same thing we did in line 1 of the algorithm to set the base prediction, but here we choose the optimal prediction for each terminal node of the newly trained decision tree."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#implementation",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#implementation",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Implementation",
    "text": "Implementation\nI did some (half-assed) searching on the interweb for an implementation of GBM that allows the user to provide a custom loss function, and you know what? I couldn‚Äôt find anything. If you find another implementation, post in the comments so we can learn from it too.\nSince we need to modify the values predicted by our decision trees‚Äô terminal nodes, we‚Äôll want to brush up on the scikit-learn decision tree structure before we get going. You can see explanations of all the necessary decision tree hacks in this notebook.\n\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor \nfrom scipy.optimize import minimize\n\nclass GradientBoostingMachine():\n    '''Gradient Boosting Machine supporting any user-supplied loss function.\n    \n    Parameters\n    ----------\n    n_trees : int\n        number of boosting rounds\n        \n    learning_rate : float\n        learning rate hyperparameter\n        \n    max_depth : int\n        maximum tree depth\n    '''\n    \n    def __init__(self, n_trees, learning_rate=0.1, max_depth=1):\n        self.n_trees=n_trees; \n        self.learning_rate=learning_rate\n        self.max_depth=max_depth;\n    \n    def fit(self, X, y, objective):\n        '''Fit the GBM using the specified loss function.\n        \n        Parameters\n        ----------\n        X : ndarray of size (number observations, number features)\n            design matrix\n            \n        y : ndarray of size (number observations,)\n            target values\n            \n        objective : loss function class instance\n            Class specifying the loss function for training.\n            Should implement two methods:\n                loss(labels: ndarray, predictions: ndarray) -&gt; float\n                negative_gradient(labels: ndarray, predictions: ndarray) -&gt; ndarray\n        '''\n        \n        self.trees = []\n        self.base_prediction = self._get_optimal_base_value(y, objective.loss)\n        current_predictions = self.base_prediction * np.ones(shape=y.shape)\n        for _ in range(self.n_trees):\n            pseudo_residuals = objective.negative_gradient(y, current_predictions)\n            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n            tree.fit(X, pseudo_residuals)\n            self._update_terminal_nodes(tree, X, y, current_predictions, objective.loss)\n            current_predictions += self.learning_rate * tree.predict(X)\n            self.trees.append(tree)\n     \n    def _get_optimal_base_value(self, y, loss):\n        '''Find the optimal initial prediction for the base model.'''\n        fun = lambda c: loss(y, c)\n        c0 = y.mean()\n        return minimize(fun=fun, x0=c0).x[0]\n        \n    def _update_terminal_nodes(self, tree, X, y, current_predictions, loss):\n        '''Update the tree's predictions according to the loss function.'''\n        # terminal node id's\n        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n        # compute leaf for each sample in ``X``.\n        leaf_node_for_each_sample = tree.apply(X)\n        for leaf in leaf_nodes:\n            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n            y_in_leaf = y.take(samples_in_this_leaf, axis=0)\n            preds_in_leaf = current_predictions.take(samples_in_this_leaf, axis=0)\n            val = self._get_optimal_leaf_value(y_in_leaf, \n                                               preds_in_leaf,\n                                               loss)\n            tree.tree_.value[leaf, 0, 0] = val\n            \n    def _get_optimal_leaf_value(self, y, current_predictions, loss):\n        '''Find the optimal prediction value for a given leaf.'''\n        fun = lambda c: loss(y, current_predictions + c)\n        c0 = y.mean()\n        return minimize(fun=fun, x0=c0).x[0]\n          \n    def predict(self, X):\n        '''Generate predictions for the given input data.'''\n        return (self.base_prediction \n                + self.learning_rate \n                * np.sum([tree.predict(X) for tree in self.trees], axis=0))\n\nIn terms of design, we implement a class for the GBM with scikit-like fit and predict methods. Notice in the below implementation that the fit method is only 10 lines long, and corresponds very closely to Friedman‚Äôs gradient boost algorithm from above. Most of the complexity comes from the helper methods for updating the leaf values according to the specified loss function.\nWhen the user wants to call the fit method, they‚Äôll need to supply the loss function they want to use for boosting. We‚Äôll make the user implement their loss (a.k.a. objective) function as a class with two methods: (1) a loss method taking the labels and the predictions and returning the loss score and (2) a negative_gradient method taking the labels and the predictions and returning an array of negative gradients."
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#testing-our-model",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#testing-our-model",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Testing our Model",
    "text": "Testing our Model\nLet‚Äôs test drive our custom-loss-ready GBM with a few different loss functions! We‚Äôll compare it to the scikit-learn GBM to sanity check our implementation.\n\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n\nrng = np.random.default_rng()\n\n# test data\ndef make_test_data(n, noise_scale):\n    x = np.linspace(0, 10, 500).reshape(-1,1)\n    y = (np.where(x &lt; 5, x, 5) + rng.normal(0, noise_scale, size=x.shape)).ravel()\n    return x, y\n    \n# print model loss scores\ndef print_model_loss_scores(obj, y, preds, sk_preds):\n    print(f'From Scratch Loss = {obj.loss(y, pred):0.4}')\n    print(f'Scikit-Learn Loss = {obj.loss(y, sk_pred):0.4}')\n\n\nMean Squared Error\nMean Squared Error (a.k.a. Least Squares) loss produces estimates of the mean target value conditioned on the feature values. Here‚Äôs the implementation.\n\nx, y = make_test_data(500, 0.4)\n\n\n# from scratch GBM\nclass SquaredErrorLoss():\n    '''User-Defined Squared Error Loss'''\n    \n    def loss(self, y, preds):\n        return np.mean((y - preds)**2)\n    \n    def negative_gradient(self, y, preds):\n        return y - preds\n    \n\ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                              max_depth=1)\ngbm.fit(x, y, SquaredErrorLoss())\npred = gbm.predict(x)\n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingRegressor(n_estimators=10,\n                                   learning_rate=0.5,\n                                   max_depth=1,\n                                   loss='squared_error')\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict(x)\n\n\nprint_model_loss_scores(SquaredErrorLoss(), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.168\nScikit-Learn Loss = 0.168\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean Absolute Error\nMean Absolute Error (a.k.a.Least Absolute Deviations) loss produces estimates of the median target value conditioned on the feature values. Here‚Äôs the implementation.\n\nx, y = make_test_data(500, 0.4)\n\n\n\n# from scratch GBM\nclass AbsoluteErrorLoss():\n    '''User-Defined Absolute Error Loss'''\n    \n    def loss(self, y, preds):\n        return np.mean(np.abs(y - preds))\n    \n    def negative_gradient(self, y, preds):\n        return np.sign(y - preds)\n\n\ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                              max_depth=1)\ngbm.fit(x, y, AbsoluteErrorLoss())\npred = gbm.predict(x)\n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingRegressor(n_estimators=10,\n                                   learning_rate=0.5,\n                                   max_depth=1,\n                                   loss='absolute_error')\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict(x)\n\n\nprint_model_loss_scores(AbsoluteErrorLoss(), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.3225\nScikit-Learn Loss = 0.3208\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantile Loss\nQuantile loss yields estimates of a given quantile of the target variable conditioned on the features. Here‚Äôs my implementation.\n\nx, y = make_test_data(500, 1)\n\n\n\n# from scratch GBM\nclass QuantileLoss():\n    '''Quantile Loss\n    \n    Parameters\n    ----------\n    alpha : float\n        quantile to be estimated, 0 &lt; alpha &lt; 1\n    '''\n    \n    def __init__(self, alpha):\n        if alpha &lt; 0 or alpha &gt;1:\n            raise ValueError('alpha must be between 0 and 1')\n        self.alpha = alpha\n        \n    def loss(self, y, preds):\n        e = y - preds\n        return np.mean(np.where(e &gt; 0, self.alpha * e, (self.alpha - 1) * e))\n    \n    def negative_gradient(self, y, preds):\n        e = y - preds \n        return np.where(e &gt; 0, self.alpha, self.alpha - 1)\n\ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                             max_depth=1)\ngbm.fit(x, y, QuantileLoss(alpha=0.9))\npred = gbm.predict(x)    \n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingRegressor(n_estimators=10,\n                                 learning_rate=0.5,\n                                 max_depth=1,\n                                 loss='quantile', alpha=0.9)\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict(x)\n\n\nprint_model_loss_scores(QuantileLoss(alpha=0.9), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.1853\nScikit-Learn Loss = 0.1856\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinary Cross Entropy Loss\nThe previous losses are useful for regression problems, where the target is numeric. But we can also solve classification problems, simply by swapping in an appropriate loss function. Here we‚Äôll implement binary cross entropy, a.k.a. binary deviance, a.k.a. negative binomial log likelihood (sometimes abusively called log loss). One thing to remember is that, as with logistic regression, our model is actually predicting the log odds ratio, not the probability of the positive class. Thus we use expit transformations (the inverse of logit) whenever probabilities are needed, e.g., when predicting the probability that an observation belongs to the positive class.\n\n# make categorical test data\n\ndef expit(t):\n    return np.exp(t) / (1 + np.exp(t))\n\nx = np.linspace(-3, 3, 500)\np = expit(x)\ny = rng.binomial(1, p, size=p.shape)\nx = x.reshape(-1,1)\n\n\n# from scratch GBM\nclass BinaryCrossEntropyLoss():\n    '''Binary Cross Entropy Loss\n    \n    Note that the predictions should be log odds ratios.\n    '''\n    \n    def __init__(self):\n        self.expit = lambda t: np.exp(t) / (1 + np.exp(t))\n    \n    def loss(self, y, preds):\n        p = self.expit(preds)\n        return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n    \n    def negative_gradient(self, y, preds):\n        p = self.expit(preds)\n        return y / p - (1 - y) / (1 - p)\n\n    \ngbm = GradientBoostingMachine(n_trees=10,\n                              learning_rate=0.5,\n                              max_depth=1)\ngbm.fit(x, y, BinaryCrossEntropyLoss())\npred = expit(gbm.predict(x))\n\n\n# scikit-learn GBM\nsk_gbm = GradientBoostingClassifier(n_estimators=10,\n                                    learning_rate=0.5,\n                                    max_depth=1,\n                                    loss='log_loss')\nsk_gbm.fit(x, y)\nsk_pred = sk_gbm.predict_proba(x)[:, 1]\n\n\nprint_model_loss_scores(BinaryCrossEntropyLoss(), y, pred, sk_pred)\n\nFrom Scratch Loss = 0.6379\nScikit-Learn Loss = 0.6403"
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#wrapping-up",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#wrapping-up",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWoohoo! We did it! We finally made it through Friedman‚Äôs paper in its entirety, and we implemented the generic gradient boosting algorithm which works with any differentiable loss function. If you made it this far, great job, gold star! By now you hopefully have a pretty solid grasp on gradient boosting, which is good, because soon we‚Äôre going to dive into the modern Newton descent gradient boosting frameworks like XGBoost. Onward!"
  },
  {
    "objectID": "posts/gradient-boosting-machine-with-any-loss-function/index.html#references",
    "href": "posts/gradient-boosting-machine-with-any-loss-function/index.html#references",
    "title": "How to Implement a Gradient Boosting Machine that Works with Any Loss Function",
    "section": "References",
    "text": "References\nFriedman‚Äôs 2001 paper: Greedy Function Approximation: A Gradient Boosting Machine"
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html",
    "href": "posts/get-down-with-gradient-descent/index.html",
    "title": "Get Down with Gradient Descent",
    "section": "",
    "text": "Ahh, gradient descent. It‚Äôs probably one of the most ubiquitous algorithms used in data science, but you‚Äôre unlikely to see it being celebrated in the limelight of the Kaggle podium. Rather than taking center stage, gradient descent operates under the hood, powering the training for a wide range of models including deep neural networks, gradient boosting trees, generalized linear models, and mixed effects models. Getting an intuition for the algorithm will reveal how model fitting actually works and help us to see the common thread connecting a wide range of seemingly unrelated models. In this post we‚Äôll get the intuition for gradient descent with a fresh analogy, develop the mathematical formulation, and ground our understanding by using it to train ourselves a linear regression model."
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#intuition",
    "href": "posts/get-down-with-gradient-descent/index.html#intuition",
    "title": "Get Down with Gradient Descent",
    "section": "Intuition",
    "text": "Intuition\nBefore we dive into the intuition for gradient descent itself, let‚Äôs get a high-level view of why it‚Äôs useful in training or fiting a model. Training a model basically means finding the model parameter values that make the model fit a given dataset well. We measure how well a model fits data using a special function variously called a loss or cost or objective function. A loss function takes the dataset and the model as arguments and returns a number that tells us how well our model fits the data. Therefore training is an optimization problem in which we search for the model parameter values that result in the minimum value of the loss function. Enter gradient descent.\nGradient descent is a numerical optimization technique that helps us find the inputs that yield the minimum value of a function. Since most explanations of the gradient descent algorithm seem to use a story about hikers being lost in some foggy mountains, we‚Äôre going to try out a new analogy.\nLet‚Äôs say you‚Äôre at a concert. Remember those? They‚Äôre these things that used to happen where people played music and everyone danced and had a great time.\n\nNOTE: Chiming in here in 2023 from a sort-of-post COVID 19 world, happily I can report that concerts and live music are back!\n\nNow suppose at this concert there‚Äôs a dance floor which has become a bit sweltering from copious amounts of ‚Äúgetting down‚Äù. But the temperature isn‚Äôt quite uniform; maybe there‚Äôs a cool spot from a ceiling fan somewhere.\n\n\n\ndance floor\n\n\nLet‚Äôs get ourselves to that cool spot using the following procedure.\n\nFrom our current location, figure out which direction feels coolest.\nTake a step (or simply shimmy) in that direction.\nRepeat steps 1 and 2 until we reach the coolest spot on the dance floor.\n\nThe crux of this procedure is figuring out, at each step, which direction yields the greatest temperature reduction. Our skin is pretty sensitive to temperature, so we can just use awareness of body sensation to sense which direction feels coolest. Luckily, we have a mathematical equivalent to our skin‚Äôs ability to sense local variation in temperature.\n\nDetermine which way to go\nLet \\(f(x,y)\\) be the temperature on the dance floor at position \\((x,y)\\). The direction of fastest decrease in temperature is going to be given by some vector in our \\((x,y)\\) space, e.g.,\n[vector component in \\(x\\) direction, vector component in \\(y\\) direction]\nTurns out that the gradient of a function evaluated at a particular location yields a vector that points in the direction of fastest increase in the function, pretty similar to what we‚Äôre looking for. The gradient of \\(f(x,y)\\) is given by\n\\[ \\nabla f(x,y) = \\left [ \\frac{\\partial f(x,y)}{\\partial x}, \\frac{\\partial f(x,y)}{\\partial y} \\right ] \\]\nThe components of the gradient vector are the partial derivatives of our function \\(f(x,y)\\), evaluated at the point \\((x,y)\\). These partial derivatives just tell us the slope of \\(f(x,y)\\) in the \\(x\\) and \\(y\\) directions respectively. The intuition is that if \\(\\frac{\\partial f(x,y)}{\\partial x}\\) is a large positive number, then moving in the positive \\(x\\) direction will make \\(f(x,y)\\) increase a lot, whereas if \\(\\frac{\\partial f(x,y)}{\\partial x}\\) is a large negative number, then moving in the negative \\(x\\) direction will make \\(f(x,y)\\) increase a lot.\nIt‚Äôs not too hard to see that the direction of fastest decrease is actually just the exact opposite direction from that of fastest increase. Since we can point a vector in the opposite direction by negating its component values, our direction of fastest temperature decrease will be given by the negative gradient of the temperature field \\(-\\nabla f(x,y)\\).\n\n\n\ndance floor with hot and cold sides\n\n\n\n\nTake a step in the right direction\nNow that we have our direction vector, we‚Äôre ready to take a step toward the cool part of the dance floor. To do this, we‚Äôll just add our direction vector to our current position. The update rule would look like this.\n\\[ [x_\\text{next}, y_\\text{next}] = [x_\\text{prev}, y_\\text{prev}] - \\nabla f (x_\\text{prev}, y_\\text{prev}) = [x_\\text{prev}, y_\\text{prev}] -  \\left [ \\frac{\\partial f (x_\\text{prev}, y_\\text{prev})}{\\partial x}, \\frac{\\partial f (x_\\text{prev}, y_\\text{prev})}{\\partial y} \\right ] \\]\nIf we iteratively apply this update rule, we‚Äôll end up tracing a trajectory through the \\((x,y)\\) space on the dance floor and we‚Äôll eventually end up at the coolest spot!\n\n\n\ndance floor with trajectory from hot side to cool side\n\n\nGreat success!"
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#general-formulation",
    "href": "posts/get-down-with-gradient-descent/index.html#general-formulation",
    "title": "Get Down with Gradient Descent",
    "section": "General Formulation",
    "text": "General Formulation\nLet‚Äôs generalize a bit to get to the form of gradient descent you‚Äôll see in references like the wikipedia article.\nFirst we modify our update equation above to handle functions with more than two arguments. We‚Äôll use a bold \\(\\mathbf{x}\\) to indicate a vector of inputs \\(\\mathbf{x} = [x_1,x_2,\\dots,x_p]\\). Our function \\(f(\\mathbf{x}): \\mathbb{R}^p \\mapsto \\mathbb{R}\\) maps a \\(p\\) dimensional input to a scalar output.\nSecond, instead of displacing our current location with the negative gradient vector itself, we‚Äôll first rescale it with a learning rate parameter. This helps address any issues with units on inputs versus outputs. Imagine the input could range between 0 and 1, but the output ranged from 0 to 1,000. We would need to rescale the partial derivatives so the update step doesn‚Äôt send us way too far off in input space.\nFinally, we‚Äôll index our updates with \\(t=0,1,\\dots\\). We‚Äôll run for some prespecified number of iterations or we‚Äôll stop the procedure once the change in \\(f(\\mathbf{x})\\) is sufficiently small from one iteration to the next. Our update equation will look like this.\n\\[\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\nabla f ( \\mathbf{x}_t) \\]\nIn pseudocode we could write it like this.\n# gradient descent\nx = initial_value_of_x \nfor t in range(n_iterations):  # or some other convergence condition\n    x -= learning_rate * gradient_of_f(x)\nNow let‚Äôs see how this algorithm gets used to train models."
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#training-a-linear-regression-model-with-gradient-descent",
    "href": "posts/get-down-with-gradient-descent/index.html#training-a-linear-regression-model-with-gradient-descent",
    "title": "Get Down with Gradient Descent",
    "section": "Training a Linear Regression Model with Gradient Descent",
    "text": "Training a Linear Regression Model with Gradient Descent\nTo get the intuition for how we use gradient descent to train models, let‚Äôs use it to train a linear regression model. Note that we wouldn‚Äôt actually use gradient descent to train a linear model in real life since there is an exact analytical solution for the best-fit parameter values.\nAnyway, in the simple linear regression problem we have numerical feature \\(x\\) and numerical target \\(y\\), and we want to find a model of the form\n\\[F(x) = \\alpha + \\beta x\\]\nThis model has two parameters, \\(\\alpha\\) and \\(\\beta\\). Here ‚Äútraining‚Äù means finding the parameter values that make \\(F(x)\\) fit our \\(y\\) data best. We measure how well, or really how poorly, our model fits the data by using a loss function that yields a small value when a model fits well. Ordinary least squares is so named because it uses mean squared error as its loss function.\n\\[L(y, F(x)) =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - F(x_i))^2  =  \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\alpha + \\beta x_i))^2 \\]\nThe loss function \\(L\\) takes four arguments: \\(x\\), \\(y\\), \\(\\alpha\\), and \\(\\beta\\). But since \\(x\\) and \\(y\\) are fixed given our dataset, we could write the loss as \\(L(\\alpha, \\beta | x, y)\\) to emphasize that \\(\\alpha\\) and \\(\\beta\\) are the only free parameters. So we‚Äôre looking for the following.\n\\[\\underset{\\alpha,\\beta}{\\operatorname{argmin}} ~ L(\\alpha,\\beta|x,y) \\]\nThat‚Äôs right, we‚Äôre looking for the values of \\(\\alpha\\) and \\(\\beta\\) that minimize scalar-valued function \\(L(\\alpha, \\beta)\\). Sounds familiar huh?\nTo solve this minimization problem with gradient descent, we can use the following update rule.\n\\[[\\alpha_{t+1}, \\beta_{t+1}] = [\\alpha_{t}, \\beta_{t}] - \\eta \\nabla L(\\alpha_t, \\beta_t | x, y) \\]\nTo get the gradient \\(\\nabla L(\\alpha,\\beta|x,y)\\), we need the partial derivatives of \\(L\\) with respect to \\(\\alpha\\) and \\(\\beta\\). Since \\(L\\) is just a big sum, it‚Äôs easy to calculate the derivatives.\n\\[ \\frac{\\partial L(\\alpha, \\beta)}{\\partial \\alpha} = \\frac{1}{n} \\sum_{i=1}^{n} -2 (y_i - (\\alpha + \\beta x_i)) \\] \\[ \\frac{\\partial L(\\alpha, \\beta)}{\\partial \\beta} = \\frac{1}{n} \\sum_{i=1}^{n} -2x_i (y_i - (\\alpha + \\beta x_i)) \\]\nGreat! We‚Äôve got everything we need to implement gradient descent to train an ordinary least squares model. Everything except data that is.\n\nToy Data\nLet‚Äôs make a friendly little linear dataset where \\(\\alpha=-10\\) and \\(\\beta=2\\), i.e.\n\\[ y = -10 + 2x + \\text{noise}\\]\n\nimport numpy as np \n\nalpha_true = -10\nbeta_true = 2\n\nrng = np.random.default_rng(42)\nx = np.linspace(0, 10, 50)\ny = alpha_true + beta_true*x + rng.normal(0, 1, size=x.shape)\n\n\n\n\n\n\n\n\n\n\n\n\nImplementation\nOur implementation will use a function to compute the gradient of the loss function. Since we have two parameters, we‚Äôll use length-2 arrays to hold their values and their partial derivatives. At each iteration, we update the parameter values by subtracting the rescaled partial derivatives.\n\n\n# linear regression using gradient descent \n\ndef gradient_of_loss(parameters, x, y):\n    alpha = parameters[0]\n    beta = parameters[1]\n    partial_alpha = np.mean(-2*(y - (alpha + beta*x)))\n    partial_beta = np.mean(-2*x*(y - (alpha + beta*x)))\n    return np.array([partial_alpha, partial_beta])\n\nlearning_rate = 0.02\nparameters = np.array([0.0, 0.0]) # initial values of alpha and beta\n\nfor _ in range(500):\n    partial_derivatives = gradient_of_loss(parameters, x, y)\n    parameters -= learning_rate * partial_derivatives\n    \nparameters\n\narray([-10.07049616,   2.03559051])\n\n\nWe can see the loss function decreasing throughout the 500 iterations.\n\n\n\n\n\n\n\n\n\nAnd we can visualize the loss function as a contour plot over \\((\\alpha,\\beta)\\) space. The blue points show the trajectory our gradient descent followed as it shimmied from the initial position to the coolest spot in \\((\\alpha, \\beta)\\) space where the loss function is nice and small.\n\n\n\n\n\n\n\n\n\nOur gradient descent settles in a spot pretty close to \\((-10, 2)\\) in \\((\\alpha,\\beta)\\) space, which gives us the final fitted model below."
  },
  {
    "objectID": "posts/get-down-with-gradient-descent/index.html#wrapping-up",
    "href": "posts/get-down-with-gradient-descent/index.html#wrapping-up",
    "title": "Get Down with Gradient Descent",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere you have it, gradient descent explained with a fresh new analogy having nothing whatsoever to do with foggy mountains, plus an implemented example fitting a linear model. While we often see gradient descent used to train models by performing an optimization in parameter space, as in generalized linear models and neural networks, there are other ways to use this powerful technique to train models. In particular, we‚Äôll soon see how our beloved gradient boosting tree models use gradient descent in prediction space, rather than parameter space. Stay tuned for that mind bender in a future post."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html",
    "href": "posts/8020-pandas-tutorial/index.html",
    "title": "The 80/20 Pandas Tutorial",
    "section": "",
    "text": "Ahh, pandas. In addition to being everyone‚Äôs favorite mostly vegetarian bear from south central China, it‚Äôs also the python library for working with tabular data, a.k.a. dataframes. When you dive into pandas, you‚Äôll quickly find out that there is a lot going on; indeed there are hundreds of methods for operating on dataframes. But luckily for us, as with many areas of life, there is a Pareto Principle, or 80/20 rule, that will help us focus on the small set of methods that collectively solve the majority of our data transformation needs.\nIf you‚Äôre like me, then pandas is not your first data-handling tool; maybe you‚Äôve been using SQL or R with data.table or dplyr. If so, that‚Äôs great because you already have a sense for the key operations we need when working with tabular data. In their book, R for Data Science, Garrett Grolemund and Hadley Wickham describe five essential operations for manipulating dataframes. I‚Äôve found that these cover the majority of my data transformation tasks to prepare data for analysis, visualization, and modeling.\nI would add that we also need a way to build up more complex transformations by chaining these fundamental operations together sequentially.\nBefore we dive in, here‚Äôs the TLDR on the pandas methods that I prefer for accomplishing these tasks, along with their equivalents from SQL and dplyr in R."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#imports-and-data",
    "href": "posts/8020-pandas-tutorial/index.html#imports-and-data",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Imports and Data",
    "text": "Imports and Data\nWe‚Äôll use the nycflights13 dataset which contains data on the roughly 300k flights that departed from New York City in 2013.\n\nimport pandas as pd\nimport numpy as np\n\nurl = 'https://www.openintro.org/book/statdata/nycflights.csv'\nstorage_options = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0'}\ndf = pd.read_csv(url, storage_options=storage_options)\n\nTypically if I‚Äôm only going to be using a single dataframe, I‚Äôll use the name ‚Äúdf‚Äù. This is a pretty strong convention in pandas, e.g.¬†you can see the name ‚Äúdf‚Äù being used all over the pandas documentation; therefore it makes your code easier for others to understand. If there will be more than one dataframe, I suggest prepending a meaningful name to the ‚Äúdf‚Äù, e.g.¬†flights_df.\nLet‚Äôs have a look at the dataframe structure using the info() method.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32735 entries, 0 to 32734\nData columns (total 16 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   year       32735 non-null  int64 \n 1   month      32735 non-null  int64 \n 2   day        32735 non-null  int64 \n 3   dep_time   32735 non-null  int64 \n 4   dep_delay  32735 non-null  int64 \n 5   arr_time   32735 non-null  int64 \n 6   arr_delay  32735 non-null  int64 \n 7   carrier    32735 non-null  object\n 8   tailnum    32735 non-null  object\n 9   flight     32735 non-null  int64 \n 10  origin     32735 non-null  object\n 11  dest       32735 non-null  object\n 12  air_time   32735 non-null  int64 \n 13  distance   32735 non-null  int64 \n 14  hour       32735 non-null  int64 \n 15  minute     32735 non-null  int64 \ndtypes: int64(12), object(4)\nmemory usage: 4.0+ MB"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#select-rows-based-on-their-values-with-query",
    "href": "posts/8020-pandas-tutorial/index.html#select-rows-based-on-their-values-with-query",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Select rows based on their values with query()",
    "text": "Select rows based on their values with query()\nquery() lets you retain a subset of rows based on the values of the data; it‚Äôs like dplyr::filter() in R or WHERE in SQL. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even check if the column value matches any items in a list.\n\n# compare one column to a value\ndf.query('month == 6')\n\n# compare two column values\ndf.query('arr_delay &gt; dep_delay')\n\n# using arithmetic\ndf.query('arr_delay &gt; 0.5 * air_time')\n\n# using \"and\"\ndf.query('month == 6 and day == 1')\n\n# using \"or\"\ndf.query('origin == \"JFK\" or dest == \"JFK\"')\n\n# column value matching any item in a list\ndf.query('carrier in [\"AA\", \"UA\"]')\n\nYou may have noticed that it seems to be much more popular to filter pandas data frames using boolean indexing. Indeed when I ask my favorite search engine how to filter a pandas dataframe on its values, I find this tutorial, this blog post, various questions on Stack Overflow, and even the pandas documentation, all espousing boolean indexing. Here‚Äôs what it looks like.\n\n# canonical boolean indexing\ndf[(df['carrier'] == \"AA\") & (df['origin'] == \"JFK\")]\n\n# the equivalent use of query()\ndf.query('carrier == \"AA\" and origin == \"JFK\"')\n\nThere are a few reasons I prefer query() over boolean indexing.\n\nquery() does not require me to type the dataframe name again, whereas boolean indexing requires me to type it every time I wish to refer to a column.\nquery() makes the code easier to read and understand, especially when expressions get complex.\nquery() is more computationally efficient than boolean indexing.\nquery() can safely be used in dot chains, which we‚Äôll see very soon."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#select-columns-by-name-with-filter",
    "href": "posts/8020-pandas-tutorial/index.html#select-columns-by-name-with-filter",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Select columns by name with filter()",
    "text": "Select columns by name with filter()\nfilter() lets you pick out a specific set of columns by name; it‚Äôs analogous to dplyr::select() in R or SELECT in SQL. You can either provide exactly the column names you want, or you can grab all columns whose names contain a given substring or which match a given regular expression. This isn‚Äôt a big deal when your dataframe has only a few columns, but is particularly useful when you have a dataframe with tens or hundreds of columns.\n\n# select a list of columns\ndf.filter(['origin', 'dest'])\n\n# select columns containing a particular substring\ndf.filter(like='time')\n\n# select columns matching a regular expression\ndf.filter(regex='e$')"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#sort-rows-with-sort_values",
    "href": "posts/8020-pandas-tutorial/index.html#sort-rows-with-sort_values",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Sort rows with sort_values()",
    "text": "Sort rows with sort_values()\nsort_values() changes the order of the rows based on the data values; it‚Äôs likedplyr::arrange() in R or ORDER BY in SQL. You can specify one or more columns on which to sort, where their order denotes the sorting priority. You can also specify whether to sort in ascending or descending order.\n\n# sort by a single column\ndf.sort_values('air_time')\n\n# sort by a single column in descending order\ndf.sort_values('air_time', ascending=False)\n\n# sort by carrier, then within carrier, sort by descending distance\ndf.sort_values(['carrier', 'distance'], ascending=[True, False])"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#add-new-columns-with-assign",
    "href": "posts/8020-pandas-tutorial/index.html#add-new-columns-with-assign",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Add new columns with assign()",
    "text": "Add new columns with assign()\nassign() adds new columns which can be functions of the existing columns; it‚Äôs like dplyr::mutate() from R.\n\n# add a new column based on other columns\ndf.assign(speed = lambda x: x.distance / x.air_time)\n\n# another new column based on existing columns\ndf.assign(gain = lambda x: x.dep_delay - x.arr_delay)\n\nIf you‚Äôre like me, this way of using assign() might seem a little strange at first. Let‚Äôs break it down. In the call to assign() the keyword argument speed tells pandas the name of our new column. The business to the right of the = is a inline lambda function that takes the dataframe we passed to assign() and returns the column we want to add.\nI like using x as the lambda argument because its easy to type and it evokes tabular data (think design matrix), which reminds me that it refers to the entire dataframe. We can then access the other columns in our dataframe using the dot like x.other_column.\nIt‚Äôs true that you can skip the whole lambda business and refer to the dataframe to which you are assigning directly inside the assign. That might look like this.\ndf.assign(speed = flights.distance / flights.air_time)\nI prefer using a lambda for the following reasons.\n\nUsing the lambda will save you from typing the name every time you want to refer to a column.\nThe lambda makes your code more portable. Since you refer to the dataframe as a generic x, you can reuse this same assignment code on a dataframe with a different name.\nMost importantly, the lambda will allow you to harness the power of dot chaining."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#chain-transformations-together-with-the-dot-chain",
    "href": "posts/8020-pandas-tutorial/index.html#chain-transformations-together-with-the-dot-chain",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Chain transformations together with the dot chain",
    "text": "Chain transformations together with the dot chain\nOne of the awesome things about pandas is that the object.method() paradigm lets us easily build up complex dataframe transformations from a sequence of method calls. In R, this is effectively accomplished by the pipe %&gt;% operator. For example, suppose we want to look at high-speed flights from JFK to Honolulu, which would require us to query for JFK to Honolulu flights, assign a speed column, and maybe sort on that new speed column.\nWe can say:\n\n# neatly chain method calls together\n(\n    df\n    .query('origin == \"JFK\"')\n    .query('dest == \"HNL\"')\n    .assign(speed = lambda x: x.distance / x.air_time)\n    .sort_values(by='speed', ascending=False)\n    .query('speed &gt; 8.0')\n)\n\nWe compose the dot chain by wrapping the entire expression in parentheses and indenting each line within. The first line is the name of the dataframe on which we are operating. Each subsequent line has a single method call.\nThere are a few great things about writing the code this way:\n\nReadability - It‚Äôs easy to scan down the left margin of the code to see what‚Äôs happening. The first line gives us our noun (the dataframe) and each subsequent line starts with a verb. You could read this as ‚Äútake df then query the rows where origin is JFK, then query for rows where destination is HNL, then assign a new column called speed, then sort the dataframe by speed, then query only for the rows where speed is greater than 8.0.\nFlexibility - It‚Äôs easy to comment out individual lines and re-run the cell. It‚Äôs also easy to reorder operations, since only one thing happens on each line.\nNeatness - We have not polluted our workspace with any intermediate variables, nor have we wasted any mental energy thinking of names for any temporary variables.\n\nBy default, dot chains do not modify the original dataframe; they just output a temporary result that we can inspect directly in the output. If you want to store the result, or pass it along to another function (e.g.¬†for plotting), you can simply assign the entire dot chain to a variable.\n\n# store the output of the dot chain in a new dataframe\nhigh_speed_flights_df = (\n    df\n    .assign(speed = lambda x: x.distance / x.air_time)\n    .query('speed &gt; 8.0')\n)"
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#collapsing-rows-into-grouped-summaries-with-groupby",
    "href": "posts/8020-pandas-tutorial/index.html#collapsing-rows-into-grouped-summaries-with-groupby",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Collapsing rows into grouped summaries with groupby()",
    "text": "Collapsing rows into grouped summaries with groupby()\ngroupby() combined with apply() gives us flexibility and control over our grouped summaries; it‚Äôs like dplyr::group_by() and dplyr::summarise() in R. This is the primary pattern I use for SQL-style groupby operations in pandas. Specifically it unlocks the following essential functionality you‚Äôre used to having in SQL.\n\nspecify the names of the aggregation columns we create\nspecify which aggregation function to use on which columns\ncompose more complex aggregations such as the proportion of rows meeting some condition\naggregate over arbitrary functions of multiple columns\n\nLet‚Äôs check out the departure delay stats for each carrier.\n\n# grouped summary with groupby and apply\n(\n    df\n    .groupby(['carrier'])\n    .apply(lambda d: pd.Series({\n        'n_flights': len(d),\n        'med_delay': d.dep_delay.median(),\n        'avg_delay': d.dep_delay.mean(),\n    }))\n    .head()\n)\n\n\n\n\n\n\n\n\nn_flights\nmed_delay\navg_delay\n\n\ncarrier\n\n\n\n\n\n\n\n9E\n1696.0\n-1.0\n17.285967\n\n\nAA\n3188.0\n-2.0\n9.142409\n\n\nAS\n66.0\n-4.5\n5.181818\n\n\nB6\n5376.0\n-1.0\n13.137091\n\n\nDL\n4751.0\n-2.0\n8.529573\n\n\n\n\n\n\n\nWhile you might be used to apply() acting over the rows or columns of a dataframe, here we‚Äôre calling apply on a grouped dataframe object, so it‚Äôs acting over the groups. According to the pandas documentation:\n\nThe function passed to apply must take a dataframe as its first argument and return a dataframe, a series or a scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method.\n\nWe need to supply apply() with a function that takes each chunk of the grouped dataframe and returns (in our case) a series object with one element for each new aggregation column. Notice that I use a lambda to specify the function we pass to apply()), and that I name its argument d, which reminds me that it‚Äôs a dataframe. My lambda returns a pandas series whose index entries specify the new aggregation column names, and whose values constitute the results of the aggregations for each group. Pandas will then stitch everything back together into a lovely dataframe.\nNotice how nice the code looks when we use this pattern. Each aggregation is specified on its own line, which makes it easy to see what aggregation columns we‚Äôre creating and allows us to comment, uncomment, and reorder the aggregations without breaking anything.\nHere are some more complex aggregations to illustrate some useful patterns.\n\n# more complex grouped summary\n(\n    df\n    .groupby(['carrier'])\n    .apply(lambda d: pd.Series({\n        'avg_gain': np.mean(d.dep_delay - d.arr_delay), \n        'pct_delay_gt_30': np.mean(d.dep_delay &gt; 30), \n        'pct_late_dep_early_arr': np.mean((d.dep_delay &gt; 0) & (d.arr_delay &lt; 0)), \n        'avg_arr_given_dep_delay_gt_0': d.query('dep_delay &gt; 0').arr_delay.mean(),\n        'cor_arr_delay_dep_delay': np.corrcoef(d.dep_delay, d.arr_delay)[0,1],\n    }))\n    .head()\n)\n\n\n\n\n\n\n\n\navg_gain\npct_delay_gt_30\npct_late_dep_early_arr\navg_arr_given_dep_delay_gt_0\ncor_arr_delay_dep_delay\n\n\ncarrier\n\n\n\n\n\n\n\n\n\n9E\n9.247642\n0.196934\n0.110259\n39.086111\n0.932485\n\n\nAA\n7.743726\n0.113237\n0.105395\n30.087165\n0.891013\n\n\nAS\n16.515152\n0.106061\n0.121212\n28.058824\n0.864565\n\n\nB6\n3.411458\n0.160528\n0.084449\n37.306866\n0.914180\n\n\nDL\n7.622816\n0.097874\n0.100821\n30.078029\n0.899327\n\n\n\n\n\n\n\nHere‚Äôs what‚Äôs happening.\n\nnp.mean(d.dep_delay - d.arr_delay) aggregates over the difference of two columns.\nnp.mean(d.dep_delay &gt; 30) computes the proportion of rows where the delay is greater than 30 minutes. Generating a boolean series based on some condition and then using mean() to find the proportion comes up all the time.\nnp.mean((d.dep_delay &gt; 0) & (d.arr_delay &lt; 0)) shows that we can compute proportions where conditions on multiple columns are met.\nd.query('dep_delay &gt; 0').arr_delay.mean() computes the average arrival delay on flights where the departure was delayed. Here we first filter each grouped dataframe down to the subset of rows where departure delay is greater than zero using query(), and then we take the mean of the remaining arrival delays.\nnp.corrcoef(d.dep_delay, d.arr_delay)[0,1] computes the correlation coefficient between departure and arrival delays. Remember we can use pretty much any reduction operation to collapse values down to a scalar.\n\nYou might have noticed that the canonical pandas approach for grouped summaries is to use agg(). That works well if you need to apply the same aggregation function on each column in the dataframe, e.g.¬†taking the mean of every column. But because of the kind of data I work with these days, it‚Äôs much more common for me to use customized aggregations like those above, so the groupby() apply() idiom works best for me."
  },
  {
    "objectID": "posts/8020-pandas-tutorial/index.html#wrapping-up",
    "href": "posts/8020-pandas-tutorial/index.html#wrapping-up",
    "title": "The 80/20 Pandas Tutorial",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere you have it, how to pull off the five most essential data transformation tasks using pandas in a style reminiscent of my beloved dplyr. Remember that part of the beauty of pandas is that since there are so many ways to do most tasks, you can develop your own style based on the kind of data you work with, what you like about other tools, how you see others using the tools, and of course your own taste and preferences.\nIf you found this post helpful or if you have your own preferred style for accomplishing any of these key transformations with pandas, do let me know about it in the comments."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "",
    "text": "Ahh, the dark art of hyperparameter tuning. It‚Äôs a key step in the machine learning workflow, and it‚Äôs an activity that can easily be overlooked or be overkill. Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils. Today I‚Äôll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework. I‚Äôll give you some intuition for how to think about the key parameters in XGBoost, and I‚Äôll show you an efficient strategy for parameter tuning GBTs. I‚Äôll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like. You can download a notebook with this tuning workflow from my data science templates repository. Finally we‚Äôll wrap up with the kind of cautionary tale data scientists tell their colleagues around the campfire about when all this fancy hyperparameter tuning can backfire catastrophically‚Äîignore at your own peril."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#xgboost-parameters",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#xgboost-parameters",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "XGBoost Parameters",
    "text": "XGBoost Parameters\nGradient boosting algorithms like XGBoost have two main types of hyperparameters: tree parameters which control the decision tree trained at each boosting round and boosting parameters which control the boosting procedure itself. Below I‚Äôll highlight my favorite parameters, but you can see the full list in the documentation.\n\nTree Parameters\nIn theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, decision trees are typically the best choice. In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.\n\nTree construction algorithm\nThe tree construction algorithm boils down to split finding, and different algorithms have different ways of generating candidate splits to consider. In XGBoost we have the parameter:\n\ntree_method - select tree construction algorithm: exact, hist, approx, or the horrifying default‚Äîauto‚Äîwhich outsources your choice of tree construction algo to XGBoost and which you should never ever use. I‚Äôve been burned by this hidden tree_method=auto default multiple times before learning my lesson. Why is the online model worse than the offline model? Why is this model suddenly taking so much longer to train? Avoid these debugging nightmares and set tree_method explicitly; the exact method tends to be slow and ironically less accurate, so I use either approx or hist.\n\n\n\nTree complexity parameters\nTree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be. I use these two parameters:\n\nmax_depth - maximum number of split levels allowed. Reasonable values are usually from 3-12.\nmin_child_weight - minimum allowable sum of hessian values over data in a node. When using the default squared error objective, this is the minimum number of samples allowed in a leaf node (see this explanation of why that‚Äôs true). For a squared error objective, values in [1, 200] usually work well.\n\nThese two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing min child weight makes trees less expressive and therefore is a powerful way to counter overfitting. Note that gamma (a.k.a. min_split_loss) also limits node splitting, but I usually don‚Äôt use it because min_child_weight seems to work well enough on its own.\n\n\nSampling parameters\nXGBoost can randomly sample rows and columns to be used for training each tree; you might think of this as bagging. We have a few parameters:\n\nsubsample - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.\ncolsample_bytree, colsample_bylevel, colsample_bynode - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split. Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.\n\n\n\nRegularization parameters\nIn XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero. I usually use:\n\nreg_lambda - L2 regularization of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. Valid values are in [0,\\(\\infty\\)), but good values typically fall in [0,10].\n\nThere is also an L1 regularization parameter called reg_alpha; feel free to use it instead. It seems that using one or the other is usually sufficient.\n\n\n\nBoosting Parameters and Early Stopping\nTrained gradient boosting models take the form:\n\\[ F(\\mathbf{x}) = b + \\eta \\sum_{k=1}^{K} f_k(\\mathbf{x}) \\]\nwhere \\(b\\) is the constant base predicted value, \\(f_k(\\cdot)\\) is the base learner for round \\(k\\), parameter \\(K\\) is the number of boosting rounds, and parameter \\(\\eta\\) is the learning rate. In XGBoost these parameters correspond with:\n\nnum_boost_round (\\(K\\)) - the number of boosting iterations\nlearning_rate (\\(\\eta\\)) - the scaling or ‚Äúshrinkage‚Äù factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. Fun fact: the \\(\\eta\\) character is called ‚Äúeta‚Äù, and learning_rate is aliased to eta in xgboost, so you can use parameter eta instead of learning_rate if you like.\n\nThese two parameters are very closely linked; the optimal value of one depends on the value of the other. To illustrate their relationship, we can train two different XGBoost models on the same training dataset, where one model has a lower learning rate than the other.\n\n\nCode\nimport xgboost as xgb \nfrom sklearn.datasets import make_regression \nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split \n\nX, y = make_regression(5000, random_state=0)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)\n\neta1, eta2 = 0.3, 0.15\nreg1 = xgb.XGBRegressor(n_estimators=1000, learning_rate=eta1, early_stopping_rounds=25)\nreg1.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], \n        verbose=0);\nreg2 = xgb.XGBRegressor(n_estimators=1000, learning_rate=eta2, early_stopping_rounds=25)\nreg2.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_valid, y_valid)], \n        verbose=0);\n\nfig, ax = plt.subplots()\n\nbest_round1, best_round2 = reg1.best_iteration, reg2.best_iteration\nobj1 = reg1.evals_result()['validation_1']['rmse']\nobj2 = reg2.evals_result()['validation_1']['rmse']\nbest_obj1 = obj1[best_round1]\nbest_obj2 = obj2[best_round1]\n\nplt.plot(reg1.evals_result()['validation_1']['rmse'], '-b', label=f'learning_rate={eta1}')\nplt.plot(reg2.evals_result()['validation_1']['rmse'], '-r', label=f'learning_rate={eta2}')\nax.annotate(f'learning_rate={eta1}\\nbest_iteration={best_round1}', \n            xy=(best_round1, best_obj1), \n            xytext=(best_round1, best_obj1+20),\n            horizontalalignment='right',\n            arrowprops=dict(facecolor='black', shrink=0.05))\nax.annotate(f'learning_rate={eta2}\\nbest_iteration={best_round2}', \n            xy=(best_round2, best_obj2), \n            xytext=(best_round2, best_obj2+20),\n            horizontalalignment='right',\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.legend()\nplt.ylabel('RMSE') \nplt.xlabel('boosting round') \nplt.title('Validation Scores by Boosting Round');\n\n\n\n\n\nHold-out validation score (RMSE) by boosting round for two XGBoost models differing only by learning rate.\n\n\n\n\nThe above figure shows root mean squared error measured on a held-out validation dataset for two different XGBoost models: one with a higher learning rate and one with a lower learning rate. The figure demonstrates two key properties of the boosting parameters:\n\nWhile training a model with a given learning rate, the evaluation score (computed on a hold-out set) tends to improve with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.\nAll else constant, a smaller learning rate leads to a model with more boosting rounds and better evaluation score.\n\nWe can leverage the first property to make our tuning more efficient by using XGBoost‚Äôs early_stopping_rounds: int argument, which terminates training after observing the specified number of boosting rounds without sufficient improvement to the evaluation metric. The models above were trained using early_stopping_rounds=50, which terminates training after 50 boosting rounds without improvement in RMSE on the validation data. For each model, the arrow indicates the boosting round with the best score.\nThe figure also exemplifies the second property, where the model with lower learning rate attains a better validation score but requires more boosting rounds to trigger early stopping. Note that smaller and smaller learning rates will provide diminishing improvements to the validation score."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#an-efficient-parameter-search-strategy-for-xgboost",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#an-efficient-parameter-search-strategy-for-xgboost",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "An Efficient Parameter Search Strategy for XGBoost",
    "text": "An Efficient Parameter Search Strategy for XGBoost\nEfficiency is the key to effective parameter tuning, because wasting less time means searching more parameter values and finding better models in a given amount of time. But as we just saw, there is a tradeoff between accuracy and training time via the learning rate. Given infinite time and compute resources, we would just choose an arbitrarily tiny learning rate and search through tree parameter values while using early stopping to choose the number of boosting rounds. The problem is that tiny learning rates require tons of boosting rounds, which will make our ideal search prohibitively slow when confronted with the reality of finite time and resources. So what can we do?\nMy approach is based on the claim that good tree parameters at one learning rate are also good tree parameters at other learning rates. The intuition is that given two models‚Äîone with good tree parameters and one with bad tree parameters‚Äîthe model with good tree parameters will score better, regardless of the learning rate. Thus, tree parameters are ‚Äúindependent‚Äù of boosting parameters‚ÄîSee this notebook for justification of this claim.\nIndependence between tree parameters and boosting parameters suggests a two-stage procedure where we first find optimal tree parameters, then we maximize performance by pushing boosting parameters to the extreme. The procedure is:\n\nTune tree parameters. Fix the learning rate at a relatively high value (like 0.3ish) and enable early stopping so that each model trains within a few seconds. Use your favorite hyperparameter tuning technique to find the optimal tree parameters.\nTune boosting parameters. Using these optimal tree parameter values, fix the learning rate as low as you want and train your model, using early stopping to identify the optimal number of boosting rounds.\n\nWhy is this a good idea? Because by starting with a high learning rate and early stopping enabled, you can burn through hundreds of model training trials and find some really good tree parameters in a few minutes. Then, with the confidence that your tree parameters are actually quite good, you can set a really low learning rate and boost a few thousand rounds to get a model with the best of both tree parameter and boosting parameter worlds.\n\nYou can check out this notebook where I justify this approach by running two parameter searches‚Äîone with high learning rate and one with low learning rate‚Äîshowing that they recover the same optimal tree parameters."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#tuning-xgboost-parameters-with-optuna",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#tuning-xgboost-parameters-with-optuna",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Tuning XGBoost Parameters with Optuna",
    "text": "Tuning XGBoost Parameters with Optuna\nOptuna is a model-agnostic python library for hyperparameter tuning. I like it because it has a flexible API that abstracts away the details of the search algorithm being used. That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more. Another massive benefit is that optuna provides a specific XGBoost integration which terminates training early on lousy parameter combinations.\nYou can install optuna with anaconda, e.g.\n$ conda install -c conda-forge optuna"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#example-tuning-the-bluebook-for-bulldozers-regression-model",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#example-tuning-the-bluebook-for-bulldozers-regression-model",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Example: Tuning the Bluebook for Bulldozers Regression Model",
    "text": "Example: Tuning the Bluebook for Bulldozers Regression Model\nTo illustrate the procedure, we‚Äôll tune the parameters for the regression model we built back in the XGBoost for regression post. First we‚Äôll load up the bulldozer data and prepare the features and target just like we did before.\n\n\nCode\nimport time \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport optuna \n\ndf = pd.read_csv('../xgboost-for-regression-in-python/Train.csv', parse_dates=['saledate']);\n\ndef encode_string_features(df):\n    out_df = df.copy()\n    for feature, feature_type in df.dtypes.items():\n        if feature_type == 'object':\n            out_df[feature] = out_df[feature].astype('category')\n    return out_df\n\ndf = encode_string_features(df)\n\ndf['saledate_days_since_epoch'] = (\n    df['saledate'] - pd.Timestamp(year=1970, month=1, day=1)\n    ).dt.days\n\ndf['logSalePrice'] = np.log1p(df['SalePrice'])\n\n\nfeatures = [\n    'SalesID',\n    'MachineID',\n    'ModelID',\n    'datasource',\n    'auctioneerID',\n    'YearMade',\n    'MachineHoursCurrentMeter',\n    'UsageBand',\n    'fiModelDesc',\n    'fiBaseModel',\n    'fiSecondaryDesc',\n    'fiModelSeries',\n    'fiModelDescriptor',\n    'ProductSize',\n    'fiProductClassDesc',\n    'state',\n    'ProductGroup',\n    'ProductGroupDesc',\n    'Drive_System',\n    'Enclosure',\n    'Forks',\n    'Pad_Type',\n    'Ride_Control',\n    'Stick',\n    'Transmission',\n    'Turbocharged',\n    'Blade_Extension',\n    'Blade_Width',\n    'Enclosure_Type',\n    'Engine_Horsepower',\n    'Hydraulics',\n    'Pushblock',\n    'Ripper',\n    'Scarifier',\n    'Tip_Control',\n    'Tire_Size',\n    'Coupler',\n    'Coupler_System',\n    'Grouser_Tracks',\n    'Hydraulics_Flow',\n    'Track_Type',\n    'Undercarriage_Pad_Width',\n    'Stick_Length',\n    'Thumb',\n    'Pattern_Changer',\n    'Grouser_Type',\n    'Backhoe_Mounting',\n    'Blade_Type',\n    'Travel_Controls',\n    'Differential_Type',\n    'Steering_Controls',\n    'saledate_days_since_epoch'\n ]\n\ntarget = 'logSalePrice'\n\n\nBut this time, since we‚Äôre going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes. We make four different xgboost.DMatrix datasets for this process: training, validation, training+validation, and test. Training and validation are for the parameter search, and training+validation and test are for the final model.\n\nn_valid = 12000\nn_test = 12000\n\nsorted_df = df.sort_values(by='saledate')\ntrain_df = sorted_df[:-(n_valid + n_test)] \nvalid_df = sorted_df[-(n_valid + n_test):-n_test] \ntest_df = sorted_df[-n_test:]\n\ndtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], \n                     enable_categorical=True)\ndvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], \n                     enable_categorical=True)\ndtest = xgb.DMatrix(data=test_df[features], label=test_df[target], \n                    enable_categorical=True)\ndtrainvalid = xgb.DMatrix(data=pd.concat([train_df, valid_df])[features], \n                          label=pd.concat([train_df, valid_df])[target], \n                          enable_categorical=True)"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#preliminaries-base-parameters-and-scoring-function",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#preliminaries-base-parameters-and-scoring-function",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Preliminaries: base parameters and scoring function",
    "text": "Preliminaries: base parameters and scoring function\nWe‚Äôll go ahead and set a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping. We‚Äôll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE.\n\nmetric = 'rmse'\nbase_params = {\n    'objective': 'reg:squarederror',\n    'eval_metric': metric,\n}\n\n\ndef score_model(model: xgb.core.Booster, dmat: xgb.core.DMatrix) -&gt; float:\n    y_true = dmat.get_label() \n    y_pred = model.predict(dmat) \n    return mean_squared_error(y_true, y_pred, squared=False)"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-1-tune-tree-parameters-with-optuna",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-1-tune-tree-parameters-with-optuna",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Stage 1: Tune Tree Parameters with Optuna",
    "text": "Stage 1: Tune Tree Parameters with Optuna\nNext we need to choose a fixed learning rate and tune the tree parameters. We want a learning rate that allows us to train within a few seconds, so we need to time model training. Start with a high learning rate (like 0.8) and work down until you find a rate that takes a few seconds. Below I end up landing at 0.3, which takes about 4 seconds to train on my little laptop.\n\nlearning_rate = 0.3\n\nparams = {\n    'tree_method': 'approx',\n    'learning_rate': learning_rate\n}\nparams.update(base_params)\ntic = time.time()\nmodel = xgb.train(params=params, dtrain=dtrain,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  num_boost_round=10000,\n                  early_stopping_rounds=50,\n                  verbose_eval=0)\nprint(f'{time.time() - tic:.1f} seconds')\n\n4.5 seconds\n\n\nThen we implement our optuna objective, a function taking an optuna study Trial object and returning the score we want to optimize. We use the suggest_categorical, suggest_float, and suggest_int methods of the Trial object to define the search space for each parameter. Note the use of the pruning callback function which we pass into the callback argument of the XGBoost train function; this is a must, since it allows optuna to terminate training on lousy models after a few boosting rounds. After training a model with the selected parameter values, we stash the optimal number of boosting rounds from early stopping into an optuna user attribute using the trial.user_attrs() method. Finally we return the score computed by our model_score function.\n\ndef objective(trial):\n    params = {\n        'tree_method': trial.suggest_categorical('tree_method', ['approx', 'hist']),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 250),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.1, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log=True),\n        'learning_rate': learning_rate,\n    }\n    num_boost_round = 10000\n    params.update(base_params)\n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, f'valid-{metric}')\n    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                      evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                      early_stopping_rounds=50,\n                      verbose_eval=0,\n                      callbacks=[pruning_callback])\n    trial.set_user_attr('best_iteration', model.best_iteration)\n    return model.best_score\n\nTo create a new optuna study and search through 50 parameter combinations, you could just run these two lines.\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nBut, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials‚Äîwho knows how long 50 trials will take. I also want the results to be reproducible. So, to set the random seed and run the optimization for around 300 seconds (long enough to go make a nice cup of tea, stretch, and come back), I do something like this:\n\nsampler = optuna.samplers.TPESampler(seed=42)\nstudy = optuna.create_study(direction='minimize', sampler=sampler)\ntic = time.time()\nwhile time.time() - tic &lt; 300:\n    study.optimize(objective, n_trials=1)\n\n\n\nCode\nprint('Stage 1 ==============================')\nprint(f'best score = {study.best_trial.value}')\nprint('boosting params ---------------------------')\nprint(f'fixed learning rate: {learning_rate}')\nprint(f'best boosting round: {study.best_trial.user_attrs[\"best_iteration\"]}')\nprint('best tree params --------------------------')\nfor k, v in study.best_trial.params.items():\n    print(k, ':', v)\n\n\nStage 1 ==============================\nbest score = 0.23107522766919256\nboosting params ---------------------------\nfixed learning rate: 0.3\nbest boosting round: 23\nbest tree params --------------------------\ntree_method : approx\nmax_depth : 10\nmin_child_weight : 6\nsubsample : 0.9729188669457949\ncolsample_bynode : 0.8491983767203796\nreg_lambda : 0.008587261143813469\n\n\nIf we decide we want to tune the tree parameters a little more, we can just call study.optimize(...) again, adding as many trials as we want to the study. Once we‚Äôre happy with the tree parameters, we can proceed to stage 2."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-2-intensify-the-boosting-parameters",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#stage-2-intensify-the-boosting-parameters",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Stage 2: Intensify the Boosting Parameters",
    "text": "Stage 2: Intensify the Boosting Parameters\nNow we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate; here I use 0.01, but you could go lower. The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you‚Äôll need to max out the evaluation metric on the validation data.\n\nlow_learning_rate = 0.01\n\nparams = {}\nparams.update(base_params)\nparams.update(study.best_trial.params)\nparams['learning_rate'] = low_learning_rate\nmodel_stage2 = xgb.train(params=params, dtrain=dtrain, \n                         num_boost_round=10000,\n                         evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                         early_stopping_rounds=50,\n                         verbose_eval=0)\n\n\n\nCode\nprint('Stage 2 ==============================')\nprint(f'best score = {score_model(model_stage2, dvalid)}')\nprint('boosting params ---------------------------')\nprint(f'fixed learning rate: {params[\"learning_rate\"]}')\nprint(f'best boosting round: {model_stage2.best_iteration}')\n\n\nStage 2 ==============================\nbest score = 0.22172991931438446\nboosting params ---------------------------\nfixed learning rate: 0.01\nbest boosting round: 1446"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#train-and-evaluate-the-final-model",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#train-and-evaluate-the-final-model",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Train and Evaluate the Final Model",
    "text": "Train and Evaluate the Final Model\nNow we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2. Then we evaluate on the held out test data.\n\nmodel_final = xgb.train(params=params, dtrain=dtrainvalid, \n                        num_boost_round=model_stage2.best_iteration,\n                        verbose_eval=0)\n\n\n\nCode\nprint('Final Model ==========================')\nprint(f'test score = {score_model(model_final, dtest)}')\nprint('parameters ---------------------------')\nfor k, v in params.items():\n    print(k, ':', v)\nprint(f'num_boost_round: {model_stage2.best_iteration}')\n\n\nFinal Model ==========================\ntest score = 0.21621863543987274\nparameters ---------------------------\nobjective : reg:squarederror\neval_metric : rmse\ntree_method : approx\nmax_depth : 10\nmin_child_weight : 6\nsubsample : 0.9729188669457949\ncolsample_bynode : 0.8491983767203796\nreg_lambda : 0.008587261143813469\nlearning_rate : 0.01\nnum_boost_round: 1446\n\n\nBack in the regression post we got an RMSE of about 0.231 just using default parameter values, which put us in 5th place on the leaderboard for the Kagle dozers competition. Now with about 10 minutes of hyperparameter tuning, our RMSE is down to 0.216 which puts us in 1st place by a huge margin. üôå"
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#what-could-possibly-go-wrong",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#what-could-possibly-go-wrong",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "What could possibly go wrong?",
    "text": "What could possibly go wrong?\nHyperparameter tuning can easily be overlooked in the move-fast-and-break-everything hustle of building an ML product, but it can also easily become overkill or even downright harmful, depending on the application. There are three key questions to ask:\n\nHow much value is created by an incremental gain in model prediction accuracy?\nWhat is the cost of increasing model prediction accuracy?\nIs my model answering the right question?\n\nSometimes a small gain in model prediction performance translates into millions of dollars of impact. The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org‚Äôs KPIs, and get mad respect, bonuses, and promoted. But the reality is that often additional model accuracy doesn‚Äôt really change business KPIs by very much. Try to figure out the actual value of improved model accuracy and proceed accordingly.\nRemember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself. It can also lead us to larger and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.\nWorst of all and quite counterintuitively, it‚Äôs possible that improving a model‚Äôs prediction accuracy can compromise overall business KPIs. I‚Äôve seen this with my own eyes at work; offline testing shows that hyperparameter tuning significantly improves a model‚Äôs prediction accuracy, but when the model goes into production, an AB test shows that the business KPIs are actually worse. What happened? In this case, the model‚Äôs prediction was being used indirectly to infer the relationship between one of the features and the prediction target to inform automatic business decisions. Answering questions about how changing an input will affect an output requires causal reasoning, and traditional ML models are not the right tool for the job. I‚Äôll have a lot more to say about that soon; let this story foreshadow an epic new epoch on Random Realizations‚Ä¶."
  },
  {
    "objectID": "posts/xgboost-parameter-tuning-with-optuna/index.html#wrapping-up",
    "href": "posts/xgboost-parameter-tuning-with-optuna/index.html#wrapping-up",
    "title": "The Ultimate Guide to XGBoost Parameter Tuning",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna. If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!"
  },
  {
    "objectID": "posts/hello-pyspark/index.html",
    "href": "posts/hello-pyspark/index.html",
    "title": "Hello PySpark!",
    "section": "",
    "text": "A big day at Playa Guiones\nWell, you guessed it: it‚Äôs time for us to learn PySpark!\nI know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes?\nThat‚Äôs a totally fair question.\nSo what happens when we‚Äôre working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory? We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details.\nEnter PySpark.\nI think it‚Äôs fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it‚Äôs like pandas but scalable. It‚Äôs built on top of Apache Spark, a unified analytics engine for large-scale data processing. PySpark is essentially a way to access the functionality of spark via python code. While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice. PySpark also has great integration with SQL, and it has a companion machine learning library called MLlib that‚Äôs more or less a scalable scikit-learn (maybe we can cover it in a future post).\nSo, here‚Äôs the plan. First we‚Äôre going to get set up to run PySpark locally in a jupyter notebook on our laptop. This is my preferred environment for interactively playing with PySpark and learning the ropes. Then we‚Äôre going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas. Once we‚Äôre comfortable running PySpark on the laptop, it‚Äôs going to be much easier to jump onto a distributed cluster and run PySpark at scale.\nLet‚Äôs do this."
  },
  {
    "objectID": "posts/hello-pyspark/index.html#how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop",
    "href": "posts/hello-pyspark/index.html#how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop",
    "title": "Hello PySpark!",
    "section": "How to Run PySpark in a Jupyter Notebook on Your Laptop",
    "text": "How to Run PySpark in a Jupyter Notebook on Your Laptop\nOk, I‚Äôm going to walk us through how to get things installed on a Mac or Linux machine where we‚Äôre using homebrew and conda to manage virtual environments. If you have a different setup, your favorite search engine will help you get PySpark set up locally.\n\n\n\n\n\n\nNote\n\n\n\nIt‚Äôs possible for Homebrew and Anaconda to interfere with one another. The simple rule of thumb is that whenever you want to use the brew command, first deactivate your conda environment by running conda deactivate. See this Stack Overflow question for more details.\n\n\n\nInstall Spark\nInstall Spark with homebrew.\nbrew install apache-spark\nNext we need to set up a SPARK_HOME environment variable in the shell. Check where Spark is installed.\nbrew info apache-spark\nYou should see something like\n==&gt; apache-spark: stable 3.3.2 (bottled), HEAD\nEngine for large-scale data processing\nhttps://spark.apache.org/\n/opt/homebrew/Cellar/apache-spark/3.3.2 (1,453 files, 320.9MB) *\n...\nSet the SPARK_HOME environment variable to your spark installation path with /libexec appended to the end. To do this I added the following line to my .zshrc file.\nexport SPARK_HOME=/opt/homebrew/Cellar/apache-spark/3.3.2/libexec\nRestart your shell, and test the installation by starting the Spark shell.\nspark-shell\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n      /_/\n         \nUsing Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 19.0.2)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt; \nIf you get the scala&gt; prompt, then you‚Äôve successfully installed Spark on your laptop!\n\n\nInstall PySpark\nUse conda to install the PySpark python package. As usual, it‚Äôs advisable to do this in a new virtual environment.\n$ conda install pyspark\nYou should be able to launch an interactive PySpark REPL by saying pyspark.\n$ pyspark\n...\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n      /_/\n\nUsing Python version 3.8.3 (default, Jul  2 2020 11:26:31)\nSpark context Web UI available at http://192.168.100.47:4041\nSpark context available as 'sc' (master = local[*], app id = local-1624127229929).\nSparkSession available as 'spark'.\n&gt;&gt;&gt; \nThis time we get a familiar python &gt;&gt;&gt; prompt. This is an interactive shell where we can easily experiment with PySpark. Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we‚Äôll get set up to run PySpark in a jupyter notebook.\n\n\n\n\n\n\nNote\n\n\n\nWhen I tried following this setup on a new Mac, I hit an error about being unable to find the Java Runtime. This stack overflow question lead me to the fix.\n\n\n\n\nThe Spark Session Object\nYou may have noticed that when we launched that PySpark interactive shell, it told us that something called SparkSession was available as 'spark'. So basically, what‚Äôs happening here is that when we launch the pyspark shell, it instantiates an object called spark which is an instance of class pyspark.sql.session.SparkSession. The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we‚Äôre going to be saying things like spark.this() and spark.that() to make stuff happen.\nThe PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically. However, when we‚Äôre using another interface to PySpark (like say a jupyter notebook running a python kernal), we‚Äôll have to make a spark session object for ourselves.\n\n\nCreate a PySpark Session in a Jupyter Notebook\nThere are a few ways to run PySpark in jupyter which you can read about here.\nFor derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a jupyter notebook running on a regular python kernel. The method we‚Äôll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session. So, first install the findspark package.\nconda install -c conda-forge findspark\nLaunch jupyter as usual.\njupyter notebook\nGo ahead and fire up a new notebook using a regular python 3 kernal. Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated. You can think of this as boilerplate code that we need to run in the first cell of a notebook where we‚Äôre going to use PySpark.\n\nimport pyspark\nimport findspark\nfrom pyspark.sql import SparkSession\n\nfindspark.init()\nspark = SparkSession.builder.appName('My Spark App').getOrCreate()\n\nFirst we‚Äôre running findspark‚Äôs init() method to find our Spark installation. If you run into errors here, make sure you got the SPARK_HOME environment variable correctly set in the install instructions above. Then we instantiate a spark session as spark. Once you run this, you‚Äôre ready to rock and roll with PySpark in your jupyter notebook.\n\n\n\n\n\n\nNote\n\n\n\nSpark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at http://localhost:4040/jobs/."
  },
  {
    "objectID": "posts/hello-pyspark/index.html#pyspark-concepts",
    "href": "posts/hello-pyspark/index.html#pyspark-concepts",
    "title": "Hello PySpark!",
    "section": "PySpark Concepts",
    "text": "PySpark Concepts\nPySpark provides two main abstractions for data: the RDD and the dataframe. RDD‚Äôs are just a distributed list of objects; we won‚Äôt go into details about them in this post. For us, the key object in PySpark is the dataframe.\nWhile PySpark dataframes expose much of the functionality you would expect from a library for tabular data manipulation, they behave a little differently from pandas dataframes, both syntactically and under-the-hood. There are a couple of key concepts that will help explain these idiosyncracies.\nImmutability - Pyspark RDD‚Äôs and dataframes are immutable. This means that if you change an object, e.g.¬†by adding a column to a dataframe, PySpark returns a reference to a new dataframe; it does not modify the existing dataframe. This is kind of nice, because we don‚Äôt have to worry about that whole view versus copy nonsense that happens in pandas.\nLazy Evaluation - Lazy evaluation means that when we start manipulating a dataframe, PySpark won‚Äôt actually perform any of the computations until we explicitly ask for the result. This is nice because it potentially allows PySpark to do fancy optimizations before executing a sequence of operations. It‚Äôs also confusing at first, because PySpark will seem to blaze through complex operations and then take forever to print a few rows of the dataframe."
  },
  {
    "objectID": "posts/hello-pyspark/index.html#pyspark-dataframe-essentials",
    "href": "posts/hello-pyspark/index.html#pyspark-dataframe-essentials",
    "title": "Hello PySpark!",
    "section": "PySpark Dataframe Essentials",
    "text": "PySpark Dataframe Essentials\n\nCreating a PySpark dataframe with createDataFrame()\nThe first thing we‚Äôll need is a way to make dataframes. createDataFrame() allows us to create PySpark dataframes from python objects like nested lists or pandas dataframes. Notice that createDataFrame() is a method of the spark session class, so we‚Äôll call it from our spark session sparkby saying spark.createDataFrame().\n\n# create pyspark dataframe from nested  lists\nmy_df = spark.createDataFrame(\n    data=[\n        [2022, \"tiger\"],\n        [2023, \"rabbit\"],\n        [2024, \"dragon\"]\n    ],\n    schema=['year', 'animal']\n)\n\nLet‚Äôs read the seaborn tips dataset into a pandas dataframe and then use it to create a PySpark dataframe.\n\nimport pandas as pd\n\n# load tips dataset into a pandas dataframe\npandas_df = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\n\n# create pyspark dataframe from a pandas dataframe\npyspark_df = spark.createDataFrame(pandas_df)\n\n\n\n\n\n\n\nNote\n\n\n\nIn real life when we‚Äôre running PySpark on a large-scale distributed system, we would not generally want to use python lists or pandas dataframes to load data into PySpark. Ideally we would want to read data directly from where it is stored on HDFS, e.g.¬†by reading parquet files, or by querying directly from a hive database using spark sql.\n\n\n\n\nPeeking at a dataframe‚Äôs contents\nThe default print method for the PySpark dataframe will just give you the schema.\n\npyspark_df\n\nDataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]\n\n\nIf we want to peek at some of the data, we‚Äôll need to use the show() method, which is analogous to the pandas head(). Remember that show() will cause PySpark to execute any operations that it‚Äôs been lazily waiting to evaluate, so sometimes it can take a while to run.\n\n# show the first few rows of the dataframe\npyspark_df.show(5)\n\n+----------+----+------+------+---+------+----+\n|total_bill| tip|   sex|smoker|day|  time|size|\n+----------+----+------+------+---+------+----+\n|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n+----------+----+------+------+---+------+----+\nonly showing top 5 rows\n\n\n\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]\n\n                                                                                \n\n\nWe thus encounter our first rude awakening. PySpark‚Äôs default representation of dataframes in the notebook isn‚Äôt as pretty as that of pandas. But no one ever said it would be pretty, they just said it would be scalable.\nYou can also use the printSchema() method for a nice vertical representation of the schema.\n\n# show the dataframe schema\npyspark_df.printSchema()\n\nroot\n |-- total_bill: double (nullable = true)\n |-- tip: double (nullable = true)\n |-- sex: string (nullable = true)\n |-- smoker: string (nullable = true)\n |-- day: string (nullable = true)\n |-- time: string (nullable = true)\n |-- size: long (nullable = true)\n\n\n\n\n\nSelect columns by name\nYou can select specific columns from a dataframe using the select() method. You can pass either a list of names, or pass names as arguments.\n\n# select some of the columns\npyspark_df.select('total_bill', 'tip')\n\n# select columns in a list\npyspark_df.select(['day', 'time', 'total_bill'])\n\n\n\nFilter rows based on column values\nAnalogous to the WHERE clause in SQL, and the query() method in pandas, PySpark provides a filter() method which returns only the rows that meet the specified conditions. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even do a SQL-like in to check if the column value matches any items in a list.\n\n## compare a column to a value\npyspark_df.filter('total_bill &gt; 20')\n\n# compare two columns with arithmetic\npyspark_df.filter('tip &gt; 0.15 * total_bill')\n\n# check equality with a string value\npyspark_df.filter('sex == \"Male\"')\n\n# check equality with any of several possible values\npyspark_df.filter('day in (\"Sat\", \"Sun\")')\n\n# use \"and\" \npyspark_df.filter('day == \"Fri\" and time == \"Lunch\"')\n\nIf you‚Äôre into boolean indexing with the brackets, PySpark does support that too, but I encourage you to use filter() instead. Check out my rant about why you shouldn‚Äôt use boolean indexing for the details. The TLDR is that filter() requires less typing, makes your code more readable and portable, and it allows you to chain method calls together using dot chains.\nHere‚Äôs the boolean indexing equivalent of the last example from above.\n\n# using boolean indexing\npyspark_df[(pyspark_df.day == 'Fri') & (pyspark_df.time == 'Lunch')]\n\nI know, it looks horrendous, but not as horrendous as the error message you‚Äôll get if you forget the parentheses.\n\n\nAdd new columns to a dataframe\nYou can add new columns which are functions of the existing columns with the withColumn() method.\n\nimport pyspark.sql.functions as f\n\n# add a new column using col() to reference other columns\npyspark_df.withColumn('tip_percent', f.col('tip') / f.col('total_bill'))\n\nNotice that we‚Äôve imported the pyspark.sql.functions module. This module contains lots of useful functions that we‚Äôll be using all over the place, so it‚Äôs probably a good idea to go ahead and import it whenever you‚Äôre using PySpark. BTW, it seems like folks usually import this module as f or F. In this example we‚Äôre using the col() function, which allows us to refer to columns in our dataframe using string representations of the column names.\nYou could also achieve the same result using the dot to reference the other columns, but this requires us to type the dataframe name over and over again, which makes it harder to reuse this code on different dataframes or in dot chains.\n\n# add a new column using the dot to reference other columns (less recommended)\npyspark_df.withColumn('tip_percent', pyspark_df.tip / pyspark_df.total_bill)\n\nIf you want to apply numerical transformations like exponents or logs, use the built-in functions in the pyspark.sql.functions module.\n\n# log \npyspark_df.withColumn('log_bill', f.log(f.col('total_bill')))\n\n# exponent\npyspark_df.withColumn('bill_squared', f.pow(f.col('total_bill'), 2))\n\nYou can implement conditional assignment like SQL‚Äôs CASE WHEN construct using the when() function and the otherwise() method.\n\n# conditional assignment (like CASE WHEN)\npyspark_df.withColumn('is_male', f.when(f.col('sex') == 'Male', True).otherwise(False))\n\n# using multiple when conditions and values\npyspark_df.withColumn('bill_size', \n    f.when(f.col('total_bill') &lt; 10, 'small')\n    .when(f.col('total_bill') &lt; 20, 'medium')\n    .otherwise('large')\n)\n\nRemember that since PySpark dataframes are immutable, calling withColumns() on a dataframe returns a new dataframe. If you want to persist the result, you‚Äôll need to make an assignment.\npyspark_df = pyspark_df.withColumns(...)\n\n\nGroup by and aggregate\nPySpark provides a groupBy() method similar to the pandas groupby(). Just like in pandas, we can call methods like count() and mean() on our grouped dataframe, and we also have a more flexible agg() method that allows us to specify column-aggregation mappings.\n\n\n# group by and count\npyspark_df.groupBy('time').count().show()\n\n+------+-----+\n|  time|count|\n+------+-----+\n|Dinner|  176|\n| Lunch|   68|\n+------+-----+\n\n\n\n\n\n# group by and specify column-aggregation mappings with agg()\npyspark_df.groupBy('time').agg({'total_bill': 'mean', 'tip': 'max'}).show()\n\n+------+--------+------------------+\n|  time|max(tip)|   avg(total_bill)|\n+------+--------+------------------+\n|Dinner|    10.0| 20.79715909090909|\n| Lunch|     6.7|17.168676470588235|\n+------+--------+------------------+\n\n\n\nIf you want to get fancier with your aggregations, it might just be easier to express them using hive syntax. Read on to find out how.\n\n\nRun Hive SQL on dataframes\nOne of the mind-blowing features of PySpark is that it allows you to write hive SQL queries on your dataframes. To take a PySpark dataframe into the SQL world, use the createOrReplaceTempView() method. This method takes one string argument which will be the dataframes name in the SQL world. Then you can use spark.sql() to run a query. The result is returned as a PySpark dataframe.\n\n\n# put pyspark dataframe in SQL world and query it\npyspark_df.createOrReplaceTempView('tips')\nspark.sql('select * from tips').show(5)\n\n+----------+----+------+------+---+------+----+\n|total_bill| tip|   sex|smoker|day|  time|size|\n+----------+----+------+------+---+------+----+\n|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n+----------+----+------+------+---+------+----+\nonly showing top 5 rows\n\n\n\nThis is awesome for a couple of reasons. First, it allows us to easily express any transformations in hive syntax. If you‚Äôre like me and you‚Äôve already been using hive, this will dramatically reduce the PySpark learning curve, because when in doubt, you can always bump a dataframe into the SQL world and simply use hive to do what you need. Second, if you have a hive deployment, PySpark‚Äôs SQL world also has access to all of your hive tables. This means you can write queries involving both hive tables and your PySpark dataframes. It also means you can run hive commands, like inserting into a table, directly from PySpark.\nLet‚Äôs do some aggregations that might be a little trickier to do using the PySpark built-in functions.\n\n\n# run hive query and save result to dataframe\ntip_stats_by_time = spark.sql(\"\"\"\n    select\n        time\n        , count(*) as n \n        , avg(tip) as avg_tip\n        , percentile_approx(tip, 0.5) as med_tip\n        , avg(case when tip &gt; 3 then 1 else 0 end) as pct_tip_gt_3\n    from \n        tips\n    group by 1\n\"\"\")\n\ntip_stats_by_time.show()\n\n+------+---+------------------+-------+-------------------+\n|  time|  n|           avg_tip|med_tip|       pct_tip_gt_3|\n+------+---+------------------+-------+-------------------+\n|Dinner|176| 3.102670454545455|    3.0|0.44886363636363635|\n| Lunch| 68|2.7280882352941176|    2.2|0.27941176470588236|\n+------+---+------------------+-------+-------------------+"
  },
  {
    "objectID": "posts/hello-pyspark/index.html#visualization-with-pyspark",
    "href": "posts/hello-pyspark/index.html#visualization-with-pyspark",
    "title": "Hello PySpark!",
    "section": "Visualization with PySpark",
    "text": "Visualization with PySpark\nThere aren‚Äôt any tools for visualization included in PySpark. But that‚Äôs no problem, because we can just use the toPandas() method on a PySpark dataframe to pull data back into pandas. Once we have a pandas dataframe, we can happily build visualizations as usual. Of course, if your PySpark dataframe is huge, you wouldn‚Äôt want to use toPandas() directly, because PySpark will attempt to read the entire contents of its huge dataframe into memory. Instead, it‚Äôs best to use PySpark to generate aggregations of your data for plotting or to pull only a sample of your full data into pandas.\n\n# read aggregated pyspark dataframe into pandas for plotting\nplot_pdf = tip_stats_by_time.toPandas()\nplot_pdf.plot.bar(x='time', y=['avg_tip', 'med_tip']);"
  },
  {
    "objectID": "posts/hello-pyspark/index.html#wrapping-up",
    "href": "posts/hello-pyspark/index.html#wrapping-up",
    "title": "Hello PySpark!",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nSo that‚Äôs a wrap on our crash course in working with PySpark. You now have a good idea of what pyspark is and how to get started manipulating dataframes with it. Stay tuned for a future post on PySpark‚Äôs companion ML library MLlib. In the meantime, may no dataframe be too large for you ever again."
  },
  {
    "objectID": "posts/shap-from-scratch/index.html",
    "href": "posts/shap-from-scratch/index.html",
    "title": "SHAP from Scratch",
    "section": "",
    "text": "Ahh, SHAP. As you know it‚Äôs become one of the leading frameworks for explaining ML model predictions. I‚Äôd guess it‚Äôs popularity is due to its appealing theoretical basis, its universal applicability to any type of ML model, and its easy-to-use python package. SHAP promises to turn your black box ML model into a nice friendly interpretable model. The hilarious irony is that, when I first started using it in my work, SHAP itself was a complete black box to me. In this post, we‚Äôll change all that by diving into the SHAP paper, illuminating the key theoretical ideas behind its development step by step, and implementing it from scratch in python. If you aren‚Äôt already familiar with how to compute and interpret SHAP values in practice, I‚Äôd recommend that you go check out the documentation for the shap python package before diving into this post."
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#what-is-shap",
    "href": "posts/shap-from-scratch/index.html#what-is-shap",
    "title": "SHAP from Scratch",
    "section": "What is SHAP?",
    "text": "What is SHAP?\nSHAP (SHapley Additive exPlanations) is a conceptual framework for creating explanations of ML model predictions. The term also refers to a set of computational methods for generating these explanations and a python library which implements them. The ‚ÄúSHAP‚Äù backronym was introduced in Lundberg and Lee 2017, which I call the SHAP paper, that expanded on several previously existing ideas which we‚Äôll build up in the following sections. The key concepts are:\n\nShapley values, a concept from cooperative game theory which originally had nothing to do with machine learning\nShapley regression values, which showed how to use Shapley values to generate explanations of model predictions\nShapley sampling values, which offered a computationally tractable way to compute Shapley regression values for any type of model.\n\nThe SHAP paper tied Shapley regression values and several other existing model explanation methods together by showing they are all members of a class called ‚Äúadditive feature attribution methods.‚Äù Under the right conditions, these additive feature attribution methods can generate Shapley values, and when they do we can call them SHAP values.\nAfter establishing this theoretical framework, the authors go on to discuss various computational methods for computing SHAP values; some are model-agnostic, meaning they work with any type of model, and others are model-specific, meaning they work for specific types of models. It turns out that the previously existing Shapley sampling values method is a model-agnostic approach, but while it‚Äôs the most intuitive, computationally speaking it‚Äôs relatively inefficient. Thus the authors propose a novel model-agnostic approach called Kernel SHAP, which is really just LIME parameterized to yield SHAP values.\nModel-specific approaches can be potentially much more efficient than model-agnostic ones by taking advantage of model idiosyncrasies. For example, there is an analytical solution for the SHAP values of linear models, so Linear SHAP is extremely efficient. Similarly, Deep SHAP (proposed in the SHAP paper) and Tree SHAP (proposed later in Lundberg et al 2020) take advantage of idiosyncrasies of deep learning and tree-based models to compute SHAP values efficiently.\nThe important thing about these different methods is that they provide computationally tractable ways to compute SHAP values, but ultimately, they are all based on the Shapley sampling values method‚Äîthe original method to compute what we now call SHAP values. Thus, for the remainder of this post, we‚Äôll focus on this method, building it up from Shapley values to Shapley regression values to Shapley sampling values and ultimately implementing it from scratch in python."
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#shapley-values",
    "href": "posts/shap-from-scratch/index.html#shapley-values",
    "title": "SHAP from Scratch",
    "section": "Shapley Values",
    "text": "Shapley Values\nThe Shapley value is named in honor of Nobel prize winning economist Loyd Shapley who introduced the idea in the field of coalitional game theory in the 1950‚Äôs. Shapley proposed a way to determine how a coalition of players can fairly share the payout they receive from a cooperative game. We‚Äôll introduce the mathematical formalism in the next section, so for now let‚Äôs just touch on the intuition for the approach. Essentially, the method distributes the payout among the players according to the expected contribution of each player across all possible combinations of the players. The thought experiment works as follows:\n\nDraw a random permutation (ordering) of the players.\nHave the first player play alone, generating some payout. Then have the first two players play together, generating some payout. Then the first three, and so on.\nAs each new player is added, attribute the change in the payout to this new player.\nRepeat this experiment for all permutations of the players. A player‚Äôs Shapley value is the average change in payout (across all permutations) when that player is added to the game.\n\nNext we‚Äôll see how this idea can be applied to model explanations."
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#shapley-regression-values",
    "href": "posts/shap-from-scratch/index.html#shapley-regression-values",
    "title": "SHAP from Scratch",
    "section": "Shapley Regression Values",
    "text": "Shapley Regression Values\nThe next idea came from Lipovetsky and Conklin 2001, who proposed a way to use Shapley values to explain the predictions of a linear regression model. Shapley regression values assign an importance value to each feature that represents the effect on the model prediction of including that feature. The basic idea is to train a second model without the feature of interest, and then to compare the predictions from the model with the feature and the model without the feature. This procedure of training two models and comparing their predictions is repeated for all possible subsets of the other features; the average difference in predictions is the Shapley value for the feature of interest.\nThe Shapley value for feature \\(i\\) on instance \\(x\\) is given by equation 4 in the SHAP paper:\n\\[\n\\phi_i = \\sum_{S \\subseteq F \\setminus \\{i\\}}\n\\frac{|S|!(|F| - |S| - 1)!}{|F|!}\n[f_{S \\cup \\{i\\}}(x_{S \\cup \\{i\\}}) - f_S(x_S) ]\n\\]\nwhere\n\n\\(\\phi_i\\) is the Shapley value for feature of interest \\(i\\),\nthe \\(\\subseteq\\) symbol indicates the item on its left is a subset of the object on its right,\n\\(F\\) is the set of all features,\nthe vertical bars indicate the number of elements in a set, e.g.¬†\\(|F|\\) is the total number of features,\n\\(F \\setminus \\{i\\}\\) is the set of all features except the feature of interest,\n\\(S\\) is a particular subset of features not including the feature of interest,\n\\(f_{S}\\) is a ‚Äúsubset model‚Äù‚Äîa model that uses only the features in \\(S\\) for both training and prediction,\nand \\(f_{S \\cup \\{i\\}}\\) is asubset model using features in \\(S\\) and the feature of interest.\n\nTo reiterate, this is the most important equation when it comes to understanding SHAP, as it defines the Shapley value; let‚Äôs make sure we understand what‚Äôs going on by implementing it in python.\nWe start with the feature subsets. Notice that the sum is indexed over all subsets of \\(F \\setminus \\{i\\}\\), which is the set of all features except the \\(i\\)th feature, the one we‚Äôre calculating the Shapley value for. Let‚Äôs write a function that takes a list of items and returns an iterable that yields all possible subsets of those items.\n\nfrom itertools import chain, combinations \n\ndef get_all_subsets(items):\n    return chain.from_iterable(combinations(items, r) for r in range(len(items)+1))\n\nfor s in  get_all_subsets([0, 1, 2]):\n    print(s)\n\n()\n(0,)\n(1,)\n(2,)\n(0, 1)\n(0, 2)\n(1, 2)\n(0, 1, 2)\n\n\nTo get all subsets of features, other than the feature of interest, we could do something like this.\n\ndef get_all_other_feature_subsets(n_features, feature_of_interest):\n    all_other_features = [j for j in range(n_features) if j != feature_of_interest]\n    return get_all_subsets(all_other_features)\n\nfor s in get_all_other_feature_subsets(n_features=4, feature_of_interest=2):\n    print(s)\n\n()\n(0,)\n(1,)\n(3,)\n(0, 1)\n(0, 3)\n(1, 3)\n(0, 1, 3)\n\n\nSo for each of the feature subsets, we‚Äôll need to calculate the summand, which is the product of a quotient with a bunch of factorials and the difference in predicted values between two subset models. Let‚Äôs start with those subset models. Subset model \\(f_{S}\\) is a model trained only on the features in subset \\(S\\). We can write a function that takes an untrained model, a training dataset, a feature subset to use, and a single instance to predict on; the function will then train a model using only features in the subset, and it will issue a prediction for the single instance we gave it.\n\ndef subset_model(model, X_train, y_train, feature_subset, instance):\n    assert len(instance.shape) == 1, 'Instance must be a 1D array'\n    if len(feature_subset) == 0:\n        return y.mean() # a model with no features predicts E[y]\n    X_subset = X_train.take(feature_subset, axis=1)\n    model.fit(X_subset, y_train)\n    return model.predict(instance.take(feature_subset).reshape(1, -1))[0]\n\nNext let‚Äôs have a look at \\(|S|!(|F|-|S|-1)!/|F|!\\). The keen reader will notice this factor kind of looks like the answers to those combinatorics questions like how many unique ways can you order the letters in the word MISSISSIPPI. The combinatorics connection is that Shapley values are defined in terms of all permutations of the players , where the included players come first, then the player of interest, followed by the excluded players. In ML models, the order of features doesn‚Äôt matter, so we can work with unordered subsets of features, scaling the prediction difference terms by the number of permutations that involve the same sets of included and excluded features. With that in mind, we can see that including the factor in each term of the sum gives us a weighted average over all feature combinations, where the numerator gives the number of permutations in which the included features come first, followed by the feature of interest, followed by the excluded features, and the denominator is the total number of feature permutations.\n\nfrom math import factorial\n\ndef permutation_factor(n_features, n_subset):\n    return factorial(n_subset) * factorial(n_features - n_subset - 1) / factorial(n_features)\n\nNow we can put these pieces together to compute equation 4‚Äîa single Shapley regression value for a single instance and feature of interest.\n\ndef compute_single_shap_value(untrained_model,\n                              X_train,\n                              y_train,\n                              feature_of_interest,\n                              instance):\n    \"Compute a single SHAP value (equation 4)\"\n    n_features = X_train.shape[1]\n    shap_value = 0\n    for subset in get_all_other_feature_subsets(n_features, feature_of_interest):\n        n_subset = len(subset)\n        prediction_without_feature = subset_model(\n            untrained_model,\n            X_train, y_train,\n            subset,\n            instance\n        )\n        prediction_with_feature = subset_model(\n            untrained_model,\n            X_train, y_train,\n            subset + (feature_of_interest,),\n            instance\n        )\n        factor = permutation_factor(n_features, n_subset)\n        shap_value += factor * (prediction_with_feature - prediction_without_feature)\n    return shap_value\n\nLet‚Äôs use this function to compute a single Shapley regression value for a linear model and a small training dataset with 3 features.\n\nfrom sklearn.datasets import make_regression \nfrom sklearn.linear_model import LinearRegression \n\nX, y = make_regression(n_samples=50, n_features=3)\n\ncompute_single_shap_value(untrained_model=LinearRegression(),\n                          X_train=X, y_train=y,\n                          feature_of_interest=2,\n                          instance=X[0, :])\n\n-0.07477140629329351\n\n\nThat gives us a single Shapley value corresponding to a single feature value in a single instance. To get useful model explanations, we‚Äôd need to compute Shapley values for each feature of each instance in some dataset of instances. You might notice there‚Äôs a big problem with the formulation above. Namely, we are going to have to train a whole bunch of new subset models‚Äîone for each subset of the features. If our model has \\(M\\) features, we‚Äôll have to train \\(2^M\\) models, so this will get impractical in a hurry, especially if we‚Äôre trying to train anything other than linear models."
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#shapley-sampling-values",
    "href": "posts/shap-from-scratch/index.html#shapley-sampling-values",
    "title": "SHAP from Scratch",
    "section": "Shapley Sampling Values",
    "text": "Shapley Sampling Values\nNext, ≈†trumbelj and Kononenko 2014 proposed Shapley sampling values, a method which provides a much more efficient way to approximate the subset models used to calculate Shapley regression values. In this approach, the effect of removing some features from the model is approximated by the conditional expectation of the model given the known features.\n\\[ f_S(x_S)  := E[f(x) | x_S]  \\]\nThis means we‚Äôre approximating the output of a subset model by averaging over outputs of the full model. That‚Äôs great because now we don‚Äôt have to train all those new subset models, we can just query our full model over some set of inputs and average over the outputs to compute these conditional expectation subset models.\nNow how exactly do we compute that conditional expectation? First we rewrite the above conditional expectation (equation 10 in the SHAP paper)\n\\[ E[f(x) | x_S]  = E_{x_{\\bar{S}}|x_S} [f(x)]\\]\nwhere \\(\\bar{S}\\) is the set of excluded or missing features. Beside this equation in the paper they give the note ‚Äúexpectation over \\(x_{\\bar{S}} | x_S\\), which means we‚Äôre taking the expectation over the missing features given the known features. Then we get another step (equation 11)\n\\[E_{x_{\\bar{S}}|x_S} [f(x)] \\approx E_{x_{\\bar{S}}} [f(x)]\\]\nNow it‚Äôs not an equality but an approximation. The authors give the note ‚Äúassume feature independence‚Äù. The intuition here is that if the missing features are correlated with the known features, then their distribution depends on the particular values taken by the known features. But here the authors make the simplifying assumption that known and missing features are independent, which allows us to replace the conditional expectation with an unconditional expectation over the missing features.\n\n\n\n\n\n\nNote\n\n\n\nSo is this assumption that features in \\(S\\) are independent from features in \\(\\bar{S}\\) a problem? The short answer is‚Ä¶ maybe ü§∑‚Äç‚ôÄÔ∏è? It‚Äôs potentially problematic enough that people have worked out some ways to relax this assumption, e.g.¬†partition masking, but that makes Owen values instead of Shapley values, so we‚Äôll save it for another post.\n\n\nAnyway, how do we compute this unconditional expectation over the missing features in practice? We‚Äôll need to use a so-called background dataset, which is just some set of observations of our feature variables that represents their distribution. A good candidate is the training data we used to train our model. ≈†trumbelj and Kononenko 2014 propose a way to estimate this conditional expectation using resampling of the background dataset.\nThe idea is to notice that the instance of interest \\(x\\) is a feature vector comprised of the set of ‚Äúknown‚Äù features \\(x_S\\) and the set of excluded features \\(x_{\\bar{S}}\\) such that \\(x=\\{x_S,x_{\\bar{S}} \\}\\). Our resampling scheme will be based on constructing ‚Äúmasked‚Äù samples \\(x^*=\\{x_S,z_{\\bar{S}} \\}\\) where \\(z_{\\bar{S}}\\) are values of the missing features drawn from some random observation in the background dataset. We can then compute an estimate \\(\\hat{f}_S(x)\\) of the conditional expectation \\(E_{x_{\\bar{S}}}[f(x)]\\) as\n\\[\\hat{f}_S(x) = \\frac{1}{n} \\sum_{k=1}^n f(\\{x_S, z_{\\bar{S}}^{(k)} \\}) \\]\nwhere \\(z_{\\bar{S}}^{(k)}\\) is the vector of values of the excluded features from the \\(k\\)-th row of the background dataset. Algorithmically, we can view this as first drawing a sample of observations from the background dataset, second ‚Äúmasking‚Äù features in \\(S\\) in the sampled background dataset by replacing the observed values \\(z_S\\) on each row with the values in the instance \\(x_S\\), third using the full model \\(f\\) to predict on each of these masked samples in the background dataset, and finally averaging over these predictions. We can implement a new subset model function that takes a fully trained model, a background dataset,a feature subset, and an instance for explanation and returns an approximation of the subset model prediction.\n\nimport numpy as np\n\ndef subset_model_approximation(trained_model, \n                               background_dataset,\n                               feature_subset,  \n                               instance):\n    \"\"\" \n    Approximate subset model prediction  (Equation 11)\n    \\hat{f}_S(x) = E_{x_{\\hat{S}}}[f_S(x)]\n    for feature subset S on single instance x\n    \"\"\"\n    masked_background_dataset = background_dataset.copy()\n    for j in range(masked_background_dataset.shape[1]):\n        if j in feature_subset:\n            masked_background_dataset[:, j] = instance[j]\n    conditional_expectation_of_model = np.mean(\n        trained_model.predict(masked_background_dataset)\n    )\n    return conditional_expectation_of_model          \n\nIf we replace our subset_model function with this new subset_model_approximation function in our compute_single_shap_value function from earlier, then we‚Äôll be computing Shapley sampling values. And according to the SHAP paper: ‚Äúif we assume feature independence when approximating conditional expectations (using Equation 11 to estimate subset model output) ‚Ä¶ then SHAP values can be estimated directly using the Shapley sampling values method.‚Äù That means we‚Äôll be computing SHAP values!"
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#how-to-implement-shap-from-scratch",
    "href": "posts/shap-from-scratch/index.html#how-to-implement-shap-from-scratch",
    "title": "SHAP from Scratch",
    "section": "How to Implement SHAP from Scratch",
    "text": "How to Implement SHAP from Scratch\nLet‚Äôs put the pieces together and implement a class for a model explainer that computes SHAP values via the Shapley sampling values method. We‚Äôll talk through a couple of points after the code.\n\nimport numpy as np \nfrom typing import Any, Callable, Iterable\nfrom math import factorial\nfrom itertools import chain, combinations\n\nclass ShapFromScratchExplainer():\n    def __init__(self,\n                 model: Callable[[np.ndarray], float], \n                 background_dataset: np.ndarray,\n                 max_samples: int = None):\n        self.model = model\n        if max_samples:\n            max_samples = min(max_samples, background_dataset.shape[0]) \n            rng = np.random.default_rng()\n            self.background_dataset = rng.choice(background_dataset, \n                                                 size=max_samples, \n                                                 replace=False, axis=0)\n        else:\n            self.background_dataset = background_dataset\n\n    def shap_values(self, X: np.ndarray) -&gt; np.ndarray:\n        \"SHAP Values for instances in DataFrame or 2D array\"\n        shap_values = np.empty(X.shape)\n        for i in range(X.shape[0]):\n            for j in range(X.shape[1]):\n                shap_values[i, j] = self._compute_single_shap_value(j, X[i, :])\n        return shap_values\n       \n    def _compute_single_shap_value(self, \n                                   feature: int,\n                                   instance: np.array) -&gt; float:\n        \"Compute a single SHAP value (equation 4)\"\n        n_features = len(instance)\n        shap_value = 0\n        for subset in self._get_all_other_feature_subsets(n_features, feature):\n            n_subset = len(subset)\n            prediction_without_feature = self._subset_model_approximation(\n                subset, \n                instance\n            )\n            prediction_with_feature = self._subset_model_approximation(\n                subset + (feature,), \n                instance\n            )\n            factor = self._permutation_factor(n_features, n_subset)\n            shap_value += factor * (prediction_with_feature - prediction_without_feature)\n        return shap_value\n    \n    def _get_all_subsets(self, items: list) -&gt; Iterable:\n        return chain.from_iterable(combinations(items, r) for r in range(len(items)+1))\n    \n    def _get_all_other_feature_subsets(self, n_features, feature_of_interest):\n        all_other_features = [j for j in range(n_features) if j != feature_of_interest]\n        return self._get_all_subsets(all_other_features)\n\n    def _permutation_factor(self, n_features, n_subset):\n        return (\n            factorial(n_subset) \n            * factorial(n_features - n_subset - 1) \n            / factorial(n_features) \n        )\n    \n    def _subset_model_approximation(self, \n                                    feature_subset: tuple[int, ...], \n                                    instance: np.array) -&gt; float:\n        masked_background_dataset = self.background_dataset.copy()\n        for j in range(masked_background_dataset.shape[1]):\n            if j in feature_subset:\n                masked_background_dataset[:, j] = instance[j]\n        conditional_expectation_of_model = np.mean(\n            self.model(masked_background_dataset)\n        )\n        return conditional_expectation_of_model          \n\nThe SHAPExplainerFromScratch API is similar to that of the KernelExplainer from the python library, taking two required arguments during instantiation:\n\nmodel: ‚ÄúUser supplied function that takes a matrix of samples (# samples x # features) and computes the output of the model for those samples.‚Äù That means if our model is a scikit-learn model, we‚Äôll need to pass in its predict method, not the model object itself.\nbackground_dataset: ‚ÄúThe background dataset to use for integrating out features.‚Äù We know about this idea from the Shapley sampling values section above; a good choice for this data could be the training dataset we used to fit the model. By default, we‚Äôll use all the rows of this background dataset, but we‚Äôll also implement the ability to sample down to the desired number of rows with an argument called max_samples.\n\nLike the KernelExplainer, this class has a method called shap_values which estimates the SHAP values for a set of instances. It takes an argument X which is ‚Äúa matrix of samples (# samples x # features) on which to explain the model‚Äôs output.‚Äù This shap_values method just loops through each feature value of each instance of the input samples X and calls an internal method named _compute_single_shap_value to compute each SHAP value. The _compute_single_shap_value method is the real workhorse of the class. It implements equation 4 from the SHAP paper as described in the Shapley regression values section above by calling a few other internal helper methods corresponding to functions that we‚Äôve already written."
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#testing-the-implementation",
    "href": "posts/shap-from-scratch/index.html#testing-the-implementation",
    "title": "SHAP from Scratch",
    "section": "Testing the Implementation",
    "text": "Testing the Implementation\nLet‚Äôs check our work by comparing SHAP values computed by our implementation with those from the SHAP python library. We‚Äôll use our old friend the diabetes dataset, training a linear model, a random forest, and a gradient boosting machine.\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n\nX, y = load_diabetes(as_frame=False, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    random_state=42)\n\nlin_model = LinearRegression().fit(X_train, y_train);\nrfr_model = RandomForestRegressor().fit(X_train, y_train);\ngbt_model = GradientBoostingRegressor().fit(X_train, y_train);\n\nHere‚Äôs a little function to compare the SHAP values generated by our implementation and those from the library KernelExplainer.\n\nimport shap\n\ndef compare_methods(model, X_background, X_instances):\n        \n    library_explainer = shap.KernelExplainer(model.predict, X_background)\n    library_shap_values = library_explainer.shap_values(X_instances)\n\n    from_scratch_explainer = ShapFromScratchExplainer(model.predict, X_background)\n    from_scratch_shap_values = from_scratch_explainer.shap_values(X_instances)\n\n    return np.allclose(library_shap_values, from_scratch_shap_values)\n\n\ncompare_methods(lin_model, \n                X_background=X_train[:100, :], \n                X_instances=X_test[:5, :])\n\n\n\n\nTrue\n\n\n\ncompare_methods(rfr_model, \n                X_background=X_train[:100, :], \n                X_instances=X_test[:5, :])\n\n\n\n\nTrue\n\n\n\ncompare_methods(gbt_model, \n                X_background=X_train[:100, :], \n                X_instances=X_test[:5, :])\n\n\n\n\nTrue\n\n\nBeautiful! Our Implementation is consistent with the SHAP library explainer!"
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#wrapping-up",
    "href": "posts/shap-from-scratch/index.html#wrapping-up",
    "title": "SHAP from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell I hope this one was helpful to you. The research phase actually took me a lot longer than I expected; it just took me a while to figure out what SHAP really is and how those different ideas and papers fit together. I thought the implementation itself was pretty fun and relatively easy. What do you think?"
  },
  {
    "objectID": "posts/shap-from-scratch/index.html#references",
    "href": "posts/shap-from-scratch/index.html#references",
    "title": "SHAP from Scratch",
    "section": "References",
    "text": "References\n\nThe SHAP Paper (Lundberg and Lee, 2017)\nInterpretable Machine Learning by Christoph Molnar"
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Hello World! And Why I‚Äôm Inspired to Start a Blog",
    "section": "",
    "text": "Matt raises his arms in joy at the world.!\nWell, I‚Äôve been thinking about getting this blog started for months now. I guess a combination of inertia, up-front investment in blogging platform selection/setup, and spending a little too much time writing and rewriting the first content post has drawn out the period from initial inspiration to making the blog a reality. Needless to say, I‚Äôm pretty excited to finally get things going.\nBefore we dive headlong into the weeds of ML algorithms, statistical methods, and whatever I happen to be learning and teaching at the moment, I figured it would be good to articulate why I‚Äôve felt inspired to get started blogging in the first place. Hopefully this will serve the dual purpose of clarifying my intentions and introducing a vastly underappreciated concept in data science that I hope to weave through the posts to come."
  },
  {
    "objectID": "posts/hello-world/index.html#learning",
    "href": "posts/hello-world/index.html#learning",
    "title": "Hello World! And Why I‚Äôm Inspired to Start a Blog",
    "section": "Learning",
    "text": "Learning\nThe initial inception about blogging probably originated from some comments about learning that Jeremy Howard makes in the Practical Deep Learning course from fastai. During one of the lectures, he mentions that it‚Äôs a great idea to start blogging. To paraphrase Jeremy:\n\nThe thing I really love about blogging is that it helps you learn; by writing things down, you synthesize your ideas.\n\nBeautiful. That definitely rings true for me. I tend to take notes and play around with code when learning new concepts anyway. One of my key hypotheses about this blogging experiment is that making the effort to transform those notes into blog posts will help me learn more effectively."
  },
  {
    "objectID": "posts/hello-world/index.html#teaching",
    "href": "posts/hello-world/index.html#teaching",
    "title": "Hello World! And Why I‚Äôm Inspired to Start a Blog",
    "section": "Teaching",
    "text": "Teaching\nAh, teaching. Yes, sometimes it‚Äôs that thing that takes time away from your research, forcing you to sit alone in a windowless room squinting at hand-written math on a fat stack of homework assignments. But sometimes it actually involves interacting with students, endeavoring to explain a concept, and watching them light up when they get it. The latter manifestation of teaching was one of my favorite things about grad school and academia in general. While I certainly still get to do some teaching as an industry data scientist, I could see myself returning to a more teaching-centric gig somewhere off in the future. Thus we have our second key hypothesis about the blogging experiment, that the writing will entertain my inclination to teach."
  },
  {
    "objectID": "posts/hello-world/index.html#contributing",
    "href": "posts/hello-world/index.html#contributing",
    "title": "Hello World! And Why I‚Äôm Inspired to Start a Blog",
    "section": "Contributing",
    "text": "Contributing\nWorking in the field of data science today is a bit like standing in front of a massive complimentary all-you-can-learn buffet. There is an abundance of free material out on the interwebs for learning pretty much anything in data science from hello world python tutorials to research papers on cutting-edge deep learning techniques. I‚Äôve personally benefited from many a blog post that helped me unpack a new concept or get started using a new tool. And let‚Äôs not forget the gigantic cyber warehouse full of freely available open source software tools that volunteer developers have straight-up donated to humanity.\nI realize that up to now, I‚Äôve simply been consuming all of this free goodness without giving anything substantive back in return. Well then, it‚Äôs time to start evening the score. Which brings us to key hypothesis number three, that through these blog posts, I might be able to create something helpful, thereby being of service to a community that has freely given so much to me."
  },
  {
    "objectID": "posts/hello-world/index.html#live-long-and-prosper-blog",
    "href": "posts/hello-world/index.html#live-long-and-prosper-blog",
    "title": "Hello World! And Why I‚Äôm Inspired to Start a Blog",
    "section": "Live Long and Prosper, Blog",
    "text": "Live Long and Prosper, Blog\nPhew, there it is, the original source of inspiration for this blogging experiment, and three reasons I think it might be a good idea. The astute reader will have noticed that these three assertions have been formulated as hypotheses which are to be tested in the laboratory of experience. And thus, we also have our first glimpse of the scientific method, an underrated concept that is going to help us put the science back in data science.\nWith that, blog, I christen thee, Random Realizations."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html",
    "href": "posts/xgboost-for-classification-in-python/index.html",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "",
    "text": "Today we continue the saga on gradient boosting with a down-to-Earth tutorial on the essentials of solving classification problems with XGBoost. We‚Äôll run through two examples: one for binary classification and another for multi-class classification. In both cases I‚Äôll show you how to train XGBoost models using either the scikit-learn interface or the native xgboost training API. Once trained, we‚Äôll evaluate the models with validation data then inspect them with feature importance and partial dependence plots. You can use the XGBoost classification notebook in my ds-templates repository to follow along with your own dataset."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#preparing-data-for-xgboost-classifier",
    "href": "posts/xgboost-for-classification-in-python/index.html#preparing-data-for-xgboost-classifier",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Preparing Data for XGBoost Classifier",
    "text": "Preparing Data for XGBoost Classifier\nOur dataset must satisfy two requirements to be used in an XGBoost classifier. First all feature data must be numeric‚Äîno strings and no datetimes; if you have non-numeric features, you need to transform your feature data. Second, the target must be integer encoded using \\(\\{0,1\\}\\) for binary targets and \\(\\{0,1,\\dots,K\\}\\) for multiclass targets. Note that if your data is encoded to positive integers (no 0 class) XGBoost will throw potentially cryptic errors. You can use the scikit-learn LabelEncoder (which we‚Äôll do below) to generate a valid target encoding."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#xgboost-training-apis",
    "href": "posts/xgboost-for-classification-in-python/index.html#xgboost-training-apis",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "XGBoost Training APIs",
    "text": "XGBoost Training APIs\nThe xgboost python library offers two API‚Äôs for training classification models: the native train function and a wrapper class called XGBClassifier, which offers an API consistent with the scikit-learn universe. I‚Äôll show you how to use both approaches in the examples below, but if you‚Äôre planning to use other utilities from scikit-learn, you might find the XGBClassifier approach to be more convenient, since the trained model object will generally play nice with sklearn functionality."
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#binary-classification-example",
    "href": "posts/xgboost-for-classification-in-python/index.html#binary-classification-example",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Binary Classification Example",
    "text": "Binary Classification Example\n\nBreast Cancer Wisconsin Dataset\nWe‚Äôll demonstrate binary classification in XGBoost using the breast cancer wisconsin data, one of scikit-learn‚Äôs built-in toy datasets. This is a tiny dataset with 569 observations of 30 features and a binary target representing whether samples are malignant or benign..\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom sklearn import datasets\nimport xgboost as xgb \n\ndbunch = datasets.load_breast_cancer(as_frame=True)\ndf = dbunch.frame\nfeatures = dbunch.feature_names \ntarget_names = dbunch.target_names \ntarget = 'target' \ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   mean radius              569 non-null    float64\n 1   mean texture             569 non-null    float64\n 2   mean perimeter           569 non-null    float64\n 3   mean area                569 non-null    float64\n 4   mean smoothness          569 non-null    float64\n 5   mean compactness         569 non-null    float64\n 6   mean concavity           569 non-null    float64\n 7   mean concave points      569 non-null    float64\n 8   mean symmetry            569 non-null    float64\n 9   mean fractal dimension   569 non-null    float64\n 10  radius error             569 non-null    float64\n 11  texture error            569 non-null    float64\n 12  perimeter error          569 non-null    float64\n 13  area error               569 non-null    float64\n 14  smoothness error         569 non-null    float64\n 15  compactness error        569 non-null    float64\n 16  concavity error          569 non-null    float64\n 17  concave points error     569 non-null    float64\n 18  symmetry error           569 non-null    float64\n 19  fractal dimension error  569 non-null    float64\n 20  worst radius             569 non-null    float64\n 21  worst texture            569 non-null    float64\n 22  worst perimeter          569 non-null    float64\n 23  worst area               569 non-null    float64\n 24  worst smoothness         569 non-null    float64\n 25  worst compactness        569 non-null    float64\n 26  worst concavity          569 non-null    float64\n 27  worst concave points     569 non-null    float64\n 28  worst symmetry           569 non-null    float64\n 29  worst fractal dimension  569 non-null    float64\n 30  target                   569 non-null    int64  \ndtypes: float64(30), int64(1)\nmemory usage: 137.9 KB\n\n\nIn this dataset, the features are all numeric, so no need to do preprocessing before passing to XGBoost. Below we‚Äôll have a look at the target to ensure it‚Äôs encoded in \\(\\{0,1\\}\\) and to check the class balance.\n\nprint(df[target].unique())\nprint(target_names)\n\n[0 1]\n['malignant' 'benign']\n\n\n\ndf.target.value_counts().sort_index().plot.bar()\nplt.xlabel('target') \nplt.ylabel('count');\n\n\n\n\nclass counts for the breast cancer dataset\n\n\n\n\nNext We randomly split data into train and validation sets.\n\nfrom sklearn.model_selection import train_test_split\n\nn_valid = 50 \n\ntrain_df, valid_df = train_test_split(df, test_size=n_valid, random_state=42)\ntrain_df.shape, valid_df.shape\n\n((519, 31), (50, 31))\n\n\n\n\nTraining with the train function\nWe need to set a couple of model parameters, most notably objective, which should be set to binary:logistic for binary classification. I also prefer to explicitly set tree_method to something other than its default of auto; usually I‚Äôll start with exact on small datasets or approx on larger ones. Note also that The train function expects to receive data as DMatrix objects, not pandas dataframes, so we need to create dense matrix objects as well.\n\nparams = {\n    'tree_method': 'exact',\n    'objective': 'binary:logistic',\n}\nnum_boost_round = 50\n\ndtrain = xgb.DMatrix(label=train_df[target], data=train_df[features])\ndvalid = xgb.DMatrix(label=valid_df[target], data=valid_df[features])\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=10)\n\n[0] train-logloss:0.46232   valid-logloss:0.49033\n[10]    train-logloss:0.04394   valid-logloss:0.13434\n[20]    train-logloss:0.01515   valid-logloss:0.12193\n[30]    train-logloss:0.00995   valid-logloss:0.11988\n[40]    train-logloss:0.00766   valid-logloss:0.12416\n[49]    train-logloss:0.00657   valid-logloss:0.12799\n\n\n\n\nTraining with XGBClassifier\nThe XGBClassifier takes dataframes or numpy arrays as input, so this time we don‚Äôt need to create those dense matrix objects.\n\nparams = {\n    'tree_method': 'exact',\n    'objective': 'binary:logistic',\n}\nnum_boost_round = 50\n\nclf = xgb.XGBClassifier(n_estimators=num_boost_round, **params)\nclf.fit(train_df[features], train_df[target], \n        eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n        verbose=10);\n\n[0] validation_0-logloss:0.46232    validation_1-logloss:0.49033\n[10]    validation_0-logloss:0.04394    validation_1-logloss:0.13434\n[20]    validation_0-logloss:0.01515    validation_1-logloss:0.12193\n[30]    validation_0-logloss:0.00995    validation_1-logloss:0.11988\n[40]    validation_0-logloss:0.00766    validation_1-logloss:0.12416\n[49]    validation_0-logloss:0.00657    validation_1-logloss:0.12799\n\n\n\n\nEvaluating the Model\nWe‚Äôll use the sklearn.metrics module to evaluate model performance on the held-out validation set. Have a look at the scikit-learn metrics for classification for examples of other metrics to use.\nOne thing to watch out for when computing metrics is the difference between the actual labels (usually called y_true), the model‚Äôs predicted labels (usually called y_pred), and the models predicted probabilities (usually called y_score). If you‚Äôre using the XGBClassifier wrapper, you can get predicted labels with the predict method and predicted probabilities with the predict_proba method. Also note that whereas predict returns a vector of size (num data), predict_proba returns a vector of size (num data, num classes); thus for binary classification, we‚Äôll take just the second column of the array which gives the probability of class 1.\n\ny_true = valid_df[target]\ny_pred = clf.predict(valid_df[features])\ny_score = clf.predict_proba(valid_df[features])[:,1]\n\nProbably the simplest classification metric is accuracy, the proportion of labels we predicted correctly.\n\nfrom sklearn import metrics \n\nmetrics.accuracy_score(y_true, y_pred)\n\n0.96\n\n\nWe can generate a classification report with several different metrics at once.\n\nprint(metrics.classification_report(y_true, y_pred, target_names=target_names))\n\n              precision    recall  f1-score   support\n\n   malignant       0.93      0.93      0.93        15\n      benign       0.97      0.97      0.97        35\n\n    accuracy                           0.96        50\n   macro avg       0.95      0.95      0.95        50\nweighted avg       0.96      0.96      0.96        50\n\n\n\nAnd we can compute the AUC, a popular classification metric based on the ROC curve, which depends on the predicted probability rather than the predicted labels.\n\nmetrics.roc_auc_score(y_true, y_score)\n\n0.9885714285714287\n\n\n\n\nFeature Importance\nBecause of the limitations of the built-in XGBoost feature importance metrics I recommend that you use either permutation feature importance or perhaps SHAP feature importance.\nHere we‚Äôll compute the permutation feature importance, which tells us by how much the model‚Äôs performance changes when we scramble a particular feature‚Äôs values at prediction time. This reflects how much the model relies on each feature when making predictions.\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import make_scorer\n\nscorer = make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)\npermu_imp = permutation_importance(clf, valid_df[features], valid_df[target], \n                                   n_repeats=30, random_state=0, scoring=scorer)\n\n\nimportances_permutation = pd.Series(permu_imp['importances_mean'], index=features)\nimportances_permutation.sort_values(ascending=True)[-10:].plot.barh()\nplt.title('Permutation Importance on Out-of-Sample Set')\nplt.xlabel('change in log likelihood');\n\n\n\n\ntop 10 features by permutation importance on validation set"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#partial-dependence",
    "href": "posts/xgboost-for-classification-in-python/index.html#partial-dependence",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Partial Dependence",
    "text": "Partial Dependence\nA partial dependence plot (PDP) is a representation of the dependence between the model output and one or more feature variables. In binary classification, the model output is the probability of the so-called positive class, i.e.¬†the class with encoded label 1, which corresponds to probability of ‚Äúbenign‚Äù in this example.. We can loosely interpret the partial dependence as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say ‚Äúloosely‚Äù because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(clf, \n                                        valid_df[features], \n                                        ['worst area', 'area error', 'mean area']);\n\n\n\n\nPDP of target probability of benign vs three features"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#multi-class-classification-example",
    "href": "posts/xgboost-for-classification-in-python/index.html#multi-class-classification-example",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Multi-Class Classification Example",
    "text": "Multi-Class Classification Example\n\nForest Cover Type Dataset\nWe‚Äôll illustrate multi-class classification using the scikit-learn forest cover type dataset, which has around 580k observations of 54 features and a target with 7 classes.\n\ndbunch = datasets.fetch_covtype(as_frame=True)\ndf = dbunch.frame\nfeatures = dbunch.feature_names \ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 581012 entries, 0 to 581011\nData columns (total 55 columns):\n #   Column                              Non-Null Count   Dtype  \n---  ------                              --------------   -----  \n 0   Elevation                           581012 non-null  float64\n 1   Aspect                              581012 non-null  float64\n 2   Slope                               581012 non-null  float64\n 3   Horizontal_Distance_To_Hydrology    581012 non-null  float64\n 4   Vertical_Distance_To_Hydrology      581012 non-null  float64\n 5   Horizontal_Distance_To_Roadways     581012 non-null  float64\n 6   Hillshade_9am                       581012 non-null  float64\n 7   Hillshade_Noon                      581012 non-null  float64\n 8   Hillshade_3pm                       581012 non-null  float64\n 9   Horizontal_Distance_To_Fire_Points  581012 non-null  float64\n 10  Wilderness_Area_0                   581012 non-null  float64\n 11  Wilderness_Area_1                   581012 non-null  float64\n 12  Wilderness_Area_2                   581012 non-null  float64\n 13  Wilderness_Area_3                   581012 non-null  float64\n 14  Soil_Type_0                         581012 non-null  float64\n 15  Soil_Type_1                         581012 non-null  float64\n 16  Soil_Type_2                         581012 non-null  float64\n 17  Soil_Type_3                         581012 non-null  float64\n 18  Soil_Type_4                         581012 non-null  float64\n 19  Soil_Type_5                         581012 non-null  float64\n 20  Soil_Type_6                         581012 non-null  float64\n 21  Soil_Type_7                         581012 non-null  float64\n 22  Soil_Type_8                         581012 non-null  float64\n 23  Soil_Type_9                         581012 non-null  float64\n 24  Soil_Type_10                        581012 non-null  float64\n 25  Soil_Type_11                        581012 non-null  float64\n 26  Soil_Type_12                        581012 non-null  float64\n 27  Soil_Type_13                        581012 non-null  float64\n 28  Soil_Type_14                        581012 non-null  float64\n 29  Soil_Type_15                        581012 non-null  float64\n 30  Soil_Type_16                        581012 non-null  float64\n 31  Soil_Type_17                        581012 non-null  float64\n 32  Soil_Type_18                        581012 non-null  float64\n 33  Soil_Type_19                        581012 non-null  float64\n 34  Soil_Type_20                        581012 non-null  float64\n 35  Soil_Type_21                        581012 non-null  float64\n 36  Soil_Type_22                        581012 non-null  float64\n 37  Soil_Type_23                        581012 non-null  float64\n 38  Soil_Type_24                        581012 non-null  float64\n 39  Soil_Type_25                        581012 non-null  float64\n 40  Soil_Type_26                        581012 non-null  float64\n 41  Soil_Type_27                        581012 non-null  float64\n 42  Soil_Type_28                        581012 non-null  float64\n 43  Soil_Type_29                        581012 non-null  float64\n 44  Soil_Type_30                        581012 non-null  float64\n 45  Soil_Type_31                        581012 non-null  float64\n 46  Soil_Type_32                        581012 non-null  float64\n 47  Soil_Type_33                        581012 non-null  float64\n 48  Soil_Type_34                        581012 non-null  float64\n 49  Soil_Type_35                        581012 non-null  float64\n 50  Soil_Type_36                        581012 non-null  float64\n 51  Soil_Type_37                        581012 non-null  float64\n 52  Soil_Type_38                        581012 non-null  float64\n 53  Soil_Type_39                        581012 non-null  float64\n 54  Cover_Type                          581012 non-null  int32  \ndtypes: float64(54), int32(1)\nmemory usage: 241.6 MB\n\n\nHere again the features are all numeric, so we don‚Äôt need to further preprocess them. Let‚Äôs have a look at the target.\n\ndf['Cover_Type'].value_counts().sort_index().plot.bar()\nplt.xlabel('cover type') \nplt.ylabel('count');\n\n\n\n\nclass counts for the forest cover type dataset\n\n\n\n\nFor multi-class classification, our target variable must take values in \\(\\{0,1,\\dots,K\\}\\). However, from the histogram of the cover type above, we see that it takes values in \\(\\{1,2,\\dots,7\\}\\). To fix this we can use the scikit-learn label encoder to create a valid target column.\n\nfrom sklearn.preprocessing import LabelEncoder \n\ntarget = 'encoded'\nenc = LabelEncoder()\ndf[target] = enc.fit_transform(df['Cover_Type'])\nprint(np.sort(df[target].unique()))\n\n[0 1 2 3 4 5 6]\n\n\nThen we can create training and validation sets.\n\nn_valid = 20000\n\ntrain_df, valid_df = train_test_split(df, test_size=n_valid, random_state=42)\ntrain_df.shape, valid_df.shape\n\n((561012, 56), (20000, 56))\n\n\n\n\nTraining with the train function\nIf you‚Äôre training with the train function, multi-class classification can be done with two objectives: multi:softmax and multi:softprob. Both use the same loss function‚Äînegative multinomial log likelihood‚Äîbut the softmax option produces a trained Booster object whose predict method returns a 1d array of predicted labels, whereas the softprob option produces a trained Booster object whose predict method returns a 2d array of predicted probabilities. In either case, you also need to explicitly tell XGBoost how many classes the target has with the num_class parameter.\n\nparams = {\n    'tree_method': 'approx',\n    'objective': 'multi:softprob',\n    'num_class': df[target].nunique()\n}\nnum_boost_round = 10\n\ndtrain = xgb.DMatrix(label=train_df[target], data=train_df[features])\ndvalid = xgb.DMatrix(label=valid_df[target], data=valid_df[features])\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=2)\n\n[0] train-mlogloss:1.42032  valid-mlogloss:1.42366\n[2] train-mlogloss:1.00541  valid-mlogloss:1.00963\n[4] train-mlogloss:0.80557  valid-mlogloss:0.81109\n[6] train-mlogloss:0.69432  valid-mlogloss:0.70085\n[8] train-mlogloss:0.62653  valid-mlogloss:0.63350\n[9] train-mlogloss:0.60111  valid-mlogloss:0.60794\n\n\n\n\nTraining with XGBClassifier\nIn multi-class classification, I think the scikit-learn XGBClassifier wrapper is quite a bit more convenient than the native train function. You can set the objective parameter to multi:softprob, and XGBClassifier.fit will produce a model having both predict and predict_proba methods. Also there is no need to explicitly set the number of classes in the target and no need to create the DMatrix objects.\n\nparams = {\n    'tree_method': 'approx',\n    'objective': 'multi:softprob',\n}\nnum_boost_round = 10\n\nclf = xgb.XGBClassifier(n_estimators=num_boost_round, **params)\nclf.fit(train_df[features], train_df[target], \n        eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n        verbose=2);\n\n[0] validation_0-mlogloss:1.42032   validation_1-mlogloss:1.42366\n[2] validation_0-mlogloss:1.00541   validation_1-mlogloss:1.00963\n[4] validation_0-mlogloss:0.80557   validation_1-mlogloss:0.81109\n[6] validation_0-mlogloss:0.69432   validation_1-mlogloss:0.70085\n[8] validation_0-mlogloss:0.62653   validation_1-mlogloss:0.63350\n[9] validation_0-mlogloss:0.60111   validation_1-mlogloss:0.60794\n\n\n\n\nEvaluating the Model\nThis time, we‚Äôll keep the entire 2d array of predicted probabilities in y_score.\n\ny_true = valid_df[target]\ny_pred = clf.predict(valid_df[features])\ny_score = clf.predict_proba(valid_df[features])\ny_true.shape, y_pred.shape, y_score.shape\n\n((20000,), (20000,), (20000, 7))\n\n\n\nmetrics.accuracy_score(y_true, y_pred)\n\n0.77425\n\n\n\nprint(metrics.classification_report(y_true, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.77      0.74      0.75      7365\n           1       0.78      0.84      0.81      9725\n           2       0.75      0.85      0.80      1207\n           3       0.82      0.78      0.80        85\n           4       0.93      0.26      0.40       317\n           5       0.76      0.31      0.44       627\n           6       0.88      0.68      0.77       674\n\n    accuracy                           0.77     20000\n   macro avg       0.81      0.64      0.68     20000\nweighted avg       0.78      0.77      0.77     20000\n\n\n\nSome binary classification metrics, like AUC, can be extended to the multi-class setting by computing the metric for each class, then averaging in some way to get an overall score. The details are controlled by the average and multi_class parameters, which are described in the documentation.\n\nmetrics.roc_auc_score(y_true, y_score, average='weighted', multi_class='ovr')\n\n0.9129422094408693\n\n\n\n\nFeature Importance\nWe can compute permutation feature importance with exactly the same code that we used for the binary classifier.\n\nscorer = make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)\npermu_imp = permutation_importance(clf, valid_df[features], valid_df[target], \n                                   n_repeats=30, random_state=0, scoring=scorer)\n\n\nimportances_permutation = pd.Series(permu_imp['importances_mean'], index=features)\nimportances_permutation.sort_values(ascending=True)[-10:].plot.barh()\nplt.title('Permutation Importance on Out-of-Sample Set')\nplt.xlabel('change in multivariate log likelihood');\n\n\n\n\ntop 10 features by permutation importance on validation set\n\n\n\n\n\n\nPartial Dependence\nRecall that partial dependence reflects how the expected model output changes with a particular feature. In the multi-class setting, the model has multiple outputs‚Äîone probability for each class‚Äîso we need to choose which class probability to show in the plots. We choose the target class with the target parameter; be sure to pass in the encoded value, e.g.¬†we need to use the label encoder to transform a raw class label back into the encoded value. Here we‚Äôll examine partial dependence for the probability of cover type 3.\n\nPartialDependenceDisplay.from_estimator(clf, \n                                        X=valid_df[features], \n                                        features=['Elevation', 'Horizontal_Distance_To_Roadways'], \n                                        target=enc.transform([3])[0]);\n\n\n\n\nPDP of target probability of cover type == 3 vs elevation and distance to roadway"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#wrapping-up",
    "href": "posts/xgboost-for-classification-in-python/index.html#wrapping-up",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, for me, those are really the minimal nuts and bolts one needs to get XGBoost models working on classification problems. If you dig this tutorial, or if you have additional insights into using XGBoost to solve classification problems, let me know about it down in the comments!"
  },
  {
    "objectID": "posts/xgboost-for-classification-in-python/index.html#go-deeper",
    "href": "posts/xgboost-for-classification-in-python/index.html#go-deeper",
    "title": "XGBoost for Binary and Multi-Class Classification in Python",
    "section": "Go Deeper",
    "text": "Go Deeper\nIf you‚Äôre feeling like Alice, and you want to go tumbling down the rabbit hole, might I recommend checking out some of the following:\n\nXGBoost Explained - for a deep dive into the math\nXGBoost from Scratch - to see how to implement all those equations in code\nMulti-Class Gradient Boosting from Scratch - to fully grok the multi-class gradient boosting algorithm"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html",
    "href": "posts/xgboost-for-regression-in-python/index.html",
    "title": "XGBoost for Regression in Python",
    "section": "",
    "text": "In this post I‚Äôm going to show you my process for solving regression problems with XGBoost in python, using either the native xgboost API or the scikit-learn interface. This is a powerful methodology that can produce world class results in a short time with minimal thought or effort. While we‚Äôll be working on an old Kagle competition for predicting the sale prices of bulldozers and other heavy machinery, you can use this flow to solve whatever tabular data regression problem you‚Äôre working on.\nThis post serves as the explanation and documentation for the XGBoost regression jupyter notebook from my ds-templates repo on GitHub, so go ahead and download the notebook and follow along with your own data.\nIf you‚Äôre not already comfortable with the ideas behind gradient boosting and XGBoost, you‚Äôll find it helpful to read some of my previous posts to get up to speed. I‚Äôd start with this introduction to gradient boosting, and then read this explanation of how XGBoost works.\nLet‚Äôs get into it! üöÄ"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#install-and-import-the-xgboost-library",
    "href": "posts/xgboost-for-regression-in-python/index.html#install-and-import-the-xgboost-library",
    "title": "XGBoost for Regression in Python",
    "section": "Install and import the xgboost library",
    "text": "Install and import the xgboost library\nIf you don‚Äôt already have it, go ahead and use conda to install the xgboost library, e.g.\n$ conda install -c conda-forge xgboost\nThen import it along with the usual suspects.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost as xgb"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#read-dataset-into-python",
    "href": "posts/xgboost-for-regression-in-python/index.html#read-dataset-into-python",
    "title": "XGBoost for Regression in Python",
    "section": "Read dataset into python",
    "text": "Read dataset into python\nIn this example we‚Äôll work on the Kagle Bluebook for Bulldozers competition, which asks us to build a regression model to predict the sale price of heavy equipment. Amazingly, you can solve your own regression problem by swapping this data out with your organization‚Äôs data before proceeding with the tutorial.\nGo ahead and download the Train.zip file from Kagle and extract it into Train.csv. Then read the data into a pandas dataframe.\n\ndf = pd.read_csv('Train.csv', parse_dates=['saledate']);\n\nNotice I cheated a little bit, checking the columns ahead of time and telling pandas to treat the saledate column as a date. In general it will make life easier to read in any date-like columns as dates.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 401125 entries, 0 to 401124\nData columns (total 53 columns):\n #   Column                    Non-Null Count   Dtype         \n---  ------                    --------------   -----         \n 0   SalesID                   401125 non-null  int64         \n 1   SalePrice                 401125 non-null  int64         \n 2   MachineID                 401125 non-null  int64         \n 3   ModelID                   401125 non-null  int64         \n 4   datasource                401125 non-null  int64         \n 5   auctioneerID              380989 non-null  float64       \n 6   YearMade                  401125 non-null  int64         \n 7   MachineHoursCurrentMeter  142765 non-null  float64       \n 8   UsageBand                 69639 non-null   object        \n 9   saledate                  401125 non-null  datetime64[ns]\n 10  fiModelDesc               401125 non-null  object        \n 11  fiBaseModel               401125 non-null  object        \n 12  fiSecondaryDesc           263934 non-null  object        \n 13  fiModelSeries             56908 non-null   object        \n 14  fiModelDescriptor         71919 non-null   object        \n 15  ProductSize               190350 non-null  object        \n 16  fiProductClassDesc        401125 non-null  object        \n 17  state                     401125 non-null  object        \n 18  ProductGroup              401125 non-null  object        \n 19  ProductGroupDesc          401125 non-null  object        \n 20  Drive_System              104361 non-null  object        \n 21  Enclosure                 400800 non-null  object        \n 22  Forks                     192077 non-null  object        \n 23  Pad_Type                  79134 non-null   object        \n 24  Ride_Control              148606 non-null  object        \n 25  Stick                     79134 non-null   object        \n 26  Transmission              183230 non-null  object        \n 27  Turbocharged              79134 non-null   object        \n 28  Blade_Extension           25219 non-null   object        \n 29  Blade_Width               25219 non-null   object        \n 30  Enclosure_Type            25219 non-null   object        \n 31  Engine_Horsepower         25219 non-null   object        \n 32  Hydraulics                320570 non-null  object        \n 33  Pushblock                 25219 non-null   object        \n 34  Ripper                    104137 non-null  object        \n 35  Scarifier                 25230 non-null   object        \n 36  Tip_Control               25219 non-null   object        \n 37  Tire_Size                 94718 non-null   object        \n 38  Coupler                   213952 non-null  object        \n 39  Coupler_System            43458 non-null   object        \n 40  Grouser_Tracks            43362 non-null   object        \n 41  Hydraulics_Flow           43362 non-null   object        \n 42  Track_Type                99153 non-null   object        \n 43  Undercarriage_Pad_Width   99872 non-null   object        \n 44  Stick_Length              99218 non-null   object        \n 45  Thumb                     99288 non-null   object        \n 46  Pattern_Changer           99218 non-null   object        \n 47  Grouser_Type              99153 non-null   object        \n 48  Backhoe_Mounting          78672 non-null   object        \n 49  Blade_Type                79833 non-null   object        \n 50  Travel_Controls           79834 non-null   object        \n 51  Differential_Type         69411 non-null   object        \n 52  Steering_Controls         69369 non-null   object        \ndtypes: datetime64[ns](1), float64(2), int64(6), object(44)\nmemory usage: 162.2+ MB"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#prepare-raw-data-for-xgboost",
    "href": "posts/xgboost-for-regression-in-python/index.html#prepare-raw-data-for-xgboost",
    "title": "XGBoost for Regression in Python",
    "section": "Prepare raw data for XGBoost",
    "text": "Prepare raw data for XGBoost\nWhen faced with a new tabular dataset for modeling, we have two format considerations: data types and missingness. From the call to df.info() above, we can see we have both mixed types and missing values.\nWhen it comes to missing values, some models like the gradient booster or random forest in scikit-learn require purely non-missing inputs. One of the great strengths of XGBoost is that it relaxes this requirement, allowing us to pass in missing feature values, so we don‚Äôt have to worry about them.\nRegarding data types, all ML models for tabular data require inputs to be numeric, either integers or floats, so we‚Äôre going to have to deal with those object columns.\n\nEncode string features\nThe simplest way to encode string variables is to map each unique string value to an integer; this is called integer encoding.\nWe can easily accomplish this by using the categorical data type in pandas. The category type is a bit like the factor type in R; pandas stores the underlying data as integers, and it keeps a mapping from the integers back to the original string values. XGBoost is able to access the numeric data underlying the categorical features for model training and prediction. This is a nice way to encode string features because it‚Äôs easy to implement and it preserves the original category levels in the data frame. If you prefer to generate your own integer mappings, you can also do it with the scikit-learn OrdinalEncoder.\n\ndef encode_string_features(df):\n    out_df = df.copy()\n    for feature, feature_type in df.dtypes.items():\n        if feature_type == 'object':\n            out_df[feature] = out_df[feature].astype('category')\n    return out_df\n\ndf = encode_string_features(df)\n\n\n\nEncode date and timestamp features\nWhile dates feel sort of numeric, they are not quite numbers, so we need to transform them into numeric columns that XGBoost can understand. Unfortunately, encoding timestamps isn‚Äôt as straightforward as encoding strings, so we actually might need to engage in a little bit of feature engineering. A single date has many different attributes, e.g.¬†days since epoch, year, quarter, month, day, day of year, day of week, is holiday, etc. Often a simple time index is the most useful information in a date column, so here we‚Äôll just start by adding a feature that gives the number of days since some epoch date.\n\ndf['saledate_days_since_epoch'] = (\n    df['saledate'] - pd.Timestamp(year=1970, month=1, day=1)\n    ).dt.days\n\n\n\nTransform the target if necessary\nIn the interest of speed and efficiency, we didn‚Äôt bother doing any EDA with the feature data. Part of my justification for this is that trees are incredibly robust to outliers, colinearity, missingness, and other assorted nonsense in the feature data. However, they are not necessarily robust to nonsense in the target variable, so it‚Äôs worth having a look at it before proceeding any further.\n\ndf.SalePrice.hist(); plt.xlabel('SalePrice');\n\n\n\n\n\n\n\n\nOften when predicting prices it makes sense to use log price, especially when they span multiple orders of magnitude or have a strong right skew. These data look pretty friendly, lacking outliers and exhibiting only a mild positive skew; we could probably get away without doing any transformation. But checking the evaluation metric used to score the Kagle competition, we see they‚Äôre using root mean squared log error. That‚Äôs equivalent to using RMSE on log-transformed target data, so let‚Äôs go ahead and work with log prices.\n\ndf['logSalePrice'] = np.log1p(df['SalePrice'])\ndf.logSalePrice.hist(); plt.xlabel('logSalePrice');"
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#train-and-evaluate-the-xgboost-regression-model",
    "href": "posts/xgboost-for-regression-in-python/index.html#train-and-evaluate-the-xgboost-regression-model",
    "title": "XGBoost for Regression in Python",
    "section": "Train and Evaluate the XGBoost regression model",
    "text": "Train and Evaluate the XGBoost regression model\nHaving prepared our dataset, we are now ready to train an XGBoost model. Let‚Äôs walk through the flow step-by-step.\n\nSplit the data into training and validation sets\nFirst we split the dataset into a training set and a validation set. Of course since we‚Äôre going to evaluate against the validation set a number of times as we iterate, it‚Äôs best practice to keep a separate test set reserved to check our final model to ensure it generalizes well. Assuming that final test set is hidden away, we can use the rest of the data for training and validation.\nThere are two main ways we might want to select the validation set. If there isn‚Äôt a temporal ordering of the observations, we might be able to randomly sample. In practice, it‚Äôs common that observations have a temporal ordering, and that models are trained on observations up to a certain time and used to predict on observations occuring after that time. Since this data is temporal, we don‚Äôt want to split randomly; instead we‚Äôll split on observation date, reserving the latest observations for the validation set.\n\n# Temporal Validation Set\ndef train_test_split_temporal(df, datetime_column, n_test):\n    idx_sort = np.argsort(df[datetime_column])\n    idx_train, idx_test = idx_sort[:-n_valid], idx_sort[-n_valid:]\n    return df.iloc[idx_train, :], df.iloc[idx_test, :]\n\n\nn_valid = 12000\n\ntrain_df, valid_df = train_test_split_temporal(df, 'saledate', n_valid)\ntrain_df.shape, valid_df.shape\n\n((389125, 55), (12000, 55))\n\n\n\n\nSpecify target and feature columns\nNext we‚Äôll put together a list of our features and define the target column. I like to have an actual list defined in the code so it‚Äôs easy to explicitly see everything we‚Äôre puting into the model and easier to add or remove features as we iterate. Just run something like list(df.columns) in a cel to get a copy-pasteable list of columns, then edit it down to the full list of features, i.e.¬†remove the target, date columns, and other non-feature columns..\n\n#  list(df.columns)\n\n\nfeatures = [\n    'SalesID',\n    'MachineID',\n    'ModelID',\n    'datasource',\n    'auctioneerID',\n    'YearMade',\n    'MachineHoursCurrentMeter',\n    'UsageBand',\n    'fiModelDesc',\n    'fiBaseModel',\n    'fiSecondaryDesc',\n    'fiModelSeries',\n    'fiModelDescriptor',\n    'ProductSize',\n    'fiProductClassDesc',\n    'state',\n    'ProductGroup',\n    'ProductGroupDesc',\n    'Drive_System',\n    'Enclosure',\n    'Forks',\n    'Pad_Type',\n    'Ride_Control',\n    'Stick',\n    'Transmission',\n    'Turbocharged',\n    'Blade_Extension',\n    'Blade_Width',\n    'Enclosure_Type',\n    'Engine_Horsepower',\n    'Hydraulics',\n    'Pushblock',\n    'Ripper',\n    'Scarifier',\n    'Tip_Control',\n    'Tire_Size',\n    'Coupler',\n    'Coupler_System',\n    'Grouser_Tracks',\n    'Hydraulics_Flow',\n    'Track_Type',\n    'Undercarriage_Pad_Width',\n    'Stick_Length',\n    'Thumb',\n    'Pattern_Changer',\n    'Grouser_Type',\n    'Backhoe_Mounting',\n    'Blade_Type',\n    'Travel_Controls',\n    'Differential_Type',\n    'Steering_Controls',\n    'saledate_days_since_epoch'\n ]\n\ntarget = 'logSalePrice'\n\n\n\nCreate DMatrix data objects\nXGBoost uses a data type called dense matrix for efficient training and prediction, so next we need to create DMatrix objects for our training and validation datasets. Remember how we decided to encode our string columns by casting them as pandas categorical types? For this to work, we need to set the enable_categoricals argument to True.\n\ndtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], \n                     enable_categorical=True)\ndvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], \n                     enable_categorical=True)\n\n\n\nSet the XGBoost parameters\nXGBoost has numerous hyperparameters. Fortunately, just a handful of them tend to be the most influential; furthermore, the default values are not bad in most situations. I like to start out with a dictionary containing the default values for the parameters I‚Äôm most likely to adjust later, with one exception. I dislike the default value of auto for the tree_method parameter, which tells XGBoost to choose a tree method on it‚Äôs own. I‚Äôve been burned by this ambiguity in the past, so now I prefer to set it to approx. For training there is one required boosting parameter called num_boost_round which I set to 50 as a starting point.\n\n# default values for important parameters\nparams = {\n    'tree_method': 'approx',\n    'learning_rate': 0.3,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'subsample': 1,\n    'colsample_bynode': 1,\n    'objective': 'reg:squarederror',\n}\nnum_boost_round = 50\n\n\n\nTrain the XGBoost model\nThe xgb.train() function takes our training dataset and parameters, and it returns a trained XGBoost model, which is an object of class xgb.core.Booster. Check out the documentation on the learning API to see all the training options. During training, I like to have XGBoost print out the evaluation metric on the train and validation set after every few boosting rounds and again at the end of training; that can be done by setting evals and verbose_eval.\n\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=10)\n\n[0] train-rmse:6.74240  valid-rmse:6.80825\n[10]    train-rmse:0.31071  valid-rmse:0.34532\n[20]    train-rmse:0.21950  valid-rmse:0.24364\n[30]    train-rmse:0.20878  valid-rmse:0.23669\n[40]    train-rmse:0.20164  valid-rmse:0.23254\n[49]    train-rmse:0.19705  valid-rmse:0.23125\n\n\n\n\nTrain the XGBoost model using the sklearn interface\nIf you prefer scikit-learn-like syntax, you can use the sklearn estimator interface to create and train XGBoost models. The XGBRegressor class, which is available in the xgboost library that we already imported, constructs an XGBRegressor object with fit and predict methods like you‚Äôre used to using in scikit-learn. The fit and predict methods take pandas dataframes, so you don‚Äôt need to create DMatrix data objects yourself; however, since these methods still have to transform input data into DMatrix objects internally, training and prediction seem to be slower via the sklearn interface.\n\n# scikit-learn interface\nreg = xgb.XGBRegressor(n_estimators=num_boost_round, enable_categorical=True, **params)\nreg.fit(train_df[features], train_df[target], \n        eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n        verbose=10);\n\n[0] validation_0-rmse:6.74240   validation_1-rmse:6.80825\n[10]    validation_0-rmse:0.31071   validation_1-rmse:0.34532\n[20]    validation_0-rmse:0.21950   validation_1-rmse:0.24364\n[30]    validation_0-rmse:0.20878   validation_1-rmse:0.23669\n[40]    validation_0-rmse:0.20164   validation_1-rmse:0.23254\n[49]    validation_0-rmse:0.19705   validation_1-rmse:0.23125\n\n\nSince not all features of XGBoost are available through the scikit-learn estimator interface, you might want to get the native xgb.core.Booster object back out of the sklearn wrapper.\n\nbooster = reg.get_booster()\n\n\n\nEvaluate the XGBoost model and check for overfitting\nWe get the model evaluation metrics on the training and validation sets printed to stdout when we use the evals argument to the training API. Typically I just look at those printed metrics, but sometimes it‚Äôs helpful to retain them in a variable for further inspection via, e.g.¬†plotting. To do that we need to train again, passing an empty dictionary to the evals_result argument. In the objective curves, I‚Äôm looking for signs of overfitting, which could include validation scores staying the same or getting worse over later iterations or huge gaps between training and validation scores.\n\nevals_result = {}\nmodel = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                  verbose_eval=10,\n                  evals_result=evals_result)\n\n[0] train-rmse:6.74240  valid-rmse:6.80825\n[10]    train-rmse:0.31071  valid-rmse:0.34532\n[20]    train-rmse:0.21950  valid-rmse:0.24364\n[30]    train-rmse:0.20878  valid-rmse:0.23669\n[40]    train-rmse:0.20164  valid-rmse:0.23254\n[49]    train-rmse:0.19705  valid-rmse:0.23125\n\n\n\npd.DataFrame({\n    'train': evals_result['train']['rmse'],\n    'valid': evals_result['valid']['rmse']\n}).plot(); plt.xlabel('boosting round'); plt.ylabel('objective');\n\n\n\n\nThese objective curves look pretty good‚Äìno obvious signs of trouble.\n\n\n\n\nWhile we could just look at the validation RMSE in the printed output from model training, let‚Äôs go ahead and compute it by hand, just to be sure.\n\nfrom sklearn.metrics import mean_squared_error\n\n# squared=False returns RMSE\nmean_squared_error(y_true=dvalid.get_label(), \n                   y_pred=model.predict(dvalid), \n                   squared=False)\n\n0.23124987\n\n\nSo, how good is that RMSLE of 0.231? Well, checking the Kagle leaderboard for this competition, we would have come in around 5th out of 474. That‚Äôs not bad for 10 minutes of work doing the bare minimum necessary to transform the raw data into a format consumable by XGBoost and then training a model using default hyperparameter values. To improve our model from here we would want to explore some feature engineering and some hyperparameter tuning, which we‚Äôll save for another post.\n\nWait, why was that so easy? Since XGBoost made it‚Äôs big Kagle debut in the 2014 Higgs Boson competition, presumably no one in this 2013 competition was using it yet. A second potential reason is that we‚Äôre using a different validation set from that used for the final leaderboard (which is long closed), but our score is likely still a decent approximation for how we would have done in the competition."
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#xgboost-model-interpretation",
    "href": "posts/xgboost-for-regression-in-python/index.html#xgboost-model-interpretation",
    "title": "XGBoost for Regression in Python",
    "section": "XGBoost Model Interpretation",
    "text": "XGBoost Model Interpretation\nNext let‚Äôs have a look at how to apply a couple of the most common model interpretation techniques, feature importance and partial dependence, to XGBoost.\n\nRemember we have two trained models floating around: one called model of class xgb.core.Booster which is compatible with xgboost library utilities and another called reg of class XGBRegressor which is compatible with scikit-learn utilities. We need to be sure to use the model that‚Äôs compatible with whatever utility we‚Äôre using.\n\n\nWhile these interpretation tools are still very common, there‚Äôs a newer, more comprehensive, and self-consistent model interpretation framework called SHAP that‚Äôs worth checking out.\n\n\nFeature Importance for XGBoost\nWhile XGBoost automatically computes feature importance by three different metrics during training, you should only use them with great care and skepticism. The three metrics are\n\nweight: the number of splits that use the feature\ngain: the average gain in the objective function from splits which use the feature\ncover: the average number of training samples affected by splits that use the feature\n\nThe first problem with these metrics is that they are computed using only the training dataset, which means they don‚Äôt reflect how useful a feature is when predicting on out-of-sample data. If your model is overfit on some nonsense feature, it will still have a high importance. Secondly, I think they are difficult to interpret; all three are specific to decision trees and reflect domain-irrelevant idiosyncrasies like whether a feature is used nearer the root or the leaves of a tree. Anyway let‚Äôs see what these metrics have to say about our features.\n\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(5, 8))\nxgb.plot_importance(model, importance_type='weight', title='importance_type=weight', \n                    max_num_features=10, show_values=False, ax=ax1, )\nxgb.plot_importance(model, importance_type='cover', title='importance_type=cover', \n                    max_num_features=10, show_values=False, ax=ax2)\nxgb.plot_importance(model, importance_type='gain', title='importance_type=gain', \n                    max_num_features=10, show_values=False, ax=ax3)\nplt.tight_layout()\n\n\n\n\ntop 10 features according to each built-in XGBoost feature importance metric\n\n\n\n\nWow, notice that the top 10 features by weight and by cover are completely different. This should forever cause you to feel skeptical whenever you see a feature importance plot.\nLuckily, there is a better way. IMHO, permutation feature importance is better aligned with our intuition about what feature importance should mean. It tells us by how much the model performance decreases when the values of a particular feature are randomly shuffled during prediction. This effectively breaks the relationship between the feature and the target, thus revealing how much the model relies on that feature for prediction. It also has the benefit that it can be computed using either training data or out-of-sample data.\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import make_scorer\n\n# make a scorer for RMSE\nscorer = make_scorer(mean_squared_error, squared=False)\npermu_imp = permutation_importance(reg, valid_df[features], valid_df[target], \n                                   n_repeats=30, random_state=0, scoring=scorer)\n\n\nimportances_permutation = pd.Series(-1 * permu_imp['importances_mean'], index=features)\nimportances_permutation.sort_values(ascending=True)[-10:].plot.barh()\nplt.title('Permutation Importance on Out-of-Sample Set')\nplt.xlabel('drop in RMSE');\n\n\n\n\ntop 10 features by permutation importance on validation set\n\n\n\n\nNow we can see which features the model relies on most for out-of-sample predictions. These are good candidate features to dig into with some EDA and conversations with any domain expert collaborators."
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#partial-dependence-plots-for-xgboost",
    "href": "posts/xgboost-for-regression-in-python/index.html#partial-dependence-plots-for-xgboost",
    "title": "XGBoost for Regression in Python",
    "section": "Partial Dependence Plots for XGBoost",
    "text": "Partial Dependence Plots for XGBoost\nA partial dependence plot (PDP) is a representation of the dependence between the target variable and one or more feature variables. We can loosely interpret it as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say ‚Äúloosely‚Äù because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\nPartialDependenceDisplay.from_estimator(reg, \n                                        valid_df[features].query('YearMade &gt;= 1960'), \n                                        ['YearMade']);\n\n\n\n\nPDP of target logSalePrice on feature YearMade\n\n\n\n\nIt looks like the log sale price tends to increase in a non-linear way with year made.\n\n\nCode\n# The PDPs for categorical features expect numeric data, not pandas categorical types,\n# so the sklearn API for partial dependence won't work directly with the dataframe we've been using.\n# The workaround is to create a new dataframe where categorical columns are encoded numerically,\n# retrain the XGBoost model using the sklearn interface, create the PDPs,\n# then add the category levels as the tick labels for the PDP.\n\ndef cat_pdp():\n    cat_feature = 'Enclosure'\n    modified_df = df.copy()\n    cat_codes = modified_df[cat_feature].cat.codes\n    cat_labels = list(modified_df[cat_feature].cat.categories)\n    cat_labels = ['NaN'] + cat_labels if -1 in cat_codes.unique() else cat_labels\n    modified_df[cat_feature] = cat_codes\n\n    n_valid = 12000\n    train_df, valid_df = train_test_split_temporal(modified_df, 'saledate', n_valid)\n    train_df.shape, valid_df.shape\n\n    # scikit-learn interface\n    reg = xgb.XGBRegressor(n_estimators=num_boost_round, enable_categorical=True, **params)\n    reg.fit(train_df[features], train_df[target], \n            eval_set=[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], \n            verbose=0);\n    PartialDependenceDisplay.from_estimator(reg, valid_df[features], [cat_feature], categorical_features=[cat_feature])\n    plt.xticks(ticks=cat_codes.unique(), labels=cat_labels)\ncat_pdp()\n\n\n\n\n\nPDP of target logSalePrice on categorical feature Enclosure\n\n\n\n\nYou can imagine how useful these model interpretation tools can be, both for understanding data and for improving your models."
  },
  {
    "objectID": "posts/xgboost-for-regression-in-python/index.html#wrapping-up",
    "href": "posts/xgboost-for-regression-in-python/index.html#wrapping-up",
    "title": "XGBoost for Regression in Python",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThere you have it, a simple flow for solving regression problems with XGBoost in python. Remember you can use the XGBoost regression notebook from my ds-templates repo to make it easy to follow this flow on your own problems. If you found this helpful, or if you have additional ideas about solving regression problems with XGBoost, let me know down in the comments."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html",
    "href": "posts/decision-tree-from-scratch/index.html",
    "title": "Decision Tree from Scratch",
    "section": "",
    "text": "Yesterday we had a lovely discussion about the key strengths and weaknesses of decision trees and why tree ensembles are so great. But today, gentle reader, we abandon our philosophizing and get down to the business of implementing one of these decision trees from scratch.\nA note before we get started. This is going to be the most involved scratch-build that we‚Äôve done at Random Realizations so far. It is not the kind of algorithm that I could just sit down and write all at once. We need to start with a basic frame and then add functionality step by step, testing all along the way to make sure things are working properly. Since I‚Äôm writing this in a jupyter notebook, I‚Äôll try to give you a sense for how I actually put the algorithm together interactively in pieces, eventually landing on a fully-functional final product.\nShall we?"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#binary-tree-data-structure",
    "href": "posts/decision-tree-from-scratch/index.html#binary-tree-data-structure",
    "title": "Decision Tree from Scratch",
    "section": "Binary Tree Data Structure",
    "text": "Binary Tree Data Structure\nA decision tree takes a dataset with features and a target, partitions the feature space into chunks, and assigns a prediction value to each chunk. Since each partitioning step divides one chunk in two, and since the partitioning is done recursively, it‚Äôs natural to use a binary tree data structure to represent a decision tree.\nThe basic idea of the binary tree is that we define a class to represent nodes in the tree. If we want to add children to a given node, we simply assign them as attributes of the parent node. The child nodes we add are themselves instances of the same class, so we can add children to them in the same way.\nLet‚Äôs start out with a simple class for our decision tree. It takes a single value called max_depth as input, which will dictate how many layers of child nodes should be inserted below the root. This controls the depth of the tree. As long as max_depth is positive, the parent will instantiate two new instances of the binary tree node class, passing along max_depth decremented by one and attaching the two children to itself as attributes called left and right.\n\nimport math\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nclass DecisionTree():\n\n        def __init__(self, max_depth):\n            assert max_depth &gt;= 0, 'max_depth must be nonnegative'\n            self.max_depth = max_depth\n            if max_depth &gt; 0:\n                self.left = DecisionTree(max_depth=max_depth-1)\n                self.right = DecisionTree(max_depth=max_depth-1)\n\nLet‚Äôs make a new instance of our decision tree class, a tree with depth 2.\n\nt = DecisionTree(max_depth=2)\n\n\n\n\nBinary tree structure diagram\n\n\nWe can access individual nodes and check their value of max_depth.\n\nt.max_depth, t.left.max_depth, t.left.right.max_depth\n\n(2, 1, 0)\n\n\nOur full decision tree can expand on this idea where each node receives some input, modifies it, creates two child nodes, and passes the modified input along to them. Specifically, each node in our decision tree will receive a dataset, determine how best to split the dataset into two parts, create two child nodes, and pass one part of the data to the left child and the other part to the right child.\nAll we have to do now is add some additional functionality to our decision tree. First we‚Äôll start by capturing all the inputs we need to grow a tree, which include the feature dataframe X, the target array y, max_depth to explicitly limit tree depth, min_samples_leaf to specify the minimum number of observations that are allowed in a leaf node, and an optional idxs which specifies the indices of data that the node should use. The indices argument is useful for users of our decision tree because it will allow them to implement row subsampling in ensemble methods like random forest. It will also be handy for internal use inside the decision tree when passing data along to child nodes; instead of passing copies of the two data subsets, we‚Äôll just pass a reference to the full dataset and pass along a set of indices to identify that node‚Äôs instance subset.\nOnce we get our input, we‚Äôll do a little bit of input validation and store things that we want to keep as object attributes. In case this is a leaf node, we‚Äôll go ahead and compute its predicted value; since this is a regression tree, the prediction is just the mean of the target y. We‚Äôll also go ahead and initialize a score metric which we‚Äôll use to help us find the best split later; since lower scores are going to be better, we‚Äôll initialize it to positive infinity. Finally, we‚Äôll push the logic to add child nodes into a method called _maybe_insert_child_nodes that we‚Äôll define next.\n\n\n\n\n\n\nNote\n\n\n\na leading underscore in a method name indicates the method is for internal use and not part of the user-facing API of the class.\n\n\n\nclass DecisionTree():\n\n    def __init__(self, X, y, min_samples_leaf=5, max_depth=6, idxs=None):\n        assert max_depth &gt;= 0, 'max_depth must be nonnegative'\n        assert min_samples_leaf &gt; 0, 'min_samples_leaf must be positive'\n        self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth\n        if isinstance(y, pd.Series): y = y.values\n        if idxs is None: idxs = np.arange(len(y))\n        self.X, self.y, self.idxs = X, y, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = np.mean(y[idxs]) # node's prediction value\n        self.best_score_so_far = float('inf') # initial loss before split finding\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n            \n    def _maybe_insert_child_nodes(self):\n        pass\n\nNow in order to test our class, we‚Äôll need some actual data. We can use the same scikit-learn diabetes data from the last post.\n\nfrom sklearn.datasets import load_diabetes\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\n\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=5)\n\nSo far, so good."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#inserting-child-nodes",
    "href": "posts/decision-tree-from-scratch/index.html#inserting-child-nodes",
    "title": "Decision Tree from Scratch",
    "section": "Inserting Child Nodes",
    "text": "Inserting Child Nodes\nOur node inserting function _maybe_insert_child_nodes needs to first find the best split; then if a valid split exists, it needs to insert the child nodes. To find the best valid split, we need to loop through the columns and search each one for the best valid split. Again we‚Äôll push the logic of finding the best split into a function that we‚Äôll define later. Next if no split was found, we need to bail by returning before trying to insert the child nodes. To check if this node is a leaf (i.e.¬†it shouldn‚Äôt have child nodes), we define a property called is_leaf which will just check if the best score so far is still infinity, in which case no split was found and the node is a leaf.\nIf a valid split was found, then we need to insert the child nodes. We‚Äôll assume that our split finding function assigned attributes called split_feature_idx and threshold to tell us the split feature‚Äôs index and the split threshold value. We then use these to compute the indices of the data to be passed to the child nodes; the left child gets instances where the split feature value is less than or equal to the threshold, and the right child node gets instances where the split feature value is greater than the threshold. Then we create two new decision trees, passing the corresponding data indices to each and assigning them to the left and right attributes of the current node.\n\n    def _maybe_insert_child_nodes(self):\n        for j in range(self.c): \n            self._find_better_split(j)\n        if self.is_leaf: # do not insert children\n            return \n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[left_idx])\n        self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[right_idx])\n\n    def _find_better_split(self, feature_idx):\n        pass\n    \n    @property\n    def is_leaf(self): return self.best_score_so_far == float('inf')\n\nTo test these new methods , we can assign them to our DecisionTree class and create a new class instance to make sure things are still working.\n\nDecisionTree._maybe_insert_child_nodes = _maybe_insert_child_nodes\nDecisionTree._find_better_split = _find_better_split\nDecisionTree.is_leaf = is_leaf\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=6)\n\nYep, we‚Äôre still looking good."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#split-finding",
    "href": "posts/decision-tree-from-scratch/index.html#split-finding",
    "title": "Decision Tree from Scratch",
    "section": "Split Finding",
    "text": "Split Finding\nNow we need to fill in the functionality of the split finding method. The overall strategy is to consider every possible way to split on the current feature, measuring the quality of each potential split with some scoring mechanism, and keeping track of the best split we‚Äôve seen so far. We‚Äôll come back to the issue of how to try all the possible splits in a moment, but let‚Äôs start by figuring out how to score a particular potential split.\nLike other machine learning models, trees are trained by attempting to minimize some loss function that measures how well the model predicts the target data. We‚Äôll be training our regression tree to minimize squared error.\n\\[ L = \\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\]\nFor a given node, we can replace \\(\\hat{y}\\) with \\(\\bar{y}\\) because each node uses the sample mean of its target instances as its prediction. We can then rewrite the loss for a given node as\n\\[ L = \\sum_{i=1}^n(y_i - \\bar{y})^2 \\] \\[  = \\sum_{i=1}^n(y_i^2 -2y_i\\bar{y} + \\bar{y}^2)  \\] \\[  = \\sum_{i=1}^ny_i^2 -2\\bar{y}\\sum_{i=1}^ny_i + n\\bar{y}^2 \\] \\[  = \\sum_{i=1}^ny_i^2 - \\frac{1}{n} \\left ( \\sum_{i=1}^ny_i \\right )^2 \\]\nWe can then evaluate potential splits by comparing the loss after splitting to the loss before splitting, where the split with the greatest loss reduction is best. Let‚Äôs work out a simple expression for the loss reduction from a given split.\nLet \\(I\\) be the set of \\(n\\) data instances in the current node, and let \\(I_L\\) and \\(I_R\\) be the instances that fall into the left and right child nodes of a proposed split. Let \\(L\\) be the total loss for all instances in the node, while \\(L_L\\) and \\(L_R\\) are the losses for the left and right child nodes. The total loss contributed by instances in \\(I\\) prior to any split is\n\\[L_{\\text{before split}} = L =  \\sum_{i \\in I} y_i^2 - \\frac{1}{n} \\left ( \\sum_{i \\in I} y_i \\right )^2 \\]\nAnd the loss after splitting \\(I\\) into \\(I_L\\) and \\(I_R\\) is\n\\[L_{\\text{after split}} = L_L + L_R =  \\sum_{i \\in I_L} y_i^2 - \\frac{1}{n_L} \\left ( \\sum_{i \\in I_L} y_i \\right )^2 + \\sum_{i \\in I_R} y_i^2 - \\frac{1}{n_R} \\left ( \\sum_{i \\in I_R} y_i \\right )^2 \\]\nThe reduction in loss from this split is\n\\[ \\Delta L = L_{\\text{after split}} -  L_{\\text{before split}} = (L_L + L_R) - L \\] \\[  = \\sum_{i \\in I_L} y_i^2 - \\frac{1}{n_L} \\left ( \\sum_{i \\in I_L} y_i \\right )^2 + \\sum_{i \\in I_R} y_i^2 - \\frac{1}{n_R} \\left ( \\sum_{i \\in I_R} y_i \\right )^2 - \\left ( \\sum_{i \\in I} y_i^2 - \\frac{1}{n} \\left ( \\sum_{i \\in I} y_i \\right )^2 \\right ) \\]\nSince \\(I = I_L \\cup I_R\\) the \\(\\sum y^2\\) terms cancel and we can simplify.\n\\[ \\Delta L = - \\frac{1}{n_L} \\left ( \\sum_{i \\in I_L} y_i \\right )^2\n- \\frac{1}{n_R} \\left ( \\sum_{i \\in I_R} y_i \\right )^2\n+ \\frac{1}{n} \\left ( \\sum_{i \\in I} y_i \\right )^2  \\]\nThis is a really nice formulation of the split scoring metric from a computational complexity perspective. We can sort the data by the feature values then, starting with the smallest min_samples_leaf instances in the left node and the rest in the right node, we check the score. Then to check the next split, we simply move a single target value from the right node into the left node, updating the score by subtracting it from the right node‚Äôs partial sum and adding it to the left node‚Äôs partial sum. The third term is constant for all splits, so we only need to compute it once. If any split‚Äôs score is lower than the best score so far, then we update the best score so far, the split feature, and the threshold value. When we‚Äôre done we can be sure we found the best possible split. The time bottleneck is the sort, which puts us at an average time complexity of \\(O(n\\log n)\\).\n\n    def _find_better_split(self, feature_idx):\n        x = self.X.values[self.idxs,feature_idx]\n        y = self.y[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_y, sort_x = y[sort_idx], x[sort_idx]\n        sum_y, n = y.sum(), len(y)\n        sum_y_right, n_right = sum_y, n\n        sum_y_left, n_left = 0., 0\n    \n        for i in range(0, self.n - self.min_samples_leaf):\n            y_i, x_i, x_i_next = sort_y[i], sort_x[i], sort_x[i + 1]\n            sum_y_left += y_i; sum_y_right -= y_i\n            n_left += 1; n_right -= 1\n            if  n_left &lt; self.min_samples_leaf or x_i == x_i_next:\n                continue\n            score = - sum_y_left**2 / n_left - sum_y_right**2 / n_right + sum_y**2 / n\n            if score &lt; self.best_score_so_far:\n                self.best_score_so_far = score\n                self.split_feature_idx = feature_idx\n                self.threshold = (x_i + x_i_next) / 2\n\nAgain, we assign the split finding method to our class and instantiate a new tree to make sure things are still working.\n\nDecisionTree._find_better_split = _find_better_split\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=6)\nX.columns[t.split_feature_idx], t.threshold\n\n('s5', -0.0037611760063045703)\n\n\nNice! Looks like the tree started with a split on the s5 feature."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#inspecting-the-tree",
    "href": "posts/decision-tree-from-scratch/index.html#inspecting-the-tree",
    "title": "Decision Tree from Scratch",
    "section": "Inspecting the Tree",
    "text": "Inspecting the Tree\nWhile we‚Äôre developing something complex like a decision tree class, we need a good way to inspect the object to help with testing and debugging. Let‚Äôs write a quick string representation method to make it easier to check what‚Äôs going on with a particular node.\n\n    def __repr__(self):\n        s = f'n: {self.n}'\n        s += f'; value:{self.value:0.2f}'\n        if not self.is_leaf:\n            split_feature_name = self.X.columns[self.split_feature_idx]\n            s += f'; split: {split_feature_name} &lt;= {self.threshold:0.3f}'\n        return s\n\nWe can assign the string representation method to the class and print a few nodes.\n\nDecisionTree.__repr__ = __repr__\nt = DecisionTree(X, y, min_samples_leaf=5, max_depth=2)\nprint(t)\nprint(t.left)\nprint(t.left.left)\n\nn: 442; value:152.13; split: s5 &lt;= -0.004\nn: 218; value:109.99; split: bmi &lt;= 0.006\nn: 171; value:96.31"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#prediction",
    "href": "posts/decision-tree-from-scratch/index.html#prediction",
    "title": "Decision Tree from Scratch",
    "section": "Prediction",
    "text": "Prediction\nWe need a public predict method that takes a feature dataframe and returns an array of predictions. We‚Äôll need to look up the predicted value for one instance at a time and stitch them together in an array. We can do that by iterating over the feature dataframe rows with a list comprehension that calls a _predict_row method to grab the prediction for each row. The row predict method needs to return the current node‚Äôs predicted value if it‚Äôs a leaf, or if not, it needs to identify the appropriate child node based on its split and ask it for a prediction.\n\n    def predict(self, X):\n        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n    \n    def _predict_row(self, row):\n        if self.is_leaf: \n            return self.value\n        child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n                else self.right\n        return child._predict_row(row)\n\nLet‚Äôs assign the predict methods and make predictions on a few rows.\n\nDecisionTree.predict = predict\nDecisionTree._predict_row = _predict_row\nt.predict(X.iloc[:3, :])\n\narray([225.87962963,  96.30994152, 225.87962963])"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#the-complete-decision-tree-implementation",
    "href": "posts/decision-tree-from-scratch/index.html#the-complete-decision-tree-implementation",
    "title": "Decision Tree from Scratch",
    "section": "The Complete Decision Tree Implementation",
    "text": "The Complete Decision Tree Implementation\nHere‚Äôs the implementation, all in one place.\n\nclass DecisionTree():\n\n    def __init__(self, X, y, min_samples_leaf=5, max_depth=6, idxs=None):\n        assert max_depth &gt;= 0, 'max_depth must be nonnegative'\n        assert min_samples_leaf &gt; 0, 'min_samples_leaf must be positive'\n        self.min_samples_leaf, self.max_depth = min_samples_leaf, max_depth\n        if isinstance(y, pd.Series): y = y.values\n        if idxs is None: idxs = np.arange(len(y))\n        self.X, self.y, self.idxs = X, y, idxs\n        self.n, self.c = len(idxs), X.shape[1]\n        self.value = np.mean(y[idxs]) # node's prediction value\n        self.best_score_so_far = float('inf') # initial loss before split finding\n        if self.max_depth &gt; 0:\n            self._maybe_insert_child_nodes()\n            \n    def _maybe_insert_child_nodes(self):\n        for j in range(self.c): \n            self._find_better_split(j)\n        if self.is_leaf: # do not insert children\n            return \n        x = self.X.values[self.idxs,self.split_feature_idx]\n        left_idx = np.nonzero(x &lt;= self.threshold)[0]\n        right_idx = np.nonzero(x &gt; self.threshold)[0]\n        self.left = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[left_idx])\n        self.right = DecisionTree(self.X, self.y, self.min_samples_leaf, \n                                  self.max_depth - 1, self.idxs[right_idx])\n    \n    @property\n    def is_leaf(self): return self.best_score_so_far == float('inf')\n    \n    def _find_better_split(self, feature_idx):\n        x = self.X.values[self.idxs,feature_idx]\n        y = self.y[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_y, sort_x = y[sort_idx], x[sort_idx]\n        sum_y, n = y.sum(), len(y)\n        sum_y_right, n_right = sum_y, n\n        sum_y_left, n_left = 0., 0\n    \n        for i in range(0, self.n - self.min_samples_leaf):\n            y_i, x_i, x_i_next = sort_y[i], sort_x[i], sort_x[i + 1]\n            sum_y_left += y_i; sum_y_right -= y_i\n            n_left += 1; n_right -= 1\n            if  n_left &lt; self.min_samples_leaf or x_i == x_i_next:\n                continue\n            score = - sum_y_left**2 / n_left - sum_y_right**2 / n_right + sum_y**2 / n\n            if score &lt; self.best_score_so_far:\n                self.best_score_so_far = score\n                self.split_feature_idx = feature_idx\n                self.threshold = (x_i + x_i_next) / 2\n                \n    def __repr__(self):\n        s = f'n: {self.n}'\n        s += f'; value:{self.value:0.2f}'\n        if not self.is_leaf:\n            split_feature_name = self.X.columns[self.split_feature_idx]\n            s += f'; split: {split_feature_name} &lt;= {self.threshold:0.3f}'\n        return s\n    \n    def predict(self, X):\n        return np.array([self._predict_row(row) for i, row in X.iterrows()])\n    \n    def _predict_row(self, row):\n        if self.is_leaf: \n            return self.value\n        child = self.left if row[self.split_feature_idx] &lt;= self.threshold \\\n                else self.right\n        return child._predict_row(row)"
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#from-scratch-versus-scikit-learn",
    "href": "posts/decision-tree-from-scratch/index.html#from-scratch-versus-scikit-learn",
    "title": "Decision Tree from Scratch",
    "section": "From Scratch versus Scikit-Learn",
    "text": "From Scratch versus Scikit-Learn\nAs usual, we‚Äôll test our homegrown handiwork by comparing it to the existing implementation in scikit-learn. First let‚Äôs train both models on the California Housing dataset which gives us 20k instances and 8 features to predict median house price by district.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\nX, y = fetch_california_housing(as_frame=True, return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)\n\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmax_depth = 8\nmin_samples_leaf = 16\n\ntree = DecisionTree(X_train, y_train, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\npred = tree.predict(X_test)\n\nsk_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\nsk_tree.fit(X_train, y_train)\nsk_pred = sk_tree.predict(X_test)\n\nprint(f'from scratch MSE: {mean_squared_error(y_test, pred):0.4f}')\nprint(f'scikit-learn MSE: {mean_squared_error(y_test, sk_pred):0.4f}')\n\nfrom scratch MSE: 0.3988\nscikit-learn MSE: 0.3988\n\n\nWe get similar accuracy on a held-out test dataset.\nLet‚Äôs benchmark the two implementations on training time.\n\n%%time\nsk_tree = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\nsk_tree.fit(X_train, y_train);\n\nCPU times: user 45.3 ms, sys: 555 ¬µs, total: 45.8 ms\nWall time: 45.3 ms\n\n\nDecisionTreeRegressor(max_depth=8, min_samples_leaf=16)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=8, min_samples_leaf=16)\n\n\n\n%%time\ntree = DecisionTree(X_train, y_train, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n\nCPU times: user 624 ms, sys: 1.65 ms, total: 625 ms\nWall time: 625 ms\n\n\nWow, the scikit-learn implementation absolutely smoked us, training an order of magnitude faster. This is to be expected, since they implement split finding in cython, which generates compiled C code that can run much faster than our native python code. Maybe we can take a look at how to optimize python code with cython here on the blog one of these days."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#wrapping-up",
    "href": "posts/decision-tree-from-scratch/index.html#wrapping-up",
    "title": "Decision Tree from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nHoly cow, we just implemented a decision tree using nothing but numpy. I hope you enjoyed the scratch build as much as I did, and I hope you got a little bit better at coding (I certainly did). That was actually way harder than I expected, but looking back at the finished product, it doesn‚Äôt seem so bad right? I almost thought we were going to get away with not implementing our own decision tree, but it turns out that this will be super helpful for us when it comes time to implement XGBoost from scratch."
  },
  {
    "objectID": "posts/decision-tree-from-scratch/index.html#references",
    "href": "posts/decision-tree-from-scratch/index.html#references",
    "title": "Decision Tree from Scratch",
    "section": "References",
    "text": "References\nThis implementation is inspired and partially adapted from Jeremy Howard‚Äôs live coding of a Random Forest as part of the fastai ML course."
  },
  {
    "objectID": "posts/random-realizations-resurrected/index.html",
    "href": "posts/random-realizations-resurrected/index.html",
    "title": "Random Realizations Resurrected",
    "section": "",
    "text": "Christ the Redeemer towers into a vast blue Brazilian sky.!\n\n\nWell it‚Äôs been over a year since I posted anything here. You see, a lot has been going on here at the Random Realizations Remote Global Headquarters that has distracted from producing the high-quality data science content that you‚Äôre used to. Mostly I went on hiatus from work and started traveling, which turns out to be it‚Äôs own full time job. I had aspirations of writing more after leaving work, but of course, after leaving, I couldn‚Äôt be bothered to sit down at my laptop and type stuff about data science to yall. After all, life is bigger than that.\nWhen I finally felt like opening up my laptop, I was confronted with an email from the maintainers of fastpages, the open source content management system (CMS) I originally used to create this blog, notifying me that the project was being deprecated and that I would need to migrate my content to some other platform.\nBoo.\nThat didn‚Äôt sound like much fun, so I spent another few months ignoring the blog. But eventually, dear reader, I decided it was time to roll up my sleeves and get this blog thriving once again.\nOk so fastpages was going to be deprecated, and I needed to find a new CMS. My requirements were pretty simple: I wanted to write the blog posts with jupyter notebook, and I wanted to host the site on my own domain. Helpfully, the former maintainers of fastpages recommended an alternative CMS called Quarto which I had never heard of. Apparently I had been living under a rock because Quarto appears to be all the rage. Quarto‚Äôs website says it‚Äôs an open-source scientific and technical publishing system. I think it‚Äôs fair to think of it as a way to render plain text or source code from languages like python, R, and julia into a variety of different published formats like websites, books, or journal articles. It was developed by the good folks over at RStudio, and the project has a pretty active following over on github, so I think it‚Äôs less likely to suddenly disappear like fastpages.\nSo anyway, I‚Äôve been migrating my content over into this new quarto universe.\nYou mayofficially consider this blog resurrected from the dead, because this is the first new post published after the migration. The site has a bit of a new look and feel, so I hope you like it. Do let me know in the comments if you find anything amiss with the new website. Otherwise we‚Äôll just assume it‚Äôs fabulous.\nI‚Äôm working on a post about how to create a blog with quarto using jupyter and python, so you can too!\nSee you in more posts real soon! Love, Matt."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "",
    "text": "Tell me dear reader, who among us, while gazing in wonder at the improbably verdant aloe vera clinging to the windswept rock at Cape Point near the southern tip of Africa, hasn‚Äôt wondered: how the heck do gradient boosting trees implement multi-class classification? Today, we‚Äôll unravel this mystery by reviewing the theory and implementing the algorithm for ourselves in python. Specifically, we‚Äôll review the multi-class gradient boosting model originally described in Friedman‚Äôs classic Greedy Function Approximation paper, and we‚Äôll implement components of the algorithm as we go along. Once we have all the pieces, we‚Äôll write a python class for multi-class gradient boosting with a similar API to the scikit-learn GradientBoostingClassifier.\nIf you need a refresher on gradient boosting before diving in here, then start with my original gradient boosting from scratch post, which is the first installment in my ongoing series on gradient boosting."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-multi-class-gradient-boosting-classification-algorithm",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-multi-class-gradient-boosting-classification-algorithm",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "The multi-class gradient boosting classification algorithm",
    "text": "The multi-class gradient boosting classification algorithm\nFriedman describes the algorithm for training a multi-class classification gradient boosting model in Algorithm 6 of the classic Greedy Function Approximation paper. If you want a step-by-step walkthrough of the ideas in the paper, have a look at my post on the generalized gradient boosting algorithm. In high-level terms, the algorithm for multi-class gradient boosting is:\n\nSet the initial model predictions.\nRepeat the following for each boosting round.\n¬†¬†¬†¬† Repeat the following for each class.\n¬†¬†¬†¬† ¬†¬†¬†¬† Compute the pseudo residuals.\n¬†¬†¬†¬† ¬†¬†¬†¬† Train a regression tree to predict the pseudo residuals.\n¬†¬†¬†¬† ¬†¬†¬†¬† Adjust the tree‚Äôs predicted values to optimize the objective function.\n¬†¬†¬†¬† ¬†¬†¬†¬† Add the new tree to the current composite model.\n\nLet‚Äôs take a look at the details for each of these steps."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#target-variable-encoding",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#target-variable-encoding",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Target variable encoding",
    "text": "Target variable encoding\nFollowing the convention in scikit-learn, when training a multi-class classifier, the target variable in the training dataset should be integer encoded so that the \\(K\\) distinct classes are mapped to the integers \\(0,1,\\dots,K-1\\). In the code for model training, however, it‚Äôs going to be more convenient to work with a one hot encoded representation of the target. Therefore we‚Äôll start by writing an internal method to transform the target variable from integer encoding to one hot encoding. Remember that eventually we‚Äôll write a class for our multi-class gradient boosting model, so I‚Äôll write this function like a class method with a leading argument called self.\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import OneHotEncoder\n\ndef _one_hot_encode_labels(self, y):\n    if isinstance(y, pd.Series): y = y.values\n    ohe = OneHotEncoder()\n    y_ohe = ohe.fit_transform(y.reshape(-1, 1)).toarray()\n    return y_ohe\nThis code takes the integer-encoded target variable, makes sure it‚Äôs a numpy array, then uses cikit-learn‚Äôs one hot encoder to encode it as a 2D array with observations along the first axis and classes along the second axis. I tend to think of the one hot encoded output as a matrix with \\(n\\) rows (the number of observations in the training data) and \\(K\\) columns (the number of classes), although it‚Äôs technically not a matrix but rather a 2D array."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#model-predictions-in-raw-space-and-probability-space",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#model-predictions-in-raw-space-and-probability-space",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Model predictions in raw space and probability space",
    "text": "Model predictions in raw space and probability space\nIn amulti-class classification problem with \\(K\\) classes, the model prediction for a particular observation returns a list of \\(K\\) probabilities, one for each class. Essentially the model prediction is a conditional probability mass function for the discrete target variable, conditioned on the feature values.\nSo, we need a way to ensure that the model output is a valid probability mass function, i.e.¬†each probability is in (0, 1) and the \\(K\\) class probabilities sum to 1. Analogous to logistic regression, we can accomplish this by using the model to first make a raw prediction which can be any real number, then using something like the inverse logit function to transform the raw model prediction into a number between 0 and 1 that can be interpreted as a probability. Again analogous to logistic regression, in the multi-class setting we use \\(K\\) different models, one for each class, to generate the raw predictions, then we transform the raw model predictions into probabilities using the softmax function,, which takes a length-\\(K\\) vector of real numbers as input and returns a probability mass function over \\(K\\) discrete classes.\nLet \\(\\{F_1(\\mathbf{x}),\\dots,F_K(\\mathbf{x})\\}=\\{F_k(\\mathbf{x})\\}_1^K\\) be the list of \\(K\\) raw model outputs, and let \\(\\{p_1(\\mathbf{x}),\\dots,p_K(\\mathbf{x})\\}=\\{p_k(\\mathbf{x})\\}_1^K\\) be the corresponding probability mass function over the \\(K\\) classes, then the softmax function is defined as\n\\[ p_k(\\mathbf{x}) = \\text{softmax}_k(\\{F_k(\\mathbf{x})\\}_1^K)\n    = \\frac{e^{F_k(\\mathbf{x})}}{\\sum_{l=1}^K e^{F_l(\\mathbf{x})}}\\]\nLet's implement an internal softmax method that transforms the raw predictions into probabilities.\ndef _softmax(self, raw_predictions):\n    numerator = np.exp(raw_predictions) \n    denominator = np.sum(np.exp(raw_predictions), axis=1).reshape(-1, 1)\n    return numerator / denominator"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#initial-model-predictions",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#initial-model-predictions",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Initial model predictions",
    "text": "Initial model predictions\nWe‚Äôre now ready to implement model training, starting with line 1 of the algorithm which sets the initial model predictions. In our code, we‚Äôll keep the raw model predictions \\(\\{F_k(\\mathbf{x})\\}_1^K\\) for the \\(n\\) observations in the training dataset in a size \\(n \\times K\\) array called raw_predictions, and we‚Äôll keep the corresponding probabilities \\(\\{p_k(\\mathbf{x})\\}_1^K\\) in another \\(n \\times K\\) array called probabilities. Perhaps the simplest reasonable initialization is to set the probabilities to \\(1/K\\), i.e.¬†\\(p_k(\\mathbf{x})=1/K\\), which implies \\(F_k(\\mathbf{x})=0\\).\nWe‚Äôll go ahead and create that one hot encoded representation of the target, then use it to set the right size for the model prediction arrays.\ny_ohe = self._one_hot_encode_labels(y)\nraw_predictions = np.zeros(shape=y_ohe.shape)\nprobabilities = self._softmax(raw_predictions)"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#boosting",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#boosting",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Boosting",
    "text": "Boosting\nLine 2 of the algorithm kicks off a loop to iteratively perform boosting rounds. Within each round, line 3 specifies that we iterate through each of the \\(K\\) classes, adding a new booster model for each class at each boosting round. We‚Äôll keep all the boosters in a list called boosters, where each element is itself a list which we‚Äôll call class_trees that contains the \\(K\\) trees we trained in a given boosting round. For each round and each class, we compute the pseudo residuals (negative gradients), train a decision tree to predict them, update the tree‚Äôs predicted values to optimize the overall objective function, then update the current raw and probability predictions before storing the new tree in that round‚Äôs list of class trees.\nself.boosters = []\nfor m in range(self.n_estimators):\n    class_trees = []\n    for k in range(self.n_classes):\n        negative_gradients = self._negative_gradients(y_ohe[:, k], probabilities[:, k])\n        hessians = self._hessians(probabilities[:, k])\n        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n        tree.fit(X, negative_gradients);\n        self._update_terminal_nodes(tree, X, negative_gradients, hessians)\n        raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n        probabilities = self._softmax(raw_predictions)\n        class_trees.append(tree)\n    self.boosters.append(class_trees)\nNext we‚Äôll dive into the details of the pseudo residual computation and the adjustment to the tree booster predicted values."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#pseudo-residuals",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#pseudo-residuals",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Pseudo Residuals",
    "text": "Pseudo Residuals\nFor each observation in the training dataset, the pseudo residual is the negative gradient of the objective function with respect to the corresponding model prediction. The objective function for multi-class classification is the Multinomial Negative Log Likelihood. For a single observation, the objective is\n\\[ J(\\{ y_k, p_k(\\mathbf{x}) \\}_1^K) = -\\sum_{k=1}^K y_k \\log p_k(\\mathbf{x}) \\]\nWe can rewrite the objective in terms of our raw model output \\(F\\) like this.\n\\[ J(\\{ y_k, F_k(\\mathbf{x}) \\}_1^K) = -\\sum_{k=1}^K y_k \\log \\frac{e^{F_k(\\mathbf{x})}}{\\sum_{l=1}^K e^{F_l(\\mathbf{x})}}\\]\nThe negative gradient of the objective with respect to raw model prediction \\(F_k(\\mathbf{x}_i)\\) for training example \\(i\\) is given by\n\\[ r_{ik} = -J'(F_k(\\mathbf{x}_i)) = -\\left[ \\frac{\\partial J(\\{ y_{il}, F_l(\\mathbf{x_i})\\}_{l=1}^K)}{\\partial F_k(\\mathbf{x}_i) } \\right]\n=y_{ik} - p_{k}(\\mathbf{x}_i)\\]\nYou can take a look at the derivation if you‚Äôre curious how to work it out yourself. Note that this formula has a nice intuition. When \\(y_{ik}=1\\), if predicted probability \\(p_k(\\mathbf{x}_i)\\) is terrible and close to 0, then the pseudo residual will be positive, and the next boosting round will try to increase the predicted probability. Otherwise if the predicted probability is already good and close to 1, the pseudo residual will be close to 0 and the next boosting round won‚Äôt change the predicted probability very much.\nWe can easily implement an internal method to compute the negative gradients over the training dataset as follows.\ndef _negative_gradients(self, y_ohe, probabilities):\n    return y_ohe - probabilities"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#adjusting-the-trees-predicted-values",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#adjusting-the-trees-predicted-values",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Adjusting the trees‚Äô predicted values",
    "text": "Adjusting the trees‚Äô predicted values\nAfter training a regression tree to predict the pseudo residuals, we need to adjust the predicted values in its terminal nodes to optimize the overall objective function. In the Greedy Function Approximation paper, Friedman actually specifies finding the optimal value using a numerical optimization routine like line search. We could express that like\n\\[ v = \\text{argmin}_v \\sum_{i \\in t} J(y_{i}, F(\\mathbf{x}_i) + v) \\]\nwhere \\(t\\) is the set of samples falling into this terminal node.\nIn the scikit-learn implementation of gradient boosting classification, the authors instead use the approach from FHT00 which uses a single Newton descent step to approximate the optimal predicted value for each terminal node. See code and comments for the function _update_terminal_regions in the scikit-learn gradient boosting module. The updated value is computed like\n\\[ v = -\\frac{\\sum_{i \\in t} J'(F(\\mathbf{x}_i))}{\\sum_{i \\in t} J''(F(\\mathbf{x}_i))} \\]\nWe already found the first derivative of the objective, so we just need to calculate the second derivative.\n\\[ J''(F_k(\\mathbf{x}_i)) =\n\\left[ \\frac{\\partial J(\\{ y_{il}, F_l(\\mathbf{x_i})\\}_{l=1}^K)}{\\partial ^2 F_k(\\mathbf{x}_i) } \\right]\n= p_k(\\mathbf{x}_i) (1 - p_k(\\mathbf{x}_i))\n\\]\nHere‚Äôs the internal method to compute the second derivative .\ndef _hessians(self, probabilities): \n    return probabilities * (1 - probabilities)\nThen we can implement the internal method for updating the tree predicted values. I give more details about how to manually set scikit-learn‚Äôs decision tree predicted values in the post on gradient boosting with any loss function.\ndef _update_terminal_nodes(self, tree, X, negative_gradients, hessians):\n    '''Update the terminal node predicted values'''\n    # terminal node id's\n    leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n    # compute leaf for each sample in ``X``.\n    leaf_node_for_each_sample = tree.apply(X)\n    for leaf in leaf_nodes:\n        samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n        negative_gradients_in_leaf = negative_gradients.take(samples_in_this_leaf, axis=0)\n        hessians_in_leaf = hessians.take(samples_in_this_leaf, axis=0)\n        val = np.sum(negative_gradients_in_leaf) / np.sum(hessians_in_leaf)\n        tree.tree_.value[leaf, 0, 0] = val"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#prediction",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#prediction",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Prediction",
    "text": "Prediction\nAt inference time, the user supplies an X with multiple observations of the feature variables, and our model needs to issue a prediction for each observation. We‚Äôll start by implementing the predict_proba method, which takes X as input and returns a length-\\(K\\) probability mass function for each observation in X. To do this, we‚Äôll initialize the raw predictions with zeros, just as we did in training, and then for each class, we‚Äôll loop through all the boosters, collecting their predictions on X, scaling by the learning rate, and summing them up. Finally, we use the softmax to transform raw predictions into the probabilities.\ndef predict_proba(self, X):\n    '''Generate probability predictions for the given input data.'''\n    raw_predictions =  np.zeros(shape=(X.shape[0], self.n_classes))\n    for k in range(self.n_classes):\n        for booster in self.boosters:\n            raw_predictions[:, k] +=self.learning_rate * booster[k].predict(X)\n    probabilities = self._softmax(raw_predictions)\n    return probabilities\nThen to get the predicted labels, we can use the predict_proba method to generate probabilities, simply returning the integer-encoded class label of the largest probability for each observation in X.\ndef predict(self, X):\n    '''Generate predicted labels (as integer-encoded array)'''\n    probabilities = self.predict_proba(X)\n    return np.argmax(probabilities, axis=1)"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-complete-multi-class-gradient-boosting-classification-model-implementation",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#the-complete-multi-class-gradient-boosting-classification-model-implementation",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "The complete multi-class gradient boosting classification model implementation",
    "text": "The complete multi-class gradient boosting classification model implementation\nNow we‚Äôre ready to implement a multi-class classification gradient boosting model class with public fit, predict_proba, and predict methods. We combine the components above into a fit method for model training, and we add the two prediction methods to complete the model‚Äôs functionality.\n\nimport numpy as np\nimport pandas as pd \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.preprocessing import OneHotEncoder\n\nclass GradientBoostingClassifierFromScratch():\n    '''Gradient Boosting Classifier from Scratch.\n    \n    Parameters\n    ----------\n    n_estimators : int\n        number of boosting rounds\n        \n    learning_rate : float\n        learning rate hyperparameter\n        \n    max_depth : int\n        maximum tree depth\n    '''\n    \n    def __init__(self, n_estimators, learning_rate=0.1, max_depth=1):\n        self.n_estimators=n_estimators; \n        self.learning_rate=learning_rate\n        self.max_depth=max_depth;\n    \n    def fit(self, X, y):\n        '''Fit the GBM\n        \n        Parameters\n        ----------\n        X : ndarray of size (number observations, number features)\n            design matrix\n            \n        y : ndarray of size (number observations,)\n            integer-encoded target labels in {0,1,...,k-1}\n        '''\n        \n        self.n_classes = pd.Series(y).nunique()\n        y_ohe = self._one_hot_encode_labels(y)\n\n        raw_predictions = np.zeros(shape=y_ohe.shape)\n        probabilities = self._softmax(raw_predictions)\n        self.boosters = []\n        for m in range(self.n_estimators):\n            class_trees = []\n            for k in range(self.n_classes):\n                negative_gradients = self._negative_gradients(y_ohe[:, k], probabilities[:, k])\n                hessians = self._hessians(probabilities[:, k])\n                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n                tree.fit(X, negative_gradients);\n                self._update_terminal_nodes(tree, X, negative_gradients, hessians)\n                raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n                probabilities = self._softmax(raw_predictions)\n                class_trees.append(tree)\n            self.boosters.append(class_trees)\n    \n    def _one_hot_encode_labels(self, y):\n        if isinstance(y, pd.Series): y = y.values\n        ohe = OneHotEncoder()\n        y_ohe = ohe.fit_transform(y.reshape(-1, 1)).toarray()\n        return y_ohe\n        \n    def _negative_gradients(self, y_ohe, probabilities):\n        return y_ohe - probabilities\n    \n    def _hessians(self, probabilities): \n        return probabilities * (1 - probabilities)\n\n    def _softmax(self, raw_predictions):\n        numerator = np.exp(raw_predictions) \n        denominator = np.sum(np.exp(raw_predictions), axis=1).reshape(-1, 1)\n        return numerator / denominator\n        \n    def _update_terminal_nodes(self, tree, X, negative_gradients, hessians):\n        '''Update the terminal node predicted values'''\n        # terminal node id's\n        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n        # compute leaf for each sample in ``X``.\n        leaf_node_for_each_sample = tree.apply(X)\n        for leaf in leaf_nodes:\n            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n            negative_gradients_in_leaf = negative_gradients.take(samples_in_this_leaf, axis=0)\n            hessians_in_leaf = hessians.take(samples_in_this_leaf, axis=0)\n            val = np.sum(negative_gradients_in_leaf) / np.sum(hessians_in_leaf)\n            tree.tree_.value[leaf, 0, 0] = val\n          \n    def predict_proba(self, X):\n        '''Generate probability predictions for the given input data.'''\n        raw_predictions =  np.zeros(shape=(X.shape[0], self.n_classes))\n        for k in range(self.n_classes):\n            for booster in self.boosters:\n                raw_predictions[:, k] +=self.learning_rate * booster[k].predict(X)\n        probabilities = self._softmax(raw_predictions)\n        return probabilities\n        \n    def predict(self, X):\n        '''Generate predicted labels (as 1-d array)'''\n        probabilities = self.predict_proba(X)\n        return np.argmax(probabilities, axis=1)"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#testing-our-implementation",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#testing-our-implementation",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Testing our implementation",
    "text": "Testing our implementation\nLet‚Äôs test our implementation alongside the scikit-learn GradientBoostingClassifier to ensure it works as expected.\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX, y = make_classification(n_samples=10000, \n                           n_classes=5, \n                           n_features=20,\n                           n_informative=10,\n                           random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(n_estimators=10, \n                                 learning_rate=0.3, \n                                 max_depth=6)\ngbc.fit(X_train, y_train)\naccuracy_score(y_test, gbc.predict(X_test))\n\n0.7756\n\n\n\ngbcfs = GradientBoostingClassifierFromScratch(n_estimators=10, \n                                              learning_rate=0.3, \n                                              max_depth=6)\ngbcfs.fit(X_train, y_train)\naccuracy_score(y_test, gbcfs.predict(X_test))\n\n0.7768\n\n\nBeautiful. Our implementation is performing comparably to the sklearn gradient boosting classifier!"
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#wrapping-up",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#wrapping-up",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWell, there you have it, another epic scratch build for the books. I think the most interesting thing about the multi-class gradient boosting algorithm is that it generates multi-dimensional predictions based on a single objective function by training multiple decision trees in each boosting round. That‚Äôs a very interesting extension of the classic gradient boosting machine! If you have questions about the implementation, or if you found this post helpful, please leave a comment below to tell me about it."
  },
  {
    "objectID": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#references",
    "href": "posts/gradient-boosting-multi-class-classification-from-scratch/index.html#references",
    "title": "Gradient Boosting Multi-Class Classification from Scratch",
    "section": "References",
    "text": "References\n\nFriedman‚Äôs Greedy Function Approximation paper\nFriedman, Hastie, and Tibshirani 2000: paper on additive logistic regression"
  },
  {
    "objectID": "posts/traditional-vs-roth-401k/index.html",
    "href": "posts/traditional-vs-roth-401k/index.html",
    "title": "Analyzing After Tax Retirement Income: Roth vs.¬†Traditional 401(k)",
    "section": "",
    "text": "Today we‚Äôre taking a break from our typical hard hitting algorithm deep dives for a quick foray into the world of personal finance. We‚Äôll take on a question I recently encountered while setting up my retirement account with my new employer‚Äîwhich is more efficient, the traditional 401(k) or the Roth 401(k)? US-based readers will recognize these as the two main types of employer-sponsored retirement accounts. When I searched for traditional vs Roth 401(k), the articles I found gave only very hand-wavy guidance on which is better in a given situation. So, today I‚Äôll share my quantitative analysis of which account type provides superior performance for a given set of personal circumstances. We‚Äôll implement the analysis in python, so you can run the numbers for your own situation and determine which employer-sponsored account type is better for you."
  },
  {
    "objectID": "posts/traditional-vs-roth-401k/index.html#traditional-401k-vs-roth-401k",
    "href": "posts/traditional-vs-roth-401k/index.html#traditional-401k-vs-roth-401k",
    "title": "Analyzing After Tax Retirement Income: Roth vs.¬†Traditional 401(k)",
    "section": "Traditional 401(k) vs Roth 401(k)",
    "text": "Traditional 401(k) vs Roth 401(k)\nI‚Äôll let JLCollins explain the background on 401(k)s; read that post first if you‚Äôre not already familiar with the concepts of taxable accounts, IRAs, 401(k)s, and the basic rules of Roth vs traditional. The key distinction is * In a traditional 401(k), money you contribute now is deducted from your taxable income, meaning you‚Äôll pay less in income tax now. During retirement however, withdrawals from the account will count toward your taxable income, so you‚Äôll pay tax then. * In a Roth 401(k), money you contribute now does count toward your taxable income, meaning you‚Äôll pay income tax on any contributions now. During retirement however, withdrawals do not count toward your taxable income and are therefore tax free.\nEssentially you can either pay tax now (Roth) or pay tax later (traditional). The hand-wavy advice points out that which account is better for you depends on your income tax rate now versus your income tax rate during retirement. High tax rate now and low tax rate during retirement could favor traditional, while low tax rate now and high tax rate during retirement could favor Roth. Let‚Äôs put some numbers on this advice.\nI‚Äôll assume that you‚Äôre following the sage advice of Mr.¬†Money Mustache and (after paying off any high-interest debt) maxing out your 401(k) contribution for the year. In 2024, the IRS has set a maximum combined contribution of $23,000; i.e.¬†the sum of your Roth and traditional contributions cannot exceed this limit. Also, once you contribute to these accounts, you may not begin withdrawals (without penalty) until the age of 59.5."
  },
  {
    "objectID": "posts/traditional-vs-roth-401k/index.html#analysis-formulation",
    "href": "posts/traditional-vs-roth-401k/index.html#analysis-formulation",
    "title": "Analyzing After Tax Retirement Income: Roth vs.¬†Traditional 401(k)",
    "section": "Analysis Formulation",
    "text": "Analysis Formulation\nLet‚Äôs state the question precisely‚Äîwhich account type will yield me the most money during retirement after withdrawal and after all taxes are paid? Let‚Äôs think through the Roth vs traditional scenarios, setting aside the same amount of money today and liquidating the entire account at retirement; we‚Äôll compare how much money we have at retirement after liquidating and settling any tax obligations.\nRoth: I contribute contribution = 23_000 now, plus I pay income tax on this contribution in the amount of current_income_tax_rate * contribution. Over the years from now to retirement retirement_age - current_age, my contribution grows at some average long term yearly rate investment_growth_rate. At retirement, I liquidate the entire account, paying no income tax on the proceeds.\nTraditional: I contribute contribution = 23_000 now. For fair comparison with the Roth, I invest an additional amount current_income_tax_rate * contribution (the extra income tax I would have paid had I chosen the Roth) in a normal taxable investment account as well. Over the time from now to retirement, the 401(k) and the taxable account both grow at the average long term rate investment_growth_rate. However, in the taxable account, I‚Äôll also need to pay income tax every year on any dividends that I earn; the S&P500 has recently paid out 1.5-2% in dividends each year, let‚Äôs call it dividend_rate. At retirement, I liquidate both accounts, paying income tax on the proceeds from the 401(k) at the rate of retirement_income_tax_rate and paying capital gains tax on the proceeds from the taxable account at the rate of retirement_capital_gains_tax_rate.\nLet‚Äôs code up a function that takes in all our parameters and returns the total liquidation value after taxes of the Roth versus traditional 401(k)s as described above.\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\n\ndef get_401k_liquidation_value(\n    current_age = 37,\n    current_income_tax_rate = 0.35,\n    contribution = 23_000,\n    retirement_age = 59.5,\n     investment_growth_rate = 0.07,\n     dividend_rate = 0.02,\n    retirement_income_tax_rate = 0.24,\n    retirement_capital_gains_tax_rate = 0.15 # 0%, 15%, 20%\n):\n\n    investment_growth_factor = (1 + investment_growth_rate) ** (retirement_age - current_age)\n    dividend_income_tax_drag_factor = (1 - dividend_rate * current_income_tax_rate) ** (retirement_age - current_age)\n\n    # Roth 401k\n    roth_401k_value = contribution \n    roth_401k_value *= investment_growth_factor\n    total_roth_401k_liquidation_value = roth_401k_value\n    \n    # traditional  401k\n    traditional_401k_value = contribution \n    taxable_account_value = current_income_tax_rate * contribution \n    traditional_401k_value *= investment_growth_factor\n    taxable_account_value *= investment_growth_factor * dividend_income_tax_drag_factor\n    traditional_401k_value *= (1 - retirement_income_tax_rate)\n    taxable_account_value *= (1 - retirement_capital_gains_tax_rate)\n    total_traditional_401k_liquidation_value = traditional_401k_value + taxable_account_value\n\n    return {\n        'traditional': round(total_traditional_401k_liquidation_value), \n        'roth': round(total_roth_401k_liquidation_value)\n    }\n\n\nget_401k_liquidation_value()\n\n{'traditional': 106882, 'roth': 105405}\n\n\nSomehow it‚Äôs not surprising that these two options seem to yield very similar after-tax performance‚Äîno arbitrage right?\nLet‚Äôs write a function to perturb some of our parameter values to see under what conditions one option dominates the other.\n\ndef plot_liquidation_value_by_parameter_values(param, grid_values, func=get_401k_liquidation_value):\n    y = [func(**{param: x}) for x in grid_values]\n    df = pd.DataFrame(y, index=pd.Series(grid_values, name=param))\n    fig, ax = plt.subplots()\n    df.plot(ax=ax)\n    plt.ylabel('liquidation value')\n    plt.title(f'Liquidation Value at Retirement by {param}')\n    return fig, ax\n\n\nIncome Tax Rate at Retirement\nIt seems that income tax rate at retirement is by far the most important determining factor in whether traditional or Roth 401(k) is a better option.\n\nplot_liquidation_value_by_parameter_values('retirement_income_tax_rate', np.linspace(0, 0.38, num=20));\n\n\n\n\n\n\n\n\nSo given the other parameters I‚Äôve set, Roth outperforms traditional when our income tax rate in retirement exceeds about 25%. According to the IRS in 2023, an individual tax payer is in the 24% bracket if their income is between about $95k and $180k. So, how much income do you expect to pull in retirement? If we‚Äôre really building FIRE wealth, the kind indicated by Mr.¬†Money Mustache and JLCollins, our income in retirement could easily exceed $180k, which would push us into the 32% bracket where Roth is more efficient than traditional.\n\n\nCapital Gains Tax Rate at Retirement\n\nplot_liquidation_value_by_parameter_values('retirement_capital_gains_tax_rate', np.linspace(0, 0.20, num=20));\n\n\n\n\n\n\n\n\nIn 2023, according to the IRS as a single tax payer, if your income is between $44k and $492k, you‚Äôll pay 15% capital gains. Over $492k you‚Äôll jump up to 20% where Roth dominates traditional.\n\n\nRetirement Age\n\nplot_liquidation_value_by_parameter_values('retirement_age', np.linspace(59.5, 75, num=20));\n\n\n\n\n\n\n\n\nFor retirement ages beyond 59.5, traditional‚Äôs edge over Roth grows slightly."
  },
  {
    "objectID": "posts/traditional-vs-roth-401k/index.html#bottom-line",
    "href": "posts/traditional-vs-roth-401k/index.html#bottom-line",
    "title": "Analyzing After Tax Retirement Income: Roth vs.¬†Traditional 401(k)",
    "section": "Bottom Line",
    "text": "Bottom Line\nWhen I plugged in my actual parameters, I found that because I was only employed for 6 months last year, my current income tax rate pushed me into the regime where Roth performs better than traditional. However for this next year, I expect to be in a higher income tax bracket where traditional will be a better deal than Roth.\nThat said, the most important factor is your income tax rate at the time of withdrawal during retirement, which is based on your taxable income at that time. But how, I hear you asking, am I supposed to know what to plug in for my post-retirement income? That quantity is unknown. This illuminates the fundamental limitation of this kind of analysis‚Äîwhat to do about uncertain inputs to the calculation? That‚Äôs a question that we might take on in a future post, so stay tuned!"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Logistic Regression with PyTorch\n\n\n\n\n\n\n\n\nJul 4, 2025\n\n\n\n\n\n\n\nBayesian Modeling Primer\n\n\n\n\n\n\n\n\nJun 4, 2025\n\n\n\n\n\n\n\nAnalyzing After Tax Retirement Income: Roth vs.¬†Traditional 401(k)\n\n\n\n\n\n\n\n\nDec 14, 2024\n\n\n\n\n\n\n\nSHAP from Scratch\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\n\n\n\n\n\nThe Ultimate Guide to XGBoost Parameter Tuning\n\n\n\n\n\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n\nXGBoost for Binary and Multi-Class Classification in Python\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\n\n\n\n\n\nGradient Boosting Multi-Class Classification from Scratch\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\nXGBoost for Regression in Python\n\n\n\n\n\n\n\n\nSep 18, 2023\n\n\n\n\n\n\n\nBlogging with Quarto and Jupyter: The Complete Guide\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\n\n\n\n\n\nRandom Realizations Resurrected\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\n\n\n\n\n\nXGBoost from Scratch\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\n\n\n\n\n\nXGBoost Explained\n\n\n\n\n\n\n\n\nMar 13, 2022\n\n\n\n\n\n\n\nDecision Tree from Scratch\n\n\n\n\n\n\n\n\nDec 13, 2021\n\n\n\n\n\n\n\nConsider the Decision Tree\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\n\n\n\n\n\nHow to Implement a Gradient Boosting Machine that Works with Any Loss Function\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\nHello PySpark!\n\n\n\n\n\n\n\n\nJun 22, 2021\n\n\n\n\n\n\n\nHow Gradient Boosting Does Gradient Descent\n\n\n\n\n\n\n\n\nApr 27, 2021\n\n\n\n\n\n\n\nGet Down with Gradient Descent\n\n\n\n\n\n\n\n\nJan 22, 2021\n\n\n\n\n\n\n\nHow to Build a Gradient Boosting Machine from Scratch\n\n\n\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n\nThe 80/20 Pandas Tutorial\n\n\n\n\n\n\n\n\nNov 25, 2020\n\n\n\n\n\n\n\nHello World! And Why I‚Äôm Inspired to Start a Blog\n\n\n\n\n\n\n\n\nNov 22, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to Random Realizations! This blog is a celebration of the fascinating world of data science. I hope you find the content useful, and I hope you enjoy reading along and learning with me!\nAnd me? Well, I‚Äôm Matt Bowers. I‚Äôm an ex data scientist at Uber where I solved problems using statistical modeling and machine learning. Over the years I‚Äôve driven product decisions through controlled experimentation and user analytics, built ML products like ETA prediction on large-scale telematics data, and cracked long-standing ML problems in marketplace pricing. Before Uber I earned an MS in Applied Statistics and a PhD in Atmospheric Sciences at Purdue University where I studied the spatial-temporal correlation structure of tropical convection systems and developed statistical methods for climate research.\nBut enough about me. Let‚Äôs get down with some data science!"
  }
]