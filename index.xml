<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Realizations</title>
<link>https://blog.mattbowers.dev/index.html</link>
<atom:link href="https://blog.mattbowers.dev/index.xml" rel="self" type="application/rss+xml"/>
<description>A blog about data science, statistics, machine learning, and the scientific method</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Fri, 06 May 2022 23:00:00 GMT</lastBuildDate>
<item>
  <title>XGBoost from scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2022/xgboost-from-scratch/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2022/xgboost-from-scratch/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A weathered tree reaches toward the sea at Playa Mal País</figcaption><p></p>
</figure>
</div>
<p>Well, dear reader, it’s that time again, time for us to do a seemingly unnecessary scratch build of a popular algorithm that most people would simply import from the library without a second thought. But readers of this blog are not most people. Of course you know that when we do scratch builds, it’s not for the hell of it, it’s for the purpose of demystification. To that end, today we are going to implement XGBoost from scratch in python, using only numpy and pandas.</p>
<p>Specifically we’re going to implement the core statistical learning algorithm of XGBoost, including most of the key hyperparameters and their functionality. Our implementation will also support user-defined custom objective functions, meaning that it can perform regression, classification, and whatever exotic learning tasks you can dream up, as long as you can write down a twice-differentiable objective function. We’ll refrain from implementing some simple features like column subsampling which will be left to you, gentle reader, as exercises. In terms of tree methods, we’re going to implement the exact tree-splitting algorithm, leaving the sparsity-aware method (used to handle missing feature values) and the approximate method (used for scalability) as exercises or maybe topics for future posts.</p>
<p>As always, if something is unclear, try backtracking through the previous posts on gradient boosting and decision trees to clarify your intuition. We’ve already built up all the statistical and computational background needed to make sense of this scratch build. Here are the most important prerequisite posts:</p>
<ol type="1">
<li><a href="../../../gradient-boosting-machine-from-scratch">Gradient Boosting Machine from Scratch</a></li>
<li><a href="../../../decision-tree-from-scratch">Decision Tree From Scratch</a></li>
<li><a href="../../../how-to-understand-xgboost">How to Understand XGBoost</a></li>
</ol>
<p>Great, let’s do this.</p>
<section id="the-xgboost-model-class" class="level2">
<h2 class="anchored" data-anchor-id="the-xgboost-model-class">The XGBoost Model Class</h2>
<p>We begin with the user-facing API for our model, a class called <code>XGBoostModel</code> which will implement gradient boosting and prediction. To be more consistent with the XGBoost library, we’ll pass hyperparameters to our model in a parameter dictionary, so our init method is going to pull relevant parameters out of the dictionary and set them as object attributes. Note the use of python’s <code>defaultdict</code> so we don’t have to worry about handling key errors if we try to access a parameter that the user didn’t set in the dictionary.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> math</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np </span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> collections <span class="im" style="color: #00769E;">import</span> defaultdict</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">class</span> XGBoostModel():</span>
<span id="cb2-2">    <span class="co" style="color: #5E5E5E;">'''XGBoost from Scratch</span></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb2-4">    </span>
<span id="cb2-5">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, params, random_seed<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb2-6">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> defaultdict(<span class="kw" style="color: #003B4F;">lambda</span>: <span class="va" style="color: #111111;">None</span>, params)</span>
<span id="cb2-7">        <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-8">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb2-9">        <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-10">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.3</span></span>
<span id="cb2-11">        <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-12">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb2-13">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-14">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb2-15">        <span class="va" style="color: #111111;">self</span>.rng <span class="op" style="color: #5E5E5E;">=</span> np.random.default_rng(seed<span class="op" style="color: #5E5E5E;">=</span>random_seed)</span></code></pre></div>
</div>
<p>The fit method, based on our classic GBM, takes a feature dataframe, a target vector, the objective function, and the number of boosting rounds as arguments. The user-supplied objective function should be an object with loss, gradient, and hessian methods, each of which takes a target vector and a prediction vector as input; the loss method should return a scalar loss score, the gradient method should return a vector of gradients, and the hessian method should return a vector of hessians.</p>
<p>In contrast to boosting in the classic GBM, instead of computing residuals between the current predictions and the target, we compute gradients and hessians of the loss function with respect to the current predictions, and instead of predicting residuals with a decision tree, we fit a special XGBoost tree booster (which we’ll implement in a moment) using the gradients and hessians. I’ve also added row subsampling by drawing a random subset of instance indices and passing them to the tree booster during each boosting round. The rest of the fit method is the same as the classic GBM, and the predict method is identical too.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, X, y, objective, num_boost_round, verbose<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb3-2">    current_predictions <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">*</span> np.ones(shape<span class="op" style="color: #5E5E5E;">=</span>y.shape)</span>
<span id="cb3-3">    <span class="va" style="color: #111111;">self</span>.boosters <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_boost_round):</span>
<span id="cb3-5">        gradients <span class="op" style="color: #5E5E5E;">=</span> objective.gradient(y, current_predictions)</span>
<span id="cb3-6">        hessians <span class="op" style="color: #5E5E5E;">=</span> objective.hessian(y, current_predictions)</span>
<span id="cb3-7">        sample_idxs <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span> <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">1.0</span> <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb3-8">            <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.rng.choice(<span class="bu" style="color: null;">len</span>(y), </span>
<span id="cb3-9">                                 size<span class="op" style="color: #5E5E5E;">=</span>math.floor(<span class="va" style="color: #111111;">self</span>.subsample<span class="op" style="color: #5E5E5E;">*</span><span class="bu" style="color: null;">len</span>(y)), </span>
<span id="cb3-10">                                 replace<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb3-11">        booster <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(X, gradients, hessians, </span>
<span id="cb3-12">                              <span class="va" style="color: #111111;">self</span>.params, <span class="va" style="color: #111111;">self</span>.max_depth, sample_idxs)</span>
<span id="cb3-13">        current_predictions <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> booster.predict(X)</span>
<span id="cb3-14">        <span class="va" style="color: #111111;">self</span>.boosters.append(booster)</span>
<span id="cb3-15">        <span class="cf" style="color: #003B4F;">if</span> verbose: </span>
<span id="cb3-16">            <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'[</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">] train loss = </span><span class="sc" style="color: #5E5E5E;">{</span>objective<span class="sc" style="color: #5E5E5E;">.</span>loss(y, current_predictions)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb3-17">            </span>
<span id="cb3-18"><span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb3-19">    <span class="cf" style="color: #003B4F;">return</span> (<span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.learning_rate </span>
<span id="cb3-20">            <span class="op" style="color: #5E5E5E;">*</span> np.<span class="bu" style="color: null;">sum</span>([booster.predict(X) <span class="cf" style="color: #003B4F;">for</span> booster <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.boosters], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>))</span>
<span id="cb3-21"></span>
<span id="cb3-22">XGBoostModel.fit <span class="op" style="color: #5E5E5E;">=</span> fit</span>
<span id="cb3-23">XGBoostModel.predict <span class="op" style="color: #5E5E5E;">=</span> predict            </span></code></pre></div>
</div>
<p>All we have to do now is implement the tree booster.</p>
</section>
<section id="the-xgboost-tree-booster" class="level2">
<h2 class="anchored" data-anchor-id="the-xgboost-tree-booster">The XGBoost Tree Booster</h2>
<p>The XGBoost tree booster is a modified version of the decision tree that we built in the decision tree from scratch post. Like the decision tree, we recursively build a binary tree structure by finding the best split rule for each node in the tree. The main difference is the criterion for evaluating splits and the way that we define a leaf’s predicted value. Instead of being functions of the target values of the instances in each node, the criterion and predicted values are functions of the instance gradients and hessians. Thus we need only make a couple of modifications to our previous decision tree implementation to create the XGBoost tree booster.</p>
<section id="initialization-and-inserting-child-nodes" class="level3">
<h3 class="anchored" data-anchor-id="initialization-and-inserting-child-nodes">Initialization and Inserting Child Nodes</h3>
<p>Most of the init method is just parsing the parameter dictionary to assign parameters as object attributes. The one notable difference from our decision tree is in the way we define the node’s predicted value. We define <code>self.value</code> according to equation 5 of the XGBoost paper, a simple function of the gradient and hessian values of the instances in the current node. Of course the init also goes on to build the tree via the maybe insert child nodes method. This method is nearly identical to the one we implemented for our decision tree. So far so good.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;">class</span> TreeBooster():</span>
<span id="cb4-2"> </span>
<span id="cb4-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, g, h, params, max_depth, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb4-4">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> params</span>
<span id="cb4-5">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> max_depth</span>
<span id="cb4-6">        <span class="cf" style="color: #003B4F;">assert</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb4-7">        <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb4-8">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb4-9">        <span class="va" style="color: #111111;">self</span>.reg_lambda <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb4-10">        <span class="va" style="color: #111111;">self</span>.gamma <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.0</span></span>
<span id="cb4-11">        <span class="va" style="color: #111111;">self</span>.colsample_bynode <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb4-12">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb4-13">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(g, pd.Series): g <span class="op" style="color: #5E5E5E;">=</span> g.values</span>
<span id="cb4-14">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(h, pd.Series): h <span class="op" style="color: #5E5E5E;">=</span> h.values</span>
<span id="cb4-15">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(g))</span>
<span id="cb4-16">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, g, h, idxs</span>
<span id="cb4-17">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb4-18">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>g[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">/</span> (h[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda) <span class="co" style="color: #5E5E5E;"># Eq (5)</span></span>
<span id="cb4-19">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb4-20">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb4-21">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb4-22"></span>
<span id="cb4-23">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb4-24">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): <span class="va" style="color: #111111;">self</span>._find_better_split(i)</span>
<span id="cb4-25">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="cf" style="color: #003B4F;">return</span></span>
<span id="cb4-26">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb4-27">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-28">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-29">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb4-30">                                <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb4-31">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb4-32">                                 <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb4-33"></span>
<span id="cb4-34">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb4-35">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb4-36"></span>
<span id="cb4-37">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb4-38">        <span class="cf" style="color: #003B4F;">pass</span></span></code></pre></div>
</div>
</section>
<section id="split-finding" class="level3">
<h3 class="anchored" data-anchor-id="split-finding">Split Finding</h3>
<p>Split finding follows the exact same pattern that we used in the decision tree, except we keep track of gradient and hessian stats instead of target value stats, and of course we use the XGBoost gain criterion (equation 7 from the paper) for evaluating splits.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb5-2">    x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs, feature_idx]</span>
<span id="cb5-3">    g, h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.g[<span class="va" style="color: #111111;">self</span>.idxs], <span class="va" style="color: #111111;">self</span>.h[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb5-4">    sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb5-5">    sort_g, sort_h, sort_x <span class="op" style="color: #5E5E5E;">=</span> g[sort_idx], h[sort_idx], x[sort_idx]</span>
<span id="cb5-6">    sum_g, sum_h <span class="op" style="color: #5E5E5E;">=</span> g.<span class="bu" style="color: null;">sum</span>(), h.<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb5-7">    sum_g_right, sum_h_right <span class="op" style="color: #5E5E5E;">=</span> sum_g, sum_h</span>
<span id="cb5-8">    sum_g_left, sum_h_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb5-9"></span>
<span id="cb5-10">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb5-11">        g_i, h_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_g[i], sort_h[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb5-12">        sum_g_left <span class="op" style="color: #5E5E5E;">+=</span> g_i<span class="op" style="color: #5E5E5E;">;</span> sum_g_right <span class="op" style="color: #5E5E5E;">-=</span> g_i</span>
<span id="cb5-13">        sum_h_left <span class="op" style="color: #5E5E5E;">+=</span> h_i<span class="op" style="color: #5E5E5E;">;</span> sum_h_right <span class="op" style="color: #5E5E5E;">-=</span> h_i</span>
<span id="cb5-14">        <span class="cf" style="color: #003B4F;">if</span> sum_h_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:<span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb5-15">        <span class="cf" style="color: #003B4F;">if</span> sum_h_right <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight: <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb5-16"></span>
<span id="cb5-17">        gain <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span> <span class="op" style="color: #5E5E5E;">*</span> ((sum_g_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_left <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb5-18">                        <span class="op" style="color: #5E5E5E;">+</span> (sum_g_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_right <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb5-19">                        <span class="op" style="color: #5E5E5E;">-</span> (sum_g<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb5-20">                        ) <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.gamma<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="co" style="color: #5E5E5E;"># Eq(7) in the xgboost paper</span></span>
<span id="cb5-21">        <span class="cf" style="color: #003B4F;">if</span> gain <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far: </span>
<span id="cb5-22">            <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb5-23">            <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> gain</span>
<span id="cb5-24">            <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb5-25">            </span>
<span id="cb5-26">TreeBooster._find_better_split <span class="op" style="color: #5E5E5E;">=</span> _find_better_split</span></code></pre></div>
</div>
</section>
<section id="prediction" class="level3">
<h3 class="anchored" data-anchor-id="prediction">Prediction</h3>
<p>Prediction works exactly the same as in our decision tree, and the methods are nearly identical.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb6-2">    <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb6-3"></span>
<span id="cb6-4"><span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb6-6">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb6-7">    child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb6-8">        <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb6-9">    <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span>
<span id="cb6-10"></span>
<span id="cb6-11">TreeBooster.predict <span class="op" style="color: #5E5E5E;">=</span> predict </span>
<span id="cb6-12">TreeBooster._predict_row <span class="op" style="color: #5E5E5E;">=</span> _predict_row </span></code></pre></div>
</div>
</section>
</section>
<section id="the-complete-xgboost-from-scratch-implementation" class="level2">
<h2 class="anchored" data-anchor-id="the-complete-xgboost-from-scratch-implementation">The Complete XGBoost From Scratch Implementation</h2>
<p>Here’s the entire implementation which produces a usable <code>XGBoostModel</code> class with fit and predict methods.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;">class</span> XGBoostModel():</span>
<span id="cb7-2">    <span class="co" style="color: #5E5E5E;">'''XGBoost from Scratch</span></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb7-4">    </span>
<span id="cb7-5">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, params, random_seed<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-6">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> defaultdict(<span class="kw" style="color: #003B4F;">lambda</span>: <span class="va" style="color: #111111;">None</span>, params)</span>
<span id="cb7-7">        <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-8">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-9">        <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-10">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.3</span></span>
<span id="cb7-11">        <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-12">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb7-13">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-14">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb7-15">        <span class="va" style="color: #111111;">self</span>.rng <span class="op" style="color: #5E5E5E;">=</span> np.random.default_rng(seed<span class="op" style="color: #5E5E5E;">=</span>random_seed)</span>
<span id="cb7-16">                </span>
<span id="cb7-17">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, X, y, objective, num_boost_round, verbose<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb7-18">        current_predictions <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">*</span> np.ones(shape<span class="op" style="color: #5E5E5E;">=</span>y.shape)</span>
<span id="cb7-19">        <span class="va" style="color: #111111;">self</span>.boosters <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb7-20">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_boost_round):</span>
<span id="cb7-21">            gradients <span class="op" style="color: #5E5E5E;">=</span> objective.gradient(y, current_predictions)</span>
<span id="cb7-22">            hessians <span class="op" style="color: #5E5E5E;">=</span> objective.hessian(y, current_predictions)</span>
<span id="cb7-23">            sample_idxs <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span> <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">1.0</span> <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-24">                <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.rng.choice(<span class="bu" style="color: null;">len</span>(y), </span>
<span id="cb7-25">                                     size<span class="op" style="color: #5E5E5E;">=</span>math.floor(<span class="va" style="color: #111111;">self</span>.subsample<span class="op" style="color: #5E5E5E;">*</span><span class="bu" style="color: null;">len</span>(y)), </span>
<span id="cb7-26">                                     replace<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb7-27">            booster <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(X, gradients, hessians, </span>
<span id="cb7-28">                                  <span class="va" style="color: #111111;">self</span>.params, <span class="va" style="color: #111111;">self</span>.max_depth, sample_idxs)</span>
<span id="cb7-29">            current_predictions <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> booster.predict(X)</span>
<span id="cb7-30">            <span class="va" style="color: #111111;">self</span>.boosters.append(booster)</span>
<span id="cb7-31">            <span class="cf" style="color: #003B4F;">if</span> verbose: </span>
<span id="cb7-32">                <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'[</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">] train loss = </span><span class="sc" style="color: #5E5E5E;">{</span>objective<span class="sc" style="color: #5E5E5E;">.</span>loss(y, current_predictions)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb7-33">            </span>
<span id="cb7-34">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb7-35">        <span class="cf" style="color: #003B4F;">return</span> (<span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.learning_rate </span>
<span id="cb7-36">                <span class="op" style="color: #5E5E5E;">*</span> np.<span class="bu" style="color: null;">sum</span>([booster.predict(X) <span class="cf" style="color: #003B4F;">for</span> booster <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.boosters], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>))</span>
<span id="cb7-37">    </span>
<span id="cb7-38"><span class="kw" style="color: #003B4F;">class</span> TreeBooster():</span>
<span id="cb7-39"> </span>
<span id="cb7-40">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, g, h, params, max_depth, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-41">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> params</span>
<span id="cb7-42">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> max_depth</span>
<span id="cb7-43">        <span class="cf" style="color: #003B4F;">assert</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb7-44">        <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-45">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-46">        <span class="va" style="color: #111111;">self</span>.reg_lambda <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-47">        <span class="va" style="color: #111111;">self</span>.gamma <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.0</span></span>
<span id="cb7-48">        <span class="va" style="color: #111111;">self</span>.colsample_bynode <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-49">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-50">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(g, pd.Series): g <span class="op" style="color: #5E5E5E;">=</span> g.values</span>
<span id="cb7-51">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(h, pd.Series): h <span class="op" style="color: #5E5E5E;">=</span> h.values</span>
<span id="cb7-52">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(g))</span>
<span id="cb7-53">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, g, h, idxs</span>
<span id="cb7-54">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb7-55">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>g[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">/</span> (h[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda) <span class="co" style="color: #5E5E5E;"># Eq (5)</span></span>
<span id="cb7-56">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb7-57">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb7-58">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb7-59"></span>
<span id="cb7-60">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb7-61">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): <span class="va" style="color: #111111;">self</span>._find_better_split(i)</span>
<span id="cb7-62">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="cf" style="color: #003B4F;">return</span></span>
<span id="cb7-63">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb7-64">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb7-65">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb7-66">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb7-67">                                <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb7-68">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb7-69">                                 <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb7-70"></span>
<span id="cb7-71">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb7-72">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb7-73">    </span>
<span id="cb7-74">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb7-75">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs, feature_idx]</span>
<span id="cb7-76">        g, h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.g[<span class="va" style="color: #111111;">self</span>.idxs], <span class="va" style="color: #111111;">self</span>.h[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb7-77">        sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb7-78">        sort_g, sort_h, sort_x <span class="op" style="color: #5E5E5E;">=</span> g[sort_idx], h[sort_idx], x[sort_idx]</span>
<span id="cb7-79">        sum_g, sum_h <span class="op" style="color: #5E5E5E;">=</span> g.<span class="bu" style="color: null;">sum</span>(), h.<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb7-80">        sum_g_right, sum_h_right <span class="op" style="color: #5E5E5E;">=</span> sum_g, sum_h</span>
<span id="cb7-81">        sum_g_left, sum_h_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb7-82"></span>
<span id="cb7-83">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb7-84">            g_i, h_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_g[i], sort_h[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb7-85">            sum_g_left <span class="op" style="color: #5E5E5E;">+=</span> g_i<span class="op" style="color: #5E5E5E;">;</span> sum_g_right <span class="op" style="color: #5E5E5E;">-=</span> g_i</span>
<span id="cb7-86">            sum_h_left <span class="op" style="color: #5E5E5E;">+=</span> h_i<span class="op" style="color: #5E5E5E;">;</span> sum_h_right <span class="op" style="color: #5E5E5E;">-=</span> h_i</span>
<span id="cb7-87">            <span class="cf" style="color: #003B4F;">if</span> sum_h_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:<span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb7-88">            <span class="cf" style="color: #003B4F;">if</span> sum_h_right <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight: <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb7-89"></span>
<span id="cb7-90">            gain <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span> <span class="op" style="color: #5E5E5E;">*</span> ((sum_g_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_left <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb7-91">                            <span class="op" style="color: #5E5E5E;">+</span> (sum_g_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_right <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb7-92">                            <span class="op" style="color: #5E5E5E;">-</span> (sum_g<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb7-93">                            ) <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.gamma<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="co" style="color: #5E5E5E;"># Eq(7) in the xgboost paper</span></span>
<span id="cb7-94">            <span class="cf" style="color: #003B4F;">if</span> gain <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far: </span>
<span id="cb7-95">                <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb7-96">                <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> gain</span>
<span id="cb7-97">                <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb7-98">                </span>
<span id="cb7-99">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb7-100">        <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb7-101"></span>
<span id="cb7-102">    <span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb7-103">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb7-104">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb7-105">        child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-106">            <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb7-107">        <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span></code></pre></div>
</div>
</section>
<section id="testing" class="level2">
<h2 class="anchored" data-anchor-id="testing">Testing</h2>
<p>Let’s take this baby for a spin and benchmark its performance against the actual XGBoost library. We use the scikit learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html">California housing dataset</a> for benchmarking.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> fetch_california_housing</span>
<span id="cb8-2"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb8-3">    </span>
<span id="cb8-4">X, y <span class="op" style="color: #5E5E5E;">=</span> fetch_california_housing(as_frame<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_X_y<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb8-5">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.3</span>, </span>
<span id="cb8-6">                                                    random_state<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">43</span>)</span></code></pre></div>
</div>
<p>Let’s start with a nice friendly squared error objective function for training. We should probably have a future post all about how to define custom objective functions in XGBoost, but for now, here’s how I define squared error.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;">class</span> SquaredErrorObjective():</span>
<span id="cb9-2">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, y, pred): <span class="cf" style="color: #003B4F;">return</span> np.mean((y <span class="op" style="color: #5E5E5E;">-</span> pred)<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb9-3">    <span class="kw" style="color: #003B4F;">def</span> gradient(<span class="va" style="color: #111111;">self</span>, y, pred): <span class="cf" style="color: #003B4F;">return</span> pred <span class="op" style="color: #5E5E5E;">-</span> y</span>
<span id="cb9-4">    <span class="kw" style="color: #003B4F;">def</span> hessian(<span class="va" style="color: #111111;">self</span>, y, pred): <span class="cf" style="color: #003B4F;">return</span> np.ones(<span class="bu" style="color: null;">len</span>(y))</span></code></pre></div>
</div>
<p>Here I use a more or less arbitrary set of hyperparameters for training. Feel free to play around with tuning and trying other parameter combinations yourself.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;">import</span> xgboost <span class="im" style="color: #00769E;">as</span> xgb</span>
<span id="cb10-2"></span>
<span id="cb10-3">params <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb10-4">    <span class="st" style="color: #20794D;">'learning_rate'</span>: <span class="fl" style="color: #AD0000;">0.1</span>,</span>
<span id="cb10-5">    <span class="st" style="color: #20794D;">'max_depth'</span>: <span class="dv" style="color: #AD0000;">5</span>,</span>
<span id="cb10-6">    <span class="st" style="color: #20794D;">'subsample'</span>: <span class="fl" style="color: #AD0000;">0.8</span>,</span>
<span id="cb10-7">    <span class="st" style="color: #20794D;">'reg_lambda'</span>: <span class="fl" style="color: #AD0000;">1.5</span>,</span>
<span id="cb10-8">    <span class="st" style="color: #20794D;">'gamma'</span>: <span class="fl" style="color: #AD0000;">0.0</span>,</span>
<span id="cb10-9">    <span class="st" style="color: #20794D;">'min_child_weight'</span>: <span class="dv" style="color: #AD0000;">25</span>,</span>
<span id="cb10-10">    <span class="st" style="color: #20794D;">'base_score'</span>: <span class="fl" style="color: #AD0000;">0.0</span>,</span>
<span id="cb10-11">    <span class="st" style="color: #20794D;">'tree_method'</span>: <span class="st" style="color: #20794D;">'exact'</span>,</span>
<span id="cb10-12">}</span>
<span id="cb10-13">num_boost_round <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">50</span></span>
<span id="cb10-14"></span>
<span id="cb10-15"><span class="co" style="color: #5E5E5E;"># train the from-scratch XGBoost model</span></span>
<span id="cb10-16">model_scratch <span class="op" style="color: #5E5E5E;">=</span> XGBoostModel(params, random_seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>)</span>
<span id="cb10-17">model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)</span>
<span id="cb10-18"></span>
<span id="cb10-19"><span class="co" style="color: #5E5E5E;"># train the library XGBoost model</span></span>
<span id="cb10-20">dtrain <span class="op" style="color: #5E5E5E;">=</span> xgb.DMatrix(X_train, label<span class="op" style="color: #5E5E5E;">=</span>y_train)</span>
<span id="cb10-21">dtest <span class="op" style="color: #5E5E5E;">=</span> xgb.DMatrix(X_test, label<span class="op" style="color: #5E5E5E;">=</span>y_test)</span>
<span id="cb10-22">model_xgb <span class="op" style="color: #5E5E5E;">=</span> xgb.train(params, dtrain, num_boost_round)</span></code></pre></div>
</div>
<p>Let’s check the models’ performance on the held out test data to benchmark our implementation.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">pred_scratch <span class="op" style="color: #5E5E5E;">=</span> model_scratch.predict(X_test)</span>
<span id="cb11-2">pred_xgb <span class="op" style="color: #5E5E5E;">=</span> model_xgb.predict(dtest)</span>
<span id="cb11-3"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'scratch score: </span><span class="sc" style="color: #5E5E5E;">{</span>SquaredErrorObjective()<span class="sc" style="color: #5E5E5E;">.</span>loss(y_test, pred_scratch)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb11-4"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'xgboost score: </span><span class="sc" style="color: #5E5E5E;">{</span>SquaredErrorObjective()<span class="sc" style="color: #5E5E5E;">.</span>loss(y_test, pred_xgb)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>scratch score: 0.2434125759558149
xgboost score: 0.24123239765807963</code></pre>
</div>
</div>
<p>Well, look at that! Our scratch-built SGBoost is looking pretty consistent with the library. Go us!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>I’d say this is a pretty good milestone for us here at Random Realizations. We’ve been hammering away at the various concepts around gradient boosting, leaving a trail of equations and scratch-built algos in our wake. Today we put all of that together to create a legit scratch build of XGBoost, something that would have been out of reach for me before we embarked on this journey together over a year ago. To anyone with the patience to read through this stuff, cheers to you! I hope you’re learning and enjoying this as much as I am.</p>
</section>
<section id="reader-exercises" class="level2">
<h2 class="anchored" data-anchor-id="reader-exercises">Reader Exercises</h2>
<p>If you want to take this a step further and deepen your understanding and coding abilities, let me recommend some exercises for you.</p>
<ol type="1">
<li>Implement column subsampling. XGBoost itself provides column subsampling by tree, by level, and by node. Try implementing by tree first, then try adding by level or by node as well. These should be pretty straightforward to do.</li>
<li>Implement sparsity aware split finding for missing feature values (Algorithm 2 in the <a href="https://arxiv.org/abs/1603.02754">XGBoost paper</a>). This will be a little more involved, since you’ll need to refactor and modify several parts of the tree booster class.</li>
</ol>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://blog.mattbowers.dev/posts/2022/xgboost-from-scratch/index.html</guid>
  <pubDate>Fri, 06 May 2022 23:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2022/xgboost-from-scratch/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How to Understand XGBoost</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2022/how-to-understand-xgboost/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2022/how-to-understand-xgboost/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Tree branches on a chilly day in Johnson City</figcaption><p></p>
</figure>
</div>
<p>Ahh, XGBoost, what an absolutely stellar implementation of gradient boosting. Once Tianqi Chen and Carlos Guestrin of the University of Washington published the <a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">XGBoost paper</a> and shared the <a href="https://github.com/dmlc/xgboost">open source code</a> in the mid 2010’s, the algorithm quickly gained adoption in the ML community, appearing in over half of winning Kagle submissions in 2015. Nowadays it’s certainly among the most popular gradient boosting libraries, along with LightGBM and CatBoost, although the highly scientific indicator of <a href="https://www.githubcompare.com/dmlc/xgboost+microsoft/lightgbm+catboost/catboost">GitHub stars per year</a> indicates that it is in fact the most beloved gradient boosting package of all. Since it was the first of the modern popular boosting frameworks, and since <a href="https://blog.dataiku.com/the-many-flavors-of-gradient-boosting-algorithms">benchmarking</a> indicates that no other boosting algorithm outperforms it, we can comfortably focus our attention on understanding XGBoost.</p>
<p>The XGBoost authors identify two key aspects of a machine learning system: (1) a flexible statistical model and (2) a scalable learning system to fit that model using data. XGBoost improves on both of these aspects, providing a more flexible and feature-rich statistical model and building a truly scalable system to fit it. In this post we’re going to focus on the statistical modeling innovations, outlining the key differences from the classic gradient boosting machine and divinginto the mathematical derivation of the XGBoost learning algorithm. If you’re not already familiar with <a href="../../../gradient-boosting-machine-from-scratch">gradient boosting</a>, go back and read the earlier posts in the series before jumping in here.</p>
<p>Buckle up, dear reader. Today we understand how XGBoost works, no hand waving required.</p>
<section id="xgboost-is-a-gradient-boosting-machine" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-is-a-gradient-boosting-machine">XGBoost is a Gradient Boosting Machine</h2>
<p>At a high level, XGBoost is an iteratively constructed composite model, just like the classic gradient boosting machine we discussed back in the <a href="../../../gradient-boosting-machine-from-scratch">GBM post</a> . The final model takes the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%20=%20F(%5Cmathbf%7Bx%7D_i)%20=%20b%20+%20%5Ceta%20%5Csum_%7Bk=1%7D%5EK%20f_k(%5Cmathbf%7Bx%7D_i)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b"> is the base prediction, <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is the learning rate hyperparameter that helps control overfitting by reducing the contributions of each booster, and each of the <img src="https://latex.codecogs.com/png.latex?K"> boosters <img src="https://latex.codecogs.com/png.latex?f_k"> is a decision tree. To help us connect the dots between theory and code, whenever we encounter new hyperparameters, I’ll point out their names from the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">XGBoost Parameter Documentation</a>. So, <img src="https://latex.codecogs.com/png.latex?b"> can be set by <code>base_score</code>, and <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is set by either <code>eta</code> or <code>learning_rate</code>.</p>
<p>XGBoost introduces two key statistical learning improvements over the classic gradient boosting model. First, it reimagines the gradient descent algorithm used for training, and second it uses a custom-built decision tree with extra functionality as its booster. We’ll dive into each of these key innovations in the following sections.</p>
</section>
<section id="descent-algorithm-innovations" class="level2">
<h2 class="anchored" data-anchor-id="descent-algorithm-innovations">Descent Algorithm Innovations</h2>
<section id="regularized-objective-function" class="level3">
<h3 class="anchored" data-anchor-id="regularized-objective-function">Regularized Objective Function</h3>
<p>In the post on <a href="../../../gradient-boosting-machine-with-any-loss-function">GBM with any loss function</a>, we looked at loss functions of the form <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20l(y_i,%5Chat%7By%7D_i)"> which compute some distance between targets <img src="https://latex.codecogs.com/png.latex?y_i"> and predictions <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> and sum them up over the training dataset. XGBoost introduces regularization into the objective function so that the objective takes the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%20=%20%5Csum_i%20l(y_i,%5Chat%7By%7D_i)%20+%20%5Csum_k%20%5COmega(f_k)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?l"> is some twice-differentiable loss function. <img src="https://latex.codecogs.com/png.latex?%5COmega"> is a regularization that penalizes the complexity of each tree booster, taking the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5COmega(f)%20=%20%5Cgamma%20T%20+%20%5Cfrac%7B1%7D%7B2%7D%20%5Clambda%20%7C%7Cw%7C%7C%5E2%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?T"> is the number of leaf nodes and <img src="https://latex.codecogs.com/png.latex?%7C%7Cw%7C%7C%5E2"> is the squared sum of the leaf prediction values. This introduces two new hyperparameters: <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> which penalizes the number of leaf nodes and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> which is the L-2 regularization parameter for leaf predicted values. These are set by <code>gamma</code> and <code>reg_lambbda</code> in the XGBoost parametrization. Together, these provide powerful new controls to reduce overfitting due to overly complex tree boosters. Note that <img src="https://latex.codecogs.com/png.latex?%5Cgamma=%5Clambda=0"> reduces the objective back to an unregularized loss function as used in the classic GBM.</p>
</section>
<section id="an-aside-on-newtons-method" class="level3">
<h3 class="anchored" data-anchor-id="an-aside-on-newtons-method">An Aside on Newton’s Method</h3>
<p>As we’ll see soon, XGBoost uses <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s Method</a> to minimize its objective function, so let’s start with a quick refresher.</p>
<p>Newton’s method is an iterative procedure for minimizing a function <img src="https://latex.codecogs.com/png.latex?s(x)">. At each step we have some input <img src="https://latex.codecogs.com/png.latex?x_t">, and our goal is to find a nudge value <img src="https://latex.codecogs.com/png.latex?u"> such that</p>
<p><img src="https://latex.codecogs.com/png.latex?%20s(x_t%20+%20u)%20%5Cle%20s(x_t)"></p>
<p>To find a good nudge value <img src="https://latex.codecogs.com/png.latex?u">, we generate a local quadratic approximation of the function in the neighborhood of the input <img src="https://latex.codecogs.com/png.latex?x_t">, and then we find the input value that would bring us to the minimum of the quadratic approximation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2022/how-to-understand-xgboost/newton.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Schematic of Newton’s method</figcaption><p></p>
</figure>
</div>
<p>The figure shows a single Newton step where we start at <img src="https://latex.codecogs.com/png.latex?x_t">, find the local quadratic approximation, and then jump a distance <img src="https://latex.codecogs.com/png.latex?u"> along the <img src="https://latex.codecogs.com/png.latex?x">-axis to land at the minimum of the quadratic. If we iterate in this way, we are likely to land close to the minimum of <img src="https://latex.codecogs.com/png.latex?s(x)">.</p>
<p>So how do we compute the quadratic approximation? We use the second order Taylor series expansion of <img src="https://latex.codecogs.com/png.latex?s(x)"> near the point <img src="https://latex.codecogs.com/png.latex?x_t">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20s(x_t%20+%20u)%20%5Capprox%20%20s(x_t)%20+%20s'(x_t)u%20+%20%5Cfrac%7B1%7D%7B2%7D%20s''(x_t)%20u%5E2%20"></p>
<p>To find the nudge value <img src="https://latex.codecogs.com/png.latex?u"> that minimizes the quadratic approximation, we can take the derivative with respect to <img src="https://latex.codecogs.com/png.latex?u">, set it to zero, and solve for <img src="https://latex.codecogs.com/png.latex?u">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%200%20=%20%5Cfrac%7Bd%7D%7Bdu%7D%20%20%5Cleft%20(%20s(x_t)%20+%20s'(x_t)u%20+%20%5Cfrac%7B1%7D%7B2%7D%20s''(x_t)%20u%5E2%20%5Cright%20)%20=%20s'(x_t)%20+%20s''(x_t)%20u%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Crightarrow%20u%5E*%20=%20-%5Cfrac%7Bs'(x_t)%7D%7Bs''(x_t)%7D%20"></p>
<p>And as long as <img src="https://latex.codecogs.com/png.latex?s''(x_t)%3E0"> (i.e., the parabola is pointing up), <img src="https://latex.codecogs.com/png.latex?s(x_t%20+%20u%5E*)%20%5Cle%20s(x_t)">.</p>
</section>
<section id="tree-boosting-with-newtons-method" class="level3">
<h3 class="anchored" data-anchor-id="tree-boosting-with-newtons-method">Tree Boosting with Newton’s Method</h3>
<p>This lands us at the heart of XGBoost, which uses Newton’s method, rather than gradient descent, to guide each round of boosting. This explanation will correspond very closely to section 2.2 of the XGBoost paper, but here I’ll explicitly spell out some of the intermediate steps which are omitted from their derivation, and you’ll get some additional commentary from me along the way.</p>
<section id="newton-descent-in-tree-space" class="level4">
<h4 class="anchored" data-anchor-id="newton-descent-in-tree-space">Newton Descent in Tree Space</h4>
<p>Suppose we’ve done <img src="https://latex.codecogs.com/png.latex?t-1"> boosting rounds, and we want to add the <img src="https://latex.codecogs.com/png.latex?t">-th booster to our composite model. Our current model’s prediction for instance <img src="https://latex.codecogs.com/png.latex?i"> is <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%5E%7B(t-1)%7D">. If we add a new tree booster <img src="https://latex.codecogs.com/png.latex?f_t"> to our model, the objective function would give</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%5E%7B(t)%7D%20=%20%5Csum_%7Bi=1%7D%5En%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%20+%20f_t(%5Cmathbf%7Bx%7D_i))%20+%20%5COmega(f_t)%20"></p>
<p>We need to choose <img src="https://latex.codecogs.com/png.latex?f_t"> so that it decreases the loss, i.e.&nbsp;we want</p>
<p><img src="https://latex.codecogs.com/png.latex?%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%20+%20f_t(%5Cmathbf%7Bx%7D_i))%20%5Cle%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)"></p>
<p>Does that sound familiar? In the previous section we used Newton’s method to find a value of <img src="https://latex.codecogs.com/png.latex?u"> that would make <img src="https://latex.codecogs.com/png.latex?s(x_t%20+%20u)%20%5Cle%20s(x_t)">. Let’s try the same thing with our loss function. To be explicit, the parallels are: <img src="https://latex.codecogs.com/png.latex?s(%5Ccdot)%20%5Crightarrow%20l(y_i,%20%5Ccdot)">, <img src="https://latex.codecogs.com/png.latex?x_t%20%5Crightarrow%20%5Chat%7By%7D_i%5E%7B(t-1)%7D">, and <img src="https://latex.codecogs.com/png.latex?u%20%5Crightarrow%20f_t(%5Cmathbf%7Bx%7D_i)">.</p>
<p>Let’s start by finding the second order Taylor series approximation for the loss around the point <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%5E%7B(t-1)%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%20+%20f_t(%5Cmathbf%7Bx%7D_i))%20%5Capprox%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)%20+%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%20"></p>
<p>where</p>
<p><img src="https://latex.codecogs.com/png.latex?%20g_i%20=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%7D%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%20h_i%20=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5E2%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%7D%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)"></p>
<p>are the first and second order partial derivatives of the loss with respect to the current predictions. The XGBoost paper calls these the gradients and hessians, respectively. Remember that when we specify an actual loss function to use, we would also specify the functional form of the gradients and hessians, so that they are directly computable.</p>
<p>Now we can go back and substitute our quadratic approximation in for the loss function to get an approximation of the objective function in the neighborhood of <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%5E%7B(t-1)%7D">..</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%5E%7B(t)%7D%20%5Capprox%20%5Csum_%7Bi=1%7D%5En%20%5Bl(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)%20+%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%5D%20+%20%5COmega(f_t)%20"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?l(y_i,%5Chat%7By%7D_i%5E%7B(t-1)%7D)"> is constant regardless of our choice of <img src="https://latex.codecogs.com/png.latex?f_t">, we can drop it and instead work with the modified objective, which gives us Equation (3) from the paper.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bi=1%7D%5En%20%5B%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%5D%20+%20%5COmega(f_t)%20"></p>
<p>Now the authors are about to do something great. They’re about to show how to directly compute the optimal prediction values for the leaf nodes of <img src="https://latex.codecogs.com/png.latex?f_t">. We’ll circle back in a moment about how we find a good structure for <img src="https://latex.codecogs.com/png.latex?f_t">, i.e.&nbsp;good node splits, but we’re going to find the optimal predicted values for any tree structure having <img src="https://latex.codecogs.com/png.latex?T"> terminal nodes. Let <img src="https://latex.codecogs.com/png.latex?I_j"> denote the set of instances <img src="https://latex.codecogs.com/png.latex?i"> that are in the <img src="https://latex.codecogs.com/png.latex?j">-th leaf node of <img src="https://latex.codecogs.com/png.latex?f_t">. Then we can rewrite the objective.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bj=1%7D%5ET%20%5Cleft%20%5B%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%20%5Cright%20%5D%20+%20%5COmega(f_t)"></p>
<p>We notice that for all instances in <img src="https://latex.codecogs.com/png.latex?I_j">, the tree yields the same predicted value <img src="https://latex.codecogs.com/png.latex?f_t(%5Cmathbf%7Bx%7D_i)=w_j">. Substituting in <img src="https://latex.codecogs.com/png.latex?w_j"> for the predicted values and expanding <img src="https://latex.codecogs.com/png.latex?%5COmega(f_t)"> we get</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bj=1%7D%5ET%20%5Cleft%20%5B%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20w_j%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20w_j%5E2%20%5Cright%20%5D%20+%20%5Cgamma%20T%20+%20%5Cfrac%7B1%7D%7B2%7D%20%5Clambda%20%5Csum_%7Bj=1%7D%5ET%20w_j%5E2"></p>
<p>Rearranging terms we obtain Equation (4).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bj=1%7D%5ET%20%5Cleft%20%5B%20w_j%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20w_j%5E2%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%5Cright%20)%20%20%5Cright%20%5D%20+%20%5Cgamma%20T%20"></p>
<p>For each leaf node <img src="https://latex.codecogs.com/png.latex?j">, our modified objective function is quadratic in <img src="https://latex.codecogs.com/png.latex?w_j">. To find the optimal predicted values we take the derivative, set to zero, and solve for <img src="https://latex.codecogs.com/png.latex?w_j">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%200%20=%20%5Cfrac%7Bd%7D%7Bdw_j%7D%20%5Cleft%20%5B%20w_j%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20w_j%5E2%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%5Cright%20)%20%20%5Cright%20%5D%20=%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%5Cright%20)%20w_j%20+%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20"></p>
<p>This yields Equation (5).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20w_j%5E*%20=%20-%20%5Cfrac%7B%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%7D%20"></p>
</section>
<section id="split-finding" class="level4">
<h4 class="anchored" data-anchor-id="split-finding">Split Finding</h4>
<p>Now that we know how to find the optimal predicted value for any leaf node, we need to identify a criterion for finding a good tree structure, which boils down to finding the best split for a given node. Back in the [decision tree from scratch](/decision-tree-from-scratch post, we derived a split evaluation metric based on the reduction in the objective function associated with a particular split.<br>
To do that, first we need a way to compute the objective function given a particular tree structure. Substituting the optimal predicted values <img src="https://latex.codecogs.com/png.latex?w_j%5E*"> into the objective function, we get Equation (6).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20-%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bj=1%7D%5ET%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%7D%20+%20%5Cgamma%20T%20"></p>
<p>We can then evaluate potential splits by comparing the objective before making a split to the objective after making a split, where the split with the maximum reduction in objective (a.k.a. gain) is best.</p>
<p>More formally, let <img src="https://latex.codecogs.com/png.latex?I"> be the set of <img src="https://latex.codecogs.com/png.latex?n"> data instances in the current node, and let <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> be the instances that fall into the left and right child nodes of a proposed split. Let <img src="https://latex.codecogs.com/png.latex?L"> be the total loss for all instances in the node, while <img src="https://latex.codecogs.com/png.latex?L_L"> and <img src="https://latex.codecogs.com/png.latex?L_R"> are the losses for the left and right child nodes. The total loss contributed by instances in node <img src="https://latex.codecogs.com/png.latex?I"> prior to any split is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bbefore%20split%7D%7D%20=%20-%5Cfrac%7B1%7D%7B2%7D%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I%7D%20h_i%20+%20%5Clambda%7D%20+%20%5Cgamma%20"></p>
<p>And the loss after splitting <img src="https://latex.codecogs.com/png.latex?I"> into <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bafter%20split%7D%7D%20=%20L_L%20+%20L_R%20=%20-%5Cfrac%7B1%7D%7B2%7D%20%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_L%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_L%7D%20h_i%20+%20%5Clambda%7D%20-%5Cfrac%7B1%7D%7B2%7D%20%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_R%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_R%7D%20h_i%20+%20%5Clambda%7D%20+%202%20%5Cgamma%20"></p>
<p>The gain from this split is then</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CDelta%20L%20=%20L_%7B%5Ctext%7Bbefore%20split%7D%7D%20-%20%20L_%7B%5Ctext%7Bafter%20split%7D%7D%20=%20L%20-%20(L_L%20+%20L_R)"> <img src="https://latex.codecogs.com/png.latex?%5CDelta%20L%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%20%5B%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_L%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_L%7D%20h_i%20+%20%5Clambda%7D%20+%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_R%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_R%7D%20h_i%20+%20%5Clambda%7D%20-%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I%7D%20h_i%20+%20%5Clambda%7D%20%5Cright%20%5D%20-%20%5Cgamma%20"></p>
<p>which is Equation (7) from the paper. In practice it makes sense to accept a split only if the gain is positive, thus the <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> parameter sets the minimum gain required to make a further split. This is why <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> can be set with the parameter <code>gamma</code> or the more descriptive<code>min_loss_split</code>.</p>
</section>
</section>
</section>
<section id="tree-booster-innovations" class="level2">
<h2 class="anchored" data-anchor-id="tree-booster-innovations">Tree Booster Innovations</h2>
<section id="missing-values-and-sparsity-aware-split-finding" class="level3">
<h3 class="anchored" data-anchor-id="missing-values-and-sparsity-aware-split-finding">Missing Values and Sparsity-Aware Split Finding</h3>
<p>The XGBoost paper also introduces a modified algorithm for tree split finding which explicitly handles missing feature values. Recall that in order to find the best threshold value for a given feature, we can simply try all possible threshold values, recording the score for each. If some feature values are missing, the XGBoost split finding algorithm simply scores each threshold twice: once with missing value instances in the left node and once with them in the right node. The best split will then specify both the threshold value and to which node instances with missing values should be assigned. The paper calls this the sparsity aware split finding routine, which is defined as Algorithm 2.</p>
</section>
<section id="preventing-further-splitting" class="level3">
<h3 class="anchored" data-anchor-id="preventing-further-splitting">Preventing Further Splitting</h3>
<p>In addition to <code>min_loss_split</code> discussed above, XGBoost offers another parameter for limiting further tree splitting called <code>min_child_weight</code>. This name is a little confusing to me because the word “weight” has various meanings. In the context of this parameter, “weight” refers to the sum of the hessians <img src="https://latex.codecogs.com/png.latex?%5Csum%20h_i"> over instances in the node. For squared error loss <img src="https://latex.codecogs.com/png.latex?h_i=1">, so this is equivalent to the number of samples. Thus this parameter generalizes the notion of the minimum number of samples allowed in a terminal node.</p>
</section>
<section id="sampling" class="level3">
<h3 class="anchored" data-anchor-id="sampling">Sampling</h3>
<p>XGBoost takes a cue from Random Forest and introduces both column and row subsampling. These sampling methods can prevent overfitting and reduce training time by limiting the amount of data to be processed during boosting.</p>
<p>Like random forest, XGBoost implements column subsampling, which limits tree split finding to randomly selected subsets of features. XGBoost provides column sampling for each tree, for each depth level within a tree, and for each split point within a tree, controlled by <code>colsample_bytree</code>, <code>colsample_bbylevel</code>, and <code>colsample_bbynode</code> respectively.</p>
<p>One interesting distinction is that XGBoost implements row sampling without replacement using <code>subbsample</code>, whereas random forest uses bootstrapping. The choice to bootstrap rows in RF probably spurred from a desire to use as much data as possible while training on the smaller datasets of the 1990’s when RF was developed. With larger datasets and the ability to generate a large number of trees, XGBoost simply takes a subsample of rows for each tree.</p>
</section>
</section>
<section id="scalability" class="level2">
<h2 class="anchored" data-anchor-id="scalability">Scalability</h2>
<p>Even though we’re focused on statistical learning, I figured I’d comment on why XGBoost is highly scalable. Basically it boils down to efficient, parallelizable, and distributable methods for growing trees. You’ll notice there is a <code>tree_method</code> parameter which allows you to choose between the greedy exact algorithm (like the one we discussed in the decision tree from scratch post) and the approximate algorithm, which offers various scalability-related functionality, notably including the ability to consider only a small number of candidate split points instead of trying all possible splits. The algorithm also uses clever tricks like pre-sorting data for split finding and caching frequently needed values.</p>
<section id="why-xgboost-is-so-successful" class="level3">
<h3 class="anchored" data-anchor-id="why-xgboost-is-so-successful">Why XGBoost is so Successful</h3>
<p>As I mentioned in the intro, XGBoost is simply a very good implementation of the gradient boosting tree model. Therefore it inherits all the benefits of <a href="../../../consider-the-decision-tree">decision trees and tree ensembles</a>, while making even further improvements over the classic gradient boosting machine. These improvements boil down to</p>
<ol type="1">
<li>more ways to control overfitting</li>
<li>elegant handling of custom objectives</li>
<li>scalability</li>
</ol>
<p>First, XGBoost introduces two new tree regularization hyperparameters <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> which are baked directly into its objective function. Combining these with the additional column and row sampling functionality provides a variety of ways to reduce overfitting.</p>
<p>Second, the XGBoost formulation provides a much more elegant way to train models on custom objective functions. Recall that for <a href="../../../gradient-boosting-machine-with-any-loss-function">custom objectives</a>, the classic GBM finds tree structure by fitting a squared error decision tree to the gradients of the loss function and then sets each leaf’s predicted value by running a numerical optimization routine to find the optimal predicted value.</p>
<p>The XGBoost formulation improves on this two-stage approach by unifying the generation of tree structure and predicted values. Both the split scoring metric and the predicted values are directly computable from the instance gradient and hessian values, which are connected directly back to the overall training objective. This also removes the need for additional numerical optimizations, which contributes to speed, stability, and scalability.</p>
<p>Finally, speaking of scalability, XGBoost emerged at a time when industrial dataset size was exploding. Many use cases require scalable ML systems, and all use cases benefit from faster training and higher model development velocity.</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, there you go, those are the salient ideas behind XGBoost, the gold standard in gradient boosting model implementations. Hopefully now we all understand the mathematical basis for the algorithm and appreciate the key improvements it makes over the classic GBM. If you want to go even deeper, you can join us for the next post where we’ll roll up our sleeves and implement XGBoost entirely from scratch.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">The XGBoost paper</a></p>
</section>
<section id="exercise" class="level2">
<h2 class="anchored" data-anchor-id="exercise">Exercise</h2>
<p>Proove that the XGBoost Newton Descent generalizes the classic GBM gradient descent. Hint: show that XGBoost with a squared error objective and no regularization reduces to the classic GBM.</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <guid>https://blog.mattbowers.dev/posts/2022/how-to-understand-xgboost/index.html</guid>
  <pubDate>Sun, 13 Mar 2022 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2022/how-to-understand-xgboost/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Decision Tree From Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2021/decision-tree-from-scratch/index.html</link>
  <description><![CDATA[ 



<p><img src="https://blog.mattbowers.dev/posts/2021/decision-tree-from-scratch/thumbnail.png" title="decision tree" class="img-fluid"></p>
<p>Yesterday we had a lovely discussion about the key strengths and weaknesses of decision trees and why tree ensembles are so great. But today, gentle reader, we abandon our philosophizing and get down to the business of implementing one of these decision trees from scratch.</p>
<p>A note before we get started. This is going to be the most involved scratch-build that we’ve done at Random Realizations so far. It is not the kind of algorithm that I could just sit down and write all at once. We need to start with a basic frame and then add functionality step by step, testing all along the way to make sure things are working properly. Since I’m writing this in a jupyter notebook, I’ll try to give you a sense for how I actually put the algorithm together interactively in pieces, eventually landing on a fully-functional final product.</p>
<p>Shall we?</p>
<section id="binary-tree-data-structure" class="level2">
<h2 class="anchored" data-anchor-id="binary-tree-data-structure">Binary Tree Data Structure</h2>
<p>A decision tree takes a dataset with features and a target, partitions the feature space into chunks, and assigns a prediction value to each chunk. Since each partitioning step divides one chunk in two, and since the partitioning is done recursively, it’s natural to use a binary tree data structure to represent a decision tree.</p>
<p>The basic idea of the binary tree is that we define a class to represent nodes in the tree. If we want to add children to a given node, we simply assign them as attributes of the parent node. The child nodes we add are themselves instances of the same class, so we can add children to them in the same way.</p>
<p>Let’s start out with a simple class for our decision tree. It takes a single value called <code>max_depth</code> as input, which will dictate how many layers of child nodes should be inserted below the root. This controls the depth of the tree. As long as <code>max_depth</code> is positive, the parent will instantiate two new instances of the binary tree node class, passing along <code>max_depth</code> decremented by one and attaching the two children to itself as attributes called <code>left</code> and <code>right</code>.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> math</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np </span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">class</span> DecisionTree():</span>
<span id="cb2-2"></span>
<span id="cb2-3">        <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, max_depth):</span>
<span id="cb2-4">            <span class="cf" style="color: #003B4F;">assert</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb2-5">            <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> max_depth</span>
<span id="cb2-6">            <span class="cf" style="color: #003B4F;">if</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb2-7">                <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-8">                <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<p>Let’s make a new instance of our decision tree class, a tree with depth 2.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2021/decision-tree-from-scratch/binary_tree.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Binary tree structure diagram</figcaption><p></p>
</figure>
</div>
<p>We can access individual nodes and check their value of <code>max_depth</code>.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">t.max_depth, t.left.max_depth, t.left.right.max_depth</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(2, 1, 0)</code></pre>
</div>
</div>
<p>Our full decision tree can expand on this idea where each node receives some input, modifies it, creates two child nodes, and passes the modified input along to them. Specifically, each node in our decision tree will receive a dataset, determine how best to split the dataset into two parts, create two child nodes, and pass one part of the data to the left child and the other part to the right child.</p>
<p>All we have to do now is add some additional functionality to our decision tree. First we’ll start by capturing all the inputs we need to grow a tree, which include the feature dataframe <code>X</code>, the target array <code>y</code>, <code>max_depth</code> to explicitly limit tree depth, <code>min_samples_leaf</code> to specify the minimum number of observations that are allowed in a leaf node, and an optional <code>idxs</code> which specifies the indices of data that the node should use. The indices argument is useful for users of our decision tree because it will allow them to implement row subsampling in ensemble methods like random forest. It will also be handy for internal use inside the decision tree when passing data along to child nodes; instead of passing copies of the two data subsets, we’ll just pass a reference to the full dataset and pass along a set of indices to identify that node’s instance subset.</p>
<p>Once we get our input, we’ll do a little bit of input validation and store things that we want to keep as object attributes. In case this is a leaf node, we’ll go ahead and compute its predicted value; since this is a regression tree, the prediction is just the mean of the target <code>y</code>. We’ll also go ahead and initialize a score metric which we’ll use to help us find the best split later; since lower scores are going to be better, we’ll initialize it to positive infinity. Finally, we’ll push the logic to add child nodes into a method called <code>_maybe_insert_child_nodes</code> that we’ll define next.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>a leading underscore in a method name indicates the method is for internal use and not part of the user-facing API of the class.</p>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">class</span> DecisionTree():</span>
<span id="cb6-2"></span>
<span id="cb6-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb6-4">        <span class="cf" style="color: #003B4F;">assert</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb6-5">        <span class="cf" style="color: #003B4F;">assert</span> min_samples_leaf <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'min_samples_leaf must be positive'</span></span>
<span id="cb6-6">        <span class="va" style="color: #111111;">self</span>.min_samples_leaf, <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> min_samples_leaf, max_depth</span>
<span id="cb6-7">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;">=</span> y.values</span>
<span id="cb6-8">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(y))</span>
<span id="cb6-9">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, y, idxs</span>
<span id="cb6-10">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb6-11">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> np.mean(y[idxs]) <span class="co" style="color: #5E5E5E;"># node's prediction value</span></span>
<span id="cb6-12">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>) <span class="co" style="color: #5E5E5E;"># initial loss before split finding</span></span>
<span id="cb6-13">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb6-14">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb6-15">            </span>
<span id="cb6-16">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb6-17">        <span class="cf" style="color: #003B4F;">pass</span></span></code></pre></div>
</div>
<p>Now in order to test our class, we’ll need some actual data. We can use the same scikit-learn diabetes data from the last post.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> load_diabetes</span>
<span id="cb7-2"></span>
<span id="cb7-3">X, y <span class="op" style="color: #5E5E5E;">=</span> load_diabetes(as_frame<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_X_y<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
</div>
<p>So far, so good.</p>
</section>
<section id="inserting-child-nodes" class="level2">
<h2 class="anchored" data-anchor-id="inserting-child-nodes">Inserting Child Nodes</h2>
<p>Our node inserting function <code>_maybe_insert_child_nodes</code> needs to first find the best split; then if a valid split exists, it needs to insert the child nodes. To find the best valid split, we need to loop through the columns and search each one for the best valid split. Again we’ll push the logic of finding the best split into a function that we’ll define later. Next if no split was found, we need to bail by returning before trying to insert the child nodes. To check if this node is a leaf (i.e.&nbsp;it shouldn’t have child nodes), we define a property called <code>is_leaf</code> which will just check if the best score so far is still infinity, in which case no split was found and the node is a leaf.</p>
<p>If a valid split was found, then we need to insert the child nodes. We’ll assume that our split finding function assigned attributes called <code>split_feature_idx</code> and <code>threshold</code> to tell us the split feature’s index and the split threshold value. We then use these to compute the indices of the data to be passed to the child nodes; the left child gets instances where the split feature value is less than or equal to the threshold, and the right child node gets instances where the split feature value is greater than the threshold. Then we create two new decision trees, passing the corresponding data indices to each and assigning them to the <code>left</code> and <code>right</code> attributes of the current node.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb9-2">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): </span>
<span id="cb9-3">            <span class="va" style="color: #111111;">self</span>._find_better_split(j)</span>
<span id="cb9-4">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="co" style="color: #5E5E5E;"># do not insert children</span></span>
<span id="cb9-5">            <span class="cf" style="color: #003B4F;">return</span> </span>
<span id="cb9-6">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb9-7">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb9-8">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb9-9">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb9-10">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb9-11">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb9-12">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb9-13"></span>
<span id="cb9-14">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb9-15">        <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb9-16">    </span>
<span id="cb9-17">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb9-18">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>)</span></code></pre></div>
</div>
<p>To test these new methods , we can assign them to our <code>DecisionTree</code> class and create a new class instance to make sure things are still working.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">DecisionTree._maybe_insert_child_nodes <span class="op" style="color: #5E5E5E;">=</span> _maybe_insert_child_nodes</span>
<span id="cb10-2">DecisionTree._find_better_split <span class="op" style="color: #5E5E5E;">=</span> _find_better_split</span>
<span id="cb10-3">DecisionTree.is_leaf <span class="op" style="color: #5E5E5E;">=</span> is_leaf</span>
<span id="cb10-4">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>)</span></code></pre></div>
</div>
<p>Yep, we’re still looking good.</p>
</section>
<section id="split-finding" class="level2">
<h2 class="anchored" data-anchor-id="split-finding">Split Finding</h2>
<p>Now we need to fill in the functionality of the split finding method. The overall strategy is to consider every possible way to split on the current feature, measuring the quality of each potential split with some scoring mechanism, and keeping track of the best split we’ve seen so far. We’ll come back to the issue of how to try all the possible splits in a moment, but let’s start by figuring out how to score a particular potential split.</p>
<p>Like other machine learning models, trees are trained by attempting to minimize some loss function that measures how well the model predicts the target data. We’ll be training our regression tree to minimize squared error.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%20=%20%5Csum_%7Bi=1%7D%5En%20(y_i-%5Chat%7By%7D_i)%5E2"></p>
<p>For a given node, we can replace <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cbar%7By%7D"> because each node uses the sample mean of its target instances as its prediction. We can then rewrite the loss for a given node as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%20=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Cbar%7By%7D)%5E2%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi=1%7D%5En(y_i%5E2%20-2y_i%5Cbar%7By%7D%20+%20%5Cbar%7By%7D%5E2)%20%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi=1%7D%5Eny_i%5E2%20-2%5Cbar%7By%7D%5Csum_%7Bi=1%7D%5Eny_i%20+%20n%5Cbar%7By%7D%5E2%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi=1%7D%5Eny_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi=1%7D%5Eny_i%20%5Cright%20)%5E2%20"></p>
<p>We can then evaluate potential splits by comparing the loss after splitting to the loss before splitting, where the split with the greatest loss reduction is best. Let’s work out a simple expression for the loss reduction from a given split.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?I"> be the set of <img src="https://latex.codecogs.com/png.latex?n"> data instances in the current node, and let <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> be the instances that fall into the left and right child nodes of a proposed split. Let <img src="https://latex.codecogs.com/png.latex?L"> be the total loss for all instances in the node, while <img src="https://latex.codecogs.com/png.latex?L_L"> and <img src="https://latex.codecogs.com/png.latex?L_R"> are the losses for the left and right child nodes. The total loss contributed by instances in <img src="https://latex.codecogs.com/png.latex?I"> prior to any split is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bbefore%20split%7D%7D%20=%20L%20=%20%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%20%5Cright%20)%5E2%20"></p>
<p>And the loss after splitting <img src="https://latex.codecogs.com/png.latex?I"> into <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bafter%20split%7D%7D%20=%20L_L%20+%20L_R%20=%20%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_L%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%20%5Cright%20)%5E2%20+%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_R%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%20%5Cright%20)%5E2%20"></p>
<p>The reduction in loss from this split is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CDelta%20L%20=%20L_%7B%5Ctext%7Bafter%20split%7D%7D%20-%20%20L_%7B%5Ctext%7Bbefore%20split%7D%7D%20=%20(L_L%20+%20L_R)%20-%20L%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_L%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%20%5Cright%20)%5E2%20+%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_R%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%20%5Cright%20)%5E2%20-%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%20%5Cright%20)%5E2%20%5Cright%20)%20"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?I%20=%20I_L%20%5Ccup%20I_R"> the <img src="https://latex.codecogs.com/png.latex?%5Csum%20y%5E2"> terms cancel and we can simplify.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CDelta%20L%20=%20-%20%5Cfrac%7B1%7D%7Bn_L%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%20%5Cright%20)%5E2%0A-%20%5Cfrac%7B1%7D%7Bn_R%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%20%5Cright%20)%5E2%0A+%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%20%5Cright%20)%5E2%20%20"></p>
<p>This is a really nice formulation of the split scoring metric from a computational complexity perspective. We can sort the data by the feature values then, starting with the smallest <code>min_samples_leaf</code> instances in the left node and the rest in the right node, we check the score. Then to check the next split, we simply move a single target value from the right node into the left node, updating the score by subtracting it from the right node’s partial sum and adding it to the left node’s partial sum. The third term is constant for all splits, so we only need to compute it once. If any split’s score is lower than the best score so far, then we update the best score so far, the split feature, and the threshold value. When we’re done we can be sure we found the best possible split. The time bottleneck is the sort, which puts us at an average time complexity of <img src="https://latex.codecogs.com/png.latex?O(n%5Clog%20n)">.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb11-2">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,feature_idx]</span>
<span id="cb11-3">        y <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.y[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb11-4">        sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb11-5">        sort_y, sort_x <span class="op" style="color: #5E5E5E;">=</span> y[sort_idx], x[sort_idx]</span>
<span id="cb11-6">        sum_y, n <span class="op" style="color: #5E5E5E;">=</span> y.<span class="bu" style="color: null;">sum</span>(), <span class="bu" style="color: null;">len</span>(y)</span>
<span id="cb11-7">        sum_y_right, n_right <span class="op" style="color: #5E5E5E;">=</span> sum_y, n</span>
<span id="cb11-8">        sum_y_left, n_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb11-9">    </span>
<span id="cb11-10">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf):</span>
<span id="cb11-11">            y_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_y[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb11-12">            sum_y_left <span class="op" style="color: #5E5E5E;">+=</span> y_i<span class="op" style="color: #5E5E5E;">;</span> sum_y_right <span class="op" style="color: #5E5E5E;">-=</span> y_i</span>
<span id="cb11-13">            n_left <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">;</span> n_right <span class="op" style="color: #5E5E5E;">-=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb11-14">            <span class="cf" style="color: #003B4F;">if</span>  n_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:</span>
<span id="cb11-15">                <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb11-16">            score <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span> sum_y_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_left <span class="op" style="color: #5E5E5E;">-</span> sum_y_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_right <span class="op" style="color: #5E5E5E;">+</span> sum_y<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n</span>
<span id="cb11-17">            <span class="cf" style="color: #003B4F;">if</span> score <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far:</span>
<span id="cb11-18">                <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> score</span>
<span id="cb11-19">                <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb11-20">                <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span></code></pre></div>
</div>
<p>Again, we assign the split finding method to our class and instantiate a new tree to make sure things are still working.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">DecisionTree._find_better_split <span class="op" style="color: #5E5E5E;">=</span> _find_better_split</span>
<span id="cb12-2">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>)</span>
<span id="cb12-3">X.columns[t.split_feature_idx], t.threshold</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>('s5', -0.0037611760063045703)</code></pre>
</div>
</div>
<p>Nice! Looks like the tree started with a split on the <code>s5</code> feature.</p>
</section>
<section id="inspecting-the-tree" class="level2">
<h2 class="anchored" data-anchor-id="inspecting-the-tree">Inspecting the Tree</h2>
<p>While we’re developing something complex like a decision tree class, we need a good way to inspect the object to help with testing and debugging. Let’s write a quick string representation method to make it easier to check what’s going on with a particular node.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb14-2">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f'n: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>n<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb14-3">        s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; value:</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>value<span class="sc" style="color: #5E5E5E;">:0.2f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb14-4">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">self</span>.is_leaf:</span>
<span id="cb14-5">            split_feature_name <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.columns[<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb14-6">            s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; split: </span><span class="sc" style="color: #5E5E5E;">{</span>split_feature_name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> &lt;= </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>threshold<span class="sc" style="color: #5E5E5E;">:0.3f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;">return</span> s</span></code></pre></div>
</div>
<p>We can assign the string representation method to the class and print a few nodes.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">DecisionTree.<span class="fu" style="color: #4758AB;">__repr__</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">__repr__</span></span>
<span id="cb15-2">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb15-3"><span class="bu" style="color: null;">print</span>(t)</span>
<span id="cb15-4"><span class="bu" style="color: null;">print</span>(t.left)</span>
<span id="cb15-5"><span class="bu" style="color: null;">print</span>(t.left.left)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>n: 442; value:152.13; split: s5 &lt;= -0.004
n: 218; value:109.99; split: bmi &lt;= 0.006
n: 171; value:96.31</code></pre>
</div>
</div>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>We need a public <code>predict</code> method that takes a feature dataframe and returns an array of predictions. We’ll need to look up the predicted value for one instance at a time and stitch them together in an array. We can do that by iterating over the feature dataframe rows with a list comprehension that calls a <code>_predict_row</code> method to grab the prediction for each row. The row predict method needs to return the current node’s predicted value if it’s a leaf, or if not, it needs to identify the appropriate child node based on its split and ask it for a prediction.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb17-2">        <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb17-3">    </span>
<span id="cb17-4">    <span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb17-5">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb17-6">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb17-7">        child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb17-8">                <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb17-9">        <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span></code></pre></div>
</div>
<p>Let’s assign the predict methods and make predictions on a few rows.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">DecisionTree.predict <span class="op" style="color: #5E5E5E;">=</span> predict</span>
<span id="cb18-2">DecisionTree._predict_row <span class="op" style="color: #5E5E5E;">=</span> _predict_row</span>
<span id="cb18-3">t.predict(X.iloc[:<span class="dv" style="color: #AD0000;">3</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>array([225.87962963,  96.30994152, 225.87962963])</code></pre>
</div>
</div>
</section>
<section id="the-complete-decision-tree-implementation" class="level2">
<h2 class="anchored" data-anchor-id="the-complete-decision-tree-implementation">The Complete Decision Tree Implementation</h2>
<p>Here’s the implementation, all in one place.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;">class</span> DecisionTree():</span>
<span id="cb20-2"></span>
<span id="cb20-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb20-4">        <span class="cf" style="color: #003B4F;">assert</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb20-5">        <span class="cf" style="color: #003B4F;">assert</span> min_samples_leaf <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'min_samples_leaf must be positive'</span></span>
<span id="cb20-6">        <span class="va" style="color: #111111;">self</span>.min_samples_leaf, <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> min_samples_leaf, max_depth</span>
<span id="cb20-7">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;">=</span> y.values</span>
<span id="cb20-8">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(y))</span>
<span id="cb20-9">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, y, idxs</span>
<span id="cb20-10">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb20-11">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> np.mean(y[idxs]) <span class="co" style="color: #5E5E5E;"># node's prediction value</span></span>
<span id="cb20-12">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>) <span class="co" style="color: #5E5E5E;"># initial loss before split finding</span></span>
<span id="cb20-13">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb20-14">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb20-15">            </span>
<span id="cb20-16">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb20-17">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): </span>
<span id="cb20-18">            <span class="va" style="color: #111111;">self</span>._find_better_split(j)</span>
<span id="cb20-19">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="co" style="color: #5E5E5E;"># do not insert children</span></span>
<span id="cb20-20">            <span class="cf" style="color: #003B4F;">return</span> </span>
<span id="cb20-21">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb20-22">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb20-23">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb20-24">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb20-25">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb20-26">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb20-27">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb20-28">    </span>
<span id="cb20-29">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb20-30">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>)</span>
<span id="cb20-31">    </span>
<span id="cb20-32">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb20-33">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,feature_idx]</span>
<span id="cb20-34">        y <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.y[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb20-35">        sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb20-36">        sort_y, sort_x <span class="op" style="color: #5E5E5E;">=</span> y[sort_idx], x[sort_idx]</span>
<span id="cb20-37">        sum_y, n <span class="op" style="color: #5E5E5E;">=</span> y.<span class="bu" style="color: null;">sum</span>(), <span class="bu" style="color: null;">len</span>(y)</span>
<span id="cb20-38">        sum_y_right, n_right <span class="op" style="color: #5E5E5E;">=</span> sum_y, n</span>
<span id="cb20-39">        sum_y_left, n_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb20-40">    </span>
<span id="cb20-41">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf):</span>
<span id="cb20-42">            y_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_y[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb20-43">            sum_y_left <span class="op" style="color: #5E5E5E;">+=</span> y_i<span class="op" style="color: #5E5E5E;">;</span> sum_y_right <span class="op" style="color: #5E5E5E;">-=</span> y_i</span>
<span id="cb20-44">            n_left <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">;</span> n_right <span class="op" style="color: #5E5E5E;">-=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb20-45">            <span class="cf" style="color: #003B4F;">if</span>  n_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:</span>
<span id="cb20-46">                <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb20-47">            score <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span> sum_y_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_left <span class="op" style="color: #5E5E5E;">-</span> sum_y_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_right <span class="op" style="color: #5E5E5E;">+</span> sum_y<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n</span>
<span id="cb20-48">            <span class="cf" style="color: #003B4F;">if</span> score <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far:</span>
<span id="cb20-49">                <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> score</span>
<span id="cb20-50">                <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb20-51">                <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb20-52">                </span>
<span id="cb20-53">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb20-54">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f'n: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>n<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb20-55">        s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; value:</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>value<span class="sc" style="color: #5E5E5E;">:0.2f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb20-56">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">self</span>.is_leaf:</span>
<span id="cb20-57">            split_feature_name <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.columns[<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb20-58">            s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; split: </span><span class="sc" style="color: #5E5E5E;">{</span>split_feature_name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> &lt;= </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>threshold<span class="sc" style="color: #5E5E5E;">:0.3f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb20-59">        <span class="cf" style="color: #003B4F;">return</span> s</span>
<span id="cb20-60">    </span>
<span id="cb20-61">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb20-62">        <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb20-63">    </span>
<span id="cb20-64">    <span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb20-65">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb20-66">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb20-67">        child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb20-68">                <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb20-69">        <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span></code></pre></div>
</div>
</section>
<section id="from-scratch-versus-scikit-learn" class="level2">
<h2 class="anchored" data-anchor-id="from-scratch-versus-scikit-learn">From Scratch versus Scikit-Learn</h2>
<p>As usual, we’ll test our homegrown handiwork by comparing it to the existing implementation in scikit-learn. First let’s train both models on the <a href="https://scikit-learn.org/stable/datasets/real_world.html">California Housing dataset</a> which gives us 20k instances and 8 features to predict median house price by district.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> fetch_california_housing</span>
<span id="cb21-2"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb21-3"></span>
<span id="cb21-4">X, y <span class="op" style="color: #5E5E5E;">=</span> fetch_california_housing(as_frame<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_X_y<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb21-5">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.3</span>, random_state<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">43</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="im" style="color: #00769E;">from</span> sklearn.tree <span class="im" style="color: #00769E;">import</span> DecisionTreeRegressor</span>
<span id="cb22-2"><span class="im" style="color: #00769E;">from</span> sklearn.metrics <span class="im" style="color: #00769E;">import</span> mean_squared_error</span>
<span id="cb22-3"></span>
<span id="cb22-4">max_depth <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">8</span></span>
<span id="cb22-5">min_samples_leaf <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">16</span></span>
<span id="cb22-6"></span>
<span id="cb22-7">tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X_train, y_train, max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span>
<span id="cb22-8">pred <span class="op" style="color: #5E5E5E;">=</span> tree.predict(X_test)</span>
<span id="cb22-9"></span>
<span id="cb22-10">sk_tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span>
<span id="cb22-11">sk_tree.fit(X_train, y_train)</span>
<span id="cb22-12">sk_pred <span class="op" style="color: #5E5E5E;">=</span> sk_tree.predict(X_test)</span>
<span id="cb22-13"></span>
<span id="cb22-14"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'from scratch MSE: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_squared_error(y_test, pred)<span class="sc" style="color: #5E5E5E;">:0.4f}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb22-15"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'scikit-learn MSE: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_squared_error(y_test, sk_pred)<span class="sc" style="color: #5E5E5E;">:0.4f}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>from scratch MSE: 0.3988
scikit-learn MSE: 0.3988</code></pre>
</div>
</div>
<p>We get similar accuracy on a held-out test dataset.</p>
<p>Let’s benchmark the two implementations on training time.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb24-2">sk_tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span>
<span id="cb24-3">sk_tree.fit(X_train, y_train)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 45.3 ms, sys: 555 µs, total: 45.8 ms
Wall time: 45.3 ms</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(max_depth=8, min_samples_leaf=16)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(max_depth=8, min_samples_leaf=16)</pre></div></div></div></div></div>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb26-2">tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X_train, y_train, max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 624 ms, sys: 1.65 ms, total: 625 ms
Wall time: 625 ms</code></pre>
</div>
</div>
<p>Wow, the scikit-learn implementation absolutely smoked us, training an order of magnitude faster. This is to be expected, since they implement split finding in cython, which generates compiled C code that can run much faster than our native python code. Maybe we can take a look at how to optimize python code with cython here on the blog one of these days.</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Holy cow, we just implemented a decision tree using nothing but numpy. I hope you enjoyed the scratch build as much as I did, and I hope you got a little bit better at coding (I certainly did). That was actually way harder than I expected, but looking back at the finished product, it doesn’t seem so bad right? I almost thought we were going to get away with not implementing our own decision tree, but it turns out that this will be super helpful for us when it comes time to implement XGBoost from scratch.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>This implementation is inspired and partially adapted from Jeremy Howard’s live coding of a <a href="https://course18.fast.ai/lessonsml1/lesson7.html">Random Forest</a> as part of the fastai ML course.</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://blog.mattbowers.dev/posts/2021/decision-tree-from-scratch/index.html</guid>
  <pubDate>Mon, 13 Dec 2021 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2021/decision-tree-from-scratch/thumbnail.png" medium="image" type="image/png" height="86" width="144"/>
</item>
<item>
  <title>Consider the Decision Tree</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/main.jpg" title="." class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">A California cypress tree abides in silence on Alameda Beach.</figcaption><p></p>
</figure>
</div>
<p>Ah, the decision tree. It’s an underrated and often overlooked hero of modern statistical learning. Trees aren’t particularly powerful learning algorithms on their own, but when utilized as building blocks in larger ensemble models like random forest and gradient boosted trees, they can achieve state of the art performance in many practical applications. Since we’ve been focusing on gradient boosting ensembles lately, let’s take a moment to consider the humble decision tree itself. This post gives a high-level intuition for how trees work, an opinionated list of their key strengths and weaknesses, and some perspective on why ensembling makes them truly shine.</p>
<p>Onward!</p>
<section id="classification-and-regression-trees" class="level2">
<h2 class="anchored" data-anchor-id="classification-and-regression-trees">Classification and Regression Trees</h2>
<p>A Decision tree is a type of statistical model that takes features or covariates as input and yields a prediction as output. The idea of the decision tree as a statistical learning tool traces back to a monograph published in 1984 by Breiman, Freidman, Olshen, and Stone called “Classification and Regression Trees” (a.k.a. CART). As the name suggests, trees come in two main varieties: classification trees which predict discrete class labels (e.g.&nbsp;<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">DecisionTreeClassifier</a>) and regression trees which predict numeric values (e.g.&nbsp;<a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">DecisionTreeRegressor</a>).</p>
<p>As I mentioned earlier, tree models are not very powerful learners on their own. You might find that an individual tree model is useful for creating a simple and highly interpretable model in specific situations, but in general, trees tend to shine most as building blocks in more complex algorithms. These composite models are called ensembles, and the most important tree ensembles are random forest and gradient boosted trees. While random forest uses either regression or classification trees depending on the type of target, gradient boosting can use regression trees to solve both classification and regression tasks.</p>
</section>
<section id="regression-tree-in-action" class="level2">
<h2 class="anchored" data-anchor-id="regression-tree-in-action">Regression Tree in Action</h2>
<p>Let’s have a closer look at regression trees by training one on the diabetes dataset from scikit learn. According to the <a href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset">documentation</a>:</p>
<blockquote class="blockquote">
<p>Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.</p>
</blockquote>
<p>First we load the data. To make our lives easier, we’ll just use two features: average blood pressure (bp) and the first blood serum measurement (s1) to predict the target. I’ll rescale the features to make the values easier for me to read, but it won’t affect our tree–more on that later.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np </span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sns</span>
<span id="cb1-5">color_palette <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"viridis"</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> load_diabetes</span>
<span id="cb2-2"></span>
<span id="cb2-3">X, y <span class="op" style="color: #5E5E5E;">=</span> load_diabetes(as_frame<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_X_y<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-4"></span>
<span id="cb2-5">X <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100</span> <span class="op" style="color: #5E5E5E;">*</span> X[[<span class="st" style="color: #20794D;">'bp'</span>, <span class="st" style="color: #20794D;">'s1'</span>]]</span></code></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="4">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-5-output-1.png" class="img-fluid" alt="Scatterplot of two features with color of the points indicating the target value for each data point"></p>
</div>
</div>
<p>Let’s grow a tree to predict the target given values of blood pressure and blood serum.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> sklearn.tree <span class="im" style="color: #00769E;">import</span> DecisionTreeRegressor</span>
<span id="cb3-2"></span>
<span id="cb3-3">tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb3-4">tree.fit(X,y)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="6">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-7-output-1.png" class="img-fluid" alt="Tree diagram showing split rules and predicted values for the fitted tree"></p>
</div>
</div>
<p>To make predictions using our fitted tree, we start at the root node (which is at the top), and we work our way down moving left if our feature is less than the split threshold and to the right if it’s greater than the split threshold. For example let’s predict the target for a new case with bp= 1 and s1 = 5. Since our blood pressure of 1 is less than 2.359, we move to the left child node. Here, since our serum of 5 is greater than the threshold at 0.875, we move to the right child node. This node has no further children, and thus we return its predicted value of 155.343.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">tree.predict(pd.DataFrame({<span class="st" style="color: #20794D;">'bp'</span>: <span class="dv" style="color: #AD0000;">1</span>, <span class="st" style="color: #20794D;">'s1'</span>: <span class="dv" style="color: #AD0000;">5</span>}, index<span class="op" style="color: #5E5E5E;">=</span>[<span class="dv" style="color: #AD0000;">0</span>]))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([155.34313725])</code></pre>
</div>
</div>
<p>Let’s overlay these splits on our feature scatterplot to see how the tree has partitioned the feature space.</p>
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-9-output-1.png" class="img-fluid" alt="Scatterplot of two features with vertical and horizontal dashed lines corresponding to the splits"></p>
</div>
</div>
<p>The tree has managed to carve out regions of feature space where the target values tend to be similar within each region, e.g.&nbsp;we have low target values in the bottom left partition and high target values in the far right region.</p>
<p>Let’s take a look at the regression surface predicted by our tree. Since the tree predicts the exact same value for all instances in a given partition, the surface has only four distinct values.</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-10-output-1.png" class="img-fluid" alt="Plot of the space spanned by two features with color indicating the model predicted value"></p>
</div>
</div>
<p>Fabulous, now that we’ve seen a tree in action, let’s talk about trees’ key strengths and weaknesses.</p>
</section>
<section id="why-trees-are-awesome" class="level2">
<h2 class="anchored" data-anchor-id="why-trees-are-awesome">Why trees are awesome</h2>
<p>Trees are awesome because they are easy to use, and trees are easy to use because they are robust, require minimal data preprocessing, and can learn complex relationships without user intervention.</p>
<section id="feature-scaling" class="level3">
<h3 class="anchored" data-anchor-id="feature-scaling">Feature Scaling</h3>
<p>Trees owe their minimal data preprocessing requirements and their robustness to the fact that split finding is controlled by the sort order of the input feature values, rather than the values themselves. This means that trees are invariant to the scaling of input features, which in turn means that we don’t need to fuss around with carefully rescaling all the numeric features before fitting a tree. It also means that trees tend to work well even if features are highly skewed or contain outliers.</p>
</section>
<section id="categoricals" class="level3">
<h3 class="anchored" data-anchor-id="categoricals">Categoricals</h3>
<p>Since trees just split data based on numeric feature values, we can easily handle most categorical features by using integer encoding. For example we might encode a size feature with small = 1, medium = 2, and large = 3. This works particularly well with ordered categories, because partitioning is consistent with the category semantics. It can also work well even if the categories have no order, because with enough splits a tree can carve each category into its own partition.</p>
</section>
<section id="missing-values" class="level3">
<h3 class="anchored" data-anchor-id="missing-values">Missing Values</h3>
<p>It’s worth calling out that different implementations of the decision tree handle missing feature values in different ways. Notably, scikit-learn handles them by throwing an error and telling you not to pull such shenanigans.</p>
<pre><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float32').</code></pre>
<p>On the other hand, XGBoost supports an elegant way to make use of missing values, which we will discuss more in a later post.</p>
</section>
<section id="interactions" class="level3">
<h3 class="anchored" data-anchor-id="interactions">Interactions</h3>
<p>Feature interactions can also be learned automatically. An interaction means that the effect of one feature on the target differs depending on the value of another feature. For example, the effect of some drug may depend on whether or not the patient exercises. After a tree splits on exercise, it can naturally learn the correct drug effects for both exercisers and non-exercisers. This intuition extends to higher-order interactions as well, as long as the tree has enough splits to parse the relationships.</p>
</section>
<section id="feature-selection" class="level3">
<h3 class="anchored" data-anchor-id="feature-selection">Feature Selection</h3>
<p>Because trees choose the best feature and threshold value at each split, they essentially perform automatic feature selection. This is great because even if we throw a lot of irrelevant features at a decision tree, it will simply tend not to use them for splits. Similarly, if two or more features are highly correlated or even redundant, the tree will simply choose one or the other when making each split; having both in the model will not cause catastrophic instability as it could in a linear model.</p>
</section>
<section id="feature-target-relationship" class="level3">
<h3 class="anchored" data-anchor-id="feature-target-relationship">Feature-Target Relationship</h3>
<p>Finally, it is possible for trees to discover complex nonlinear feature-target relationships without the need for user-specification of the relationships. This is because trees use local piecewise constant approximations without making any parametric assumptions. With enough splits, the tree can approximate arbitrary feature-target relationships.</p>
<div class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-12-output-1.png" class="img-fluid" alt="Figure showing sinusoidal scatter data and a piecewise constant approximation from a decision tree"></p>
</div>
</div>
</section>
</section>
<section id="why-trees-are-not-so-awesome" class="level2">
<h2 class="anchored" data-anchor-id="why-trees-are-not-so-awesome">Why trees are not so awesome</h2>
<p>The main weakness of the decision tree is that, on its own, it tends to have poor predictive performance compared to other algorithms. The main reasons for this are the tendency to overfit and prediction quantization issues.</p>
<section id="overfitting" class="level3">
<h3 class="anchored" data-anchor-id="overfitting">Overfitting</h3>
<p>If we grow a decision tree until each leaf has exactly one instance in it, we will have simply memorized the training data, and our model will not generalize well. Basically the only defense against overfitting is to reduce the number of leaf nodes in the tree, either by using hyperparameters to stop splitting earlier or by removing certain leaf nodes after growing a deep tree. The problem here is that some of the benefits of trees, like ability to approximate arbitrary target patterns and ability to learn interaction effects, depend on having enough splits for the task. We can sometimes find ourselves in a situation where we cannot learn these complex relationships without overfitting the tree.</p>
</section>
<section id="quantization" class="level3">
<h3 class="anchored" data-anchor-id="quantization">Quantization</h3>
<p>Because regression trees use piecewise constant functions to approximate the target, prediction accuracy can deteriorate near split boundaries. For example, if the target is increasing with the feature, a tree might tend to overpredict the target on the left side of split boundaries and overpredict on the right side of split boundaries.</p>
<div class="cell" data-scrolled="true" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-13-output-1.png" class="img-fluid" alt="Figure showing scatter data and a decision tree fit where data points near splits are either over or under predicted"></p>
</div>
</div>
</section>
<section id="extrapolation" class="level3">
<h3 class="anchored" data-anchor-id="extrapolation">Extrapolation</h3>
<p>Because they are trained by partitioning the feature space in a training dataset, trees cannot intelligently extrapolate beyond the data on which they are trained. For example if we query a tree for predictions beyond the greatest feature value encountered in training, it will just return the prediction corresponding to the largest in-sample feature values.</p>
<div class="cell" data-scrolled="true" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-14-output-1.png" class="img-fluid" alt="Figure showing scatter data and a decision tree fit with a constant predicted value outside the data range"></p>
</div>
</div>
</section>
<section id="the-dark-side-of-convenience" class="level3">
<h3 class="anchored" data-anchor-id="the-dark-side-of-convenience">The Dark Side of Convenience</h3>
<p>Finally, there is always a price to pay for convenience. While trees can work well even with a messy dataset containing outliers, redundant features, and thoughtlessly encoded categoricals, we will rarely achieve the best performance under these conditions. Taking the time to deal with outliers, removing redundant information, purposefully choosing appropriate categorical encodings, and building an understanding of the data will often lead to much better results.</p>
</section>
</section>
<section id="how-ensembling-makes-trees-shine" class="level2">
<h2 class="anchored" data-anchor-id="how-ensembling-makes-trees-shine">How ensembling makes trees shine</h2>
<p>We can go a long way toward addressing the issues of overfitting and prediction quantization by using trees as building blocks in larger algorithms called tree ensembles, the most popular examples being random forest and gradient boosted trees. A tree ensemble is a collection of different individual tree models whose predictions are averaged to generate an overall prediction.</p>
<p>Ensembling helps address overfitting because even if each individual tree is overfitted, the average of their individual noisy predictions will tend to be more stable. Think of it in terms of the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias variance tradeoff</a>, where bias refers to a model’s failure to capture certain patterns and variance refers to how different a model prediction would be if the model were trained on a different sample of training data. Since the ensemble is averaging over the predictions of all the individual models, training it on a different sample of training data would change the individual models predictions, but their overall average prediction will tend to remain stable. Thus, ensembling helps reduce the effects of overfitting by reducing model variance without increasing bias.</p>
<p>Ensembling also helps address prediction quantization issues. While each individual tree’s predictions might express large jumps in the regression surface, averaging many different trees’ predictions together effectively generates a surface with more partitions and smaller jumps between them. This provides a smoother approximation of the feature-target relationship.</p>
<div class="cell" data-scrolled="true" data-execution_count="14">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/consider-the-decision-tree_files/figure-html/cell-15-output-1.png" class="img-fluid" alt="Figure showing scatter data and a smooth model prediction from a tree ensemble model"></p>
</div>
</div>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, there you go, that’s my take on the high-level overview of the decision tree and its main strengths and weaknesses. As we’ve seen, ensembling allows us to keep the conveniences of the decision tree while mitigating its core weakness of relatively weak predictive power. This is why tree ensembles are so popular in practical applications. We glossed over pretty much all details of how trees actually do their magic, but fear not, next time we’re going to get rowdy and build one of these things from scratch.</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <guid>https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/index.html</guid>
  <pubDate>Sun, 12 Dec 2021 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2021/consider-the-decision-tree/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How to Implement a Gradient Boosting Machine that Works with Any Loss Function</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Cold water cascades over the rocks in Erwin, Tennessee.</figcaption><p></p>
</figure>
</div>
<p>Friends, this is going to be an epic post! Today, we bring together all the ideas we’ve built up over the past few posts to nail down our understanding of the key ideas in Jerome Friedman’s seminal 2001 paper: “<a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine</a>.” In particular, we’ll summarize the highlights from the paper, and we’ll build an in-house python implementation of his generic gradient boosting algorithm which can train with any differentiable loss function. What’s more, we’ll go ahead and take our generic gradient boosting machine for a spin by training it with several of the most popular loss functions used in practice.</p>
<p>Are you freaking stoked or what?</p>
<p>Sweet. Let’s do this.</p>
<section id="friedman-2001-tldr" class="level2">
<h2 class="anchored" data-anchor-id="friedman-2001-tldr">Friedman 2001: TL;DR</h2>
<p>I’ve mentioned this paper a couple of times before, but as far as I can tell, this is the origin of gradient boosting; it is therefore, a seminal work worth reading. You know what, I think you might like to pick up <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">the paper</a> and read it yourself. Like many papers, there is a lot of scary looking math in the first few pages, but if you’ve been following along on this blog, you’ll find that it’s actually totally approachable. This is the kind of thing that cures imposter syndrome, so give it a shot. That said, here’s the TL;DR as I see it.</p>
<p>The first part of the paper introduces the idea of fitting models by doing gradient descent in function space, an ingenious idea we spent <a href="../../../how-gradient-boosting-does-gradient-descent">an entire post</a> demystifying earlier. Friedman goes on to introduce the generic gradient boost algorithm, which works with any differentiable loss function, as well as specific variants for minimizing absolute error, Huber loss, and binary deviance. In terms of hyperparameters, he points out that the learning rate can be used to reduce overfitting, while increased tree depth can help capture more complex interactions among features. He even discusses feature importance and partial dependence methods for interpreting fitted gradient boosting models.</p>
<p>Friedman concludes by musing about the advantages of gradient boosting with trees. He notes some key advantages afforded by the use of decision trees including no need to rescale input data, robustness against irrelevant input features, and elegant handling of missing feature values. He points out that gradient boosting manages to capitalize on the benefits of decision trees while minimizing their key weakness (crappy accuracy). I think this offers a great insight into why gradient boosting models have become so widespread and successful in practical ML applications.</p>
</section>
<section id="friedmans-generic-gradient-boosting-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="friedmans-generic-gradient-boosting-algorithm">Friedman’s Generic Gradient Boosting Algorithm</h2>
<p>Let’s take a closer look at Friedman’s original gradient boost algorithm, Alg. 1 in Section 3 of <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">the paper</a> (translated into the notation we’ve been using so far).</p>
<p>Like last time, we have training data <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7By%7D,%20%5Cmathbf%7BX%7D)"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> is a length-<img src="https://latex.codecogs.com/png.latex?n"> vector of target values, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p"> matrix with <img src="https://latex.codecogs.com/png.latex?n"> observations of <img src="https://latex.codecogs.com/png.latex?p"> features. We also have a differentiable loss function <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D)%20=%20%5Csum_%7Bi=1%7D%5En%20l(y_i,%20%5Chat%7By%7D_i)">, a “learning rate” hyperparameter <img src="https://latex.codecogs.com/png.latex?%5Ceta">, and a fixed number of model iterations <img src="https://latex.codecogs.com/png.latex?M">.</p>
<p><strong>Algorithm:</strong> <em>gradient_boost</em><img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7BX%7D,%5Cmathbf%7By%7D,L,%5Ceta,%20M)"> returns: model <img src="https://latex.codecogs.com/png.latex?F_M"></p>
<ol type="1">
<li><p>Let base model <img src="https://latex.codecogs.com/png.latex?F_0(%5Cmathbf%7Bx%7D)%20=%20c">, where <img src="https://latex.codecogs.com/png.latex?c%20=%20%5Ctext%7Bargmin%7D_%7Bc%7D%20%5Csum_%7Bi=1%7D%5En%20l(y_i,%20c)"></p></li>
<li><p><code>for</code> <img src="https://latex.codecogs.com/png.latex?m"> = <img src="https://latex.codecogs.com/png.latex?0"> to <img src="https://latex.codecogs.com/png.latex?M-1">:</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; Let “pseudo-residual” vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D_m%20=%20-%5Cnabla_%7B%5Cmathbf%7B%5Chat%7By%7D%7D_m%7D%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D_m)"></p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; Train decision tree regressor <img src="https://latex.codecogs.com/png.latex?h_m(%5Cmathbf%7BX%7D)"> to predict <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D_m"> (minimizing squared error)</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; <code>foreach</code> terminal leaf node <img src="https://latex.codecogs.com/png.latex?t%20%5Cin%20h_m">:</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Let <img src="https://latex.codecogs.com/png.latex?v%20=%20%5Ctext%7Bargmin%7D_v%20%5Csum_%7Bi%20%5Cin%20t%7D%20l(y_i,%20F_m(%5Cmathbf%7Bx%7D_i)%20+%20v)"></p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Set terminal leaf node <img src="https://latex.codecogs.com/png.latex?t"> to predict value <img src="https://latex.codecogs.com/png.latex?v"></p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; <img src="https://latex.codecogs.com/png.latex?F_%7Bm+1%7D(%5Cmathbf%7BX%7D)%20=%20F_%7Bm%7D(%5Cmathbf%7BX%7D)%20+%20%5Ceta%20h_m(%5Cmathbf%7BX%7D)"></p></li>
<li><p>Return composite model <img src="https://latex.codecogs.com/png.latex?F_M"></p></li>
</ol>
<p>By now, most of this is already familiar to us. We begin by setting the base model <img src="https://latex.codecogs.com/png.latex?F_0"> equal to the constant prediction value that minimizes the loss over all examples in the training dataset (line 1). Then we begin the boosting iterations (line 2), each time computing the negative gradients of the loss with respect to the current model predictions (known as the pseudo residuals) (line 3). We then fit our next decision tree regressor to predict the pseudo residuals (line 4).</p>
<p>Then we encounter something new on lines 5-7. When we fit a vanilla decision tree regressor to predict pseudo residuals, we’re using mean squared error as the loss function to train the tree. As you might imagine, this works well when the global loss function is also squared error. But if we want to use a global loss other than squared error, there is an additional trick we can use to further increase the composite model’s accuracy. The idea is to continue using squared error to train each decision tree, keeping its structure and split conditions but altering the predicted value in each leaf to help minimize the global loss function. Instead of using the mean target value as the prediction for each node (as we would do when minimizing squared error), we use a numerical optimization method like line search to choose the constant value for that leaf that leads to the best overall loss. This is the same thing we did in line 1 of the algorithm to set the base prediction, but here we choose the optimal prediction for each terminal node of the newly trained decision tree.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>I did some (half-assed) searching on the interweb for an implementation of GBM that allows the user to provide a custom loss function, and you know what? I couldn’t find anything. If you find another implementation, post in the comments so we can learn from it too.</p>
<p>Since we need to modify the values predicted by our decision trees’ terminal nodes, we’ll want to brush up on the scikit-learn <a href="https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html">decision tree structure</a> before we get going. You can see explanations of all the necessary decision tree hacks in this <a href="https://github.com/mcb00/blog/blob/master/supplemental/friedman-gbm-implementation.ipynb">notebook</a>.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> sklearn.tree <span class="im" style="color: #00769E;">import</span> DecisionTreeRegressor </span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> scipy.optimize <span class="im" style="color: #00769E;">import</span> minimize</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;">class</span> GradientBoostingMachine():</span>
<span id="cb1-6">    <span class="co" style="color: #5E5E5E;">'''Gradient Boosting Machine supporting any user-supplied loss function.</span></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;">    </span></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;">    Parameters</span></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;">    ----------</span></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;">    n_trees : int</span></span>
<span id="cb1-11"><span class="co" style="color: #5E5E5E;">        number of boosting rounds</span></span>
<span id="cb1-12"><span class="co" style="color: #5E5E5E;">        </span></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;">    learning_rate : float</span></span>
<span id="cb1-14"><span class="co" style="color: #5E5E5E;">        learning rate hyperparameter</span></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;">        </span></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;">    max_depth : int</span></span>
<span id="cb1-17"><span class="co" style="color: #5E5E5E;">        maximum tree depth</span></span>
<span id="cb1-18"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb1-19">    </span>
<span id="cb1-20">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_trees, learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.1</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb1-21">        <span class="va" style="color: #111111;">self</span>.n_trees<span class="op" style="color: #5E5E5E;">=</span>n_trees<span class="op" style="color: #5E5E5E;">;</span> </span>
<span id="cb1-22">        <span class="va" style="color: #111111;">self</span>.learning_rate<span class="op" style="color: #5E5E5E;">=</span>learning_rate</span>
<span id="cb1-23">        <span class="va" style="color: #111111;">self</span>.max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth<span class="op" style="color: #5E5E5E;">;</span></span>
<span id="cb1-24">    </span>
<span id="cb1-25">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, X, y, objective):</span>
<span id="cb1-26">        <span class="co" style="color: #5E5E5E;">'''Fit the GBM using the specified loss function.</span></span>
<span id="cb1-27"><span class="co" style="color: #5E5E5E;">        </span></span>
<span id="cb1-28"><span class="co" style="color: #5E5E5E;">        Parameters</span></span>
<span id="cb1-29"><span class="co" style="color: #5E5E5E;">        ----------</span></span>
<span id="cb1-30"><span class="co" style="color: #5E5E5E;">        X : ndarray of size (number observations, number features)</span></span>
<span id="cb1-31"><span class="co" style="color: #5E5E5E;">            design matrix</span></span>
<span id="cb1-32"><span class="co" style="color: #5E5E5E;">            </span></span>
<span id="cb1-33"><span class="co" style="color: #5E5E5E;">        y : ndarray of size (number observations,)</span></span>
<span id="cb1-34"><span class="co" style="color: #5E5E5E;">            target values</span></span>
<span id="cb1-35"><span class="co" style="color: #5E5E5E;">            </span></span>
<span id="cb1-36"><span class="co" style="color: #5E5E5E;">        objective : loss function class instance</span></span>
<span id="cb1-37"><span class="co" style="color: #5E5E5E;">            Class specifying the loss function for training.</span></span>
<span id="cb1-38"><span class="co" style="color: #5E5E5E;">            Should implement two methods:</span></span>
<span id="cb1-39"><span class="co" style="color: #5E5E5E;">                loss(labels: ndarray, predictions: ndarray) -&gt; float</span></span>
<span id="cb1-40"><span class="co" style="color: #5E5E5E;">                negative_gradient(labels: ndarray, predictions: ndarray) -&gt; ndarray</span></span>
<span id="cb1-41"><span class="co" style="color: #5E5E5E;">        '''</span></span>
<span id="cb1-42">        </span>
<span id="cb1-43">        <span class="va" style="color: #111111;">self</span>.trees <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb1-44">        <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._get_optimal_base_value(y, objective.loss)</span>
<span id="cb1-45">        current_predictions <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">*</span> np.ones(shape<span class="op" style="color: #5E5E5E;">=</span>y.shape)</span>
<span id="cb1-46">        <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.n_trees):</span>
<span id="cb1-47">            pseudo_residuals <span class="op" style="color: #5E5E5E;">=</span> objective.negative_gradient(y, current_predictions)</span>
<span id="cb1-48">            tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.max_depth)</span>
<span id="cb1-49">            tree.fit(X, pseudo_residuals)</span>
<span id="cb1-50">            <span class="va" style="color: #111111;">self</span>._update_terminal_nodes(tree, X, y, current_predictions, objective.loss)</span>
<span id="cb1-51">            current_predictions <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> tree.predict(X)</span>
<span id="cb1-52">            <span class="va" style="color: #111111;">self</span>.trees.append(tree)</span>
<span id="cb1-53">     </span>
<span id="cb1-54">    <span class="kw" style="color: #003B4F;">def</span> _get_optimal_base_value(<span class="va" style="color: #111111;">self</span>, y, loss):</span>
<span id="cb1-55">        <span class="co" style="color: #5E5E5E;">'''Find the optimal initial prediction for the base model.'''</span></span>
<span id="cb1-56">        fun <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> c: loss(y, c)</span>
<span id="cb1-57">        c0 <span class="op" style="color: #5E5E5E;">=</span> y.mean()</span>
<span id="cb1-58">        <span class="cf" style="color: #003B4F;">return</span> minimize(fun<span class="op" style="color: #5E5E5E;">=</span>fun, x0<span class="op" style="color: #5E5E5E;">=</span>c0).x[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb1-59">        </span>
<span id="cb1-60">    <span class="kw" style="color: #003B4F;">def</span> _update_terminal_nodes(<span class="va" style="color: #111111;">self</span>, tree, X, y, current_predictions, loss):</span>
<span id="cb1-61">        <span class="co" style="color: #5E5E5E;">'''Update the tree's predictions according to the loss function.'''</span></span>
<span id="cb1-62">        <span class="co" style="color: #5E5E5E;"># terminal node id's</span></span>
<span id="cb1-63">        leaf_nodes <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(tree.tree_.children_left <span class="op" style="color: #5E5E5E;">==</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb1-64">        <span class="co" style="color: #5E5E5E;"># compute leaf for each sample in ``X``.</span></span>
<span id="cb1-65">        leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;">=</span> tree.<span class="bu" style="color: null;">apply</span>(X)</span>
<span id="cb1-66">        <span class="cf" style="color: #003B4F;">for</span> leaf <span class="kw" style="color: #003B4F;">in</span> leaf_nodes:</span>
<span id="cb1-67">            samples_in_this_leaf <span class="op" style="color: #5E5E5E;">=</span> np.where(leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;">==</span> leaf)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb1-68">            y_in_leaf <span class="op" style="color: #5E5E5E;">=</span> y.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb1-69">            preds_in_leaf <span class="op" style="color: #5E5E5E;">=</span> current_predictions.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb1-70">            val <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._get_optimal_leaf_value(y_in_leaf, </span>
<span id="cb1-71">                                               preds_in_leaf,</span>
<span id="cb1-72">                                               loss)</span>
<span id="cb1-73">            tree.tree_.value[leaf, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>] <span class="op" style="color: #5E5E5E;">=</span> val</span>
<span id="cb1-74">            </span>
<span id="cb1-75">    <span class="kw" style="color: #003B4F;">def</span> _get_optimal_leaf_value(<span class="va" style="color: #111111;">self</span>, y, current_predictions, loss):</span>
<span id="cb1-76">        <span class="co" style="color: #5E5E5E;">'''Find the optimal prediction value for a given leaf.'''</span></span>
<span id="cb1-77">        fun <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> c: loss(y, current_predictions <span class="op" style="color: #5E5E5E;">+</span> c)</span>
<span id="cb1-78">        c0 <span class="op" style="color: #5E5E5E;">=</span> y.mean()</span>
<span id="cb1-79">        <span class="cf" style="color: #003B4F;">return</span> minimize(fun<span class="op" style="color: #5E5E5E;">=</span>fun, x0<span class="op" style="color: #5E5E5E;">=</span>c0).x[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb1-80">          </span>
<span id="cb1-81">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb1-82">        <span class="co" style="color: #5E5E5E;">'''Generate predictions for the given input data.'''</span></span>
<span id="cb1-83">        <span class="cf" style="color: #003B4F;">return</span> (<span class="va" style="color: #111111;">self</span>.base_prediction </span>
<span id="cb1-84">                <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.learning_rate </span>
<span id="cb1-85">                <span class="op" style="color: #5E5E5E;">*</span> np.<span class="bu" style="color: null;">sum</span>([tree.predict(X) <span class="cf" style="color: #003B4F;">for</span> tree <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.trees], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>))</span></code></pre></div>
</div>
<p>In terms of design, we implement a class for the GBM with scikit-like <code>fit</code> and <code>predict</code> methods. Notice in the below implementation that the <code>fit</code> method is only 10 lines long, and corresponds very closely to Friedman’s gradient boost algorithm from above. Most of the complexity comes from the helper methods for updating the leaf values according to the specified loss function.</p>
<p>When the user wants to call the <code>fit</code> method, they’ll need to supply the loss function they want to use for boosting. We’ll make the user implement their loss (a.k.a. objective) function as a class with two methods: (1) a <code>loss</code> method taking the labels and the predictions and returning the loss score and (2) a <code>negative_gradient</code> method taking the labels and the predictions and returning an array of negative gradients.</p>
</section>
<section id="testing-our-model" class="level2">
<h2 class="anchored" data-anchor-id="testing-our-model">Testing our Model</h2>
<p>Let’s test drive our custom-loss-ready GBM with a few different loss functions! We’ll compare it to the scikit-learn GBM to sanity check our implementation.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;">import</span> GradientBoostingRegressor, GradientBoostingClassifier</span>
<span id="cb2-2"></span>
<span id="cb2-3">rng <span class="op" style="color: #5E5E5E;">=</span> np.random.default_rng()</span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="co" style="color: #5E5E5E;"># test data</span></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;">def</span> make_test_data(n, noise_scale):</span>
<span id="cb2-7">    x <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">500</span>).reshape(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-8">    y <span class="op" style="color: #5E5E5E;">=</span> (np.where(x <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="dv" style="color: #AD0000;">5</span>, x, <span class="dv" style="color: #AD0000;">5</span>) <span class="op" style="color: #5E5E5E;">+</span> rng.normal(<span class="dv" style="color: #AD0000;">0</span>, noise_scale, size<span class="op" style="color: #5E5E5E;">=</span>x.shape)).ravel()</span>
<span id="cb2-9">    <span class="cf" style="color: #003B4F;">return</span> x, y</span>
<span id="cb2-10">    </span>
<span id="cb2-11"><span class="co" style="color: #5E5E5E;"># print model loss scores</span></span>
<span id="cb2-12"><span class="kw" style="color: #003B4F;">def</span> print_model_loss_scores(obj, y, preds, sk_preds):</span>
<span id="cb2-13">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'From Scratch Loss = </span><span class="sc" style="color: #5E5E5E;">{</span>obj<span class="sc" style="color: #5E5E5E;">.</span>loss(y, pred)<span class="sc" style="color: #5E5E5E;">:0.4}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb2-14">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'Scikit-Learn Loss = </span><span class="sc" style="color: #5E5E5E;">{</span>obj<span class="sc" style="color: #5E5E5E;">.</span>loss(y, sk_pred)<span class="sc" style="color: #5E5E5E;">:0.4}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
</div>
<section id="mean-squared-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error">Mean Squared Error</h3>
<p>Mean Squared Error (a.k.a. Least Squares) loss produces estimates of the mean target value conditioned on the feature values. Here’s the implementation.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">x, y <span class="op" style="color: #5E5E5E;">=</span> make_test_data(<span class="dv" style="color: #AD0000;">500</span>, <span class="fl" style="color: #AD0000;">0.4</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># from scratch GBM</span></span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;">class</span> SquaredErrorLoss():</span>
<span id="cb4-3">    <span class="co" style="color: #5E5E5E;">'''User-Defined Squared Error Loss'''</span></span>
<span id="cb4-4">    </span>
<span id="cb4-5">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb4-6">        <span class="cf" style="color: #003B4F;">return</span> np.mean((y <span class="op" style="color: #5E5E5E;">-</span> preds)<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb4-7">    </span>
<span id="cb4-8">    <span class="kw" style="color: #003B4F;">def</span> negative_gradient(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb4-9">        <span class="cf" style="color: #003B4F;">return</span> y <span class="op" style="color: #5E5E5E;">-</span> preds</span>
<span id="cb4-10">    </span>
<span id="cb4-11"></span>
<span id="cb4-12">gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingMachine(n_trees<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb4-13">                              learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb4-14">                              max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-15">gbm.fit(x, y, SquaredErrorLoss())</span>
<span id="cb4-16">pred <span class="op" style="color: #5E5E5E;">=</span> gbm.predict(x)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># scikit-learn GBM</span></span>
<span id="cb5-2">sk_gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingRegressor(n_estimators<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb5-3">                                   learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb5-4">                                   max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb5-5">                                   loss<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'squared_error'</span>)</span>
<span id="cb5-6">sk_gbm.fit(x, y)</span>
<span id="cb5-7">sk_pred <span class="op" style="color: #5E5E5E;">=</span> sk_gbm.predict(x)</span></code></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">print_model_loss_scores(SquaredErrorLoss(), y, pred, sk_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>From Scratch Loss = 0.168
Scikit-Learn Loss = 0.168</code></pre>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/gbm-any-loss_files/figure-html/cell-8-output-1.png" class="img-fluid" alt="Scatterplot showing data and model prediction of y given x"></p>
</div>
</div>
</section>
<section id="mean-absolute-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-absolute-error">Mean Absolute Error</h3>
<p>Mean Absolute Error (a.k.a.Least Absolute Deviations) loss produces estimates of the median target value conditioned on the feature values. Here’s the implementation.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">x, y <span class="op" style="color: #5E5E5E;">=</span> make_test_data(<span class="dv" style="color: #AD0000;">500</span>, <span class="fl" style="color: #AD0000;">0.4</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;"># from scratch GBM</span></span>
<span id="cb9-3"><span class="kw" style="color: #003B4F;">class</span> AbsoluteErrorLoss():</span>
<span id="cb9-4">    <span class="co" style="color: #5E5E5E;">'''User-Defined Absolute Error Loss'''</span></span>
<span id="cb9-5">    </span>
<span id="cb9-6">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb9-7">        <span class="cf" style="color: #003B4F;">return</span> np.mean(np.<span class="bu" style="color: null;">abs</span>(y <span class="op" style="color: #5E5E5E;">-</span> preds))</span>
<span id="cb9-8">    </span>
<span id="cb9-9">    <span class="kw" style="color: #003B4F;">def</span> negative_gradient(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb9-10">        <span class="cf" style="color: #003B4F;">return</span> np.sign(y <span class="op" style="color: #5E5E5E;">-</span> preds)</span>
<span id="cb9-11"></span>
<span id="cb9-12"></span>
<span id="cb9-13">gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingMachine(n_trees<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb9-14">                              learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb9-15">                              max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb9-16">gbm.fit(x, y, AbsoluteErrorLoss())</span>
<span id="cb9-17">pred <span class="op" style="color: #5E5E5E;">=</span> gbm.predict(x)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;"># scikit-learn GBM</span></span>
<span id="cb10-2">sk_gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingRegressor(n_estimators<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb10-3">                                   learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb10-4">                                   max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb10-5">                                   loss<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'absolute_error'</span>)</span>
<span id="cb10-6">sk_gbm.fit(x, y)</span>
<span id="cb10-7">sk_pred <span class="op" style="color: #5E5E5E;">=</span> sk_gbm.predict(x)</span></code></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">print_model_loss_scores(AbsoluteErrorLoss(), y, pred, sk_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>From Scratch Loss = 0.3225
Scikit-Learn Loss = 0.3208</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/gbm-any-loss_files/figure-html/cell-14-output-1.png" class="img-fluid" alt="Figure showing scatterplot of data and model prediction of median of y given x"></p>
</div>
</div>
</section>
<section id="quantile-loss" class="level3">
<h3 class="anchored" data-anchor-id="quantile-loss">Quantile Loss</h3>
<p>Quantile loss yields estimates of a given quantile of the target variable conditioned on the features. Here’s my implementation.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">x, y <span class="op" style="color: #5E5E5E;">=</span> make_test_data(<span class="dv" style="color: #AD0000;">500</span>, <span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"></span>
<span id="cb14-2"><span class="co" style="color: #5E5E5E;"># from scratch GBM</span></span>
<span id="cb14-3"><span class="kw" style="color: #003B4F;">class</span> QuantileLoss():</span>
<span id="cb14-4">    <span class="co" style="color: #5E5E5E;">'''Quantile Loss</span></span>
<span id="cb14-5"><span class="co" style="color: #5E5E5E;">    </span></span>
<span id="cb14-6"><span class="co" style="color: #5E5E5E;">    Parameters</span></span>
<span id="cb14-7"><span class="co" style="color: #5E5E5E;">    ----------</span></span>
<span id="cb14-8"><span class="co" style="color: #5E5E5E;">    alpha : float</span></span>
<span id="cb14-9"><span class="co" style="color: #5E5E5E;">        quantile to be estimated, 0 &lt; alpha &lt; 1</span></span>
<span id="cb14-10"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb14-11">    </span>
<span id="cb14-12">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, alpha):</span>
<span id="cb14-13">        <span class="cf" style="color: #003B4F;">if</span> alpha <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="dv" style="color: #AD0000;">0</span> <span class="kw" style="color: #003B4F;">or</span> alpha <span class="op" style="color: #5E5E5E;">&gt;</span><span class="dv" style="color: #AD0000;">1</span>:</span>
<span id="cb14-14">            <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">ValueError</span>(<span class="st" style="color: #20794D;">'alpha must be between 0 and 1'</span>)</span>
<span id="cb14-15">        <span class="va" style="color: #111111;">self</span>.alpha <span class="op" style="color: #5E5E5E;">=</span> alpha</span>
<span id="cb14-16">        </span>
<span id="cb14-17">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb14-18">        e <span class="op" style="color: #5E5E5E;">=</span> y <span class="op" style="color: #5E5E5E;">-</span> preds</span>
<span id="cb14-19">        <span class="cf" style="color: #003B4F;">return</span> np.mean(np.where(e <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.alpha <span class="op" style="color: #5E5E5E;">*</span> e, (<span class="va" style="color: #111111;">self</span>.alpha <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="op" style="color: #5E5E5E;">*</span> e))</span>
<span id="cb14-20">    </span>
<span id="cb14-21">    <span class="kw" style="color: #003B4F;">def</span> negative_gradient(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb14-22">        e <span class="op" style="color: #5E5E5E;">=</span> y <span class="op" style="color: #5E5E5E;">-</span> preds </span>
<span id="cb14-23">        <span class="cf" style="color: #003B4F;">return</span> np.where(e <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.alpha, <span class="va" style="color: #111111;">self</span>.alpha <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb14-24"></span>
<span id="cb14-25">gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingMachine(n_trees<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb14-26">                              learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb14-27">                             max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb14-28">gbm.fit(x, y, QuantileLoss(alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>))</span>
<span id="cb14-29">pred <span class="op" style="color: #5E5E5E;">=</span> gbm.predict(x)    </span></code></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;"># scikit-learn GBM</span></span>
<span id="cb15-2">sk_gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingRegressor(n_estimators<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb15-3">                                 learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb15-4">                                 max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb15-5">                                 loss<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'quantile'</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>)</span>
<span id="cb15-6">sk_gbm.fit(x, y)</span>
<span id="cb15-7">sk_pred <span class="op" style="color: #5E5E5E;">=</span> sk_gbm.predict(x)</span></code></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">print_model_loss_scores(QuantileLoss(alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>), y, pred, sk_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>From Scratch Loss = 0.1853
Scikit-Learn Loss = 0.1856</code></pre>
</div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="17">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/gbm-any-loss_files/figure-html/cell-20-output-1.png" class="img-fluid" alt="Figure showing scatterplot of data and model prediction of 0.9 quantile of y given x"></p>
</div>
</div>
</section>
<section id="binary-cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="binary-cross-entropy-loss">Binary Cross Entropy Loss</h3>
<p>The previous losses are useful for regression problems, where the target is numeric. But we can also solve classification problems, simply by swapping in an appropriate loss function. Here we’ll implement binary cross entropy, a.k.a. binary deviance, a.k.a. negative binomial log likelihood (sometimes abusively called log loss). One thing to remember is that, as with logistic regression, our model is actually predicting the log odds ratio, not the probability of the positive class. Thus we use expit transformations (the inverse of logit) whenever probabilities are needed, e.g., when predicting the probability that an observation belongs to the positive class.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="co" style="color: #5E5E5E;"># make categorical test data</span></span>
<span id="cb18-2"></span>
<span id="cb18-3"><span class="kw" style="color: #003B4F;">def</span> expit(t):</span>
<span id="cb18-4">    <span class="cf" style="color: #003B4F;">return</span> np.exp(t) <span class="op" style="color: #5E5E5E;">/</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">+</span> np.exp(t))</span>
<span id="cb18-5"></span>
<span id="cb18-6">x <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">500</span>)</span>
<span id="cb18-7">p <span class="op" style="color: #5E5E5E;">=</span> expit(x)</span>
<span id="cb18-8">y <span class="op" style="color: #5E5E5E;">=</span> rng.binomial(<span class="dv" style="color: #AD0000;">1</span>, p, size<span class="op" style="color: #5E5E5E;">=</span>p.shape)</span>
<span id="cb18-9">x <span class="op" style="color: #5E5E5E;">=</span> x.reshape(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;"># from scratch GBM</span></span>
<span id="cb19-2"><span class="kw" style="color: #003B4F;">class</span> BinaryCrossEntropyLoss():</span>
<span id="cb19-3">    <span class="co" style="color: #5E5E5E;">'''Binary Cross Entropy Loss</span></span>
<span id="cb19-4"><span class="co" style="color: #5E5E5E;">    </span></span>
<span id="cb19-5"><span class="co" style="color: #5E5E5E;">    Note that the predictions should be log odds ratios.</span></span>
<span id="cb19-6"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb19-7">    </span>
<span id="cb19-8">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb19-9">        <span class="va" style="color: #111111;">self</span>.expit <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> t: np.exp(t) <span class="op" style="color: #5E5E5E;">/</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">+</span> np.exp(t))</span>
<span id="cb19-10">    </span>
<span id="cb19-11">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb19-12">        p <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.expit(preds)</span>
<span id="cb19-13">        <span class="cf" style="color: #003B4F;">return</span> <span class="op" style="color: #5E5E5E;">-</span>np.mean(y <span class="op" style="color: #5E5E5E;">*</span> np.log(p) <span class="op" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">-</span> y) <span class="op" style="color: #5E5E5E;">*</span> np.log(<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">-</span> p))</span>
<span id="cb19-14">    </span>
<span id="cb19-15">    <span class="kw" style="color: #003B4F;">def</span> negative_gradient(<span class="va" style="color: #111111;">self</span>, y, preds):</span>
<span id="cb19-16">        p <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.expit(preds)</span>
<span id="cb19-17">        <span class="cf" style="color: #003B4F;">return</span> y <span class="op" style="color: #5E5E5E;">/</span> p <span class="op" style="color: #5E5E5E;">-</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">-</span> y) <span class="op" style="color: #5E5E5E;">/</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">-</span> p)</span>
<span id="cb19-18"></span>
<span id="cb19-19">    </span>
<span id="cb19-20">gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingMachine(n_trees<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb19-21">                              learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb19-22">                              max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb19-23">gbm.fit(x, y, BinaryCrossEntropyLoss())</span>
<span id="cb19-24">pred <span class="op" style="color: #5E5E5E;">=</span> expit(gbm.predict(x))</span></code></pre></div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="co" style="color: #5E5E5E;"># scikit-learn GBM</span></span>
<span id="cb20-2">sk_gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingClassifier(n_estimators<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>,</span>
<span id="cb20-3">                                    learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>,</span>
<span id="cb20-4">                                    max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb20-5">                                    loss<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'log_loss'</span>)</span>
<span id="cb20-6">sk_gbm.fit(x, y)</span>
<span id="cb20-7">sk_pred <span class="op" style="color: #5E5E5E;">=</span> sk_gbm.predict_proba(x)[:, <span class="dv" style="color: #AD0000;">1</span>]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">print_model_loss_scores(BinaryCrossEntropyLoss(), y, pred, sk_pred)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>From Scratch Loss = 0.6379
Scikit-Learn Loss = 0.6403</code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/gbm-any-loss_files/figure-html/cell-26-output-1.png" class="img-fluid" alt="Figure showing data and model prediction of probability that y equals one given x"></p>
</div>
</div>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Woohoo! We did it! We finally made it through Friedman’s paper in its entirety, and we implemented the generic gradient boosting algorithm which works with any differentiable loss function. If you made it this far, great job, gold star! By now you hopefully have a pretty solid grasp on gradient boosting, which is good, because soon we’re going to dive into the modern Newton descent gradient boosting frameworks like XGBoost. Onward!</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Friedman’s 2001 paper: <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine</a></p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/index.html</guid>
  <pubDate>Fri, 22 Oct 2021 23:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2021/gradient-boosting-machine-with-any-loss-function/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Hello PySpark!</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2021/hello-pyspark/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2021/hello-pyspark/guiones_wave.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A big day at Playa Guiones</figcaption><p></p>
</figure>
</div>
<p>Well, you guessed it: it’s time for us to learn PySpark!</p>
<p>I know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes?</p>
<p>That’s a totally fair question.</p>
<p>So what happens when we’re working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory? We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details.</p>
<p>Enter PySpark.</p>
<p>I think it’s fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it’s like pandas but scalable. It’s built on top of <a href="https://spark.apache.org/">Apache Spark</a>, a unified analytics engine for large-scale data processing. <a href="https://spark.apache.org/docs/latest/api/python/">PySpark</a> is essentially a way to access the functionality of spark via python code. While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice. PySpark also has great integration with <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">SQL</a>, and it has a companion machine learning library called <a href="https://spark.apache.org/mllib/">MLlib</a> that’s more or less a scalable scikit-learn (maybe we can cover it in a future post).</p>
<p>So, here’s the plan. First we’re going to get set up to run PySpark locally in a jupyter notebook on our laptop. This is my preferred environment for interactively playing with PySpark and learning the ropes. Then we’re going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas. Once we’re comfortable running PySpark on the laptop, it’s going to be much easier to jump onto a distributed cluster and run PySpark at scale.</p>
<p>Let’s do this.</p>
<section id="how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop" class="level2">
<h2 class="anchored" data-anchor-id="how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop">How to Run PySpark in a Jupyter Notebook on Your Laptop</h2>
<p>Ok, I’m going to walk us through how to get things installed on a Mac or Linux machine where we’re using homebrew and conda to manage virtual environments. If you have a different setup, your favorite search engine will help you get PySpark set up locally.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It’s possible for Homebrew and Anaconda to interfere with one another. The simple rule of thumb is that whenever you want to use the <code>brew</code> command, first deactivate your conda environment by running <code>conda deactivate</code>. See this <a href="https://stackoverflow.com/questions/42859781/best-practices-with-anaconda-and-brew">Stack Overflow question</a> for more details.</p>
</div>
</div>
<section id="install-spark" class="level3">
<h3 class="anchored" data-anchor-id="install-spark">Install Spark</h3>
<p>Install Spark with homebrew.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb1-1"><span class="ex" style="color: null;">brew</span> install apache-spark</span></code></pre></div>
<p>Next we need to set up a <code>SPARK_HOME</code> environment variable in the shell. Check where Spark is installed.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb2-1"><span class="ex" style="color: null;">brew</span> info apache-spark</span></code></pre></div>
<p>You should see something like</p>
<pre><code>==&gt; apache-spark: stable 3.3.2 (bottled), HEAD
Engine for large-scale data processing
https://spark.apache.org/
/opt/homebrew/Cellar/apache-spark/3.3.2 (1,453 files, 320.9MB) *
...</code></pre>
<p>Set the <code>SPARK_HOME</code> environment variable to your spark installation path with <code>/libexec</code> appended to the end. To do this I added the following line to my <code>.zshrc</code> file.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb4-1"><span class="bu" style="color: null;">export</span> <span class="va" style="color: #111111;">SPARK_HOME</span><span class="op" style="color: #5E5E5E;">=</span>/opt/homebrew/Cellar/apache-spark/3.3.2/libexec</span></code></pre></div>
<p>Restart your shell, and test the installation by starting the Spark shell.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb5-1"><span class="ex" style="color: null;">spark-shell</span></span></code></pre></div>
<pre><code>...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.2
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 19.0.2)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; </code></pre>
<p>If you get the <code>scala&gt;</code> prompt, then you’ve successfully installed Spark on your laptop!</p>
</section>
<section id="install-pyspark" class="level3">
<h3 class="anchored" data-anchor-id="install-pyspark">Install PySpark</h3>
<p>Use conda to install the PySpark python package. As usual, it’s advisable to do this in a new virtual environment.</p>
<pre><code>$ conda install pyspark</code></pre>
<p>You should be able to launch an interactive PySpark REPL by saying pyspark.</p>
<pre><code>$ pyspark
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/

Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)
Spark context Web UI available at http://192.168.100.47:4041
Spark context available as 'sc' (master = local[*], app id = local-1624127229929).
SparkSession available as 'spark'.
&gt;&gt;&gt; </code></pre>
<p>This time we get a familiar python <code>&gt;&gt;&gt;</code> prompt. This is an interactive shell where we can easily experiment with PySpark. Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we’ll get set up to run PySpark in a jupyter notebook.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When I tried following this setup on a new Mac, I hit an error about being unable to find the Java Runtime. This <a href="https://stackoverflow.com/questions/75021908/cannot-start-pyspark-unable-to-locate-a-java-runtime">stack overflow question</a> lead me to the fix.</p>
</div>
</div>
</section>
<section id="the-spark-session-object" class="level3">
<h3 class="anchored" data-anchor-id="the-spark-session-object">The Spark Session Object</h3>
<p>You may have noticed that when we launched that PySpark interactive shell, it told us that something called <code>SparkSession</code> was available as <code>'spark'</code>. So basically, what’s happening here is that when we launch the pyspark shell, it instantiates an object called <code>spark</code> which is an instance of class <code>pyspark.sql.session.SparkSession</code>. The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we’re going to be saying things like <code>spark.this()</code> and <code>spark.that()</code> to make stuff happen.</p>
<p>The PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically. However, when we’re using another interface to PySpark (like say a jupyter notebook running a python kernal), we’ll have to make a spark session object for ourselves.</p>
</section>
<section id="create-a-pyspark-session-in-a-jupyter-notebook" class="level3">
<h3 class="anchored" data-anchor-id="create-a-pyspark-session-in-a-jupyter-notebook">Create a PySpark Session in a Jupyter Notebook</h3>
<p>There are a few ways to run PySpark in jupyter which you can read about <a href="https://www.datacamp.com/community/tutorials/apache-spark-python">here</a>.</p>
<p>For derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a jupyter notebook running on a regular python kernel. The method we’ll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session. So, first install the findspark package.</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb9-1"><span class="ex" style="color: null;">conda</span> install <span class="at" style="color: #657422;">-c</span> conda-forge findspark</span></code></pre></div>
<p>Launch jupyter as usual.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb10-1"><span class="ex" style="color: null;">jupyter</span> notebook</span></code></pre></div>
<p>Go ahead and fire up a new notebook using a regular python 3 kernal. Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated. You can think of this as boilerplate code that we need to run in the first cell of a notebook where we’re going to use PySpark.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;">import</span> pyspark</span>
<span id="cb11-2"><span class="im" style="color: #00769E;">import</span> findspark</span>
<span id="cb11-3"><span class="im" style="color: #00769E;">from</span> pyspark.sql <span class="im" style="color: #00769E;">import</span> SparkSession</span>
<span id="cb11-4"></span>
<span id="cb11-5">findspark.init()</span>
<span id="cb11-6">spark <span class="op" style="color: #5E5E5E;">=</span> SparkSession.builder.appName(<span class="st" style="color: #20794D;">'My Spark App'</span>).getOrCreate()</span></code></pre></div>
</div>
<p>First we’re running findspark’s <code>init()</code> method to find our Spark installation. If you run into errors here, make sure you got the <code>SPARK_HOME</code> environment variable correctly set in the install instructions above. Then we instantiate a spark session as <code>spark</code>. Once you run this, you’re ready to rock and roll with PySpark in your jupyter notebook.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Spark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at <a href="http://localhost:4040/jobs/">http://localhost:4040/jobs/</a>.</p>
</div>
</div>
</section>
</section>
<section id="pyspark-concepts" class="level2">
<h2 class="anchored" data-anchor-id="pyspark-concepts">PySpark Concepts</h2>
<p>PySpark provides two main abstractions for data: the RDD and the dataframe. <strong>RDD</strong>’s are just a distributed list of objects; we won’t go into details about them in this post. For us, the key object in PySpark is the <strong>dataframe</strong>.</p>
<p>While PySpark dataframes expose much of the functionality you would expect from a library for tabular data manipulation, they behave a little differently from pandas dataframes, both syntactically and under-the-hood. There are a couple of key concepts that will help explain these idiosyncracies.</p>
<p><strong>Immutability</strong> - Pyspark RDD’s and dataframes are immutable. This means that if you change an object, e.g.&nbsp;by adding a column to a dataframe, PySpark returns a reference to a new dataframe; it does not modify the existing dataframe. This is kind of nice, because we don’t have to worry about that whole <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy">view versus copy</a> nonsense that happens in pandas.</p>
<p><strong>Lazy Evaluation</strong> - Lazy evaluation means that when we start manipulating a dataframe, PySpark won’t actually perform any of the computations until we explicitly ask for the result. This is nice because it potentially allows PySpark to do fancy optimizations before executing a sequence of operations. It’s also confusing at first, because PySpark will seem to blaze through complex operations and then take forever to print a few rows of the dataframe.</p>
</section>
<section id="pyspark-dataframe-essentials" class="level2">
<h2 class="anchored" data-anchor-id="pyspark-dataframe-essentials">PySpark Dataframe Essentials</h2>
<section id="creating-a-pyspark-dataframe-with-createdataframe" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-pyspark-dataframe-with-createdataframe">Creating a PySpark dataframe with <code>createDataFrame()</code></h3>
<p>The first thing we’ll need is a way to make dataframes. <a href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html"><code>createDataFrame()</code></a> allows us to create PySpark dataframes from python objects like nested lists or pandas dataframes. Notice that <code>createDataFrame()</code> is a method of the spark session class, so we’ll call it from our spark session <code>spark</code>by saying <code>spark.createDataFrame()</code>.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;"># create pyspark dataframe from nested  lists</span></span>
<span id="cb12-2">my_df <span class="op" style="color: #5E5E5E;">=</span> spark.createDataFrame(</span>
<span id="cb12-3">    data<span class="op" style="color: #5E5E5E;">=</span>[</span>
<span id="cb12-4">        [<span class="dv" style="color: #AD0000;">2022</span>, <span class="st" style="color: #20794D;">"tiger"</span>],</span>
<span id="cb12-5">        [<span class="dv" style="color: #AD0000;">2023</span>, <span class="st" style="color: #20794D;">"rabbit"</span>],</span>
<span id="cb12-6">        [<span class="dv" style="color: #AD0000;">2024</span>, <span class="st" style="color: #20794D;">"dragon"</span>]</span>
<span id="cb12-7">    ],</span>
<span id="cb12-8">    schema<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">'year'</span>, <span class="st" style="color: #20794D;">'animal'</span>]</span>
<span id="cb12-9">)</span></code></pre></div>
</div>
<p>Let’s read the seaborn tips dataset into a pandas dataframe and then use it to create a PySpark dataframe.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="co" style="color: #5E5E5E;"># load tips dataset into a pandas dataframe</span></span>
<span id="cb13-4">pandas_df <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(<span class="st" style="color: #20794D;">'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv'</span>)</span>
<span id="cb13-5"></span>
<span id="cb13-6"><span class="co" style="color: #5E5E5E;"># create pyspark dataframe from a pandas dataframe</span></span>
<span id="cb13-7">pyspark_df <span class="op" style="color: #5E5E5E;">=</span> spark.createDataFrame(pandas_df)</span></code></pre></div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In real life when we’re running PySpark on a large-scale distributed system, we would not generally want to use python lists or pandas dataframes to load data into PySpark. Ideally we would want to read data directly from where it is stored on HDFS, e.g.&nbsp;by reading <a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">parquet files</a>, or by querying directly from a hive database using <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">spark sql</a>.</p>
</div>
</div>
</section>
<section id="peeking-at-a-dataframes-contents" class="level3">
<h3 class="anchored" data-anchor-id="peeking-at-a-dataframes-contents">Peeking at a dataframe’s contents</h3>
<p>The default print method for the PySpark dataframe will just give you the schema.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">pyspark_df</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]</code></pre>
</div>
</div>
<p>If we want to peek at some of the data, we’ll need to use the <code>show()</code> method, which is analogous to the pandas <code>head()</code>. Remember that <code>show()</code> will cause PySpark to execute any operations that it’s been lazily waiting to evaluate, so sometimes it can take a while to run.</p>
<div class="cell" data-scrolled="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;"># show the first few rows of the dataframe</span></span>
<span id="cb16-2">pyspark_df.show(<span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------+----+------+------+---+------+----+
|total_bill| tip|   sex|smoker|day|  time|size|
+----------+----+------+------+---+------+----+
|     16.99|1.01|Female|    No|Sun|Dinner|   2|
|     10.34|1.66|  Male|    No|Sun|Dinner|   3|
|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|
|     23.68|3.31|  Male|    No|Sun|Dinner|   2|
|     24.59|3.61|Female|    No|Sun|Dinner|   4|
+----------+----+------+------+---+------+----+
only showing top 5 rows
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                </code></pre>
</div>
</div>
<p>We thus encounter our first rude awakening. PySpark’s default representation of dataframes in the notebook isn’t as pretty as that of pandas. But no one ever said it would be pretty, they just said it would be scalable.</p>
<p>You can also use the <code>printSchema()</code> method for a nice vertical representation of the schema.</p>
<div class="cell" data-scrolled="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;"># show the dataframe schema</span></span>
<span id="cb19-2">pyspark_df.printSchema()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>root
 |-- total_bill: double (nullable = true)
 |-- tip: double (nullable = true)
 |-- sex: string (nullable = true)
 |-- smoker: string (nullable = true)
 |-- day: string (nullable = true)
 |-- time: string (nullable = true)
 |-- size: long (nullable = true)
</code></pre>
</div>
</div>
</section>
<section id="select-columns-by-name" class="level3">
<h3 class="anchored" data-anchor-id="select-columns-by-name">Select columns by name</h3>
<p>You can select specific columns from a dataframe using the <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html"><code>select()</code></a> method. You can pass either a list of names, or pass names as arguments.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="co" style="color: #5E5E5E;"># select some of the columns</span></span>
<span id="cb21-2">pyspark_df.select(<span class="st" style="color: #20794D;">'total_bill'</span>, <span class="st" style="color: #20794D;">'tip'</span>)</span>
<span id="cb21-3"></span>
<span id="cb21-4"><span class="co" style="color: #5E5E5E;"># select columns in a list</span></span>
<span id="cb21-5">pyspark_df.select([<span class="st" style="color: #20794D;">'day'</span>, <span class="st" style="color: #20794D;">'time'</span>, <span class="st" style="color: #20794D;">'total_bill'</span>])</span></code></pre></div>
</div>
</section>
<section id="filter-rows-based-on-column-values" class="level3">
<h3 class="anchored" data-anchor-id="filter-rows-based-on-column-values">Filter rows based on column values</h3>
<p>Analogous to the <code>WHERE</code> clause in SQL, and the <code>query()</code> method in pandas, PySpark provides a <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html"><code>filter()</code></a> method which returns only the rows that meet the specified conditions. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using <code>and</code> and <code>or</code>, and you can even do a SQL-like <code>in</code> to check if the column value matches any items in a list.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="co" style="color: #5E5E5E;">## compare a column to a value</span></span>
<span id="cb22-2">pyspark_df.<span class="bu" style="color: null;">filter</span>(<span class="st" style="color: #20794D;">'total_bill &gt; 20'</span>)</span>
<span id="cb22-3"></span>
<span id="cb22-4"><span class="co" style="color: #5E5E5E;"># compare two columns with arithmetic</span></span>
<span id="cb22-5">pyspark_df.<span class="bu" style="color: null;">filter</span>(<span class="st" style="color: #20794D;">'tip &gt; 0.15 * total_bill'</span>)</span>
<span id="cb22-6"></span>
<span id="cb22-7"><span class="co" style="color: #5E5E5E;"># check equality with a string value</span></span>
<span id="cb22-8">pyspark_df.<span class="bu" style="color: null;">filter</span>(<span class="st" style="color: #20794D;">'sex == "Male"'</span>)</span>
<span id="cb22-9"></span>
<span id="cb22-10"><span class="co" style="color: #5E5E5E;"># check equality with any of several possible values</span></span>
<span id="cb22-11">pyspark_df.<span class="bu" style="color: null;">filter</span>(<span class="st" style="color: #20794D;">'day in ("Sat", "Sun")'</span>)</span>
<span id="cb22-12"></span>
<span id="cb22-13"><span class="co" style="color: #5E5E5E;"># use "and" </span></span>
<span id="cb22-14">pyspark_df.<span class="bu" style="color: null;">filter</span>(<span class="st" style="color: #20794D;">'day == "Fri" and time == "Lunch"'</span>)</span></code></pre></div>
</div>
<p>If you’re into boolean indexing with the brackets, PySpark does support that too, but I encourage you to use <code>filter()</code> instead. Check out my rant about <a href="../../../8020-pandas-tutorial#select-rows-based-on-their-values-with-query">why you shouldn’t use boolean indexing</a> for the details. The TLDR is that <code>filter()</code> requires less typing, makes your code more readable and portable, and it allows you to chain method calls together using dot chains.</p>
<p>Here’s the boolean indexing equivalent of the last example from above.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;"># using boolean indexing</span></span>
<span id="cb23-2">pyspark_df[(pyspark_df.day <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'Fri'</span>) <span class="op" style="color: #5E5E5E;">&amp;</span> (pyspark_df.time <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'Lunch'</span>)]</span></code></pre></div>
</div>
<p>I know, it looks horrendous, but not as horrendous as the error message you’ll get if you forget the parentheses.</p>
</section>
<section id="add-new-columns-to-a-dataframe" class="level3">
<h3 class="anchored" data-anchor-id="add-new-columns-to-a-dataframe">Add new columns to a dataframe</h3>
<p>You can add new columns which are functions of the existing columns with the <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html"><code>withColumn()</code></a> method.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="im" style="color: #00769E;">import</span> pyspark.sql.functions <span class="im" style="color: #00769E;">as</span> f</span>
<span id="cb24-2"></span>
<span id="cb24-3"><span class="co" style="color: #5E5E5E;"># add a new column using col() to reference other columns</span></span>
<span id="cb24-4">pyspark_df.withColumn(<span class="st" style="color: #20794D;">'tip_percent'</span>, f.col(<span class="st" style="color: #20794D;">'tip'</span>) <span class="op" style="color: #5E5E5E;">/</span> f.col(<span class="st" style="color: #20794D;">'total_bill'</span>))</span></code></pre></div>
</div>
<p>Notice that we’ve imported the <a href="[pyspark.sql.functions](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)"><code>pyspark.sql.functions</code></a> module. This module contains lots of useful functions that we’ll be using all over the place, so it’s probably a good idea to go ahead and import it whenever you’re using PySpark. BTW, it seems like folks usually import this module as <code>f</code> or <code>F</code>. In this example we’re using the <code>col()</code> function, which allows us to refer to columns in our dataframe using string representations of the column names.</p>
<p>You could also achieve the same result using the dot to reference the other columns, but this requires us to type the dataframe name over and over again, which makes it harder to reuse this code on different dataframes or in <a href="https://blog.mattbowers.dev/8020-pandas-tutorial#Chain-transformations-together-with-the-dot-chain">dot chains</a>.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="co" style="color: #5E5E5E;"># add a new column using the dot to reference other columns (less recommended)</span></span>
<span id="cb25-2">pyspark_df.withColumn(<span class="st" style="color: #20794D;">'tip_percent'</span>, pyspark_df.tip <span class="op" style="color: #5E5E5E;">/</span> pyspark_df.total_bill)</span></code></pre></div>
</div>
<p>If you want to apply numerical transformations like exponents or logs, use the built-in functions in the <code>pyspark.sql.functions</code> module.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="co" style="color: #5E5E5E;"># log </span></span>
<span id="cb26-2">pyspark_df.withColumn(<span class="st" style="color: #20794D;">'log_bill'</span>, f.log(f.col(<span class="st" style="color: #20794D;">'total_bill'</span>)))</span>
<span id="cb26-3"></span>
<span id="cb26-4"><span class="co" style="color: #5E5E5E;"># exponent</span></span>
<span id="cb26-5">pyspark_df.withColumn(<span class="st" style="color: #20794D;">'bill_squared'</span>, f.<span class="bu" style="color: null;">pow</span>(f.col(<span class="st" style="color: #20794D;">'total_bill'</span>), <span class="dv" style="color: #AD0000;">2</span>))</span></code></pre></div>
</div>
<p>You can implement conditional assignment like SQL’s <code>CASE WHEN</code> construct using the <code>when()</code> function and the <code>otherwise()</code> method.</p>
<div class="cell" data-scrolled="true" data-execution_count="13">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="co" style="color: #5E5E5E;"># conditional assignment (like CASE WHEN)</span></span>
<span id="cb27-2">pyspark_df.withColumn(<span class="st" style="color: #20794D;">'is_male'</span>, f.when(f.col(<span class="st" style="color: #20794D;">'sex'</span>) <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'Male'</span>, <span class="va" style="color: #111111;">True</span>).otherwise(<span class="va" style="color: #111111;">False</span>))</span>
<span id="cb27-3"></span>
<span id="cb27-4"><span class="co" style="color: #5E5E5E;"># using multiple when conditions and values</span></span>
<span id="cb27-5">pyspark_df.withColumn(<span class="st" style="color: #20794D;">'bill_size'</span>, </span>
<span id="cb27-6">    f.when(f.col(<span class="st" style="color: #20794D;">'total_bill'</span>) <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="dv" style="color: #AD0000;">10</span>, <span class="st" style="color: #20794D;">'small'</span>)</span>
<span id="cb27-7">    .when(f.col(<span class="st" style="color: #20794D;">'total_bill'</span>) <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="dv" style="color: #AD0000;">20</span>, <span class="st" style="color: #20794D;">'medium'</span>)</span>
<span id="cb27-8">    .otherwise(<span class="st" style="color: #20794D;">'large'</span>)</span>
<span id="cb27-9">)</span></code></pre></div>
</div>
<p>Remember that since PySpark dataframes are immutable, calling <code>withColumns()</code> on a dataframe returns a new dataframe. If you want to persist the result, you’ll need to make an assignment.</p>
<pre><code>pyspark_df = pyspark_df.withColumns(...)</code></pre>
</section>
<section id="group-by-and-aggregate" class="level3">
<h3 class="anchored" data-anchor-id="group-by-and-aggregate">Group by and aggregate</h3>
<p>PySpark provides a <code>groupBy()</code> method similar to the pandas <code>groupby()</code>. Just like in pandas, we can call methods like <code>count()</code> and <code>mean()</code> on our grouped dataframe, and we also have a more flexible <code>agg()</code> method that allows us to specify column-aggregation mappings.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"></span>
<span id="cb29-2"><span class="co" style="color: #5E5E5E;"># group by and count</span></span>
<span id="cb29-3">pyspark_df.groupBy(<span class="st" style="color: #20794D;">'time'</span>).count().show()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+-----+
|  time|count|
+------+-----+
|Dinner|  176|
| Lunch|   68|
+------+-----+
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"></span>
<span id="cb31-2"><span class="co" style="color: #5E5E5E;"># group by and specify column-aggregation mappings with agg()</span></span>
<span id="cb31-3">pyspark_df.groupBy(<span class="st" style="color: #20794D;">'time'</span>).agg({<span class="st" style="color: #20794D;">'total_bill'</span>: <span class="st" style="color: #20794D;">'mean'</span>, <span class="st" style="color: #20794D;">'tip'</span>: <span class="st" style="color: #20794D;">'max'</span>}).show()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+--------+------------------+
|  time|max(tip)|   avg(total_bill)|
+------+--------+------------------+
|Dinner|    10.0| 20.79715909090909|
| Lunch|     6.7|17.168676470588235|
+------+--------+------------------+
</code></pre>
</div>
</div>
<p>If you want to get fancier with your aggregations, it might just be easier to express them using hive syntax. Read on to find out how.</p>
</section>
<section id="run-hive-sql-on-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="run-hive-sql-on-dataframes">Run Hive SQL on dataframes</h3>
<p>One of the mind-blowing features of PySpark is that it allows you to write hive SQL queries on your dataframes. To take a PySpark dataframe into the SQL world, use the <code>createOrReplaceTempView()</code> method. This method takes one string argument which will be the dataframes name in the SQL world. Then you can use <code>spark.sql()</code> to run a query. The result is returned as a PySpark dataframe.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"></span>
<span id="cb33-2"><span class="co" style="color: #5E5E5E;"># put pyspark dataframe in SQL world and query it</span></span>
<span id="cb33-3">pyspark_df.createOrReplaceTempView(<span class="st" style="color: #20794D;">'tips'</span>)</span>
<span id="cb33-4">spark.sql(<span class="st" style="color: #20794D;">'select * from tips'</span>).show(<span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------+----+------+------+---+------+----+
|total_bill| tip|   sex|smoker|day|  time|size|
+----------+----+------+------+---+------+----+
|     16.99|1.01|Female|    No|Sun|Dinner|   2|
|     10.34|1.66|  Male|    No|Sun|Dinner|   3|
|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|
|     23.68|3.31|  Male|    No|Sun|Dinner|   2|
|     24.59|3.61|Female|    No|Sun|Dinner|   4|
+----------+----+------+------+---+------+----+
only showing top 5 rows
</code></pre>
</div>
</div>
<p>This is awesome for a couple of reasons. First, it allows us to easily express any transformations in hive syntax. If you’re like me and you’ve already been using hive, this will dramatically reduce the PySpark learning curve, because when in doubt, you can always bump a dataframe into the SQL world and simply use hive to do what you need. Second, if you have a hive deployment, PySpark’s SQL world also has access to all of your hive tables. This means you can write queries involving both hive tables and your PySpark dataframes. It also means you can run hive commands, like inserting into a table, directly from PySpark.</p>
<p>Let’s do some aggregations that might be a little trickier to do using the PySpark built-in functions.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"></span>
<span id="cb35-2"><span class="co" style="color: #5E5E5E;"># run hive query and save result to dataframe</span></span>
<span id="cb35-3">tip_stats_by_time <span class="op" style="color: #5E5E5E;">=</span> spark.sql(<span class="st" style="color: #20794D;">"""</span></span>
<span id="cb35-4"><span class="st" style="color: #20794D;">    select</span></span>
<span id="cb35-5"><span class="st" style="color: #20794D;">        time</span></span>
<span id="cb35-6"><span class="st" style="color: #20794D;">        , count(*) as n </span></span>
<span id="cb35-7"><span class="st" style="color: #20794D;">        , avg(tip) as avg_tip</span></span>
<span id="cb35-8"><span class="st" style="color: #20794D;">        , percentile_approx(tip, 0.5) as med_tip</span></span>
<span id="cb35-9"><span class="st" style="color: #20794D;">        , avg(case when tip &gt; 3 then 1 else 0 end) as pct_tip_gt_3</span></span>
<span id="cb35-10"><span class="st" style="color: #20794D;">    from </span></span>
<span id="cb35-11"><span class="st" style="color: #20794D;">        tips</span></span>
<span id="cb35-12"><span class="st" style="color: #20794D;">    group by 1</span></span>
<span id="cb35-13"><span class="st" style="color: #20794D;">"""</span>)</span>
<span id="cb35-14"></span>
<span id="cb35-15">tip_stats_by_time.show()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+---+------------------+-------+-------------------+
|  time|  n|           avg_tip|med_tip|       pct_tip_gt_3|
+------+---+------------------+-------+-------------------+
|Dinner|176| 3.102670454545455|    3.0|0.44886363636363635|
| Lunch| 68|2.7280882352941176|    2.2|0.27941176470588236|
+------+---+------------------+-------+-------------------+
</code></pre>
</div>
</div>
</section>
</section>
<section id="visualization-with-pyspark" class="level2">
<h2 class="anchored" data-anchor-id="visualization-with-pyspark">Visualization with PySpark</h2>
<p>There aren’t any tools for visualization included in PySpark. But that’s no problem, because we can just use the <code>toPandas()</code> method on a PySpark dataframe to pull data back into pandas. Once we have a pandas dataframe, we can happily build visualizations as usual. Of course, if your PySpark dataframe is huge, you wouldn’t want to use <code>toPandas()</code> directly, because PySpark will attempt to read the entire contents of its huge dataframe into memory. Instead, it’s best to use PySpark to generate aggregations of your data for plotting or to pull only a sample of your full data into pandas.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="co" style="color: #5E5E5E;"># read aggregated pyspark dataframe into pandas for plotting</span></span>
<span id="cb37-2">plot_pdf <span class="op" style="color: #5E5E5E;">=</span> tip_stats_by_time.toPandas()</span>
<span id="cb37-3">plot_pdf.plot.bar(x<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'time'</span>, y<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">'avg_tip'</span>, <span class="st" style="color: #20794D;">'med_tip'</span>])<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/hello-pyspark/hello-pyspark_files/figure-html/cell-21-output-1.png" class="img-fluid" alt="Figure showing a bar plot of average and median tips by time"></p>
</div>
</div>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>So that’s a wrap on our crash course in working with PySpark. You now have a good idea of what pyspark is and how to get started manipulating dataframes with it. Stay tuned for a future post on PySpark’s companion ML library MLlib. In the meantime, may no dataframe be too large for you ever again.</p>


</section>

 ]]></description>
  <category>PySpark</category>
  <category>tutorial</category>
  <guid>https://blog.mattbowers.dev/posts/2021/hello-pyspark/index.html</guid>
  <pubDate>Mon, 21 Jun 2021 23:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2021/hello-pyspark/guiones_wave.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How Gradient Boosting Does Gradient Descent</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2021/how-gradient-boosting-does-gradient-descent/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2021/how-gradient-boosting-does-gradient-descent/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A whiteboard session at Playa Pelada</figcaption><p></p>
</figure>
</div>
<p>In the last two posts, we learned the basics of <a href="../../../gradient-boosting-machine-from-scratch">gradient boosting machines</a> and the <a href="../../../get-down-with-gradient-descent">gradient descent algorithm</a>. But we still haven’t explicitly addressed what puts the “gradient” in gradient boosting. It turns out that gradient boosting models are using a sort of gradient descent to minimize their loss function; according to <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Friedman’s classic paper</a>, they’re doing gradient descent in “function space”. If you’re like me, and this is your first encounter with this idea, then the phrase “gradient descent in function space” is going to sound a little, ahem, mysterious. No worries, friends; we’re about to make sense of it all.</p>
<p>Understanding the underlying mechanics of gradient boosting as a form of gradient descent will empower us to train our models with custom loss functions. This opens up many interesting possibilities including doing not only regression and classification, but also predicting quantiles, prediction intervals, and even the conditional probability distribution of the response variable.</p>
<section id="generalized-intuition-for-gradient-boosting" class="level2">
<h2 class="anchored" data-anchor-id="generalized-intuition-for-gradient-boosting">Generalized intuition for gradient boosting</h2>
<p>In my earlier post on [building a gradient boosting model from scratch](/gradient-boosting-machine-from-scratch, we established the intuition for how gradient boosting works in a regression problem. In this post we’re going to generalize the ideas we encountered in the regression context, so check out the earlier post if you’re not already familiar with gradient boosting for regression. In the following sections we’ll build up the intuition for gradient boosting in general terms, and then we’ll be able to state the gradient boosting algorithm in a form that can fit models to customized loss functions.</p>
<section id="the-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="the-loss-function">The loss function</h3>
<p>You recall that we measure how well a model fits data by using a <em>loss function</em> that yields small values when a model fits well. “Training” essentially means finding the model that minimizes our loss function. A loss function takes the correct target values and the predicted target values, and it returns a scalar loss score. For example, in the last post on gradient descent we used a mean squared error (MSE) loss</p>
<p><img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%20%5Chat%7B%5Cmathbf%7By%7D%7D)%20=%20%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(y_i%20-%20%5Chat%7By%7D_i)%5E2%20"></p>
<p>where we express the correct targets and predicted values as the vector arguments <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D=%5By_1,y_2,%5Cdots,y_n%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7By%7D%7D=%5B%5Chat%7By%7D_1,%5Chat%7By%7D_2,%5Cdots,%5Chat%7By%7D_n%5D"> respectively.</p>
</section>
<section id="which-way-to-nudge-a-prediction-to-get-a-better-model" class="level3">
<h3 class="anchored" data-anchor-id="which-way-to-nudge-a-prediction-to-get-a-better-model">Which way to nudge a prediction to get a better model</h3>
<p>Now, let’s say we have a model <img src="https://latex.codecogs.com/png.latex?F(%5Cmathbf%7BX%7D)=%5Cmathbf%7B%5Chat%7By%7D%7D"> that we want to improve. One approach is that we could figure out whether each prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> needed to be higher or lower to get a better loss score. We could then nudge each prediction in the right direction, thereby decreasing our model’s loss score.</p>
<p>To figure out whether we should increase or decrease a particular prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> (and by how much), we can compute the partial derivative of the loss function with respect to that prediction. Recall the partial derivative just tells us the rate of change in a function when we change one of its arguments. Since we want to make the loss <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)"> decrease, we can use the negative partial derivative of the loss function with respect to a given prediction to help us choose the right nudge for that prediction.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctext%7Bnudge%20for%20%7D%20%5Chat%7By%7D_i%20=%20-%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_i%7D"></p>
<p>Sometimes it can get a little intense when there are partial derivatives flying around, but it doesn’t have to be that way. Remember that in practice <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_i%7D"> is just an expression that evaluates to a number like 2.7 or -0.5, and here it’s telling us how to nudge <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> to decrease our loss score.</p>
<p>The intuition is that if <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_i%7D"> is negative, then increasing the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> will make the loss decrease. We then notice that the negative of the partial derivative tells us whether to increase or decrease <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i">. For example, if <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_i%7D"> is positive, then <em>increasing</em> the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> will make the loss decrease; whereas if <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_i%7D"> is negative, then <em>decreasing</em> the prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> will make the loss decrease.</p>
<p>Since we’ll want to find the right nudge for each of the <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i">’s, we can use the negative gradient of the loss function <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)"> with respect to the vector argument <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7By%7D%7D"> to get the vector of all the partial derivatives. Let’s call this vector of desired nudge values <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D%20=%20-%5Cnabla_%7B%5Chat%7B%5Cmathbf%7By%7D%7D%7D%20L(%5Cmathbf%7By%7D,%20%5Chat%7B%5Cmathbf%7By%7D%7D)%20=%20%5Cleft%20%5B%20-%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_1%7D,%20-%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_2%7D,%20%5Ccdots,%20-%5Cfrac%7B%5Cpartial%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)%7D%7B%5Cpartial%20%5Chat%7By%7D_n%7D%5Cright%20%5D"></p>
</section>
<section id="nudging-predictions-in-the-right-direction" class="level3">
<h3 class="anchored" data-anchor-id="nudging-predictions-in-the-right-direction">Nudging predictions in the right direction</h3>
<p>Great, now that we know we should nudge each prediction in the direction of the negative partial derivative of the loss with respect to that prediction, we need to figure out how to do the actual nudging. Remember that we already have an initial model <img src="https://latex.codecogs.com/png.latex?F(%5Cmathbf%7BX%7D)=%5Cmathbf%7B%5Chat%7By%7D%7D">.</p>
<p>At this point we might be tempted to simply add the vector of nudge values to our predictions to get better predictions.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bwe%20might%20be%20tempted%20to%20try%20%7D%20%5Cmathbf%7B%5Chat%7By%7D%7D_%7B%5Ctext%7Bnew%7D%7D%20=%20%5Cmathbf%7B%5Chat%7By%7D%7D%20+%20%5Cmathbf%7Br%7D"></p>
<p>Sure, based on our reasoning in the previous section, plugging the vector of nudged predictions into the loss function would yield a lower loss score.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D%20+%20%5Cmathbf%7Br%7D)%20%5Cle%20L(%5Cmathbf%7By%7D,%5Cmathbf%7B%5Chat%7By%7D%7D)"></p>
<p>The problem is that this will only work for in-sample data, because we only know the nudge values for the cases which are present in the training dataset. In order for our model to generalize to unseen test data, we need a way to get the nudge values for new observations of the independent variables. So how can we do that?</p>
<p>Well what if we fit another model <img src="https://latex.codecogs.com/png.latex?h(%5Cmathbf%7BX%7D)"> that used our same features <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> to predict our desired nudge values <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D">, and then we added that new model to our original model <img src="https://latex.codecogs.com/png.latex?F(%5Cmathbf%7BX%7D)">. For a given prediction the nudge model <img src="https://latex.codecogs.com/png.latex?h(%5Cmathbf%7BX%7D)"> would essentially return an approximation of the desired nudge, so adding it would push the prediction in the right direction to decrease the loss function. Furthermore, the nudge model can return predictions of the nudges for out-of-sample cases which are not present in the training dataset. Since both the initial model <img src="https://latex.codecogs.com/png.latex?F(%5Cmathbf%7BX%7D)"> and the nudge model <img src="https://latex.codecogs.com/png.latex?h(%5Cmathbf%7BX%7D)"> are functions of our features <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">, we can add the two functions to get an updated model that can generalize beyond the training data.</p>
<p><img src="https://latex.codecogs.com/png.latex?F_%7B%5Ctext%7Bnew%7D%7D%20(%5Cmathbf%7BX%7D)%20=%20F(%5Cmathbf%7BX%7D)%20+%20h(%5Cmathbf%7BX%7D)"></p>
</section>
</section>
<section id="a-generalized-gradient-boosting-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="a-generalized-gradient-boosting-algorithm">A generalized gradient boosting algorithm</h2>
<p>Ok, let’s put these pieces of intuition together to create a more general gradient boosting algorithm recipe.</p>
<p>We begin with training data <img src="https://latex.codecogs.com/png.latex?(%5Cmathbf%7By%7D,%20%5Cmathbf%7BX%7D)"> where <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D"> is a length-<img src="https://latex.codecogs.com/png.latex?n"> vector of target values, and <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D"> is an <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20p"> matrix with <img src="https://latex.codecogs.com/png.latex?n"> observations of <img src="https://latex.codecogs.com/png.latex?p"> features. We also have a differentiable loss function <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D)">, a “learning rate” hyperparameter <img src="https://latex.codecogs.com/png.latex?%5Ceta">, and a fixed number of model iterations <img src="https://latex.codecogs.com/png.latex?M">.</p>
<p>We create an initial model <img src="https://latex.codecogs.com/png.latex?F_0(%5Cmathbf%7BX%7D)"> that predicts a constant value. We choose the constant value that would give the best loss score.</p>
<p><img src="https://latex.codecogs.com/png.latex?F_0(%5Cmathbf%7BX%7D)%20=%20%5Cunderset%7Bc%7D%7B%5Coperatorname%7Bargmin%7D%7D%20L(%5Cmathbf%7By%7D,%20c)"></p>
<p>Then we iteratively update the initial model with <img src="https://latex.codecogs.com/png.latex?M"> nudge models.</p>
<p>For <img src="https://latex.codecogs.com/png.latex?m"> in 0 to <img src="https://latex.codecogs.com/png.latex?M-1">:</p>
<ul>
<li>Compute current composite model predictions <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Chat%7By%7D%7D_%7Bm%7D%20=%20F_%7Bm%7D(%5Cmathbf%7BX%7D)">.</li>
<li>Compute the desired nudge values given by the negative gradient of the loss function with respect to each prediction <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D_m%20=%20-%20%5Cnabla_%7B%5Cmathbf%7B%5Chat%7By%7D%7D_m%7D%20L%20(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D_m)">.</li>
<li>Fit a weak model (e.g.&nbsp;shallow decision tree) <img src="https://latex.codecogs.com/png.latex?h_%7Bm%7D(%5Cmathbf%7BX%7D)"> that predicts the nudge values <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D_%7Bm%7D"> using features <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">.</li>
<li>Update the composite model.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?F_%7Bm+1%7D(%5Cmathbf%7BX%7D)%20=%20F_%7Bm%7D(%5Cmathbf%7BX%7D)%20+%20%5Ceta%20h_%7Bm%7D(%5Cmathbf%7BX%7D)"></p>
<p>After <img src="https://latex.codecogs.com/png.latex?M"> iterations, we are left with the final composite model <img src="https://latex.codecogs.com/png.latex?F_M(%5Cmathbf%7BX%7D)">.</p>
</section>
<section id="wait-in-what-sense-is-this-doing-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="wait-in-what-sense-is-this-doing-gradient-descent">Wait, in what sense is this doing gradient descent?</h2>
<p>In my <a href="../../../get-down-with-gradient-descent">previous post</a>, we learned how to use gradient descent to iteratively update model parameters to find a model that minimizes the loss function. We could write the update rule as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbf%7B%5Ctheta%7D_%7Bt+1%7D%20=%20%5Cmathbf%7B%5Ctheta%7D_%7Bt%7D%20+%20%5Ceta%20(%20-%20%5Cnabla_%7B%5Cmathbf%7B%5Ctheta%7D%7D%20L(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D_%7B%5Cmathbf%7B%5Ctheta%7D_%7Bt%7D%7D)%20)%20"></p>
<p>where the predictions <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Chat%7By%7D%7D"> depend on the model parameters <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D">, and we’re trying to find the value of the parameter vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D"> that minimizes the loss function <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D_%7B%5Cmathbf%7B%5Ctheta%7D_%7Bt%7D%7D)">, so we nudge the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D_t"> by the negative gradient of <img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D_%7B%5Cmathbf%7B%5Ctheta%7D_%7Bt%7D%7D)"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Ctheta%7D_t">. Compare that with the boosting model update rule we obtained in the previous section.</p>
<p><img src="https://latex.codecogs.com/png.latex?F_%7Bm+1%7D(%5Cmathbf%7BX%7D)%20=%20F_%7Bm%7D(%5Cmathbf%7BX%7D)%20+%20%5Ceta%20h_%7Bm%7D(%5Cmathbf%7BX%7D)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?h_%7Bm%7D(%5Cmathbf%7BX%7D)%20%5Capprox%20-%20%5Cnabla_%7B%5Cmathbf%7B%5Chat%7By%7D%7D_m%7D%20L%20(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D_m)">.</p>
<p>If we replace <img src="https://latex.codecogs.com/png.latex?F(%5Cmathbf%7BX%7D)"> with its prediction vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Chat%7By%7D%7D">, and we replace the nudge model <img src="https://latex.codecogs.com/png.latex?h(%5Cmathbf%7BX%7D)"> with the negative gradient of the loss function (which it approximates), the likeness to the parameter gradient descent update rule becomes more obvious.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Chat%7By%7D%7D_%7Bm+1%7D%20%5Capprox%20%5Cmathbf%7B%5Chat%7By%7D%7D_m%20+%20%5Ceta%20(-%20%5Cnabla_%7B%5Cmathbf%7B%5Chat%7By%7D%7D_m%7D%20L%20(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D_m))"></p>
<p>Indeed, gradient boosting is performing gradient descent to obtain a good model by minimizing a loss function. But there are a couple of key differences between gradient boosting and the parameter gradient descent that we discussed in the previous post.</p>
</section>
<section id="gradient-boosting-versus-parameter-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosting-versus-parameter-gradient-descent">Gradient boosting versus parameter gradient descent</h2>
<p>The generic gradient boosting algorithm outlined above implies two key differences from parameter gradient descent.</p>
<ol type="1">
<li>Instead of nudging parameters, we nudge each individual prediction, thus instead of taking the gradient of loss with respect to the parameters, we take the gradient with respect to the predictions.</li>
<li>Instead of directly adding the negative gradient to our current parameter values, we create a functional approximation of the negative gradient and add that to our model. Our functional approximation is just a crappy model that tries to use the model features to predict the negative gradient of the loss with respect to our current model predictions.</li>
</ol>
<p>The true genius of the gradient boosting algorithm is in chasing the negative gradient of the loss with crappy models, rather than using it to directly update our predictions. If we just directly added the negative gradient of the loss to our predictions, and plugged them into the loss function we could get a lower loss score, but our updated model would be useless since it couldn’t make predictions on new out-of-sample data. Instead we train a crappy model to predict the negative gradient of the loss with respect to the current model predictions, thus we can iteratively update our composite model by adding these crappy models to it.</p>
</section>
<section id="gradient-boosting-is-gradient-descent-in-function-space-a.k.a.-prediction-space" class="level2">
<h2 class="anchored" data-anchor-id="gradient-boosting-is-gradient-descent-in-function-space-a.k.a.-prediction-space">Gradient boosting is gradient descent in function space, a.k.a. prediction space</h2>
<p>Let’s address the statement in <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Friedman’s classic paper</a> that gradient boosting is doing gradient descent in function space. Again we’ll use parameter gradient descent as a basis for comparison.</p>
<p>In parameter gradient descent, we have a vector of parameter values which, when plugged into the loss function, return some loss score. At each step of gradient descent, we compute the negative gradient of the loss function with respect to each parameter; that tells us which way to nudge each parameter value to achieve a lower loss score. We then add this vector of parameter nudge values to our previous parameter vector to get the new parameter vector. We could view this sequence of successive parameter vectors as a trajectory passing through <em>parameter space</em>, the space spanned by all possible parameter values. Therefore parameter gradient descent operates in parameter space.</p>
<p>In contrast, when we do gradient boosting, at each step we have a model, a.k.a. a function, that maps feature values to predictions. Given our training dataset, this model yields predictions which can be plugged into our loss function to get a loss score. At each boosting iteration, we compute the negative gradient of the loss with respect to each of the predictions; that tells us which way to nudge each prediction to achieve a lower loss score. We then create a function (a crappy model) that takes feature values and returns an approximation of the corresponding prediction’s nudge value. We then add this crappy model (a function) to our current composite model (also a function) to get the new composite model (you guessed it; also a function). And so by analogy with parameter vectors in parameter space, we can view this sequence of successive model functions as a trajectory passing through <em>function space</em>, the space spanned by all possible functions that map feature values to predictions. Therefore, gradient boosting does gradient descent in function space.</p>
<p>If this talk about function space still feels a little abstract, you could just use the same substitution trick we used above and swap the model <img src="https://latex.codecogs.com/png.latex?F(%5Cmathbf%7BX%7D)"> for its predictions <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5Chat%7By%7D%7D"> which is just a vector of numbers. The target values for our nudge models are given by the negative gradient of the loss with respect to this prediction vector. From here, we can see that each time we add a new nudge model to our composite model, we get a new prediction vector. We can view this sequence of successive prediction vectors as a trajectory passing through <em>prediction space</em>, the space spanned by all possible prediction vector values. Therefore we can also say that gradient boosting does gradient descent in prediction space.</p>
</section>
<section id="so-why-did-we-fit-the-crappy-models-to-the-residuals-in-our-regression-gbm" class="level2">
<h2 class="anchored" data-anchor-id="so-why-did-we-fit-the-crappy-models-to-the-residuals-in-our-regression-gbm">So why did we fit the crappy models to the residuals in our regression GBM?</h2>
<p>In my first post on [gradient boosting machines](/gradient-boosting-machine-from-scratch, in the interest of simplicity I left one key aspect of the problem unaddressed, that is, what loss function were we using to train that GBM? It turns out that because of the way we built our GBM, without knowing it we were actually using a mean squared error (MSE) loss function.</p>
<p><img src="https://latex.codecogs.com/png.latex?L(%5Cmathbf%7By%7D,%20%5Chat%7B%5Cmathbf%7By%7D%7D)%20=%20%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(y_i%20-%20%5Chat%7By%7D_i)%5E2%20"></p>
<p>If the GBM was using gradient descent to find a <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7By%7D%7D"> vector that minimized this loss function, then at each iteration it would have to nudge the current <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7By%7D%7D"> by the negative gradient of the loss function with respect to <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7By%7D%7D">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?-%5Cnabla_%7B%5Chat%7B%5Cmathbf%7By%7D%7D%7D%20L(%5Cmathbf%7By%7D,%20%5Chat%7B%5Cmathbf%7By%7D%7D)">. Since our loss function takes a length <img src="https://latex.codecogs.com/png.latex?n"> vector of predictions <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7By%7D%7D"> as input, the gradient will be a length-<img src="https://latex.codecogs.com/png.latex?n"> vector of partial derivatives with respect to each of the predictions <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i">. Let’s start by taking the negative partial derivative with respect to a particular prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_j">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Barray%7D%7Brcl%7D%0A-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Chat%7By%7D_j%7D%20L(%5Cmathbf%7By%7D,%20%5Cmathbf%7B%5Chat%7By%7D%7D)%0A%20%20%20%20&amp;%20=%20&amp;%20-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Chat%7By%7D_j%7D%20%5Cleft%20(%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(y_i%20-%20%5Chat%7By%7D_i)%5E2%20%5Cright%20)%20%5C%5C%0A%20%20%20%20&amp;%20=%20&amp;%20-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Chat%7By%7D_j%7D%20%5Cleft%20(%20%5Cfrac%7B1%7D%7Bn%7D%20(y_j%20-%20%5Chat%7By%7D_j)%5E2%20%5Cright%20)%20%5C%5C%0A%20%20%20%20&amp;%20=%20&amp;%20-%5Cfrac%7B1%7D%7Bn%7D%20(2)(y_j%20-%20%5Chat%7By%7D_j)%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Chat%7By%7D_j%7D%20(y_j%20-%20%5Chat%7By%7D_j)%20%5C%5C%0A%20%20%20%20&amp;%20=%20&amp;%20%5Cfrac%7B2%7D%7Bn%7D%20(y_j%20-%20%5Chat%7By%7D_j)%20%5C%5C%0A%5Cend%7Barray%7D%0A"></p>
<p>It turns out that the negative partial derivative of the MSE loss function with respect to a particular prediction <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> is proportional to the residual <img src="https://latex.codecogs.com/png.latex?y_i%20-%20%5Chat%7By%7D_i">! This is a pretty intuitive result, because if we nudge a prediction by it’s residual, we’ll end up with the correct target value.</p>
<p>We can go ahead and write the nudge vector as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D%20=%20-%5Cnabla_%7B%5Chat%7B%5Cmathbf%7By%7D%7D%7D%20L(%5Cmathbf%7By%7D,%20%5Chat%7B%5Cmathbf%7By%7D%7D)%20=%20%5Cfrac%7B2%7D%7Bn%7D(%5Cmathbf%7By%7D%20-%20%5Chat%7B%5Cmathbf%7By%7D%7D)"></p>
<p>which is proportional to the residual vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D%20-%20%5Chat%7B%5Cmathbf%7By%7D%7D">. This means that when we use the mean squared error loss function, our nudge values are given by the current model residuals, and therefore each new crappy model targets the previous model’s residuals.</p>
<p>And this result brings us full circle, back to our original intuition from the first GBM post about chasing residuals with crappy models. Now we see that intuitive idea is just a special case of the more general and, dare I say, even more beautiful idea of chasing the negative gradient of the loss function with crappy models.</p>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<p>We covered a lot of conceptual ground in this post, so let’s recap the key ideas.</p>
<ol type="1">
<li>Gradient boosting can use gradient descent to minimize any differentiable loss function in service of creating a good final model.</li>
<li>There are two key differences between gradient boosting and parameter gradient descent:
<ul>
<li>In gradient boosting, we nudge prediction values rather than parameter values, so to find the desired nudge values, we take the negative gradient of the loss function with respect to the predictions.</li>
<li>In gradient boosting, we nudge our predictions by adding a crappy model that approximates the nudge values, rather than adding the nudge values directly to the predictions.</li>
</ul></li>
<li>Gradient boosting does gradient descent in function space. But since the model predictions are just numeric vectors, and since we take the gradient of the loss function with respect to the prediction vector, it’s also valid and probably easier to think of gradient boosting as gradient descent in prediction space.</li>
<li>We saw that iteratively fitting crappy models to the previous model residuals, as we did in the regression GBM from scratch post, is just a special case of fitting crappy models to the negative gradient of the loss function (in this case the mean squared error loss).</li>
</ol>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Phew, there it is, how gradient boosting models do gradient descent in function space. Understanding how the general form of gradient boosting works opens up the possibility for us to use any differentiable loss function for model training. That is pretty exciting because it means that we can get a lot of mileage out of this one class of learning algorithms. Stay tuned for more on some of the awesome things we can do with these ideas in future posts!</p>
<p>There are a couple of resources I found to be super helpful while researching the content in this post. Definitely check them out if you want to read more about gradient boosting and gradient descent.</p>
<p><a href="https://explained.ai/gradient-boosting/">How to explain gradient boosting</a> by Terence Parr and Jeremy Howard</p>
<p><a href="http://nicolas-hug.com/blog/gradient_boosting_descent">Understanding Gradient Boosting as Gradient Descent</a> by Nicolas Hug</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <guid>https://blog.mattbowers.dev/posts/2021/how-gradient-boosting-does-gradient-descent/index.html</guid>
  <pubDate>Mon, 26 Apr 2021 23:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2021/how-gradient-boosting-does-gradient-descent/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Get Down with Gradient Descent</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/index.html</link>
  <description><![CDATA[ 



<p><img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/gd_get_down.png" title="dancer getting down" class="img-fluid"></p>
<p>Ahh, gradient descent. It’s probably one of the most ubiquitous algorithms used in data science, but you’re unlikely to see it being celebrated in the limelight of the Kaggle podium. Rather than taking center stage, gradient descent operates under the hood, powering the training for a wide range of models including deep neural networks, gradient boosting trees, generalized linear models, and mixed effects models. Getting an intuition for the algorithm will reveal how model fitting actually works and help us to see the common thread connecting a wide range of seemingly unrelated models. In this post we’ll get the intuition for gradient descent with a <em>fresh</em> analogy, develop the mathematical formulation, and ground our understanding by using it to train ourselves a linear regression model.</p>
<section id="intuition" class="level2">
<h2 class="anchored" data-anchor-id="intuition">Intuition</h2>
<p>Before we dive into the intuition for gradient descent itself, let’s get a high-level view of why it’s useful in <em>training</em> or <em>fiting</em> a model. Training a model basically means finding the model parameter values that make the model fit a given dataset well. We measure how well a model fits data using a special function variously called a <em>loss</em> or <em>cost</em> or <em>objective</em> function. A loss function takes the dataset and the model as arguments and returns a number that tells us how well our model fits the data. Therefore training is an optimization problem in which we search for the model parameter values that result in the minimum value of the loss function. Enter <em>gradient descent</em>.</p>
<p>Gradient descent is a numerical optimization technique that helps us find the inputs that yield the minimum value of a function. Since most explanations of the gradient descent algorithm seem to use a story about hikers being lost in some foggy mountains, we’re going to try out a new analogy.</p>
<p>Let’s say you’re at a concert. Remember those? They’re these things that used to happen where people played music and everyone danced and had a great time.</p>
<blockquote class="blockquote">
<p><strong><em>NOTE:</em></strong> Chiming in here in 2023 from a sort-of-post COVID 19 world, happily I can report that concerts and live music are back!</p>
</blockquote>
<p>Now suppose at this concert there’s a dance floor which has become a bit sweltering from copious amounts of “getting down”. But the temperature isn’t quite uniform; maybe there’s a cool spot from a ceiling fan somewhere.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/gd_dance_floor.png" title="dance floor" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">dance floor</figcaption><p></p>
</figure>
</div>
<p>Let’s get ourselves to that cool spot using the following procedure.</p>
<ol type="1">
<li>From our current location, figure out which direction feels coolest.</li>
<li>Take a step (or simply shimmy) in that direction.</li>
<li>Repeat steps 1 and 2 until we reach the coolest spot on the dance floor.</li>
</ol>
<p>The crux of this procedure is figuring out, at each step, which direction yields the greatest temperature reduction. Our skin is pretty sensitive to temperature, so we can just use awareness of body sensation to sense which direction feels coolest. Luckily, we have a mathematical equivalent to our skin’s ability to sense local variation in temperature.</p>
<section id="determine-which-way-to-go" class="level3">
<h3 class="anchored" data-anchor-id="determine-which-way-to-go">Determine which way to go</h3>
<p>Let <img src="https://latex.codecogs.com/png.latex?f(x,y)"> be the temperature on the dance floor at position <img src="https://latex.codecogs.com/png.latex?(x,y)">. The direction of fastest decrease in temperature is going to be given by some vector in our <img src="https://latex.codecogs.com/png.latex?(x,y)"> space, e.g.,</p>
<p>[vector component in <img src="https://latex.codecogs.com/png.latex?x"> direction, vector component in <img src="https://latex.codecogs.com/png.latex?y"> direction]</p>
<p>Turns out that the gradient of a function evaluated at a particular location yields a vector that points in the direction of fastest <em>increase</em> in the function, pretty similar to what we’re looking for. The gradient of <img src="https://latex.codecogs.com/png.latex?f(x,y)"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cnabla%20f(x,y)%20=%20%5Cleft%20%5B%20%5Cfrac%7B%5Cpartial%20f(x,y)%7D%7B%5Cpartial%20x%7D,%20%5Cfrac%7B%5Cpartial%20f(x,y)%7D%7B%5Cpartial%20y%7D%20%5Cright%20%5D%20"></p>
<p>The components of the gradient vector are the partial derivatives of our function <img src="https://latex.codecogs.com/png.latex?f(x,y)">, evaluated at the point <img src="https://latex.codecogs.com/png.latex?(x,y)">. These partial derivatives just tell us the slope of <img src="https://latex.codecogs.com/png.latex?f(x,y)"> in the <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> directions respectively. The intuition is that if <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f(x,y)%7D%7B%5Cpartial%20x%7D"> is a large positive number, then moving in the positive <img src="https://latex.codecogs.com/png.latex?x"> direction will make <img src="https://latex.codecogs.com/png.latex?f(x,y)"> increase a lot, whereas if <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20f(x,y)%7D%7B%5Cpartial%20x%7D"> is a large negative number, then moving in the <em>negative</em> <img src="https://latex.codecogs.com/png.latex?x"> direction will make <img src="https://latex.codecogs.com/png.latex?f(x,y)"> increase a lot.</p>
<p>It’s not too hard to see that the direction of fastest decrease is actually just the exact opposite direction from that of fastest increase. Since we can point a vector in the opposite direction by negating its component values, our direction of fastest temperature decrease will be given by the negative gradient of the temperature field <img src="https://latex.codecogs.com/png.latex?-%5Cnabla%20f(x,y)">.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/gd_local_change.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">dance floor with hot and cold sides</figcaption><p></p>
</figure>
</div>
</section>
<section id="take-a-step-in-the-right-direction" class="level3">
<h3 class="anchored" data-anchor-id="take-a-step-in-the-right-direction">Take a step in the right direction</h3>
<p>Now that we have our direction vector, we’re ready to take a step toward the cool part of the dance floor. To do this, we’ll just add our direction vector to our current position. The update rule would look like this.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Bx_%5Ctext%7Bnext%7D,%20y_%5Ctext%7Bnext%7D%5D%20=%20%5Bx_%5Ctext%7Bprev%7D,%20y_%5Ctext%7Bprev%7D%5D%20-%20%5Cnabla%20f%20(x_%5Ctext%7Bprev%7D,%20y_%5Ctext%7Bprev%7D)%20=%20%5Bx_%5Ctext%7Bprev%7D,%20y_%5Ctext%7Bprev%7D%5D%20-%20%20%5Cleft%20%5B%20%5Cfrac%7B%5Cpartial%20f%20(x_%5Ctext%7Bprev%7D,%20y_%5Ctext%7Bprev%7D)%7D%7B%5Cpartial%20x%7D,%20%5Cfrac%7B%5Cpartial%20f%20(x_%5Ctext%7Bprev%7D,%20y_%5Ctext%7Bprev%7D)%7D%7B%5Cpartial%20y%7D%20%5Cright%20%5D%20"></p>
<p>If we iteratively apply this update rule, we’ll end up tracing a trajectory through the <img src="https://latex.codecogs.com/png.latex?(x,y)"> space on the dance floor and we’ll eventually end up at the coolest spot!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/gd_trajectory.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">dance floor with trajectory from hot side to cool side</figcaption><p></p>
</figure>
</div>
<p>Great success!</p>
</section>
</section>
<section id="general-formulation" class="level2">
<h2 class="anchored" data-anchor-id="general-formulation">General Formulation</h2>
<p>Let’s generalize a bit to get to the form of gradient descent you’ll see in references like <a href="https://en.wikipedia.org/wiki/Gradient_descent">the wikipedia article</a>.</p>
<p>First we modify our update equation above to handle functions with more than two arguments. We’ll use a bold <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D"> to indicate a vector of inputs <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D%20=%20%5Bx_1,x_2,%5Cdots,x_p%5D">. Our function <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D):%20%5Cmathbb%7BR%7D%5Ep%20%5Cmapsto%20%5Cmathbb%7BR%7D"> maps a <img src="https://latex.codecogs.com/png.latex?p"> dimensional input to a scalar output.</p>
<p>Second, instead of displacing our current location with the negative gradient vector itself, we’ll first rescale it with a learning rate parameter. This helps address any issues with units on inputs versus outputs. Imagine the input could range between 0 and 1, but the output ranged from 0 to 1,000. We would need to rescale the partial derivatives so the update step doesn’t send us way too far off in input space.</p>
<p>Finally, we’ll index our updates with <img src="https://latex.codecogs.com/png.latex?t=0,1,%5Cdots">. We’ll run for some prespecified number of iterations or we’ll stop the procedure once the change in <img src="https://latex.codecogs.com/png.latex?f(%5Cmathbf%7Bx%7D)"> is sufficiently small from one iteration to the next. Our update equation will look like this.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bx%7D_%7Bt+1%7D%20=%20%5Cmathbf%7Bx%7D_t%20-%20%5Ceta%20%5Cnabla%20f%20(%20%5Cmathbf%7Bx%7D_t)%20"></p>
<p>In pseudocode we could write it like this.</p>
<pre><code># gradient descent
x = initial_value_of_x 
for t in range(n_iterations):  # or some other convergence condition
    x -= learning_rate * gradient_of_f(x)</code></pre>
<p>Now let’s see how this algorithm gets used to train models.</p>
</section>
<section id="training-a-linear-regression-model-with-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="training-a-linear-regression-model-with-gradient-descent">Training a Linear Regression Model with Gradient Descent</h2>
<p>To get the intuition for how we use gradient descent to train models, let’s use it to train a linear regression model. Note that we wouldn’t actually use gradient descent to train a linear model in real life since there is an exact <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">analytical solution</a> for the best-fit parameter values.</p>
<p>Anyway, in the simple linear regression problem we have numerical feature <img src="https://latex.codecogs.com/png.latex?x"> and numerical target <img src="https://latex.codecogs.com/png.latex?y">, and we want to find a model of the form</p>
<p><img src="https://latex.codecogs.com/png.latex?F(x)%20=%20%5Calpha%20+%20%5Cbeta%20x"></p>
<p>This model has two parameters, <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Here “training” means finding the parameter values that make <img src="https://latex.codecogs.com/png.latex?F(x)"> fit our <img src="https://latex.codecogs.com/png.latex?y"> data best. We measure how well, or really how poorly, our model fits the data by using a <em>loss function</em> that yields a small value when a model fits well. Ordinary least squares is so named because it uses mean squared error as its loss function.</p>
<p><img src="https://latex.codecogs.com/png.latex?L(y,%20F(x))%20=%20%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(y_i%20-%20F(x_i))%5E2%20%20=%20%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(y_i%20-%20(%5Calpha%20+%20%5Cbeta%20x_i))%5E2%20"></p>
<p>The loss function <img src="https://latex.codecogs.com/png.latex?L"> takes four arguments: <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?y">, <img src="https://latex.codecogs.com/png.latex?%5Calpha">, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. But since <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> are fixed given our dataset, we could write the loss as <img src="https://latex.codecogs.com/png.latex?L(%5Calpha,%20%5Cbeta%20%7C%20x,%20y)"> to emphasize that <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are the only free parameters. So we’re looking for the following.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cunderset%7B%5Calpha,%5Cbeta%7D%7B%5Coperatorname%7Bargmin%7D%7D%20~%20L(%5Calpha,%5Cbeta%7Cx,y)%20"></p>
<p>That’s right, we’re looking for the values of <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> that minimize scalar-valued function <img src="https://latex.codecogs.com/png.latex?L(%5Calpha,%20%5Cbeta)">. Sounds familiar huh?</p>
<p>To solve this minimization problem with gradient descent, we can use the following update rule.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5B%5Calpha_%7Bt+1%7D,%20%5Cbeta_%7Bt+1%7D%5D%20=%20%5B%5Calpha_%7Bt%7D,%20%5Cbeta_%7Bt%7D%5D%20-%20%5Ceta%20%5Cnabla%20L(%5Calpha_t,%20%5Cbeta_t%20%7C%20x,%20y)%20"></p>
<p>To get the gradient <img src="https://latex.codecogs.com/png.latex?%5Cnabla%20L(%5Calpha,%5Cbeta%7Cx,y)">, we need the partial derivatives of <img src="https://latex.codecogs.com/png.latex?L"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. Since <img src="https://latex.codecogs.com/png.latex?L"> is just a big sum, it’s easy to calculate the derivatives.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%5Cpartial%20L(%5Calpha,%20%5Cbeta)%7D%7B%5Cpartial%20%5Calpha%7D%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20-2%20(y_i%20-%20(%5Calpha%20+%20%5Cbeta%20x_i))%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Cfrac%7B%5Cpartial%20L(%5Calpha,%20%5Cbeta)%7D%7B%5Cpartial%20%5Cbeta%7D%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20-2x_i%20(y_i%20-%20(%5Calpha%20+%20%5Cbeta%20x_i))%20"></p>
<p>Great! We’ve got everything we need to implement gradient descent to train an ordinary least squares model. Everything except data that is.</p>
<section id="toy-data" class="level3">
<h3 class="anchored" data-anchor-id="toy-data">Toy Data</h3>
<p>Let’s make a friendly little linear dataset where <img src="https://latex.codecogs.com/png.latex?%5Calpha=-10"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta=2">, i.e.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20y%20=%20-10%20+%202x%20+%20%5Ctext%7Bnoise%7D"></p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np </span>
<span id="cb2-2"></span>
<span id="cb2-3">alpha_true <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb2-4">beta_true <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb2-5"></span>
<span id="cb2-6">rng <span class="op" style="color: #5E5E5E;">=</span> np.random.default_rng(<span class="dv" style="color: #AD0000;">42</span>)</span>
<span id="cb2-7">x <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">50</span>)</span>
<span id="cb2-8">y <span class="op" style="color: #5E5E5E;">=</span> alpha_true <span class="op" style="color: #5E5E5E;">+</span> beta_true<span class="op" style="color: #5E5E5E;">*</span>x <span class="op" style="color: #5E5E5E;">+</span> rng.normal(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, size<span class="op" style="color: #5E5E5E;">=</span>x.shape)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/get-down_files/figure-html/cell-4-output-1.png" class="img-fluid" alt="Figure showing a scatterplot of x and y data with y increasing linearly in x"></p>
</div>
</div>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p>Our implementation will use a function to compute the gradient of the loss function. Since we have two parameters, we’ll use length-2 arrays to hold their values and their partial derivatives. At each iteration, we update the parameter values by subtracting the rescaled partial derivatives.</p>
<div class="cell" data-scrolled="true" data-execution_count="13">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"></span>
<span id="cb3-2"><span class="co" style="color: #5E5E5E;"># linear regression using gradient descent </span></span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="kw" style="color: #003B4F;">def</span> gradient_of_loss(parameters, x, y):</span>
<span id="cb3-5">    alpha <span class="op" style="color: #5E5E5E;">=</span> parameters[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb3-6">    beta <span class="op" style="color: #5E5E5E;">=</span> parameters[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb3-7">    partial_alpha <span class="op" style="color: #5E5E5E;">=</span> np.mean(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>(y <span class="op" style="color: #5E5E5E;">-</span> (alpha <span class="op" style="color: #5E5E5E;">+</span> beta<span class="op" style="color: #5E5E5E;">*</span>x)))</span>
<span id="cb3-8">    partial_beta <span class="op" style="color: #5E5E5E;">=</span> np.mean(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>x<span class="op" style="color: #5E5E5E;">*</span>(y <span class="op" style="color: #5E5E5E;">-</span> (alpha <span class="op" style="color: #5E5E5E;">+</span> beta<span class="op" style="color: #5E5E5E;">*</span>x)))</span>
<span id="cb3-9">    <span class="cf" style="color: #003B4F;">return</span> np.array([partial_alpha, partial_beta])</span>
<span id="cb3-10"></span>
<span id="cb3-11">learning_rate <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.02</span></span>
<span id="cb3-12">parameters <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.0</span>]) <span class="co" style="color: #5E5E5E;"># initial values of alpha and beta</span></span>
<span id="cb3-13"></span>
<span id="cb3-14"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">500</span>):</span>
<span id="cb3-15">    partial_derivatives <span class="op" style="color: #5E5E5E;">=</span> gradient_of_loss(parameters, x, y)</span>
<span id="cb3-16">    parameters <span class="op" style="color: #5E5E5E;">-=</span> learning_rate <span class="op" style="color: #5E5E5E;">*</span> partial_derivatives</span>
<span id="cb3-17">    </span>
<span id="cb3-18">parameters</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>array([-10.07049616,   2.03559051])</code></pre>
</div>
</div>
<p>We can see the loss function decreasing throughout the 500 iterations.</p>
<div class="cell" data-scrolled="true" data-execution_count="15">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/get-down_files/figure-html/cell-7-output-1.png" class="img-fluid" alt="Figure showing the loss function decreasing with each iteration"></p>
</div>
</div>
<p>And we can visualize the loss function as a contour plot over <img src="https://latex.codecogs.com/png.latex?(%5Calpha,%5Cbeta)"> space. The blue points show the trajectory our gradient descent followed as it shimmied from the initial position to the coolest spot in <img src="https://latex.codecogs.com/png.latex?(%5Calpha,%20%5Cbeta)"> space where the loss function is nice and small.</p>
<div class="cell" data-scrolled="true" data-execution_count="16">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/get-down_files/figure-html/cell-8-output-1.png" class="img-fluid" alt="Figure showing contour plot of the loss function and a trace moving toward the minimum"></p>
</div>
</div>
<p>Our gradient descent settles in a spot pretty close to <img src="https://latex.codecogs.com/png.latex?(-10,%202)"> in <img src="https://latex.codecogs.com/png.latex?(%5Calpha,%5Cbeta)"> space, which gives us the final fitted model below.</p>
<div class="cell" data-scrolled="true" data-execution_count="18">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/get-down_files/figure-html/cell-10-output-1.png" class="img-fluid" alt="Figure showing the original x and y data along with our fitted line"></p>
</div>
</div>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There you have it, gradient descent explained with a fresh new analogy having nothing whatsoever to do with foggy mountains, plus an implemented example fitting a linear model. While we often see gradient descent used to train models by performing an optimization in parameter space, as in generalized linear models and neural networks, there are other ways to use this powerful technique to train models. In particular, we’ll soon see how our beloved gradient boosting tree models use gradient descent in <em>prediction space</em>, rather than parameter space. Stay tuned for that mind bender in a future post.</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <guid>https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/index.html</guid>
  <pubDate>Fri, 22 Jan 2021 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2021/get-down-with-gradient-descent/thumbnail.png" medium="image" type="image/png" height="108" width="144"/>
</item>
<item>
  <title>How to Build a Gradient Boosting Machine from Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">SF buzzes silently in the distance</figcaption><p></p>
</figure>
</div>
<p>Ahh, gradient boosting. In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. Like its cousin random forest, gradient boosting is an ensemble technique that generates a single strong model by combining many simple models, usually decision trees. These tree ensemble methods perform very well on tabular data prediction problems and are therefore widely used in industrial applications and machine learning competitions.</p>
<p>There are several noteworthy variants of gradient boosting out there in the wild including <a href="https://xgboost.readthedocs.io/en/latest/">XGBoost</a>, <a href="https://stanfordmlgroup.github.io/projects/ngboost/">NGBoost</a>, <a href="https://lightgbm.readthedocs.io/en/latest/">LightGBM</a>, and of course the classic <a href="https://en.wikipedia.org/wiki/Gradient_boosting">gradient boosting machine</a> (GBM). While XGBoost and LightGBM tend to have a marginal performance edge on the classic GBM, they are all based on a similar, very clever, idea about how to ensemble decision trees. Let’s avail ourselves of the intuition behind that clever idea, and then we’ll be able to build our very own GBM from scratch.</p>
<section id="toy-data" class="level2">
<h2 class="anchored" data-anchor-id="toy-data">Toy Data</h2>
<p>We begin our boosting adventure with a deceptively simple toy dataset having one feature <img src="https://latex.codecogs.com/png.latex?x"> and target <img src="https://latex.codecogs.com/png.latex?y">.</p>
<div class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-4-output-1.png" class="img-fluid" alt="Figure showing a scatterplot of x and y toy data. y increases linearly in x then becomes constant in x."></p>
</div>
</div>
<p>Notice that <img src="https://latex.codecogs.com/png.latex?y"> increases with <img src="https://latex.codecogs.com/png.latex?x"> for a while, then flattens out. This is a pattern that happens all the time in real data, and it’s one that linear models epically fail to capture. Let’s build a gradient boosting machine to model it.</p>
</section>
<section id="intuition" class="level2">
<h2 class="anchored" data-anchor-id="intuition">Intuition</h2>
<p>Suppose we have a crappy model <img src="https://latex.codecogs.com/png.latex?F_0(x)"> that uses features <img src="https://latex.codecogs.com/png.latex?x"> to predict target <img src="https://latex.codecogs.com/png.latex?y">. A crappy but reasonable choice of <img src="https://latex.codecogs.com/png.latex?F_0(x)"> would be a model that always predicts the mean of <img src="https://latex.codecogs.com/png.latex?y">.</p>
<p><img src="https://latex.codecogs.com/png.latex?F_0(x)%20=%20%5Cbar%7By%7D"></p>
<p>That would look like this.</p>
<div class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-6-output-1.png" class="img-fluid" alt="Figure showing the toy data with a flat model prediction."></p>
</div>
</div>
<p><img src="https://latex.codecogs.com/png.latex?F_0(x)"> by itself is not a great model, so its residuals <img src="https://latex.codecogs.com/png.latex?y%20-%20F_0(x)"> are still pretty big and they still exhibit meaningful structure that we should try to capture with our model.</p>
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-7-output-1.png" class="img-fluid" alt="Figure showing residuals from the initial flat model."></p>
</div>
</div>
<p>Well what if I had another crappy model <img src="https://latex.codecogs.com/png.latex?h_1(x)"> that could predict the residuals <img src="https://latex.codecogs.com/png.latex?y%20-%20F_0(x)">?</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Features</th>
<th style="text-align: center;">Target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?h_1(x)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?x"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?y-F_0(x)"></td>
</tr>
</tbody>
</table>
<p>It’s worth noting that the crappiness of this new model is essential; in fact in this boosting context, it’s usually called a <em>weak learner</em>. To get a model that’s only slightly better than nothing, let’s use a very simple <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision tree</a> with a single split, a.k.a. a stump. This model basically divides our feature <img src="https://latex.codecogs.com/png.latex?x"> into two regions and predicts the mean value of <img src="https://latex.codecogs.com/png.latex?y"> for all of the <img src="https://latex.codecogs.com/png.latex?x">’s in each region. It might look like this.</p>
<div class="cell" data-scrolled="true" data-execution_count="8">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-8-output-1.png" class="img-fluid" alt="Figure showing a fit to the residuals."></p>
</div>
</div>
<p>We could make a composite model by adding the predictions of the base model <img src="https://latex.codecogs.com/png.latex?F_0(x)"> to the predictions of the supplemental model <img src="https://latex.codecogs.com/png.latex?h_1(x)"> (which will pick up some of the slack left by <img src="https://latex.codecogs.com/png.latex?F_0(x)">). We’d get a new model <img src="https://latex.codecogs.com/png.latex?F_1(x)">:</p>
<p><img src="https://latex.codecogs.com/png.latex?F_1(x)%20=%20F_0(x)%20+%20h_1(x)"></p>
<p>which is better at predicting <img src="https://latex.codecogs.com/png.latex?y"> than the original model <img src="https://latex.codecogs.com/png.latex?F_0(x)"> alone.</p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-9-output-1.png" class="img-fluid" alt="Figure showing a composite model with one step."></p>
</div>
</div>
<p>Why stop there? Our composite model <img src="https://latex.codecogs.com/png.latex?F_1(x)"> might still be kind of crappy, and so its residuals <img src="https://latex.codecogs.com/png.latex?y%20-%20F_1(x)"> might still be pretty big and structurey. Let’s add another model <img src="https://latex.codecogs.com/png.latex?h_2(x)"> to predict those residuals.</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Features</th>
<th style="text-align: center;">Target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?h_2(x)"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?x"></td>
<td style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?y-F_1(x)"></td>
</tr>
</tbody>
</table>
<p>The new composite model is</p>
<p><img src="https://latex.codecogs.com/png.latex?F_2(x)%20=%20F_1(x)%20+%20h_2(x)."></p>
<div class="cell" data-scrolled="true" data-execution_count="10">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-10-output-1.png" class="img-fluid" alt="Figure with two panels. On the left a residual plot with model fit to the residuals. On the right the toy data with updated composite model fit."></p>
</div>
</div>
<p>If we keep doing this, at each stage we’ll train a new model <img src="https://latex.codecogs.com/png.latex?h_m(x)"> on the previous composite model’s residuals <img src="https://latex.codecogs.com/png.latex?y-F_%7Bm-1%7D(x)">, and we’ll get a new composite model</p>
<p><img src="https://latex.codecogs.com/png.latex?F_m(x)%20=%20F_%7Bm-1%7D(x)%20+%20h_m(x)."></p>
<p>If we add <img src="https://latex.codecogs.com/png.latex?M"> crappy models constructed in this way to our original crappy model <img src="https://latex.codecogs.com/png.latex?F_0(x)">, we might actually end up with a pretty good model <img src="https://latex.codecogs.com/png.latex?F_M(x)"> that looks like</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AF_M(x)%20=%20F_0(x)%20+%20%5Csum_%7Bm%20=%201%7D%5E%7BM%7D%20h_m(x)%0A"></p>
<p>Here’s how our model would evolve up to <img src="https://latex.codecogs.com/png.latex?M=6">.</p>
<div class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-11-output-1.png" class="img-fluid" alt="Figure with two columns and several rows of panels. The left column contains residual plots. The right column contains updated composite models. Each subsequent row shows the results of another boosting round where the composite model becomes more accurate after each round."></p>
</div>
</div>
<p>Voila! That, friends, is boosting!</p>
</section>
<section id="learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="learning-rate">Learning Rate</h2>
<p>Let’s talk about overfitting. In real life, if we just add our new weak learner <img src="https://latex.codecogs.com/png.latex?h_m(x)"> directly to our existing composite model <img src="https://latex.codecogs.com/png.latex?F_%7Bm-1%7D(x)">, then we’re likely to end up overfitting on our training data. That’s because if we add enough of these weak learners, they’re going to chase down y so closely that all the remaining residuals are pretty much zero, and we will have successfully memorized the training data. To prevent that, we’ll scale them down a bit by a parameter <img src="https://latex.codecogs.com/png.latex?%5Ceta"> called the learning rate.</p>
<p>With the learning rate <img src="https://latex.codecogs.com/png.latex?%5Ceta">, the update step will then look like</p>
<p><img src="https://latex.codecogs.com/png.latex?F_%7Bm%7D(x)%20=%20F_%7Bm-1%7D(x)%20+%20%5Ceta%20h_m(x),"></p>
<p>and our composite model will look like</p>
<p><img src="https://latex.codecogs.com/png.latex?F_M(x)%20=%20F_0(x)%20+%20%5Ceta%20%5Csum_%7Bm%20=%201%7D%5E%7BM%7D%20h_m(x)"></p>
<p>Note that since the learning rate can be factored out of the sum, it looks kinda like we could just build our models without it and slap it on at the end when we sum up the weak learners to make the final composite model. But that won’t work, since at each stage we train the next weak learner on the residuals from the current composite model, and the current composite model depends on the learning rate.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>Ok, we’re ready to implement this thing from “scratch”. Well, sort of. To quote Carl Sagan,</p>
<div class="quarto-video ratio ratio-16x9"><iframe src="https://www.youtube.com/embed/7s664NsLeFM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<blockquote class="blockquote">
<p>If you wish to make an apple pie from scratch, you must first invent the universe.</p>
</blockquote>
<p>We will not be inventing a universe that contains the Earth, apple trees, computers, python, numpy, and sklearn. To keep the “scratch” implementation clean, we’ll allow ourselves the luxury of numpy and an off-the-shelf sklearn decision tree which we’ll use as our weak learner.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> sklearn.tree <span class="im" style="color: #00769E;">import</span> DecisionTreeRegressor</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="co" style="color: #5E5E5E;"># model hyperparameters</span></span>
<span id="cb1-4">learning_rate <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.3</span></span>
<span id="cb1-5">n_trees <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb1-6">max_depth <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;"># Training</span></span>
<span id="cb1-9">F0 <span class="op" style="color: #5E5E5E;">=</span> y.mean() </span>
<span id="cb1-10">Fm <span class="op" style="color: #5E5E5E;">=</span> F0</span>
<span id="cb1-11">trees <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb1-12"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(n_trees):</span>
<span id="cb1-13">    tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth)</span>
<span id="cb1-14">    tree.fit(x, y <span class="op" style="color: #5E5E5E;">-</span> Fm)</span>
<span id="cb1-15">    Fm <span class="op" style="color: #5E5E5E;">+=</span> learning_rate <span class="op" style="color: #5E5E5E;">*</span> tree.predict(x)</span>
<span id="cb1-16">    trees.append(tree)</span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="co" style="color: #5E5E5E;"># Prediction</span></span>
<span id="cb1-19">y_hat <span class="op" style="color: #5E5E5E;">=</span> F0 <span class="op" style="color: #5E5E5E;">+</span> learning_rate <span class="op" style="color: #5E5E5E;">*</span> np.<span class="bu" style="color: null;">sum</span>([t.predict(x) <span class="cf" style="color: #003B4F;">for</span> t <span class="kw" style="color: #003B4F;">in</span> trees], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<p>We first define our hyperparameters: - <code>learning_rate</code> is (<img src="https://latex.codecogs.com/png.latex?%5Ceta">) - <code>n_trees</code> is the number of weak learner trees to add (<img src="https://latex.codecogs.com/png.latex?M">) - <code>max_depth</code> controls the depth of the trees; here we set to 1 for stumps</p>
<p>We define our base model predictions <code>F0</code> to simply predict the mean value of <code>y</code>. <code>Fm</code> corresponds to the current composite model <img src="https://latex.codecogs.com/png.latex?F_m(x)"> as we iteratively add weak learners, so we’ll initialize it with <code>F0</code>. <code>trees</code> is an empty list that we’ll use to hold our weak learners.</p>
<p>Next we iteratively add <code>n_trees</code> weak learners to our composite model. At each iteration, we create a new decision tree and train it on <code>x</code> to predict the current residuals <code>y - Fm</code>. We update <code>Fm</code> with the newly trained learner’s predictions scaled by the learning rate, and we append the new weak learner <img src="https://latex.codecogs.com/png.latex?h_m(x)"> in the <code>trees</code> list. We generate final predictions <code>y_hat</code> on the training data by summing up the predictions from each weak learner, scaling by the learning rate, and adding to the base model (a.k.a. the mean of <code>y</code>).</p>
<div class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<p><img src="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/gradient-boosting-machine-from-scratch_files/figure-html/cell-13-output-1.png" class="img-fluid" alt="Figure showing the toy data with a composite model after several boosting rounds. The model fits the scatter data well."></p>
</div>
</div>
<p>Nice! Our GBM fits that nonlinear data pretty well.</p>
<p>Now that we have a working implementation, let’s go ahead and implement it as a class with <code>fit</code> and <code>predict</code> methods like we’re used to having in sklearn.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">class</span> GradientBoostingFromScratch():</span>
<span id="cb2-2">    </span>
<span id="cb2-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_trees, learning_rate, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb2-4">        <span class="va" style="color: #111111;">self</span>.n_trees<span class="op" style="color: #5E5E5E;">=</span>n_trees<span class="op" style="color: #5E5E5E;">;</span> <span class="va" style="color: #111111;">self</span>.learning_rate<span class="op" style="color: #5E5E5E;">=</span>learning_rate<span class="op" style="color: #5E5E5E;">;</span> <span class="va" style="color: #111111;">self</span>.max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth<span class="op" style="color: #5E5E5E;">;</span></span>
<span id="cb2-5">        </span>
<span id="cb2-6">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, x, y):</span>
<span id="cb2-7">        <span class="va" style="color: #111111;">self</span>.trees <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb2-8">        <span class="va" style="color: #111111;">self</span>.F0 <span class="op" style="color: #5E5E5E;">=</span> y.mean()</span>
<span id="cb2-9">        Fm <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.F0 </span>
<span id="cb2-10">        <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.n_trees):</span>
<span id="cb2-11">            tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">self</span>.max_depth)</span>
<span id="cb2-12">            tree.fit(x, y <span class="op" style="color: #5E5E5E;">-</span> Fm)</span>
<span id="cb2-13">            Fm <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> tree.predict(x)</span>
<span id="cb2-14">            <span class="va" style="color: #111111;">self</span>.trees.append(tree)</span>
<span id="cb2-15">            </span>
<span id="cb2-16">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, x):</span>
<span id="cb2-17">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.F0 <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> np.<span class="bu" style="color: null;">sum</span>([tree.predict(x) <span class="cf" style="color: #003B4F;">for</span> tree <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.trees], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)</span></code></pre></div>
</div>
<p>Let’s compare the performance of our implementation with the sklearn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"><code>GradientBoostingRegressor</code></a>.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;">import</span> GradientBoostingRegressor</span>
<span id="cb3-2"><span class="im" style="color: #00769E;">from</span> sklearn.metrics <span class="im" style="color: #00769E;">import</span> mean_squared_error</span>
<span id="cb3-3"></span>
<span id="cb3-4">sklearn_gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingRegressor(n_estimators <span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">25</span>, learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.3</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb3-5">sklearn_gbm.fit(x,y)</span>
<span id="cb3-6"></span>
<span id="cb3-7">scratch_gbm <span class="op" style="color: #5E5E5E;">=</span> GradientBoostingFromScratch(n_trees<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">25</span>, learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.3</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb3-8">scratch_gbm.fit(x,y)</span>
<span id="cb3-9"></span>
<span id="cb3-10">mean_squared_error(y, sklearn_gbm.predict(x)), mean_squared_error(y, scratch_gbm.predict(x))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>(0.08622324648703916, 0.0862232464870392)</code></pre>
</div>
</div>
<p>Heck yeah! Our homemade GBM is consistent with the sklearn implementation!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Alright, there you have it, the intuition behind basic gradient boosting and a from scratch implementation of the gradient boosting machine. I tried to keep this explanation as simple as possible while giving a complete intuition for the basic GBM. But it turns out that the rabbit hole goes pretty deep on these gradient boosting algorithms. We can actually wave our magic generalization wand over some custom loss functions and end up with algorithms that can do gradient descent in function space (whatever that means). We’ll get into what that means and why it’s so baller in future posts. For now, go forth and boost!</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/index.html</guid>
  <pubDate>Tue, 08 Dec 2020 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2020/gradient-boosting-machine-from-scratch/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The 80/20 Pandas Tutorial</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2020/8020-pandas-tutorial/index.html</link>
  <description><![CDATA[ 



<p><img src="https://blog.mattbowers.dev/posts/2020/8020-pandas-tutorial/80_20_pandas.png" title="8020 pandas" class="img-fluid"></p>
<p>Ahh, pandas. In addition to being everyone’s favorite mostly vegetarian bear from south central China, it’s also <em>the</em> python library for working with tabular data, a.k.a. dataframes. When you dive into pandas, you’ll quickly find out that there is a lot going on; indeed there are <a href="https://pandas.pydata.org/docs/reference/frame.html">hundreds</a> of methods for operating on dataframes. But luckily for us, as with many areas of life, there is a <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto Principle</a>, or 80/20 rule, that will help us focus on the small set of methods that collectively solve the majority of our data transformation needs.</p>
<p>If you’re like me, then pandas is not your first data-handling tool; maybe you’ve been using SQL or R with <code>data.table</code> or <code>dplyr</code>. If so, that’s great because you already have a sense for the key operations we need when working with tabular data. In their book, <a href="https://r4ds.had.co.nz/">R for Data Science</a>, Garrett Grolemund and Hadley Wickham describe five essential operations for manipulating dataframes. I’ve found that these cover the majority of my data transformation tasks to prepare data for analysis, visualization, and modeling.</p>
<ol type="1">
<li>filtering rows based on data values</li>
<li>sorting rows based on data values</li>
<li>selecting columns by name</li>
<li>adding new columns based on the existing columns</li>
<li>creating grouped summaries of the dataset</li>
</ol>
<p>I would add that we also need a way to build up more complex transformations by chaining these fundamental operations together sequentially.</p>
<p>Before we dive in, here’s the TLDR on the pandas methods that I prefer for accomplishing these tasks, along with their equivalents from SQL and <code>dplyr</code> in R.</p>
<table class="table">
<colgroup>
<col style="width: 49%">
<col style="width: 21%">
<col style="width: 3%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>description</th>
<th>pandas</th>
<th>SQL</th>
<th>dplyr</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>filter rows based on data values</td>
<td><code>query()</code></td>
<td><code>WHERE</code></td>
<td><code>filter()</code></td>
</tr>
<tr class="even">
<td>sort rows based on data values</td>
<td><code>sort_values()</code></td>
<td><code>ORDER BY</code></td>
<td><code>arrange()</code></td>
</tr>
<tr class="odd">
<td>select columns by name</td>
<td><code>filter()</code></td>
<td><code>SELECT</code></td>
<td><code>select()</code></td>
</tr>
<tr class="even">
<td>add new columns based on the existing columns</td>
<td><code>assign()</code></td>
<td><code>AS</code></td>
<td><code>mutate()</code></td>
</tr>
<tr class="odd">
<td>create grouped summaries of the dataset</td>
<td><code>groupby()</code> <br> <code>apply()</code></td>
<td><code>GROUP BY</code></td>
<td><code>group_by()</code> <br> <code>summarise()</code></td>
</tr>
<tr class="even">
<td>chain operations together</td>
<td><code>.</code></td>
<td></td>
<td><code>%&gt;%</code></td>
</tr>
</tbody>
</table>
<section id="imports-and-data" class="level2">
<h2 class="anchored" data-anchor-id="imports-and-data">Imports and Data</h2>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span></code></pre></div>
</div>
<p>We’ll use the <a href="https://github.com/hadley/nycflights13">nycflights13</a> dataset which contains data on the 336,776 flights that departed from New York City in 2013.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">url <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'https://www.openintro.org/book/statdata/nycflights.csv'</span></span>
<span id="cb2-2">storage_options <span class="op" style="color: #5E5E5E;">=</span> {<span class="st" style="color: #20794D;">'User-Agent'</span>: <span class="st" style="color: #20794D;">'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0'</span>}</span>
<span id="cb2-3">flights <span class="op" style="color: #5E5E5E;">=</span> pd.read_csv(url, storage_options<span class="op" style="color: #5E5E5E;">=</span>storage_options)</span></code></pre></div>
</div>
</section>
<section id="select-rows-based-on-their-values-with-query" class="level2">
<h2 class="anchored" data-anchor-id="select-rows-based-on-their-values-with-query">Select rows based on their values with <code>query()</code></h2>
<p><code>query()</code> lets you retain a subset of rows based on the values of the data; it’s like <code>dplyr::filter()</code> in R or <code>WHERE</code> in SQL. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>, <code>==</code> (equal), and <code>~=</code> (not equal). You can specify compound expressions using <code>and</code> and <code>or</code>, and you can even check if the column value matches any items in a list.</p>
<div class="cell" data-scrolled="true" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># compare one column to a value</span></span>
<span id="cb3-2">flights.query(<span class="st" style="color: #20794D;">'month == 6'</span>)</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;"># compare two column values</span></span>
<span id="cb3-5">flights.query(<span class="st" style="color: #20794D;">'arr_delay &gt; dep_delay'</span>)</span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;"># using arithmetic</span></span>
<span id="cb3-8">flights.query(<span class="st" style="color: #20794D;">'arr_delay &gt; 0.5 * air_time'</span>)</span>
<span id="cb3-9"></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;"># using "and"</span></span>
<span id="cb3-11">flights.query(<span class="st" style="color: #20794D;">'month == 6 and day == 1'</span>)</span>
<span id="cb3-12"></span>
<span id="cb3-13"><span class="co" style="color: #5E5E5E;"># using "or"</span></span>
<span id="cb3-14">flights.query(<span class="st" style="color: #20794D;">'origin == "JFK" or dest == "JFK"'</span>)</span>
<span id="cb3-15"></span>
<span id="cb3-16"><span class="co" style="color: #5E5E5E;"># column value matching any item in a list</span></span>
<span id="cb3-17">flights.query(<span class="st" style="color: #20794D;">'carrier in ["AA", "UA"]'</span>)</span></code></pre></div>
</div>
<p>You may have noticed that it seems to be much more popular to filter pandas data frames using boolean indexing. Indeed when I ask my favorite search engine how to filter a pandas dataframe on its values, I find <a href="https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/">this tutorial</a>, <a href="https://medium.com/swlh/3-ways-to-filter-pandas-dataframe-by-column-values-dfb6609b31de">this blog post</a>, <a href="https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values">various</a> <a href="https://stackoverflow.com/questions/11869910/pandas-filter-rows-of-dataframe-with-operator-chaining">questions</a> on Stack Overflow, and even <a href="https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/03_subset_data.html">the pandas documentation</a>, all espousing boolean indexing. Here’s what it looks like.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># canonical boolean indexing</span></span>
<span id="cb4-2">flights[(flights[<span class="st" style="color: #20794D;">'carrier'</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"AA"</span>) <span class="op" style="color: #5E5E5E;">&amp;</span> (flights[<span class="st" style="color: #20794D;">'origin'</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"JFK"</span>)]</span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="co" style="color: #5E5E5E;"># the equivalent use of query()</span></span>
<span id="cb4-5">flights.query(<span class="st" style="color: #20794D;">'carrier == "AA" and origin == "JFK"'</span>)</span></code></pre></div>
</div>
<p>There are a few reasons I prefer <code>query()</code> over boolean indexing.</p>
<ol type="1">
<li><code>query()</code> does not require me to type the dataframe name again, whereas boolean indexing requires me to type it every time I wish to refer to a column.</li>
<li><code>query()</code> makes the code easier to read and understand, especially when expressions get complex.</li>
<li><code>query()</code> is <a href="https://jakevdp.github.io/PythonDataScienceHandbook/03.12-performance-eval-and-query.html">more computationally efficient</a> than boolean indexing.</li>
<li><code>query()</code> can safely be used in dot chains, which we’ll see very soon.</li>
</ol>
</section>
<section id="select-columns-by-name-with-filter" class="level2">
<h2 class="anchored" data-anchor-id="select-columns-by-name-with-filter">Select columns by name with <code>filter()</code></h2>
<p><code>filter()</code> lets you pick out a specific set of columns by name; it’s analogous to <code>dplyr::select()</code> in R or <code>SELECT</code> in SQL. You can either provide exactly the column names you want, or you can grab all columns whose names contain a given substring or which match a given regular expression. This isn’t a big deal when your dataframe has only a few columns, but is particularly useful when you have a dataframe with tens or hundreds of columns.</p>
<div class="cell" data-scrolled="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># select a list of columns</span></span>
<span id="cb5-2">flights.<span class="bu" style="color: null;">filter</span>([<span class="st" style="color: #20794D;">'origin'</span>, <span class="st" style="color: #20794D;">'dest'</span>])</span>
<span id="cb5-3"></span>
<span id="cb5-4"><span class="co" style="color: #5E5E5E;"># select columns containing a particular substring</span></span>
<span id="cb5-5">flights.<span class="bu" style="color: null;">filter</span>(like<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'time'</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7"><span class="co" style="color: #5E5E5E;"># select columns matching a regular expression</span></span>
<span id="cb5-8">flights.<span class="bu" style="color: null;">filter</span>(regex<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'e$'</span>)</span></code></pre></div>
</div>
</section>
<section id="sort-rows-with-sort_values" class="level2">
<h2 class="anchored" data-anchor-id="sort-rows-with-sort_values">Sort rows with <code>sort_values()</code></h2>
<p><code>sort_values()</code> changes the order of the rows based on the data values; it’s like<code>dplyr::arrange()</code> in R or <code>ORDER BY</code> in SQL. You can specify one or more columns on which to sort, where their order denotes the sorting priority. You can also specify whether to sort in ascending or descending order.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;"># sort by a single column</span></span>
<span id="cb6-2">flights.sort_values(<span class="st" style="color: #20794D;">'air_time'</span>)</span>
<span id="cb6-3"></span>
<span id="cb6-4"><span class="co" style="color: #5E5E5E;"># sort by a single column in descending order</span></span>
<span id="cb6-5">flights.sort_values(<span class="st" style="color: #20794D;">'air_time'</span>, ascending<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb6-6"></span>
<span id="cb6-7"><span class="co" style="color: #5E5E5E;"># sort by carrier, then within carrier, sort by descending distance</span></span>
<span id="cb6-8">flights.sort_values([<span class="st" style="color: #20794D;">'carrier'</span>, <span class="st" style="color: #20794D;">'distance'</span>], ascending<span class="op" style="color: #5E5E5E;">=</span>[<span class="va" style="color: #111111;">True</span>, <span class="va" style="color: #111111;">False</span>])</span></code></pre></div>
</div>
</section>
<section id="add-new-columns-with-assign" class="level2">
<h2 class="anchored" data-anchor-id="add-new-columns-with-assign">Add new columns with <code>assign()</code></h2>
<p><code>assign()</code> adds new columns which can be functions of the existing columns; it’s like <code>dplyr::mutate()</code> from R.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># add a new column based on other columns</span></span>
<span id="cb7-2">flights.assign(speed <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.distance <span class="op" style="color: #5E5E5E;">/</span> x.air_time)</span>
<span id="cb7-3"></span>
<span id="cb7-4"><span class="co" style="color: #5E5E5E;"># another new column based on existing columns</span></span>
<span id="cb7-5">flights.assign(gain <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.dep_delay <span class="op" style="color: #5E5E5E;">-</span> x.arr_delay)</span></code></pre></div>
</div>
<p>If you’re like me, this way of using <code>assign()</code> might seem a little strange at first. Let’s break it down. In the call to <code>assign()</code> the keyword argument <code>speed</code> tells pandas the name of our new column. The business to the right of the <code>=</code> is a inline lambda function that takes the dataframe we passed to <code>assign()</code> and returns the column we want to add.</p>
<p>I like using <code>x</code> as the lambda argument because its easy to type and it evokes tabular data (think <a href="https://en.wikipedia.org/wiki/Design_matrix">design matrix</a>), which reminds me that it refers to the entire dataframe. We can then access the other columns in our dataframe using the dot like <code>x.other_column</code>.</p>
<p>It’s true that you can skip the whole lambda business and refer to the dataframe to which you are assigning directly inside the assign. That might look like this.</p>
<pre><code>flights.assign(speed = flights.distance / flights.air_time)</code></pre>
<p>I prefer using a lambda for the following reasons.</p>
<ol type="1">
<li>If you gave your dataframe a good name, using the lambda will save you from typing the name every time you want to refer to a column.</li>
<li>The lambda makes your code more portable. Since you refer to the dataframe as a generic <code>x</code>, you can reuse this same assignment code on a dataframe with a different name.</li>
<li>Most importantly, the lambda will allow you to harness the power of dot chaining.</li>
</ol>
</section>
<section id="chain-transformations-together-with-the-dot-chain" class="level2">
<h2 class="anchored" data-anchor-id="chain-transformations-together-with-the-dot-chain">Chain transformations together with the dot chain</h2>
<p>One of the awesome things about pandas is that the <code>object.method()</code> paradigm lets us easily build up complex dataframe transformations from a sequence of method calls. In R, this is effectively accomplished by the pipe <code>%&gt;%</code> operator. For example, suppose we want to look at high-speed flights from JFK to Honolulu, which would require us to query for JFK to Honolulu flights, assign a speed column, and maybe sort on that new speed column.</p>
<p>We can say:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;"># neatly chain method calls together</span></span>
<span id="cb9-2">(</span>
<span id="cb9-3">    flights</span>
<span id="cb9-4">    .query(<span class="st" style="color: #20794D;">'origin == "JFK"'</span>)</span>
<span id="cb9-5">    .query(<span class="st" style="color: #20794D;">'dest == "HNL"'</span>)</span>
<span id="cb9-6">    .assign(speed <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.distance <span class="op" style="color: #5E5E5E;">/</span> x.air_time)</span>
<span id="cb9-7">    .sort_values(by<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'speed'</span>, ascending<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb9-8">    .query(<span class="st" style="color: #20794D;">'speed &gt; 8.0'</span>)</span>
<span id="cb9-9">)</span></code></pre></div>
</div>
<p>We compose the dot chain by wrapping the entire expression in parentheses and indenting each line within. The first line is the name of the dataframe on which we are operating. Each subsequent line has a single method call.</p>
<p>There are a few great things about writing the code this way: 1. Readability. It’s easy to scan down the left margin of the code to see what’s happening. The first line gives us our noun (the dataframe) and each subsequent line starts with a verb. You could read this as “take <code>flights</code> then query the rows where origin is JFK, then query for rows where destination is HNL, then assign a new column called speed, then sort the dataframe by speed, then query only for the rows where speed is greater than 8.0. 1. Flexibility - It’s easy to comment out individual lines and re-run the cell. It’s also easy to reorder operations, since only one thing happens on each line. 1. Neatness - We have not polluted our workspace with any intermediate variables, nor have we wasted any mental energy thinking of names for any temporary variables.</p>
<p>By default, dot chains do not modify the original dataframe; they just output a temporary result that we can inspect directly in the output. If you want to store the result, or pass it along to another function (e.g.&nbsp;for plotting), you can simply assign the entire dot chain to a variable.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;"># sotre the output of the dot chain in a new dataframe</span></span>
<span id="cb10-2">flights_high_speed <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb10-3">    flights</span>
<span id="cb10-4">    .assign(speed <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> x: x.distance <span class="op" style="color: #5E5E5E;">/</span> x.air_time)</span>
<span id="cb10-5">    .query(<span class="st" style="color: #20794D;">'speed &gt; 8.0'</span>)</span>
<span id="cb10-6">)</span></code></pre></div>
</div>
</section>
<section id="collapsing-rows-into-grouped-summaries-with-groupby" class="level2">
<h2 class="anchored" data-anchor-id="collapsing-rows-into-grouped-summaries-with-groupby">Collapsing rows into grouped summaries with <code>groupby()</code></h2>
<p><code>groupby()</code> combined with <code>apply()</code> gives us flexibility and control over our grouped summaries; it’s like <code>dplyr::group_by()</code> and <code>dplyr::summarise()</code> in R. This is the primary pattern I use for SQL-style groupby operations in pandas. Specifically it unlocks the following essential functionality you’re used to having in SQL. 1. specify the names of the aggregation columns we create 1. specify which aggregation function to use on which columns 1. compose more complex aggregations such as the proportion of rows meeting some condition 1. aggregate over arbitrary functions of multiple columns</p>
<p>Let’s check out the departure delay stats for each carrier.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="co" style="color: #5E5E5E;"># grouped summary with groupby and apply</span></span>
<span id="cb11-2">(</span>
<span id="cb11-3">    flights</span>
<span id="cb11-4">    .groupby([<span class="st" style="color: #20794D;">'carrier'</span>])</span>
<span id="cb11-5">    .<span class="bu" style="color: null;">apply</span>(<span class="kw" style="color: #003B4F;">lambda</span> d: pd.Series({</span>
<span id="cb11-6">        <span class="st" style="color: #20794D;">'n_flights'</span>: <span class="bu" style="color: null;">len</span>(d),</span>
<span id="cb11-7">        <span class="st" style="color: #20794D;">'med_delay'</span>: d.dep_delay.median(),</span>
<span id="cb11-8">        <span class="st" style="color: #20794D;">'avg_delay'</span>: d.dep_delay.mean(),</span>
<span id="cb11-9">    }))</span>
<span id="cb11-10">    .head()</span>
<span id="cb11-11">)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>n_flights</th>
      <th>med_delay</th>
      <th>avg_delay</th>
    </tr>
    <tr>
      <th>carrier</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9E</th>
      <td>1696.0</td>
      <td>-1.0</td>
      <td>17.285967</td>
    </tr>
    <tr>
      <th>AA</th>
      <td>3188.0</td>
      <td>-2.0</td>
      <td>9.142409</td>
    </tr>
    <tr>
      <th>AS</th>
      <td>66.0</td>
      <td>-4.5</td>
      <td>5.181818</td>
    </tr>
    <tr>
      <th>B6</th>
      <td>5376.0</td>
      <td>-1.0</td>
      <td>13.137091</td>
    </tr>
    <tr>
      <th>DL</th>
      <td>4751.0</td>
      <td>-2.0</td>
      <td>8.529573</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>While you might be used to <code>apply()</code> acting over the rows or columns of a dataframe, here we’re calling apply on a grouped dataframe object, so it’s acting over the <em>groups</em>. According to the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html">pandas documentation</a>:</p>
<blockquote class="blockquote">
<p>The function passed to apply must take a dataframe as its first argument and return a dataframe, a series or a scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method.</p>
</blockquote>
<p>We need to supply <code>apply()</code> with a function that takes each chunk of the grouped dataframe and returns (in our case) a series object with one element for each new aggregation column. Notice that I use a lambda to specify the function we pass to <code>apply())</code>, and that I name its argument <code>d</code>, which reminds me that it’s a dataframe. My lambda returns a pandas series whose index entries specify the new aggregation column names, and whose values constitute the results of the aggregations for each group. Pandas will then stitch everything back together into a lovely dataframe.</p>
<p>Notice how nice the code looks when we use this pattern. Each aggregation is specified on its own line, which makes it easy to see what aggregation columns we’re creating and allows us to comment, uncomment, and reorder the aggregations without breaking anything.</p>
<p>Here are some more complex aggregations to illustrate some useful patterns.</p>
<div class="cell" data-scrolled="true" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;"># more complex grouped summary</span></span>
<span id="cb12-2">(</span>
<span id="cb12-3">    flights</span>
<span id="cb12-4">    .groupby([<span class="st" style="color: #20794D;">'carrier'</span>])</span>
<span id="cb12-5">    .<span class="bu" style="color: null;">apply</span>(<span class="kw" style="color: #003B4F;">lambda</span> d: pd.Series({</span>
<span id="cb12-6">        <span class="st" style="color: #20794D;">'avg_gain'</span>: np.mean(d.dep_delay <span class="op" style="color: #5E5E5E;">-</span> d.arr_delay), </span>
<span id="cb12-7">        <span class="st" style="color: #20794D;">'pct_delay_gt_30'</span>: np.mean(d.dep_delay <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">30</span>), </span>
<span id="cb12-8">        <span class="st" style="color: #20794D;">'pct_late_dep_early_arr'</span>: np.mean((d.dep_delay <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>) <span class="op" style="color: #5E5E5E;">&amp;</span> (d.arr_delay <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="dv" style="color: #AD0000;">0</span>)), </span>
<span id="cb12-9">        <span class="st" style="color: #20794D;">'avg_arr_given_dep_delay_gt_0'</span>: d.query(<span class="st" style="color: #20794D;">'dep_delay &gt; 0'</span>).arr_delay.mean(),</span>
<span id="cb12-10">        <span class="st" style="color: #20794D;">'cor_arr_delay_dep_delay'</span>: np.corrcoef(d.dep_delay, d.arr_delay)[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb12-11">    }))</span>
<span id="cb12-12">    .head()</span>
<span id="cb12-13">)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>avg_gain</th>
      <th>pct_delay_gt_30</th>
      <th>pct_late_dep_early_arr</th>
      <th>avg_arr_given_dep_delay_gt_0</th>
      <th>cor_arr_delay_dep_delay</th>
    </tr>
    <tr>
      <th>carrier</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9E</th>
      <td>9.247642</td>
      <td>0.196934</td>
      <td>0.110259</td>
      <td>39.086111</td>
      <td>0.932485</td>
    </tr>
    <tr>
      <th>AA</th>
      <td>7.743726</td>
      <td>0.113237</td>
      <td>0.105395</td>
      <td>30.087165</td>
      <td>0.891013</td>
    </tr>
    <tr>
      <th>AS</th>
      <td>16.515152</td>
      <td>0.106061</td>
      <td>0.121212</td>
      <td>28.058824</td>
      <td>0.864565</td>
    </tr>
    <tr>
      <th>B6</th>
      <td>3.411458</td>
      <td>0.160528</td>
      <td>0.084449</td>
      <td>37.306866</td>
      <td>0.914180</td>
    </tr>
    <tr>
      <th>DL</th>
      <td>7.622816</td>
      <td>0.097874</td>
      <td>0.100821</td>
      <td>30.078029</td>
      <td>0.899327</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Here’s what’s happening.</p>
<ul>
<li><code>np.mean(d.dep_delay - d.arr_delay)</code> aggregates over the difference of two columns.</li>
<li><code>np.mean(d.dep_delay &gt; 30)</code> computes the proportion of rows where the delay is greater than 30 minutes. Generating a boolean series based on some condition and then using <code>mean()</code> to find the proportion comes up all the time.</li>
<li><code>np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0))</code> shows that we can compute proportions where conditions on multiple columns are met.</li>
<li><code>d.query('dep_delay &gt; 0').arr_delay.mean()</code> computes the average arrival delay on flights where the departure was delayed. Here we first filter each grouped dataframe down to the subset of rows where departure delay is greater than zero using <code>query()</code>, and then we take the mean of the remaining arrival delays.</li>
<li><code>np.corrcoef(d.dep_delay, d.arr_delay)[0,1]</code> computes the correlation coefficient between departure and arrival delays. Remember we can use pretty much any reduction operation to collapse values down to a scalar.</li>
</ul>
<p>You might have noticed that the canonical pandas approach for grouped summaries is to use <code>agg()</code>. That works well if you need to apply the same aggregation function on each column in the dataframe, e.g.&nbsp;taking the mean of every column. But because of the kind of data I work with these days, it’s much more common for me to use customized aggregations like those above, so the <code>groupby()</code> <code>apply()</code> idiom works best for me.</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There you have it, how to pull off the five most essential data transformation tasks using pandas in a style reminiscent of my beloved <code>dplyr</code>. Remember that part of the beauty of pandas is that since there are so many ways to do most tasks, you can develop your own style based on the kind of data you work with, what you like about other tools, how you see others using the tools, and of course your own taste and preferences.</p>
<p>If you found this post helpful or if you have your own preferred style for accomplishing any of these key transformations with pandas, do let me know about it in the comments.</p>


</section>

 ]]></description>
  <category>pandas</category>
  <category>tutorial</category>
  <guid>https://blog.mattbowers.dev/posts/2020/8020-pandas-tutorial/index.html</guid>
  <pubDate>Wed, 25 Nov 2020 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2020/8020-pandas-tutorial/80_20_pandas.png" medium="image" type="image/png" height="73" width="144"/>
</item>
<item>
  <title>Hello World! And Why I’m Inspired to Start a Blog</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2020/hello-world/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2020/hello-world/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Matt raises his arms in joy at the world.!</figcaption><p></p>
</figure>
</div>
<p>Well, I’ve been thinking about getting this blog started for months now. I guess a combination of inertia, up-front investment in blogging platform selection/setup, and spending a little too much time writing and rewriting the first content post has drawn out the period from initial inspiration to making the blog a reality. Needless to say, I’m pretty excited to finally get things going.</p>
<p>Before we dive headlong into the weeds of ML algorithms, statistical methods, and whatever I happen to be learning and teaching at the moment, I figured it would be good to articulate why I’ve felt inspired to get started blogging in the first place. Hopefully this will serve the dual purpose of clarifying my intentions and introducing a vastly underappreciated concept in data science that I hope to weave through the posts to come.</p>
<section id="learning" class="level2">
<h2 class="anchored" data-anchor-id="learning">Learning</h2>
<p>The initial inception about blogging probably originated from some comments about learning that Jeremy Howard makes in the <a href="https://course.fast.ai/">Practical Deep Learning</a> course from <a href="https://www.fast.ai/">fastai</a>. During one of the lectures, he mentions that it’s a great idea to start blogging. To paraphrase Jeremy:</p>
<blockquote class="blockquote">
<p>The thing I really love about blogging is that it helps you learn; by writing things down, you synthesize your ideas.</p>
</blockquote>
<p>Beautiful. That definitely rings true for me. I tend to take notes and play around with code when learning new concepts anyway. One of my key hypotheses about this blogging experiment is that making the effort to transform those notes into blog posts will help me learn more effectively.</p>
</section>
<section id="teaching" class="level2">
<h2 class="anchored" data-anchor-id="teaching">Teaching</h2>
<p>Ah, teaching. Yes, sometimes it’s that thing that takes time away from your research, forcing you to sit alone in a windowless room squinting at hand-written math on a fat stack of homework assignments. But sometimes it actually involves interacting with students, endeavoring to explain a concept, and watching them light up when they get it. The latter manifestation of teaching was one of my favorite things about grad school and academia in general. While I certainly still get to do some teaching as an industry data scientist, I could see myself returning to a more teaching-centric gig somewhere off in the future. Thus we have our second key hypothesis about the blogging experiment, that the writing will entertain my inclination to teach.</p>
</section>
<section id="contributing" class="level2">
<h2 class="anchored" data-anchor-id="contributing">Contributing</h2>
<p>Working in the field of data science today is a bit like standing in front of a massive complimentary all-you-can-learn buffet. There is an abundance of free material out on the interwebs for learning pretty much anything in data science from hello world python tutorials to research papers on cutting-edge deep learning techniques. I’ve personally benefited from many a blog post that helped me unpack a new concept or get started using a new tool. And let’s not forget the gigantic cyber warehouse full of freely available open source software tools that volunteer developers have straight-up donated to humanity.</p>
<p>I realize that up to now, I’ve simply been consuming all of this free goodness without giving anything substantive back in return. Well then, it’s time to start evening the score. Which brings us to key hypothesis number three, that through these blog posts, I might be able to create something helpful, thereby being of service to a community that has freely given so much to me.</p>
</section>
<section id="live-long-and-prosper-blog" class="level2">
<h2 class="anchored" data-anchor-id="live-long-and-prosper-blog">Live Long and Prosper, Blog</h2>
<p>Phew, there it is, the original source of inspiration for this blogging experiment, and three reasons I think it might be a good idea. The astute reader will have noticed that these three assertions have been formulated as hypotheses which are to be tested in the laboratory of experience. And thus, we also have our first glimpse of the <a href="https://en.wikipedia.org/wiki/Scientific_method">scientific method</a>, an underrated concept that is going to help us put the <em>science</em> back in data science.</p>
<p>With that, blog, I christen thee, <em>Random Realizations</em>.</p>


</section>

 ]]></description>
  <category>blogging</category>
  <guid>https://blog.mattbowers.dev/posts/2020/hello-world/index.html</guid>
  <pubDate>Sun, 22 Nov 2020 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2020/hello-world/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
