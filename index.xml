<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Realizations</title>
<link>https://blog.mattbowers.dev/index.html</link>
<atom:link href="https://blog.mattbowers.dev/index.xml" rel="self" type="application/rss+xml"/>
<description>A blog about data science, statistics, machine learning, and the scientific method</description>
<generator>quarto-1.2.335</generator>
<lastBuildDate>Fri, 06 May 2022 23:00:00 GMT</lastBuildDate>
<item>
  <title>XGBoost from scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2022-05-07-xgboost-from-scratch/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2022-05-07-xgboost-from-scratch/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A weathered tree reaches toward the sea at Playa Mal País</figcaption><p></p>
</figure>
</div>
<p>Well, dear reader, it’s that time again, time for us to do a seemingly unnecessary scratch build of a popular algorithm that most people would simply import from the library without a second thought. But readers of this blog are not most people. Of course you know that when we do scratch builds, it’s not for the hell of it, it’s for the purpose of demystification. To that end, today we are going to implement XGBoost from scratch in python, using only numpy and pandas.</p>
<p>Specifically we’re going to implement the core statistical learning algorithm of XGBoost, including most of the key hyperparameters and their functionality. Our implementation will also support user-defined custom objective functions, meaning that it can perform regression, classification, and whatever exotic learning tasks you can dream up, as long as you can write down a twice-differentiable objective function. We’ll refrain from implementing some simple features like column subsampling which will be left to you, gentle reader, as exercises. In terms of tree methods, we’re going to implement the exact tree-splitting algorithm, leaving the sparsity-aware method (used to handle missing feature values) and the approximate method (used for scalability) as exercises or maybe topics for future posts.</p>
<p>As always, if something is unclear, try backtracking through the previous posts on gradient boosting and decision trees to clarify your intuition. We’ve already built up all the statistical and computational background needed to make sense of this scratch build. Here are the most important prerequisite posts:</p>
<ol type="1">
<li><a href="../../gradient-boosting-machine-from-scratch">Gradient Boosting Machine from Scratch</a></li>
<li><a href="../../decision-tree-from-scratch">Decision Tree From Scratch</a></li>
<li><a href="../../how-to-understand-xgboost">How to Understand XGBoost</a></li>
</ol>
<p>Great, let’s do this.</p>
<section id="the-xgboost-model-class" class="level2">
<h2 class="anchored" data-anchor-id="the-xgboost-model-class">The XGBoost Model Class</h2>
<p>We begin with the user-facing API for our model, a class called <code>XGBoostModel</code> which will implement gradient boosting and prediction. To be more consistent with the XGBoost library, we’ll pass hyperparameters to our model in a parameter dictionary, so our init method is going to pull relevant parameters out of the dictionary and set them as object attributes. Note the use of python’s <code>defaultdict</code> so we don’t have to worry about handling key errors if we try to access a parameter that the user didn’t set in the dictionary.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> math</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np </span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> collections <span class="im" style="color: #00769E;">import</span> defaultdict</span></code></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">class</span> XGBoostModel():</span>
<span id="cb2-2">    <span class="co" style="color: #5E5E5E;">'''XGBoost from Scratch</span></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb2-4">    </span>
<span id="cb2-5">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, params, random_seed<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb2-6">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> defaultdict(<span class="kw" style="color: #003B4F;">lambda</span>: <span class="va" style="color: #111111;">None</span>, params)</span>
<span id="cb2-7">        <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-8">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb2-9">        <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-10">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.3</span></span>
<span id="cb2-11">        <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-12">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb2-13">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb2-14">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb2-15">        <span class="va" style="color: #111111;">self</span>.rng <span class="op" style="color: #5E5E5E;">=</span> np.random.default_rng(seed<span class="op" style="color: #5E5E5E;">=</span>random_seed)</span></code></pre></div>
</div>
<p>The fit method, based on our classic GBM, takes a feature dataframe, a target vector, the objective function, and the number of boosting rounds as arguments. The user-supplied objective function should be an object with loss, gradient, and hessian methods, each of which takes a target vector and a prediction vector as input; the loss method should return a scalar loss score, the gradient method should return a vector of gradients, and the hessian method should return a vector of hessians.</p>
<p>In contrast to boosting in the classic GBM, instead of computing residuals between the current predictions and the target, we compute gradients and hessians of the loss function with respect to the current predictions, and instead of predicting residuals with a decision tree, we fit a special XGBoost tree booster (which we’ll implement in a moment) using the gradients and hessians. I’ve also added row subsampling by drawing a random subset of instance indices and passing them to the tree booster during each boosting round. The rest of the fit method is the same as the classic GBM, and the predict method is identical too.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, X, y, objective, num_boost_round, verbose<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb3-2">    current_predictions <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">*</span> np.ones(shape<span class="op" style="color: #5E5E5E;">=</span>y.shape)</span>
<span id="cb3-3">    <span class="va" style="color: #111111;">self</span>.boosters <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_boost_round):</span>
<span id="cb3-5">        gradients <span class="op" style="color: #5E5E5E;">=</span> objective.gradient(y, current_predictions)</span>
<span id="cb3-6">        hessians <span class="op" style="color: #5E5E5E;">=</span> objective.hessian(y, current_predictions)</span>
<span id="cb3-7">        sample_idxs <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span> <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">1.0</span> <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb3-8">            <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.rng.choice(<span class="bu" style="color: null;">len</span>(y), </span>
<span id="cb3-9">                                 size<span class="op" style="color: #5E5E5E;">=</span>math.floor(<span class="va" style="color: #111111;">self</span>.subsample<span class="op" style="color: #5E5E5E;">*</span><span class="bu" style="color: null;">len</span>(y)), </span>
<span id="cb3-10">                                 replace<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb3-11">        booster <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(X, gradients, hessians, </span>
<span id="cb3-12">                              <span class="va" style="color: #111111;">self</span>.params, <span class="va" style="color: #111111;">self</span>.max_depth, sample_idxs)</span>
<span id="cb3-13">        current_predictions <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> booster.predict(X)</span>
<span id="cb3-14">        <span class="va" style="color: #111111;">self</span>.boosters.append(booster)</span>
<span id="cb3-15">        <span class="cf" style="color: #003B4F;">if</span> verbose: </span>
<span id="cb3-16">            <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'[</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">] train loss = </span><span class="sc" style="color: #5E5E5E;">{</span>objective<span class="sc" style="color: #5E5E5E;">.</span>loss(y, current_predictions)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb3-17">            </span>
<span id="cb3-18"><span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb3-19">    <span class="cf" style="color: #003B4F;">return</span> (<span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.learning_rate </span>
<span id="cb3-20">            <span class="op" style="color: #5E5E5E;">*</span> np.<span class="bu" style="color: null;">sum</span>([booster.predict(X) <span class="cf" style="color: #003B4F;">for</span> booster <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.boosters], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>))</span>
<span id="cb3-21"></span>
<span id="cb3-22">XGBoostModel.fit <span class="op" style="color: #5E5E5E;">=</span> fit</span>
<span id="cb3-23">XGBoostModel.predict <span class="op" style="color: #5E5E5E;">=</span> predict            </span></code></pre></div>
</div>
<p>All we have to do now is implement the tree booster.</p>
</section>
<section id="the-xgboost-tree-booster" class="level2">
<h2 class="anchored" data-anchor-id="the-xgboost-tree-booster">The XGBoost Tree Booster</h2>
<p>The XGBoost tree booster is a modified version of the decision tree that we built in the decision tree from scratch post. Like the decision tree, we recursively build a binary tree structure by finding the best split rule for each node in the tree. The main difference is the criterion for evaluating splits and the way that we define a leaf’s predicted value. Instead of being functions of the target values of the instances in each node, the criterion and predicted values are functions of the instance gradients and hessians. Thus we need only make a couple of modifications to our previous decision tree implementation to create the XGBoost tree booster.</p>
<section id="initialization-and-inserting-child-nodes" class="level3">
<h3 class="anchored" data-anchor-id="initialization-and-inserting-child-nodes">Initialization and Inserting Child Nodes</h3>
<p>Most of the init method is just parsing the parameter dictionary to assign parameters as object attributes. The one notable difference from our decision tree is in the way we define the node’s predicted value. We define <code>self.value</code> according to equation 5 of the XGBoost paper, a simple function of the gradient and hessian values of the instances in the current node. Of course the init also goes on to build the tree via the maybe insert child nodes method. This method is nearly identical to the one we implemented for our decision tree. So far so good.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;">class</span> TreeBooster():</span>
<span id="cb4-2"> </span>
<span id="cb4-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, g, h, params, max_depth, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb4-4">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> params</span>
<span id="cb4-5">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> max_depth</span>
<span id="cb4-6">        <span class="cf" style="color: #003B4F;">assert</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb4-7">        <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb4-8">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb4-9">        <span class="va" style="color: #111111;">self</span>.reg_lambda <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb4-10">        <span class="va" style="color: #111111;">self</span>.gamma <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.0</span></span>
<span id="cb4-11">        <span class="va" style="color: #111111;">self</span>.colsample_bynode <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb4-12">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb4-13">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(g, pd.Series): g <span class="op" style="color: #5E5E5E;">=</span> g.values</span>
<span id="cb4-14">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(h, pd.Series): h <span class="op" style="color: #5E5E5E;">=</span> h.values</span>
<span id="cb4-15">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(g))</span>
<span id="cb4-16">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, g, h, idxs</span>
<span id="cb4-17">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb4-18">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>g[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">/</span> (h[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda) <span class="co" style="color: #5E5E5E;"># Eq (5)</span></span>
<span id="cb4-19">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb4-20">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb4-21">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb4-22"></span>
<span id="cb4-23">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb4-24">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): <span class="va" style="color: #111111;">self</span>._find_better_split(i)</span>
<span id="cb4-25">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="cf" style="color: #003B4F;">return</span></span>
<span id="cb4-26">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb4-27">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-28">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb4-29">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb4-30">                                <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb4-31">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb4-32">                                 <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb4-33"></span>
<span id="cb4-34">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb4-35">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb4-36"></span>
<span id="cb4-37">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb4-38">        <span class="cf" style="color: #003B4F;">pass</span></span></code></pre></div>
</div>
</section>
<section id="split-finding" class="level3">
<h3 class="anchored" data-anchor-id="split-finding">Split Finding</h3>
<p>Split finding follows the exact same pattern that we used in the decision tree, except we keep track of gradient and hessian stats instead of target value stats, and of course we use the XGBoost gain criterion (equation 7 from the paper) for evaluating splits.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb5-2">    x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs, feature_idx]</span>
<span id="cb5-3">    g, h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.g[<span class="va" style="color: #111111;">self</span>.idxs], <span class="va" style="color: #111111;">self</span>.h[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb5-4">    sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb5-5">    sort_g, sort_h, sort_x <span class="op" style="color: #5E5E5E;">=</span> g[sort_idx], h[sort_idx], x[sort_idx]</span>
<span id="cb5-6">    sum_g, sum_h <span class="op" style="color: #5E5E5E;">=</span> g.<span class="bu" style="color: null;">sum</span>(), h.<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb5-7">    sum_g_right, sum_h_right <span class="op" style="color: #5E5E5E;">=</span> sum_g, sum_h</span>
<span id="cb5-8">    sum_g_left, sum_h_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb5-9"></span>
<span id="cb5-10">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb5-11">        g_i, h_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_g[i], sort_h[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb5-12">        sum_g_left <span class="op" style="color: #5E5E5E;">+=</span> g_i<span class="op" style="color: #5E5E5E;">;</span> sum_g_right <span class="op" style="color: #5E5E5E;">-=</span> g_i</span>
<span id="cb5-13">        sum_h_left <span class="op" style="color: #5E5E5E;">+=</span> h_i<span class="op" style="color: #5E5E5E;">;</span> sum_h_right <span class="op" style="color: #5E5E5E;">-=</span> h_i</span>
<span id="cb5-14">        <span class="cf" style="color: #003B4F;">if</span> sum_h_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:<span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb5-15">        <span class="cf" style="color: #003B4F;">if</span> sum_h_right <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight: <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb5-16"></span>
<span id="cb5-17">        gain <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span> <span class="op" style="color: #5E5E5E;">*</span> ((sum_g_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_left <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb5-18">                        <span class="op" style="color: #5E5E5E;">+</span> (sum_g_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_right <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb5-19">                        <span class="op" style="color: #5E5E5E;">-</span> (sum_g<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb5-20">                        ) <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.gamma<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="co" style="color: #5E5E5E;"># Eq(7) in the xgboost paper</span></span>
<span id="cb5-21">        <span class="cf" style="color: #003B4F;">if</span> gain <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far: </span>
<span id="cb5-22">            <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb5-23">            <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> gain</span>
<span id="cb5-24">            <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb5-25">            </span>
<span id="cb5-26">TreeBooster._find_better_split <span class="op" style="color: #5E5E5E;">=</span> _find_better_split</span></code></pre></div>
</div>
</section>
<section id="prediction" class="level3">
<h3 class="anchored" data-anchor-id="prediction">Prediction</h3>
<p>Prediction works exactly the same as in our decision tree, and the methods are nearly identical.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb6-2">    <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb6-3"></span>
<span id="cb6-4"><span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb6-5">    <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb6-6">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb6-7">    child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb6-8">        <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb6-9">    <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span>
<span id="cb6-10"></span>
<span id="cb6-11">TreeBooster.predict <span class="op" style="color: #5E5E5E;">=</span> predict </span>
<span id="cb6-12">TreeBooster._predict_row <span class="op" style="color: #5E5E5E;">=</span> _predict_row </span></code></pre></div>
</div>
</section>
</section>
<section id="the-complete-xgboost-from-scratch-implementation" class="level2">
<h2 class="anchored" data-anchor-id="the-complete-xgboost-from-scratch-implementation">The Complete XGBoost From Scratch Implementation</h2>
<p>Here’s the entire implementation which produces a usable <code>XGBoostModel</code> class with fit and predict methods.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;">class</span> XGBoostModel():</span>
<span id="cb7-2">    <span class="co" style="color: #5E5E5E;">'''XGBoost from Scratch</span></span>
<span id="cb7-3"><span class="co" style="color: #5E5E5E;">    '''</span></span>
<span id="cb7-4">    </span>
<span id="cb7-5">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, params, random_seed<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-6">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> defaultdict(<span class="kw" style="color: #003B4F;">lambda</span>: <span class="va" style="color: #111111;">None</span>, params)</span>
<span id="cb7-7">        <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-8">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'subsample'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-9">        <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-10">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'learning_rate'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.3</span></span>
<span id="cb7-11">        <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-12">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'base_score'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb7-13">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-14">            <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.params[<span class="st" style="color: #20794D;">'max_depth'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb7-15">        <span class="va" style="color: #111111;">self</span>.rng <span class="op" style="color: #5E5E5E;">=</span> np.random.default_rng(seed<span class="op" style="color: #5E5E5E;">=</span>random_seed)</span>
<span id="cb7-16">                </span>
<span id="cb7-17">    <span class="kw" style="color: #003B4F;">def</span> fit(<span class="va" style="color: #111111;">self</span>, X, y, objective, num_boost_round, verbose<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb7-18">        current_predictions <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">*</span> np.ones(shape<span class="op" style="color: #5E5E5E;">=</span>y.shape)</span>
<span id="cb7-19">        <span class="va" style="color: #111111;">self</span>.boosters <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb7-20">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(num_boost_round):</span>
<span id="cb7-21">            gradients <span class="op" style="color: #5E5E5E;">=</span> objective.gradient(y, current_predictions)</span>
<span id="cb7-22">            hessians <span class="op" style="color: #5E5E5E;">=</span> objective.hessian(y, current_predictions)</span>
<span id="cb7-23">            sample_idxs <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span> <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.subsample <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">1.0</span> <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-24">                <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.rng.choice(<span class="bu" style="color: null;">len</span>(y), </span>
<span id="cb7-25">                                     size<span class="op" style="color: #5E5E5E;">=</span>math.floor(<span class="va" style="color: #111111;">self</span>.subsample<span class="op" style="color: #5E5E5E;">*</span><span class="bu" style="color: null;">len</span>(y)), </span>
<span id="cb7-26">                                     replace<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb7-27">            booster <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(X, gradients, hessians, </span>
<span id="cb7-28">                                  <span class="va" style="color: #111111;">self</span>.params, <span class="va" style="color: #111111;">self</span>.max_depth, sample_idxs)</span>
<span id="cb7-29">            current_predictions <span class="op" style="color: #5E5E5E;">+=</span> <span class="va" style="color: #111111;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;">*</span> booster.predict(X)</span>
<span id="cb7-30">            <span class="va" style="color: #111111;">self</span>.boosters.append(booster)</span>
<span id="cb7-31">            <span class="cf" style="color: #003B4F;">if</span> verbose: </span>
<span id="cb7-32">                <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'[</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">] train loss = </span><span class="sc" style="color: #5E5E5E;">{</span>objective<span class="sc" style="color: #5E5E5E;">.</span>loss(y, current_predictions)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb7-33">            </span>
<span id="cb7-34">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb7-35">        <span class="cf" style="color: #003B4F;">return</span> (<span class="va" style="color: #111111;">self</span>.base_prediction <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.learning_rate </span>
<span id="cb7-36">                <span class="op" style="color: #5E5E5E;">*</span> np.<span class="bu" style="color: null;">sum</span>([booster.predict(X) <span class="cf" style="color: #003B4F;">for</span> booster <span class="kw" style="color: #003B4F;">in</span> <span class="va" style="color: #111111;">self</span>.boosters], axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>))</span>
<span id="cb7-37">    </span>
<span id="cb7-38"><span class="kw" style="color: #003B4F;">class</span> TreeBooster():</span>
<span id="cb7-39"> </span>
<span id="cb7-40">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, g, h, params, max_depth, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb7-41">        <span class="va" style="color: #111111;">self</span>.params <span class="op" style="color: #5E5E5E;">=</span> params</span>
<span id="cb7-42">        <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> max_depth</span>
<span id="cb7-43">        <span class="cf" style="color: #003B4F;">assert</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb7-44">        <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-45">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'min_child_weight'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-46">        <span class="va" style="color: #111111;">self</span>.reg_lambda <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'reg_lambda'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-47">        <span class="va" style="color: #111111;">self</span>.gamma <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'gamma'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">0.0</span></span>
<span id="cb7-48">        <span class="va" style="color: #111111;">self</span>.colsample_bynode <span class="op" style="color: #5E5E5E;">=</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-49">            <span class="cf" style="color: #003B4F;">if</span> params[<span class="st" style="color: #20794D;">'colsample_bynode'</span>] <span class="cf" style="color: #003B4F;">else</span> <span class="fl" style="color: #AD0000;">1.0</span></span>
<span id="cb7-50">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(g, pd.Series): g <span class="op" style="color: #5E5E5E;">=</span> g.values</span>
<span id="cb7-51">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(h, pd.Series): h <span class="op" style="color: #5E5E5E;">=</span> h.values</span>
<span id="cb7-52">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(g))</span>
<span id="cb7-53">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, g, h, idxs</span>
<span id="cb7-54">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb7-55">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>g[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">/</span> (h[idxs].<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda) <span class="co" style="color: #5E5E5E;"># Eq (5)</span></span>
<span id="cb7-56">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb7-57">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb7-58">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb7-59"></span>
<span id="cb7-60">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb7-61">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): <span class="va" style="color: #111111;">self</span>._find_better_split(i)</span>
<span id="cb7-62">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="cf" style="color: #003B4F;">return</span></span>
<span id="cb7-63">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb7-64">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb7-65">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb7-66">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb7-67">                                <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb7-68">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> TreeBooster(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.g, <span class="va" style="color: #111111;">self</span>.h, <span class="va" style="color: #111111;">self</span>.params, </span>
<span id="cb7-69">                                 <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb7-70"></span>
<span id="cb7-71">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb7-72">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb7-73">    </span>
<span id="cb7-74">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb7-75">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs, feature_idx]</span>
<span id="cb7-76">        g, h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.g[<span class="va" style="color: #111111;">self</span>.idxs], <span class="va" style="color: #111111;">self</span>.h[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb7-77">        sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb7-78">        sort_g, sort_h, sort_x <span class="op" style="color: #5E5E5E;">=</span> g[sort_idx], h[sort_idx], x[sort_idx]</span>
<span id="cb7-79">        sum_g, sum_h <span class="op" style="color: #5E5E5E;">=</span> g.<span class="bu" style="color: null;">sum</span>(), h.<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb7-80">        sum_g_right, sum_h_right <span class="op" style="color: #5E5E5E;">=</span> sum_g, sum_h</span>
<span id="cb7-81">        sum_g_left, sum_h_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">0.</span></span>
<span id="cb7-82"></span>
<span id="cb7-83">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb7-84">            g_i, h_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_g[i], sort_h[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb7-85">            sum_g_left <span class="op" style="color: #5E5E5E;">+=</span> g_i<span class="op" style="color: #5E5E5E;">;</span> sum_g_right <span class="op" style="color: #5E5E5E;">-=</span> g_i</span>
<span id="cb7-86">            sum_h_left <span class="op" style="color: #5E5E5E;">+=</span> h_i<span class="op" style="color: #5E5E5E;">;</span> sum_h_right <span class="op" style="color: #5E5E5E;">-=</span> h_i</span>
<span id="cb7-87">            <span class="cf" style="color: #003B4F;">if</span> sum_h_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:<span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb7-88">            <span class="cf" style="color: #003B4F;">if</span> sum_h_right <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_child_weight: <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb7-89"></span>
<span id="cb7-90">            gain <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span> <span class="op" style="color: #5E5E5E;">*</span> ((sum_g_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_left <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb7-91">                            <span class="op" style="color: #5E5E5E;">+</span> (sum_g_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h_right <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb7-92">                            <span class="op" style="color: #5E5E5E;">-</span> (sum_g<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> (sum_h <span class="op" style="color: #5E5E5E;">+</span> <span class="va" style="color: #111111;">self</span>.reg_lambda))</span>
<span id="cb7-93">                            ) <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.gamma<span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span> <span class="co" style="color: #5E5E5E;"># Eq(7) in the xgboost paper</span></span>
<span id="cb7-94">            <span class="cf" style="color: #003B4F;">if</span> gain <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far: </span>
<span id="cb7-95">                <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb7-96">                <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> gain</span>
<span id="cb7-97">                <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb7-98">                </span>
<span id="cb7-99">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb7-100">        <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb7-101"></span>
<span id="cb7-102">    <span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb7-103">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb7-104">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb7-105">        child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb7-106">            <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb7-107">        <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span></code></pre></div>
</div>
</section>
<section id="testing" class="level2">
<h2 class="anchored" data-anchor-id="testing">Testing</h2>
<p>Let’s take this baby for a spin and benchmark its performance against the actual XGBoost library. We use the scikit learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html">California housing dataset</a> for benchmarking.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> fetch_california_housing</span>
<span id="cb8-2"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb8-3">    </span>
<span id="cb8-4">X, y <span class="op" style="color: #5E5E5E;">=</span> fetch_california_housing(as_frame<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_X_y<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb8-5">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.3</span>, </span>
<span id="cb8-6">                                                    random_state<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">43</span>)</span></code></pre></div>
</div>
<p>Let’s start with a nice friendly squared error objective function for training. We should probably have a future post all about how to define custom objective functions in XGBoost, but for now, here’s how I define squared error.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;">class</span> SquaredErrorObjective():</span>
<span id="cb9-2">    <span class="kw" style="color: #003B4F;">def</span> loss(<span class="va" style="color: #111111;">self</span>, y, pred): <span class="cf" style="color: #003B4F;">return</span> np.mean((y <span class="op" style="color: #5E5E5E;">-</span> pred)<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb9-3">    <span class="kw" style="color: #003B4F;">def</span> gradient(<span class="va" style="color: #111111;">self</span>, y, pred): <span class="cf" style="color: #003B4F;">return</span> pred <span class="op" style="color: #5E5E5E;">-</span> y</span>
<span id="cb9-4">    <span class="kw" style="color: #003B4F;">def</span> hessian(<span class="va" style="color: #111111;">self</span>, y, pred): <span class="cf" style="color: #003B4F;">return</span> np.ones(<span class="bu" style="color: null;">len</span>(y))</span></code></pre></div>
</div>
<p>Here I use a more or less arbitrary set of hyperparameters for training. Feel free to play around with tuning and trying other parameter combinations yourself.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;">import</span> xgboost <span class="im" style="color: #00769E;">as</span> xgb</span>
<span id="cb10-2"></span>
<span id="cb10-3">params <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb10-4">    <span class="st" style="color: #20794D;">'learning_rate'</span>: <span class="fl" style="color: #AD0000;">0.1</span>,</span>
<span id="cb10-5">    <span class="st" style="color: #20794D;">'max_depth'</span>: <span class="dv" style="color: #AD0000;">5</span>,</span>
<span id="cb10-6">    <span class="st" style="color: #20794D;">'subsample'</span>: <span class="fl" style="color: #AD0000;">0.8</span>,</span>
<span id="cb10-7">    <span class="st" style="color: #20794D;">'reg_lambda'</span>: <span class="fl" style="color: #AD0000;">1.5</span>,</span>
<span id="cb10-8">    <span class="st" style="color: #20794D;">'gamma'</span>: <span class="fl" style="color: #AD0000;">0.0</span>,</span>
<span id="cb10-9">    <span class="st" style="color: #20794D;">'min_child_weight'</span>: <span class="dv" style="color: #AD0000;">25</span>,</span>
<span id="cb10-10">    <span class="st" style="color: #20794D;">'base_score'</span>: <span class="fl" style="color: #AD0000;">0.0</span>,</span>
<span id="cb10-11">    <span class="st" style="color: #20794D;">'tree_method'</span>: <span class="st" style="color: #20794D;">'exact'</span>,</span>
<span id="cb10-12">}</span>
<span id="cb10-13">num_boost_round <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">50</span></span>
<span id="cb10-14"></span>
<span id="cb10-15"><span class="co" style="color: #5E5E5E;"># train the from-scratch XGBoost model</span></span>
<span id="cb10-16">model_scratch <span class="op" style="color: #5E5E5E;">=</span> XGBoostModel(params, random_seed<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">42</span>)</span>
<span id="cb10-17">model_scratch.fit(X_train, y_train, SquaredErrorObjective(), num_boost_round)</span>
<span id="cb10-18"></span>
<span id="cb10-19"><span class="co" style="color: #5E5E5E;"># train the library XGBoost model</span></span>
<span id="cb10-20">dtrain <span class="op" style="color: #5E5E5E;">=</span> xgb.DMatrix(X_train, label<span class="op" style="color: #5E5E5E;">=</span>y_train)</span>
<span id="cb10-21">dtest <span class="op" style="color: #5E5E5E;">=</span> xgb.DMatrix(X_test, label<span class="op" style="color: #5E5E5E;">=</span>y_test)</span>
<span id="cb10-22">model_xgb <span class="op" style="color: #5E5E5E;">=</span> xgb.train(params, dtrain, num_boost_round)</span></code></pre></div>
</div>
<p>Let’s check the models’ performance on the held out test data to benchmark our implementation.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">pred_scratch <span class="op" style="color: #5E5E5E;">=</span> model_scratch.predict(X_test)</span>
<span id="cb11-2">pred_xgb <span class="op" style="color: #5E5E5E;">=</span> model_xgb.predict(dtest)</span>
<span id="cb11-3"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'scratch score: </span><span class="sc" style="color: #5E5E5E;">{</span>SquaredErrorObjective()<span class="sc" style="color: #5E5E5E;">.</span>loss(y_test, pred_scratch)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb11-4"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'xgboost score: </span><span class="sc" style="color: #5E5E5E;">{</span>SquaredErrorObjective()<span class="sc" style="color: #5E5E5E;">.</span>loss(y_test, pred_xgb)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>scratch score: 0.2434125759558149
xgboost score: 0.24123239765807963</code></pre>
</div>
</div>
<p>Well, look at that! Our scratch-built SGBoost is looking pretty consistent with the library. Go us!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>I’d say this is a pretty good milestone for us here at Random Realizations. We’ve been hammering away at the various concepts around gradient boosting, leaving a trail of equations and scratch-built algos in our wake. Today we put all of that together to create a legit scratch build of XGBoost, something that would have been out of reach for me before we embarked on this journey together over a year ago. To anyone with the patience to read through this stuff, cheers to you! I hope you’re learning and enjoying this as much as I am.</p>
</section>
<section id="reader-exercises" class="level2">
<h2 class="anchored" data-anchor-id="reader-exercises">Reader Exercises</h2>
<p>If you want to take this a step further and deepen your understanding and coding abilities, let me recommend some exercises for you.</p>
<ol type="1">
<li>Implement column subsampling. XGBoost itself provides column subsampling by tree, by level, and by node. Try implementing by tree first, then try adding by level or by node as well. These should be pretty straightforward to do.</li>
<li>Implement sparsity aware split finding for missing feature values (Algorithm 2 in the <a href="https://arxiv.org/abs/1603.02754">XGBoost paper</a>). This will be a little more involved, since you’ll need to refactor and modify several parts of the tree booster class.</li>
</ol>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://blog.mattbowers.dev/posts/2022-05-07-xgboost-from-scratch/index.html</guid>
  <pubDate>Fri, 06 May 2022 23:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2022-05-07-xgboost-from-scratch/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How to Understand XGBoost</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2022-03-13-how-to-understand-xgboost/index.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2022-03-13-how-to-understand-xgboost/main.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Tree branches on a chilly day in Johnson City</figcaption><p></p>
</figure>
</div>
<p>Ahh, XGBoost, what an absolutely stellar implementation of gradient boosting. Once Tianqi Chen and Carlos Guestrin of the University of Washington published the <a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">XGBoost paper</a> and shared the <a href="https://github.com/dmlc/xgboost">open source code</a> in the mid 2010’s, the algorithm quickly gained adoption in the ML community, appearing in over half of winning Kagle submissions in 2015. Nowadays it’s certainly among the most popular gradient boosting libraries, along with LightGBM and CatBoost, although the highly scientific indicator of <a href="https://www.githubcompare.com/dmlc/xgboost+microsoft/lightgbm+catboost/catboost">GitHub stars per year</a> indicates that it is in fact the most beloved gradient boosting package of all. Since it was the first of the modern popular boosting frameworks, and since <a href="https://blog.dataiku.com/the-many-flavors-of-gradient-boosting-algorithms">benchmarking</a> indicates that no other boosting algorithm outperforms it, we can comfortably focus our attention on understanding XGBoost.</p>
<p>The XGBoost authors identify two key aspects of a machine learning system: (1) a flexible statistical model and (2) a scalable learning system to fit that model using data. XGBoost improves on both of these aspects, providing a more flexible and feature-rich statistical model and building a truly scalable system to fit it. In this post we’re going to focus on the statistical modeling innovations, outlining the key differences from the classic gradient boosting machine and divinginto the mathematical derivation of the XGBoost learning algorithm. If you’re not already familiar with <a href="../../gradient-boosting-machine-from-scratch">gradient boosting</a>, go back and read the earlier posts in the series before jumping in here.</p>
<p>Buckle up, dear reader. Today we understand how XGBoost works, no hand waving required.</p>
<section id="xgboost-is-a-gradient-boosting-machine" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-is-a-gradient-boosting-machine">XGBoost is a Gradient Boosting Machine</h2>
<p>At a high level, XGBoost is an iteratively constructed composite model, just like the classic gradient boosting machine we discussed back in the <a href="../../gradient-boosting-machine-from-scratch">GBM post</a> . The final model takes the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%20=%20F(%5Cmathbf%7Bx%7D_i)%20=%20b%20+%20%5Ceta%20%5Csum_%7Bk=1%7D%5EK%20f_k(%5Cmathbf%7Bx%7D_i)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b"> is the base prediction, <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is the learning rate hyperparameter that helps control overfitting by reducing the contributions of each booster, and each of the <img src="https://latex.codecogs.com/png.latex?K"> boosters <img src="https://latex.codecogs.com/png.latex?f_k"> is a decision tree. To help us connect the dots between theory and code, whenever we encounter new hyperparameters, I’ll point out their names from the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">XGBoost Parameter Documentation</a>. So, <img src="https://latex.codecogs.com/png.latex?b"> can be set by <code>base_score</code>, and <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is set by either <code>eta</code> or <code>learning_rate</code>.</p>
<p>XGBoost introduces two key statistical learning improvements over the classic gradient boosting model. First, it reimagines the gradient descent algorithm used for training, and second it uses a custom-built decision tree with extra functionality as its booster. We’ll dive into each of these key innovations in the following sections.</p>
</section>
<section id="descent-algorithm-innovations" class="level2">
<h2 class="anchored" data-anchor-id="descent-algorithm-innovations">Descent Algorithm Innovations</h2>
<section id="regularized-objective-function" class="level3">
<h3 class="anchored" data-anchor-id="regularized-objective-function">Regularized Objective Function</h3>
<p>In the post on <a href="../../gradient-boosting-machine-with-any-loss-function">GBM with any loss function</a>, we looked at loss functions of the form <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20l(y_i,%5Chat%7By%7D_i)"> which compute some distance between targets <img src="https://latex.codecogs.com/png.latex?y_i"> and predictions <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i"> and sum them up over the training dataset. XGBoost introduces regularization into the objective function so that the objective takes the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%20=%20%5Csum_i%20l(y_i,%5Chat%7By%7D_i)%20+%20%5Csum_k%20%5COmega(f_k)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?l"> is some twice-differentiable loss function. <img src="https://latex.codecogs.com/png.latex?%5COmega"> is a regularization that penalizes the complexity of each tree booster, taking the form</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5COmega(f)%20=%20%5Cgamma%20T%20+%20%5Cfrac%7B1%7D%7B2%7D%20%5Clambda%20%7C%7Cw%7C%7C%5E2%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?T"> is the number of leaf nodes and <img src="https://latex.codecogs.com/png.latex?%7C%7Cw%7C%7C%5E2"> is the squared sum of the leaf prediction values. This introduces two new hyperparameters: <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> which penalizes the number of leaf nodes and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> which is the L-2 regularization parameter for leaf predicted values. These are set by <code>gamma</code> and <code>reg_lambbda</code> in the XGBoost parametrization. Together, these provide powerful new controls to reduce overfitting due to overly complex tree boosters. Note that <img src="https://latex.codecogs.com/png.latex?%5Cgamma=%5Clambda=0"> reduces the objective back to an unregularized loss function as used in the classic GBM.</p>
</section>
<section id="an-aside-on-newtons-method" class="level3">
<h3 class="anchored" data-anchor-id="an-aside-on-newtons-method">An Aside on Newton’s Method</h3>
<p>As we’ll see soon, XGBoost uses <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s Method</a> to minimize its objective function, so let’s start with a quick refresher.</p>
<p>Newton’s method is an iterative procedure for minimizing a function <img src="https://latex.codecogs.com/png.latex?s(x)">. At each step we have some input <img src="https://latex.codecogs.com/png.latex?x_t">, and our goal is to find a nudge value <img src="https://latex.codecogs.com/png.latex?u"> such that</p>
<p><img src="https://latex.codecogs.com/png.latex?%20s(x_t%20+%20u)%20%5Cle%20s(x_t)"></p>
<p>To find a good nudge value <img src="https://latex.codecogs.com/png.latex?u">, we generate a local quadratic approximation of the function in the neighborhood of the input <img src="https://latex.codecogs.com/png.latex?x_t">, and then we find the input value that would bring us to the minimum of the quadratic approximation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2022-03-13-how-to-understand-xgboost/newton.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Schematic of Newton’s method</figcaption><p></p>
</figure>
</div>
<p>The figure shows a single Newton step where we start at <img src="https://latex.codecogs.com/png.latex?x_t">, find the local quadratic approximation, and then jump a distance <img src="https://latex.codecogs.com/png.latex?u"> along the <img src="https://latex.codecogs.com/png.latex?x">-axis to land at the minimum of the quadratic. If we iterate in this way, we are likely to land close to the minimum of <img src="https://latex.codecogs.com/png.latex?s(x)">.</p>
<p>So how do we compute the quadratic approximation? We use the second order Taylor series expansion of <img src="https://latex.codecogs.com/png.latex?s(x)"> near the point <img src="https://latex.codecogs.com/png.latex?x_t">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20s(x_t%20+%20u)%20%5Capprox%20%20s(x_t)%20+%20s'(x_t)u%20+%20%5Cfrac%7B1%7D%7B2%7D%20s''(x_t)%20u%5E2%20"></p>
<p>To find the nudge value <img src="https://latex.codecogs.com/png.latex?u"> that minimizes the quadratic approximation, we can take the derivative with respect to <img src="https://latex.codecogs.com/png.latex?u">, set it to zero, and solve for <img src="https://latex.codecogs.com/png.latex?u">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%200%20=%20%5Cfrac%7Bd%7D%7Bdu%7D%20%20%5Cleft%20(%20s(x_t)%20+%20s'(x_t)u%20+%20%5Cfrac%7B1%7D%7B2%7D%20s''(x_t)%20u%5E2%20%5Cright%20)%20=%20s'(x_t)%20+%20s''(x_t)%20u%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Crightarrow%20u%5E*%20=%20-%5Cfrac%7Bs'(x_t)%7D%7Bs''(x_t)%7D%20"></p>
<p>And as long as <img src="https://latex.codecogs.com/png.latex?s''(x_t)%3E0"> (i.e., the parabola is pointing up), <img src="https://latex.codecogs.com/png.latex?s(x_t%20+%20u%5E*)%20%5Cle%20s(x_t)">.</p>
</section>
<section id="tree-boosting-with-newtons-method" class="level3">
<h3 class="anchored" data-anchor-id="tree-boosting-with-newtons-method">Tree Boosting with Newton’s Method</h3>
<p>This lands us at the heart of XGBoost, which uses Newton’s method, rather than gradient descent, to guide each round of boosting. This explanation will correspond very closely to section 2.2 of the XGBoost paper, but here I’ll explicitly spell out some of the intermediate steps which are omitted from their derivation, and you’ll get some additional commentary from me along the way.</p>
<section id="newton-descent-in-tree-space" class="level4">
<h4 class="anchored" data-anchor-id="newton-descent-in-tree-space">Newton Descent in Tree Space</h4>
<p>Suppose we’ve done <img src="https://latex.codecogs.com/png.latex?t-1"> boosting rounds, and we want to add the <img src="https://latex.codecogs.com/png.latex?t">-th booster to our composite model. Our current model’s prediction for instance <img src="https://latex.codecogs.com/png.latex?i"> is <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%5E%7B(t-1)%7D">. If we add a new tree booster <img src="https://latex.codecogs.com/png.latex?f_t"> to our model, the objective function would give</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%5E%7B(t)%7D%20=%20%5Csum_%7Bi=1%7D%5En%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%20+%20f_t(%5Cmathbf%7Bx%7D_i))%20+%20%5COmega(f_t)%20"></p>
<p>We need to choose <img src="https://latex.codecogs.com/png.latex?f_t"> so that it decreases the loss, i.e.&nbsp;we want</p>
<p><img src="https://latex.codecogs.com/png.latex?%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%20+%20f_t(%5Cmathbf%7Bx%7D_i))%20%5Cle%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)"></p>
<p>Does that sound familiar? In the previous section we used Newton’s method to find a value of <img src="https://latex.codecogs.com/png.latex?u"> that would make <img src="https://latex.codecogs.com/png.latex?s(x_t%20+%20u)%20%5Cle%20s(x_t)">. Let’s try the same thing with our loss function. To be explicit, the parallels are: <img src="https://latex.codecogs.com/png.latex?s(%5Ccdot)%20%5Crightarrow%20l(y_i,%20%5Ccdot)">, <img src="https://latex.codecogs.com/png.latex?x_t%20%5Crightarrow%20%5Chat%7By%7D_i%5E%7B(t-1)%7D">, and <img src="https://latex.codecogs.com/png.latex?u%20%5Crightarrow%20f_t(%5Cmathbf%7Bx%7D_i)">.</p>
<p>Let’s start by finding the second order Taylor series approximation for the loss around the point <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%5E%7B(t-1)%7D">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%20+%20f_t(%5Cmathbf%7Bx%7D_i))%20%5Capprox%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)%20+%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%20"></p>
<p>where</p>
<p><img src="https://latex.codecogs.com/png.latex?%20g_i%20=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%7D%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%20h_i%20=%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%5E2%20%5Chat%7By%7D_i%5E%7B(t-1)%7D%7D%20l(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)"></p>
<p>are the first and second order partial derivatives of the loss with respect to the current predictions. The XGBoost paper calls these the gradients and hessians, respectively. Remember that when we specify an actual loss function to use, we would also specify the functional form of the gradients and hessians, so that they are directly computable.</p>
<p>Now we can go back and substitute our quadratic approximation in for the loss function to get an approximation of the objective function in the neighborhood of <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D_i%5E%7B(t-1)%7D">..</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%5E%7B(t)%7D%20%5Capprox%20%5Csum_%7Bi=1%7D%5En%20%5Bl(y_i,%20%5Chat%7By%7D_i%5E%7B(t-1)%7D)%20+%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%5D%20+%20%5COmega(f_t)%20"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?l(y_i,%5Chat%7By%7D_i%5E%7B(t-1)%7D)"> is constant regardless of our choice of <img src="https://latex.codecogs.com/png.latex?f_t">, we can drop it and instead work with the modified objective, which gives us Equation (3) from the paper.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bi=1%7D%5En%20%5B%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%5D%20+%20%5COmega(f_t)%20"></p>
<p>Now the authors are about to do something great. They’re about to show how to directly compute the optimal prediction values for the leaf nodes of <img src="https://latex.codecogs.com/png.latex?f_t">. We’ll circle back in a moment about how we find a good structure for <img src="https://latex.codecogs.com/png.latex?f_t">, i.e.&nbsp;good node splits, but we’re going to find the optimal predicted values for any tree structure having <img src="https://latex.codecogs.com/png.latex?T"> terminal nodes. Let <img src="https://latex.codecogs.com/png.latex?I_j"> denote the set of instances <img src="https://latex.codecogs.com/png.latex?i"> that are in the <img src="https://latex.codecogs.com/png.latex?j">-th leaf node of <img src="https://latex.codecogs.com/png.latex?f_t">. Then we can rewrite the objective.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bj=1%7D%5ET%20%5Cleft%20%5B%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20f_t(%5Cmathbf%7Bx%7D_i)%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20f_t(%5Cmathbf%7Bx%7D_i)%5E2%20%5Cright%20%5D%20+%20%5COmega(f_t)"></p>
<p>We notice that for all instances in <img src="https://latex.codecogs.com/png.latex?I_j">, the tree yields the same predicted value <img src="https://latex.codecogs.com/png.latex?f_t(%5Cmathbf%7Bx%7D_i)=w_j">. Substituting in <img src="https://latex.codecogs.com/png.latex?w_j"> for the predicted values and expanding <img src="https://latex.codecogs.com/png.latex?%5COmega(f_t)"> we get</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bj=1%7D%5ET%20%5Cleft%20%5B%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20w_j%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20w_j%5E2%20%5Cright%20%5D%20+%20%5Cgamma%20T%20+%20%5Cfrac%7B1%7D%7B2%7D%20%5Clambda%20%5Csum_%7Bj=1%7D%5ET%20w_j%5E2"></p>
<p>Rearranging terms we obtain Equation (4).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20%5Csum_%7Bj=1%7D%5ET%20%5Cleft%20%5B%20w_j%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20w_j%5E2%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%5Cright%20)%20%20%5Cright%20%5D%20+%20%5Cgamma%20T%20"></p>
<p>For each leaf node <img src="https://latex.codecogs.com/png.latex?j">, our modified objective function is quadratic in <img src="https://latex.codecogs.com/png.latex?w_j">. To find the optimal predicted values we take the derivative, set to zero, and solve for <img src="https://latex.codecogs.com/png.latex?w_j">.</p>
<p><img src="https://latex.codecogs.com/png.latex?%200%20=%20%5Cfrac%7Bd%7D%7Bdw_j%7D%20%5Cleft%20%5B%20w_j%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20+%20%5Cfrac%7B1%7D%7B2%7D%20%20w_j%5E2%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%5Cright%20)%20%20%5Cright%20%5D%20=%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%5Cright%20)%20w_j%20+%20%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20"></p>
<p>This yields Equation (5).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20w_j%5E*%20=%20-%20%5Cfrac%7B%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%20%7D%20"></p>
</section>
<section id="split-finding" class="level4">
<h4 class="anchored" data-anchor-id="split-finding">Split Finding</h4>
<p>Now that we know how to find the optimal predicted value for any leaf node, we need to identify a criterion for finding a good tree structure, which boils down to finding the best split for a given node. Back in the [decision tree from scratch](/decision-tree-from-scratch post, we derived a split evaluation metric based on the reduction in the objective function associated with a particular split.<br>
To do that, first we need a way to compute the objective function given a particular tree structure. Substituting the optimal predicted values <img src="https://latex.codecogs.com/png.latex?w_j%5E*"> into the objective function, we get Equation (6).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctilde%7BL%7D%5E%7B(t)%7D%20=%20-%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bj=1%7D%5ET%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_j%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_j%7D%20h_i%20+%20%5Clambda%7D%20+%20%5Cgamma%20T%20"></p>
<p>We can then evaluate potential splits by comparing the objective before making a split to the objective after making a split, where the split with the maximum reduction in objective (a.k.a. gain) is best.</p>
<p>More formally, let <img src="https://latex.codecogs.com/png.latex?I"> be the set of <img src="https://latex.codecogs.com/png.latex?n"> data instances in the current node, and let <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> be the instances that fall into the left and right child nodes of a proposed split. Let <img src="https://latex.codecogs.com/png.latex?L"> be the total loss for all instances in the node, while <img src="https://latex.codecogs.com/png.latex?L_L"> and <img src="https://latex.codecogs.com/png.latex?L_R"> are the losses for the left and right child nodes. The total loss contributed by instances in node <img src="https://latex.codecogs.com/png.latex?I"> prior to any split is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bbefore%20split%7D%7D%20=%20-%5Cfrac%7B1%7D%7B2%7D%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I%7D%20h_i%20+%20%5Clambda%7D%20+%20%5Cgamma%20"></p>
<p>And the loss after splitting <img src="https://latex.codecogs.com/png.latex?I"> into <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bafter%20split%7D%7D%20=%20L_L%20+%20L_R%20=%20-%5Cfrac%7B1%7D%7B2%7D%20%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_L%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_L%7D%20h_i%20+%20%5Clambda%7D%20-%5Cfrac%7B1%7D%7B2%7D%20%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_R%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_R%7D%20h_i%20+%20%5Clambda%7D%20+%202%20%5Cgamma%20"></p>
<p>The gain from this split is then</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CDelta%20L%20=%20L_%7B%5Ctext%7Bbefore%20split%7D%7D%20-%20%20L_%7B%5Ctext%7Bafter%20split%7D%7D%20=%20L%20-%20(L_L%20+%20L_R)"> <img src="https://latex.codecogs.com/png.latex?%5CDelta%20L%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Cleft%20%5B%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_L%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_L%7D%20h_i%20+%20%5Clambda%7D%20+%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I_R%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I_R%7D%20h_i%20+%20%5Clambda%7D%20-%20%5Cfrac%7B%20(%5Csum_%7Bi%20%5Cin%20I%7D%20g_i%20)%5E2%20%7D%20%7B%5Csum_%7Bi%20%5Cin%20I%7D%20h_i%20+%20%5Clambda%7D%20%5Cright%20%5D%20-%20%5Cgamma%20"></p>
<p>which is Equation (7) from the paper. In practice it makes sense to accept a split only if the gain is positive, thus the <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> parameter sets the minimum gain required to make a further split. This is why <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> can be set with the parameter <code>gamma</code> or the more descriptive<code>min_loss_split</code>.</p>
</section>
</section>
</section>
<section id="tree-booster-innovations" class="level2">
<h2 class="anchored" data-anchor-id="tree-booster-innovations">Tree Booster Innovations</h2>
<section id="missing-values-and-sparsity-aware-split-finding" class="level3">
<h3 class="anchored" data-anchor-id="missing-values-and-sparsity-aware-split-finding">Missing Values and Sparsity-Aware Split Finding</h3>
<p>The XGBoost paper also introduces a modified algorithm for tree split finding which explicitly handles missing feature values. Recall that in order to find the best threshold value for a given feature, we can simply try all possible threshold values, recording the score for each. If some feature values are missing, the XGBoost split finding algorithm simply scores each threshold twice: once with missing value instances in the left node and once with them in the right node. The best split will then specify both the threshold value and to which node instances with missing values should be assigned. The paper calls this the sparsity aware split finding routine, which is defined as Algorithm 2.</p>
</section>
<section id="preventing-further-splitting" class="level3">
<h3 class="anchored" data-anchor-id="preventing-further-splitting">Preventing Further Splitting</h3>
<p>In addition to <code>min_loss_split</code> discussed above, XGBoost offers another parameter for limiting further tree splitting called <code>min_child_weight</code>. This name is a little confusing to me because the word “weight” has various meanings. In the context of this parameter, “weight” refers to the sum of the hessians <img src="https://latex.codecogs.com/png.latex?%5Csum%20h_i"> over instances in the node. For squared error loss <img src="https://latex.codecogs.com/png.latex?h_i=1">, so this is equivalent to the number of samples. Thus this parameter generalizes the notion of the minimum number of samples allowed in a terminal node.</p>
</section>
<section id="sampling" class="level3">
<h3 class="anchored" data-anchor-id="sampling">Sampling</h3>
<p>XGBoost takes a cue from Random Forest and introduces both column and row subsampling. These sampling methods can prevent overfitting and reduce training time by limiting the amount of data to be processed during boosting.</p>
<p>Like random forest, XGBoost implements column subsampling, which limits tree split finding to randomly selected subsets of features. XGBoost provides column sampling for each tree, for each depth level within a tree, and for each split point within a tree, controlled by <code>colsample_bytree</code>, <code>colsample_bbylevel</code>, and <code>colsample_bbynode</code> respectively.</p>
<p>One interesting distinction is that XGBoost implements row sampling without replacement using <code>subbsample</code>, whereas random forest uses bootstrapping. The choice to bootstrap rows in RF probably spurred from a desire to use as much data as possible while training on the smaller datasets of the 1990’s when RF was developed. With larger datasets and the ability to generate a large number of trees, XGBoost simply takes a subsample of rows for each tree.</p>
</section>
</section>
<section id="scalability" class="level2">
<h2 class="anchored" data-anchor-id="scalability">Scalability</h2>
<p>Even though we’re focused on statistical learning, I figured I’d comment on why XGBoost is highly scalable. Basically it boils down to efficient, parallelizable, and distributable methods for growing trees. You’ll notice there is a <code>tree_method</code> parameter which allows you to choose between the greedy exact algorithm (like the one we discussed in the decision tree from scratch post) and the approximate algorithm, which offers various scalability-related functionality, notably including the ability to consider only a small number of candidate split points instead of trying all possible splits. The algorithm also uses clever tricks like pre-sorting data for split finding and caching frequently needed values.</p>
<section id="why-xgboost-is-so-successful" class="level3">
<h3 class="anchored" data-anchor-id="why-xgboost-is-so-successful">Why XGBoost is so Successful</h3>
<p>As I mentioned in the intro, XGBoost is simply a very good implementation of the gradient boosting tree model. Therefore it inherits all the benefits of <a href="../../consider-the-decision-tree">decision trees and tree ensembles</a>, while making even further improvements over the classic gradient boosting machine. These improvements boil down to</p>
<ol type="1">
<li>more ways to control overfitting</li>
<li>elegant handling of custom objectives</li>
<li>scalability</li>
</ol>
<p>First, XGBoost introduces two new tree regularization hyperparameters <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and <img src="https://latex.codecogs.com/png.latex?%5Clambda"> which are baked directly into its objective function. Combining these with the additional column and row sampling functionality provides a variety of ways to reduce overfitting.</p>
<p>Second, the XGBoost formulation provides a much more elegant way to train models on custom objective functions. Recall that for <a href="../../gradient-boosting-machine-with-any-loss-function">custom objectives</a>, the classic GBM finds tree structure by fitting a squared error decision tree to the gradients of the loss function and then sets each leaf’s predicted value by running a numerical optimization routine to find the optimal predicted value.</p>
<p>The XGBoost formulation improves on this two-stage approach by unifying the generation of tree structure and predicted values. Both the split scoring metric and the predicted values are directly computable from the instance gradient and hessian values, which are connected directly back to the overall training objective. This also removes the need for additional numerical optimizations, which contributes to speed, stability, and scalability.</p>
<p>Finally, speaking of scalability, XGBoost emerged at a time when industrial dataset size was exploding. Many use cases require scalable ML systems, and all use cases benefit from faster training and higher model development velocity.</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, there you go, those are the salient ideas behind XGBoost, the gold standard in gradient boosting model implementations. Hopefully now we all understand the mathematical basis for the algorithm and appreciate the key improvements it makes over the classic GBM. If you want to go even deeper, you can join us for the next post where we’ll roll up our sleeves and implement XGBoost entirely from scratch.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">The XGBoost paper</a></p>
</section>
<section id="exercise" class="level2">
<h2 class="anchored" data-anchor-id="exercise">Exercise</h2>
<p>Proove that the XGBoost Newton Descent generalizes the classic GBM gradient descent. Hint: show that XGBoost with a squared error objective and no regularization reduces to the classic GBM.</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <guid>https://blog.mattbowers.dev/posts/2022-03-13-how-to-understand-xgboost/index.html</guid>
  <pubDate>Sun, 13 Mar 2022 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2022-03-13-how-to-understand-xgboost/thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Decision Tree From Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://blog.mattbowers.dev/posts/2021-12-13-decision-tree-from-scratch/index.html</link>
  <description><![CDATA[ 



<p><img src="https://blog.mattbowers.dev/posts/2021-12-13-decision-tree-from-scratch/thumbnail.png" title="decision tree" class="img-fluid"></p>
<p>Yesterday we had a lovely discussion about the key strengths and weaknesses of decision trees and why tree ensembles are so great. But today, gentle reader, we abandon our philosophizing and get down to the business of implementing one of these decision trees from scratch.</p>
<p>A note before we get started. This is going to be the most involved scratch-build that we’ve done at Random Realizations so far. It is not the kind of algorithm that I could just sit down and write all at once. We need to start with a basic frame and then add functionality step by step, testing all along the way to make sure things are working properly. Since I’m writing this in a jupyter notebook, I’ll try to give you a sense for how I actually put the algorithm together interactively in pieces, eventually landing on a fully-functional final product.</p>
<p>Shall we?</p>
<section id="binary-tree-data-structure" class="level2">
<h2 class="anchored" data-anchor-id="binary-tree-data-structure">Binary Tree Data Structure</h2>
<p>A decision tree takes a dataset with features and a target, partitions the feature space into chunks, and assigns a prediction value to each chunk. Since each partitioning step divides one chunk in two, and since the partitioning is done recursively, it’s natural to use a binary tree data structure to represent a decision tree.</p>
<p>The basic idea of the binary tree is that we define a class to represent nodes in the tree. If we want to add children to a given node, we simply assign them as attributes of the parent node. The child nodes we add are themselves instances of the same class, so we can add children to them in the same way.</p>
<p>Let’s start out with a simple class for our decision tree. It takes a single value called <code>max_depth</code> as input, which will dictate how many layers of child nodes should be inserted below the root. This controls the depth of the tree. As long as <code>max_depth</code> is positive, the parent will instantiate two new instances of the binary tree node class, passing along <code>max_depth</code> decremented by one and attaching the two children to itself as attributes called <code>left</code> and <code>right</code>.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> math</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np </span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">class</span> DecisionTree():</span>
<span id="cb2-2"></span>
<span id="cb2-3">        <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, max_depth):</span>
<span id="cb2-4">            <span class="cf" style="color: #003B4F;">assert</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb2-5">            <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> max_depth</span>
<span id="cb2-6">            <span class="cf" style="color: #003B4F;">if</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb2-7">                <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-8">                <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</div>
<p>Let’s make a new instance of our decision tree class, a tree with depth 2.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.mattbowers.dev/posts/2021-12-13-decision-tree-from-scratch/binary_tree.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Binary tree structure diagram</figcaption><p></p>
</figure>
</div>
<p>We can access individual nodes and check their value of <code>max_depth</code>.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">t.max_depth, t.left.max_depth, t.left.right.max_depth</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>(2, 1, 0)</code></pre>
</div>
</div>
<p>Our full decision tree can expand on this idea where each node receives some input, modifies it, creates two child nodes, and passes the modified input along to them. Specifically, each node in our decision tree will receive a dataset, determine how best to split the dataset into two parts, create two child nodes, and pass one part of the data to the left child and the other part to the right child.</p>
<p>All we have to do now is add some additional functionality to our decision tree. First we’ll start by capturing all the inputs we need to grow a tree, which include the feature dataframe <code>X</code>, the target array <code>y</code>, <code>max_depth</code> to explicitly limit tree depth, <code>min_samples_leaf</code> to specify the minimum number of observations that are allowed in a leaf node, and an optional <code>idxs</code> which specifies the indices of data that the node should use. The indices argument is useful for users of our decision tree because it will allow them to implement row subsampling in ensemble methods like random forest. It will also be handy for internal use inside the decision tree when passing data along to child nodes; instead of passing copies of the two data subsets, we’ll just pass a reference to the full dataset and pass along a set of indices to identify that node’s instance subset.</p>
<p>Once we get our input, we’ll do a little bit of input validation and store things that we want to keep as object attributes. In case this is a leaf node, we’ll go ahead and compute its predicted value; since this is a regression tree, the prediction is just the mean of the target <code>y</code>. We’ll also go ahead and initialize a score metric which we’ll use to help us find the best split later; since lower scores are going to be better, we’ll initialize it to positive infinity. Finally, we’ll push the logic to add child nodes into a method called <code>_maybe_insert_child_nodes</code> that we’ll define next.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>a leading underscore in a method name indicates the method is for internal use and not part of the user-facing API of the class.</p>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;">class</span> DecisionTree():</span>
<span id="cb6-2"></span>
<span id="cb6-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb6-4">        <span class="cf" style="color: #003B4F;">assert</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb6-5">        <span class="cf" style="color: #003B4F;">assert</span> min_samples_leaf <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'min_samples_leaf must be positive'</span></span>
<span id="cb6-6">        <span class="va" style="color: #111111;">self</span>.min_samples_leaf, <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> min_samples_leaf, max_depth</span>
<span id="cb6-7">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;">=</span> y.values</span>
<span id="cb6-8">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(y))</span>
<span id="cb6-9">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, y, idxs</span>
<span id="cb6-10">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb6-11">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> np.mean(y[idxs]) <span class="co" style="color: #5E5E5E;"># node's prediction value</span></span>
<span id="cb6-12">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>) <span class="co" style="color: #5E5E5E;"># initial loss before split finding</span></span>
<span id="cb6-13">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb6-14">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb6-15">            </span>
<span id="cb6-16">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb6-17">        <span class="cf" style="color: #003B4F;">pass</span></span></code></pre></div>
</div>
<p>Now in order to test our class, we’ll need some actual data. We can use the same scikit-learn diabetes data from the last post.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> load_diabetes</span>
<span id="cb7-2"></span>
<span id="cb7-3">X, y <span class="op" style="color: #5E5E5E;">=</span> load_diabetes(as_frame<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_X_y<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span></code></pre></div>
</div>
<p>So far, so good.</p>
</section>
<section id="inserting-child-nodes" class="level2">
<h2 class="anchored" data-anchor-id="inserting-child-nodes">Inserting Child Nodes</h2>
<p>Our node inserting function <code>_maybe_insert_child_nodes</code> needs to first find the best split; then if a valid split exists, it needs to insert the child nodes. To find the best valid split, we need to loop through the columns and search each one for the best valid split. Again we’ll push the logic of finding the best split into a function that we’ll define later. Next if no split was found, we need to bail by returning before trying to insert the child nodes. To check if this node is a leaf (i.e.&nbsp;it shouldn’t have child nodes), we define a property called <code>is_leaf</code> which will just check if the best score so far is still infinity, in which case no split was found and the node is a leaf.</p>
<p>If a valid split was found, then we need to insert the child nodes. We’ll assume that our split finding function assigned attributes called <code>split_feature_idx</code> and <code>threshold</code> to tell us the split feature’s index and the split threshold value. We then use these to compute the indices of the data to be passed to the child nodes; the left child gets instances where the split feature value is less than or equal to the threshold, and the right child node gets instances where the split feature value is greater than the threshold. Then we create two new decision trees, passing the corresponding data indices to each and assigning them to the <code>left</code> and <code>right</code> attributes of the current node.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb9-2">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): </span>
<span id="cb9-3">            <span class="va" style="color: #111111;">self</span>._find_better_split(j)</span>
<span id="cb9-4">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="co" style="color: #5E5E5E;"># do not insert children</span></span>
<span id="cb9-5">            <span class="cf" style="color: #003B4F;">return</span> </span>
<span id="cb9-6">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb9-7">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb9-8">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb9-9">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb9-10">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb9-11">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb9-12">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb9-13"></span>
<span id="cb9-14">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb9-15">        <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb9-16">    </span>
<span id="cb9-17">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb9-18">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>)</span></code></pre></div>
</div>
<p>To test these new methods , we can assign them to our <code>DecisionTree</code> class and create a new class instance to make sure things are still working.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">DecisionTree._maybe_insert_child_nodes <span class="op" style="color: #5E5E5E;">=</span> _maybe_insert_child_nodes</span>
<span id="cb10-2">DecisionTree._find_better_split <span class="op" style="color: #5E5E5E;">=</span> _find_better_split</span>
<span id="cb10-3">DecisionTree.is_leaf <span class="op" style="color: #5E5E5E;">=</span> is_leaf</span>
<span id="cb10-4">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>)</span></code></pre></div>
</div>
<p>Yep, we’re still looking good.</p>
</section>
<section id="split-finding" class="level2">
<h2 class="anchored" data-anchor-id="split-finding">Split Finding</h2>
<p>Now we need to fill in the functionality of the split finding method. The overall strategy is to consider every possible way to split on the current feature, measuring the quality of each potential split with some scoring mechanism, and keeping track of the best split we’ve seen so far. We’ll come back to the issue of how to try all the possible splits in a moment, but let’s start by figuring out how to score a particular potential split.</p>
<p>Like other machine learning models, trees are trained by attempting to minimize some loss function that measures how well the model predicts the target data. We’ll be training our regression tree to minimize squared error.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%20=%20%5Csum_%7Bi=1%7D%5En%20(y_i-%5Chat%7By%7D_i)%5E2"></p>
<p>For a given node, we can replace <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D"> with <img src="https://latex.codecogs.com/png.latex?%5Cbar%7By%7D"> because each node uses the sample mean of its target instances as its prediction. We can then rewrite the loss for a given node as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%20=%20%5Csum_%7Bi=1%7D%5En(y_i%20-%20%5Cbar%7By%7D)%5E2%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi=1%7D%5En(y_i%5E2%20-2y_i%5Cbar%7By%7D%20+%20%5Cbar%7By%7D%5E2)%20%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi=1%7D%5Eny_i%5E2%20-2%5Cbar%7By%7D%5Csum_%7Bi=1%7D%5Eny_i%20+%20n%5Cbar%7By%7D%5E2%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi=1%7D%5Eny_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi=1%7D%5Eny_i%20%5Cright%20)%5E2%20"></p>
<p>We can then evaluate potential splits by comparing the loss after splitting to the loss before splitting, where the split with the greatest loss reduction is best. Let’s work out a simple expression for the loss reduction from a given split.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?I"> be the set of <img src="https://latex.codecogs.com/png.latex?n"> data instances in the current node, and let <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> be the instances that fall into the left and right child nodes of a proposed split. Let <img src="https://latex.codecogs.com/png.latex?L"> be the total loss for all instances in the node, while <img src="https://latex.codecogs.com/png.latex?L_L"> and <img src="https://latex.codecogs.com/png.latex?L_R"> are the losses for the left and right child nodes. The total loss contributed by instances in <img src="https://latex.codecogs.com/png.latex?I"> prior to any split is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bbefore%20split%7D%7D%20=%20L%20=%20%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%20%5Cright%20)%5E2%20"></p>
<p>And the loss after splitting <img src="https://latex.codecogs.com/png.latex?I"> into <img src="https://latex.codecogs.com/png.latex?I_L"> and <img src="https://latex.codecogs.com/png.latex?I_R"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?L_%7B%5Ctext%7Bafter%20split%7D%7D%20=%20L_L%20+%20L_R%20=%20%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_L%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%20%5Cright%20)%5E2%20+%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_R%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%20%5Cright%20)%5E2%20"></p>
<p>The reduction in loss from this split is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CDelta%20L%20=%20L_%7B%5Ctext%7Bafter%20split%7D%7D%20-%20%20L_%7B%5Ctext%7Bbefore%20split%7D%7D%20=%20(L_L%20+%20L_R)%20-%20L%20"> <img src="https://latex.codecogs.com/png.latex?%20%20=%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_L%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%20%5Cright%20)%5E2%20+%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn_R%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%20%5Cright%20)%5E2%20-%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%5E2%20-%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%20%5Cright%20)%5E2%20%5Cright%20)%20"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?I%20=%20I_L%20%5Ccup%20I_R"> the <img src="https://latex.codecogs.com/png.latex?%5Csum%20y%5E2"> terms cancel and we can simplify.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CDelta%20L%20=%20-%20%5Cfrac%7B1%7D%7Bn_L%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_L%7D%20y_i%20%5Cright%20)%5E2%0A-%20%5Cfrac%7B1%7D%7Bn_R%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I_R%7D%20y_i%20%5Cright%20)%5E2%0A+%20%5Cfrac%7B1%7D%7Bn%7D%20%5Cleft%20(%20%5Csum_%7Bi%20%5Cin%20I%7D%20y_i%20%5Cright%20)%5E2%20%20"></p>
<p>This is a really nice formulation of the split scoring metric from a computational complexity perspective. We can sort the data by the feature values then, starting with the smallest <code>min_samples_leaf</code> instances in the left node and the rest in the right node, we check the score. Then to check the next split, we simply move a single target value from the right node into the left node, updating the score by subtracting it from the right node’s partial sum and adding it to the left node’s partial sum. The third term is constant for all splits, so we only need to compute it once. If any split’s score is lower than the best score so far, then we update the best score so far, the split feature, and the threshold value. When we’re done we can be sure we found the best possible split. The time bottleneck is the sort, which puts us at an average time complexity of <img src="https://latex.codecogs.com/png.latex?O(n%5Clog%20n)">.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb11-2">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,feature_idx]</span>
<span id="cb11-3">        y <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.y[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb11-4">        sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb11-5">        sort_y, sort_x <span class="op" style="color: #5E5E5E;">=</span> y[sort_idx], x[sort_idx]</span>
<span id="cb11-6">        sum_y, n <span class="op" style="color: #5E5E5E;">=</span> y.<span class="bu" style="color: null;">sum</span>(), <span class="bu" style="color: null;">len</span>(y)</span>
<span id="cb11-7">        sum_y_right, n_right <span class="op" style="color: #5E5E5E;">=</span> sum_y, n</span>
<span id="cb11-8">        sum_y_left, n_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb11-9">    </span>
<span id="cb11-10">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf):</span>
<span id="cb11-11">            y_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_y[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb11-12">            sum_y_left <span class="op" style="color: #5E5E5E;">+=</span> y_i<span class="op" style="color: #5E5E5E;">;</span> sum_y_right <span class="op" style="color: #5E5E5E;">-=</span> y_i</span>
<span id="cb11-13">            n_left <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">;</span> n_right <span class="op" style="color: #5E5E5E;">-=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb11-14">            <span class="cf" style="color: #003B4F;">if</span>  n_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:</span>
<span id="cb11-15">                <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb11-16">            score <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span> sum_y_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_left <span class="op" style="color: #5E5E5E;">-</span> sum_y_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_right <span class="op" style="color: #5E5E5E;">+</span> sum_y<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n</span>
<span id="cb11-17">            <span class="cf" style="color: #003B4F;">if</span> score <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far:</span>
<span id="cb11-18">                <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> score</span>
<span id="cb11-19">                <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb11-20">                <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span></code></pre></div>
</div>
<p>Again, we assign the split finding method to our class and instantiate a new tree to make sure things are still working.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">DecisionTree._find_better_split <span class="op" style="color: #5E5E5E;">=</span> _find_better_split</span>
<span id="cb12-2">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>)</span>
<span id="cb12-3">X.columns[t.split_feature_idx], t.threshold</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>('s5', -0.0037611760063045703)</code></pre>
</div>
</div>
<p>Nice! Looks like the tree started with a split on the <code>s5</code> feature.</p>
</section>
<section id="inspecting-the-tree" class="level2">
<h2 class="anchored" data-anchor-id="inspecting-the-tree">Inspecting the Tree</h2>
<p>While we’re developing something complex like a decision tree class, we need a good way to inspect the object to help with testing and debugging. Let’s write a quick string representation method to make it easier to check what’s going on with a particular node.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb14-2">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f'n: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>n<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb14-3">        s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; value:</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>value<span class="sc" style="color: #5E5E5E;">:0.2f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb14-4">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">self</span>.is_leaf:</span>
<span id="cb14-5">            split_feature_name <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.columns[<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb14-6">            s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; split: </span><span class="sc" style="color: #5E5E5E;">{</span>split_feature_name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> &lt;= </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>threshold<span class="sc" style="color: #5E5E5E;">:0.3f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb14-7">        <span class="cf" style="color: #003B4F;">return</span> s</span></code></pre></div>
</div>
<p>We can assign the string representation method to the class and print a few nodes.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">DecisionTree.<span class="fu" style="color: #4758AB;">__repr__</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="fu" style="color: #4758AB;">__repr__</span></span>
<span id="cb15-2">t <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb15-3"><span class="bu" style="color: null;">print</span>(t)</span>
<span id="cb15-4"><span class="bu" style="color: null;">print</span>(t.left)</span>
<span id="cb15-5"><span class="bu" style="color: null;">print</span>(t.left.left)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>n: 442; value:152.13; split: s5 &lt;= -0.004
n: 218; value:109.99; split: bmi &lt;= 0.006
n: 171; value:96.31</code></pre>
</div>
</div>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>We need a public <code>predict</code> method that takes a feature dataframe and returns an array of predictions. We’ll need to look up the predicted value for one instance at a time and stitch them together in an array. We can do that by iterating over the feature dataframe rows with a list comprehension that calls a <code>_predict_row</code> method to grab the prediction for each row. The row predict method needs to return the current node’s predicted value if it’s a leaf, or if not, it needs to identify the appropriate child node based on its split and ask it for a prediction.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb17-2">        <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb17-3">    </span>
<span id="cb17-4">    <span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb17-5">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb17-6">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb17-7">        child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb17-8">                <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb17-9">        <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span></code></pre></div>
</div>
<p>Let’s assign the predict methods and make predictions on a few rows.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">DecisionTree.predict <span class="op" style="color: #5E5E5E;">=</span> predict</span>
<span id="cb18-2">DecisionTree._predict_row <span class="op" style="color: #5E5E5E;">=</span> _predict_row</span>
<span id="cb18-3">t.predict(X.iloc[:<span class="dv" style="color: #AD0000;">3</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>array([225.87962963,  96.30994152, 225.87962963])</code></pre>
</div>
</div>
</section>
<section id="the-complete-decision-tree-implementation" class="level2">
<h2 class="anchored" data-anchor-id="the-complete-decision-tree-implementation">The Complete Decision Tree Implementation</h2>
<p>Here’s the implementation, all in one place.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;">class</span> DecisionTree():</span>
<span id="cb20-2"></span>
<span id="cb20-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, X, y, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, max_depth<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">6</span>, idxs<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>):</span>
<span id="cb20-4">        <span class="cf" style="color: #003B4F;">assert</span> max_depth <span class="op" style="color: #5E5E5E;">&gt;=</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'max_depth must be nonnegative'</span></span>
<span id="cb20-5">        <span class="cf" style="color: #003B4F;">assert</span> min_samples_leaf <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>, <span class="st" style="color: #20794D;">'min_samples_leaf must be positive'</span></span>
<span id="cb20-6">        <span class="va" style="color: #111111;">self</span>.min_samples_leaf, <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">=</span> min_samples_leaf, max_depth</span>
<span id="cb20-7">        <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;">=</span> y.values</span>
<span id="cb20-8">        <span class="cf" style="color: #003B4F;">if</span> idxs <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span>: idxs <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="bu" style="color: null;">len</span>(y))</span>
<span id="cb20-9">        <span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.idxs <span class="op" style="color: #5E5E5E;">=</span> X, y, idxs</span>
<span id="cb20-10">        <span class="va" style="color: #111111;">self</span>.n, <span class="va" style="color: #111111;">self</span>.c <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(idxs), X.shape[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb20-11">        <span class="va" style="color: #111111;">self</span>.value <span class="op" style="color: #5E5E5E;">=</span> np.mean(y[idxs]) <span class="co" style="color: #5E5E5E;"># node's prediction value</span></span>
<span id="cb20-12">        <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>) <span class="co" style="color: #5E5E5E;"># initial loss before split finding</span></span>
<span id="cb20-13">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb20-14">            <span class="va" style="color: #111111;">self</span>._maybe_insert_child_nodes()</span>
<span id="cb20-15">            </span>
<span id="cb20-16">    <span class="kw" style="color: #003B4F;">def</span> _maybe_insert_child_nodes(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb20-17">        <span class="cf" style="color: #003B4F;">for</span> j <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.c): </span>
<span id="cb20-18">            <span class="va" style="color: #111111;">self</span>._find_better_split(j)</span>
<span id="cb20-19">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: <span class="co" style="color: #5E5E5E;"># do not insert children</span></span>
<span id="cb20-20">            <span class="cf" style="color: #003B4F;">return</span> </span>
<span id="cb20-21">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb20-22">        left_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb20-23">        right_idx <span class="op" style="color: #5E5E5E;">=</span> np.nonzero(x <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="va" style="color: #111111;">self</span>.threshold)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb20-24">        <span class="va" style="color: #111111;">self</span>.left <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb20-25">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[left_idx])</span>
<span id="cb20-26">        <span class="va" style="color: #111111;">self</span>.right <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(<span class="va" style="color: #111111;">self</span>.X, <span class="va" style="color: #111111;">self</span>.y, <span class="va" style="color: #111111;">self</span>.min_samples_leaf, </span>
<span id="cb20-27">                                  <span class="va" style="color: #111111;">self</span>.max_depth <span class="op" style="color: #5E5E5E;">-</span> <span class="dv" style="color: #AD0000;">1</span>, <span class="va" style="color: #111111;">self</span>.idxs[right_idx])</span>
<span id="cb20-28">    </span>
<span id="cb20-29">    <span class="at" style="color: #657422;">@property</span></span>
<span id="cb20-30">    <span class="kw" style="color: #003B4F;">def</span> is_leaf(<span class="va" style="color: #111111;">self</span>): <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">float</span>(<span class="st" style="color: #20794D;">'inf'</span>)</span>
<span id="cb20-31">    </span>
<span id="cb20-32">    <span class="kw" style="color: #003B4F;">def</span> _find_better_split(<span class="va" style="color: #111111;">self</span>, feature_idx):</span>
<span id="cb20-33">        x <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.values[<span class="va" style="color: #111111;">self</span>.idxs,feature_idx]</span>
<span id="cb20-34">        y <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.y[<span class="va" style="color: #111111;">self</span>.idxs]</span>
<span id="cb20-35">        sort_idx <span class="op" style="color: #5E5E5E;">=</span> np.argsort(x)</span>
<span id="cb20-36">        sort_y, sort_x <span class="op" style="color: #5E5E5E;">=</span> y[sort_idx], x[sort_idx]</span>
<span id="cb20-37">        sum_y, n <span class="op" style="color: #5E5E5E;">=</span> y.<span class="bu" style="color: null;">sum</span>(), <span class="bu" style="color: null;">len</span>(y)</span>
<span id="cb20-38">        sum_y_right, n_right <span class="op" style="color: #5E5E5E;">=</span> sum_y, n</span>
<span id="cb20-39">        sum_y_left, n_left <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.</span>, <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb20-40">    </span>
<span id="cb20-41">        <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="va" style="color: #111111;">self</span>.n <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf):</span>
<span id="cb20-42">            y_i, x_i, x_i_next <span class="op" style="color: #5E5E5E;">=</span> sort_y[i], sort_x[i], sort_x[i <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb20-43">            sum_y_left <span class="op" style="color: #5E5E5E;">+=</span> y_i<span class="op" style="color: #5E5E5E;">;</span> sum_y_right <span class="op" style="color: #5E5E5E;">-=</span> y_i</span>
<span id="cb20-44">            n_left <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">;</span> n_right <span class="op" style="color: #5E5E5E;">-=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb20-45">            <span class="cf" style="color: #003B4F;">if</span>  n_left <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.min_samples_leaf <span class="kw" style="color: #003B4F;">or</span> x_i <span class="op" style="color: #5E5E5E;">==</span> x_i_next:</span>
<span id="cb20-46">                <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb20-47">            score <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span> sum_y_left<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_left <span class="op" style="color: #5E5E5E;">-</span> sum_y_right<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n_right <span class="op" style="color: #5E5E5E;">+</span> sum_y<span class="op" style="color: #5E5E5E;">**</span><span class="dv" style="color: #AD0000;">2</span> <span class="op" style="color: #5E5E5E;">/</span> n</span>
<span id="cb20-48">            <span class="cf" style="color: #003B4F;">if</span> score <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="va" style="color: #111111;">self</span>.best_score_so_far:</span>
<span id="cb20-49">                <span class="va" style="color: #111111;">self</span>.best_score_so_far <span class="op" style="color: #5E5E5E;">=</span> score</span>
<span id="cb20-50">                <span class="va" style="color: #111111;">self</span>.split_feature_idx <span class="op" style="color: #5E5E5E;">=</span> feature_idx</span>
<span id="cb20-51">                <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">=</span> (x_i <span class="op" style="color: #5E5E5E;">+</span> x_i_next) <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb20-52">                </span>
<span id="cb20-53">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__repr__</span>(<span class="va" style="color: #111111;">self</span>):</span>
<span id="cb20-54">        s <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f'n: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>n<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb20-55">        s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; value:</span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>value<span class="sc" style="color: #5E5E5E;">:0.2f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb20-56">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">self</span>.is_leaf:</span>
<span id="cb20-57">            split_feature_name <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.X.columns[<span class="va" style="color: #111111;">self</span>.split_feature_idx]</span>
<span id="cb20-58">            s <span class="op" style="color: #5E5E5E;">+=</span> <span class="ss" style="color: #20794D;">f'; split: </span><span class="sc" style="color: #5E5E5E;">{</span>split_feature_name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> &lt;= </span><span class="sc" style="color: #5E5E5E;">{</span><span class="va" style="color: #111111;">self</span><span class="sc" style="color: #5E5E5E;">.</span>threshold<span class="sc" style="color: #5E5E5E;">:0.3f}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb20-59">        <span class="cf" style="color: #003B4F;">return</span> s</span>
<span id="cb20-60">    </span>
<span id="cb20-61">    <span class="kw" style="color: #003B4F;">def</span> predict(<span class="va" style="color: #111111;">self</span>, X):</span>
<span id="cb20-62">        <span class="cf" style="color: #003B4F;">return</span> np.array([<span class="va" style="color: #111111;">self</span>._predict_row(row) <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> X.iterrows()])</span>
<span id="cb20-63">    </span>
<span id="cb20-64">    <span class="kw" style="color: #003B4F;">def</span> _predict_row(<span class="va" style="color: #111111;">self</span>, row):</span>
<span id="cb20-65">        <span class="cf" style="color: #003B4F;">if</span> <span class="va" style="color: #111111;">self</span>.is_leaf: </span>
<span id="cb20-66">            <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.value</span>
<span id="cb20-67">        child <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.left <span class="cf" style="color: #003B4F;">if</span> row[<span class="va" style="color: #111111;">self</span>.split_feature_idx] <span class="op" style="color: #5E5E5E;">&lt;=</span> <span class="va" style="color: #111111;">self</span>.threshold <span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb20-68">                <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.right</span>
<span id="cb20-69">        <span class="cf" style="color: #003B4F;">return</span> child._predict_row(row)</span></code></pre></div>
</div>
</section>
<section id="from-scratch-versus-scikit-learn" class="level2">
<h2 class="anchored" data-anchor-id="from-scratch-versus-scikit-learn">From Scratch versus Scikit-Learn</h2>
<p>As usual, we’ll test our homegrown handiwork by comparing it to the existing implementation in scikit-learn. First let’s train both models on the <a href="https://scikit-learn.org/stable/datasets/real_world.html">California Housing dataset</a> which gives us 20k instances and 8 features to predict median house price by district.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> fetch_california_housing</span>
<span id="cb21-2"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb21-3"></span>
<span id="cb21-4">X, y <span class="op" style="color: #5E5E5E;">=</span> fetch_california_housing(as_frame<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_X_y<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb21-5">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.3</span>, random_state<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">43</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="im" style="color: #00769E;">from</span> sklearn.tree <span class="im" style="color: #00769E;">import</span> DecisionTreeRegressor</span>
<span id="cb22-2"><span class="im" style="color: #00769E;">from</span> sklearn.metrics <span class="im" style="color: #00769E;">import</span> mean_squared_error</span>
<span id="cb22-3"></span>
<span id="cb22-4">max_depth <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">8</span></span>
<span id="cb22-5">min_samples_leaf <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">16</span></span>
<span id="cb22-6"></span>
<span id="cb22-7">tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X_train, y_train, max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span>
<span id="cb22-8">pred <span class="op" style="color: #5E5E5E;">=</span> tree.predict(X_test)</span>
<span id="cb22-9"></span>
<span id="cb22-10">sk_tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span>
<span id="cb22-11">sk_tree.fit(X_train, y_train)</span>
<span id="cb22-12">sk_pred <span class="op" style="color: #5E5E5E;">=</span> sk_tree.predict(X_test)</span>
<span id="cb22-13"></span>
<span id="cb22-14"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'from scratch MSE: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_squared_error(y_test, pred)<span class="sc" style="color: #5E5E5E;">:0.4f}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb22-15"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'scikit-learn MSE: </span><span class="sc" style="color: #5E5E5E;">{</span>mean_squared_error(y_test, sk_pred)<span class="sc" style="color: #5E5E5E;">:0.4f}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>from scratch MSE: 0.3988
scikit-learn MSE: 0.3988</code></pre>
</div>
</div>
<p>We get similar accuracy on a held-out test dataset.</p>
<p>Let’s benchmark the two implementations on training time.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb24-2">sk_tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span>
<span id="cb24-3">sk_tree.fit(X_train, y_train)<span class="op" style="color: #5E5E5E;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 45.3 ms, sys: 555 µs, total: 45.8 ms
Wall time: 45.3 ms</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="20">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(max_depth=8, min_samples_leaf=16)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(max_depth=8, min_samples_leaf=16)</pre></div></div></div></div></div>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="op" style="color: #5E5E5E;">%%</span>time</span>
<span id="cb26-2">tree <span class="op" style="color: #5E5E5E;">=</span> DecisionTree(X_train, y_train, max_depth<span class="op" style="color: #5E5E5E;">=</span>max_depth, min_samples_leaf<span class="op" style="color: #5E5E5E;">=</span>min_samples_leaf)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CPU times: user 624 ms, sys: 1.65 ms, total: 625 ms
Wall time: 625 ms</code></pre>
</div>
</div>
<p>Wow, the scikit-learn implementation absolutely smoked us, training an order of magnitude faster. This is to be expected, since they implement split finding in cython, which generates compiled C code that can run much faster than our native python code. Maybe we can take a look at how to optimize python code with cython here on the blog one of these days.</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Holy cow, we just implemented a decision tree using nothing but numpy. I hope you enjoyed the scratch build as much as I did, and I hope you got a little bit better at coding (I certainly did). That was actually way harder than I expected, but looking back at the finished product, it doesn’t seem so bad right? I almost thought we were going to get away with not implementing our own decision tree, but it turns out that this will be super helpful for us when it comes time to implement XGBoost from scratch.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>This implementation is inspired and partially adapted from Jeremy Howard’s live coding of a <a href="https://course18.fast.ai/lessonsml1/lesson7.html">Random Forest</a> as part of the fastai ML course.</p>


</section>

 ]]></description>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://blog.mattbowers.dev/posts/2021-12-13-decision-tree-from-scratch/index.html</guid>
  <pubDate>Mon, 13 Dec 2021 00:00:00 GMT</pubDate>
  <media:content url="https://blog.mattbowers.dev/posts/2021-12-13-decision-tree-from-scratch/thumbnail.png" medium="image" type="image/png" height="86" width="144"/>
</item>
</channel>
</rss>
