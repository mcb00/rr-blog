<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<atom:link href="https://randomrealizations.com/archive.xml" rel="self" type="application/rss+xml"/>
<description>A blog about data science, statistics, machine learning, and the scientific method</description>
<image>
<url>https://randomrealizations.com/opengraph.png</url>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<height>76</height>
<width>144</width>
</image>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Fri, 04 Jul 2025 07:00:00 GMT</lastBuildDate>
<item>
  <title>Logistic Regression with PyTorch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/logistic-regression-with-pytorch/</link>
  <description><![CDATA[ 




<!-- Well, dear reader, while I know you've grown to appreciate the meticulous craftsmanship of the typical Random Realizations article, I've been curious lately whether we might both benefit from a more rapid publication cadence with more incremental and perhaps slightly less polished posts.
So today we'll give it a try, and you can let me know how it's working for you. -->
<p>Note from December 2025: Well dear reader, it looks like I wrote this post back in July and forgot to publish it, so here’s my early Christmas present to you. Enjoy!</p>
<p>In this post we’ll bridge the gap between traditional ML and deep learning by showing that logistic regression is a special case of a neural network, and we’ll compare the classic scikit-learn logistic regression to a neural network implementation that we’ll build in PyTorch. Then we’ll add some hidden layers to our PyTorch model to go from logistic regression to the multi-layer perceptron, a simple deep neural network that’s like the major scale of deep learning model architectures.</p>
<!-- ![Some MNIST Digits](digits.png "") -->
<section id="multiclass-logistic-regressiontraditional-ml-vs-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="multiclass-logistic-regressiontraditional-ml-vs-neural-network">Multiclass Logistic Regression—Traditional ML vs Neural Network</h2>
<p>We want to classify <img src="https://latex.codecogs.com/png.latex?N"> instances, each a <img src="https://latex.codecogs.com/png.latex?D"> dimensional input, into one of <img src="https://latex.codecogs.com/png.latex?K"> discrete classes by predicting the probability mass function over the <img src="https://latex.codecogs.com/png.latex?K"> classes. In matrix notation, the classical ML model is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20z%20=%20%20X%20W%5ET%20+%20b%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Chat%7Bp%7D%20=%20%5Ctext%7Bsoftmax%7D(z)%20=%20%5Cfrac%7B%5Cexp%20(z)%7D%20%7B%5Csum_%7Bk=1%7D%5EK%20%5Cexp%20(z_k)%7D"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20D%7D"> is the <img src="https://latex.codecogs.com/png.latex?D"> dimensional input data for each instance</li>
<li><img src="https://latex.codecogs.com/png.latex?W%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BK%20%5Ctimes%20D%7D"> is the coefficient matrix (<img src="https://latex.codecogs.com/png.latex?D"> coefficients for each class)</li>
<li><img src="https://latex.codecogs.com/png.latex?b%20%5Cin%20%5Cmathbb%7BR%7D%5EK"> is the intercept for each class</li>
<li><img src="https://latex.codecogs.com/png.latex?z%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20K%7D"> are the <img src="https://latex.codecogs.com/png.latex?K"> raw logits or linear scores for each instance</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsoftmax%7D(%5Ccdot):%5Cmathbb%7BR%7D%5EK%5Crightarrow(0,1)%5EK"> is applied to each instance to transform the logits in <img src="https://latex.codecogs.com/png.latex?(-%5Cinfty,%20%5Cinfty)"> to probabilities in <img src="https://latex.codecogs.com/png.latex?(0,1)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D%20%5Cin%20(0,1)%5E%7BN%20%5Ctimes%20K%7D"> are the <img src="https://latex.codecogs.com/png.latex?K"> class probabilities predicted for each instance</li>
</ul>
<p>In neural network terms we can express the above formulation as a network with</p>
<ul>
<li>Input layer: <img src="https://latex.codecogs.com/png.latex?X"></li>
<li>Linear layer: <img src="https://latex.codecogs.com/png.latex?z=XW%5ET+b"></li>
<li>Non-linear activation: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsoftmax%7D(z)"></li>
</ul>
<p>In both cases, model training is guided by a negative log likelihood loss function.</p>
<blockquote class="blockquote">
<p>FYI these formulations are also closely related to multi-class gradient boosting, which We talked about back in the <a href="../../posts/gradient-boosting-multi-class-classification-from-scratch/">gradient boosting for multi-class classification from scratch</a> post. You can go back and reread that post for some additional intuition on how multi-class classification works.</p>
</blockquote>
<p>Let’s implement logistic regression as a traditional ML model and as a neural network.</p>
</section>
<section id="mnist-data" class="level2">
<h2 class="anchored" data-anchor-id="mnist-data">MNIST Data</h2>
<p>We’ll train our logistic regression models to classify the handwritten digits in the classic <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>. Adapting this <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html">scikit-learn example</a>, we’ll load up the data, plot some of the digits, normalize the input images, and then fit a classical logistic regression model.</p>
<div id="50537e0a" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> fetch_openml</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LogisticRegression</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> check_random_state</span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load data from https://www.openml.org/d/554</span></span>
<span id="cb1-11">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_openml(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mnist_784"</span>, version<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, return_X_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Shuffle the data</span></span>
<span id="cb1-14">random_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> check_random_state(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-15">permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random_state.permutation(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-16">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[permutation]</span>
<span id="cb1-17">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[permutation]</span>
<span id="cb1-18">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.reshape((X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># train test split</span></span>
<span id="cb1-21">train_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span></span>
<span id="cb1-22">test_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span></span>
<span id="cb1-23">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(</span>
<span id="cb1-24">    X, y, train_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_samples, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_samples</span>
<span id="cb1-25">)</span>
<span id="cb1-26"></span>
<span id="cb1-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Normalize image data</span></span>
<span id="cb1-28">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb1-29">X_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(X_train)</span>
<span id="cb1-30">X_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.transform(X_test)</span></code></pre></div>
</div>
<div id="bde8a220" class="cell" data-execution_count="152">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Number of classes: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(np.unique(y))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shape of X: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>X<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of classes: 10
Shape of X: (70000, 784)</code></pre>
</div>
</div>
<p>We have <img src="https://latex.codecogs.com/png.latex?K=10"> classes corresponding to the digits 0-9. The image data in <code>X</code> is stored as a <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20D"> array with <img src="https://latex.codecogs.com/png.latex?N=70000"> images and each image having <img src="https://latex.codecogs.com/png.latex?D=784"> pixels. In this raw form, the images are flattened out into a single dimension, which is ideal for modeling. To visualize them, we’ll need to reshape each image from <img src="https://latex.codecogs.com/png.latex?1%20%5Ctimes%20784"> to <img src="https://latex.codecogs.com/png.latex?28%20%5Ctimes%2028">.</p>
<div id="9c38bb45" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_digits(X, n_rows, n_cols):</span>
<span id="cb4-2">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb4-3">    n_images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n_rows <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n_cols</span>
<span id="cb4-4">    fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_cols, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_rows))</span>
<span id="cb4-5"></span>
<span id="cb4-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Flatten axs to iterate easily</span></span>
<span id="cb4-7">    axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs.flatten()</span>
<span id="cb4-8"></span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_images):</span>
<span id="cb4-10">        axs[i].imshow(X[i], cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gray"</span>)</span>
<span id="cb4-11">        axs[i].axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"off"</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># turn off axes completely</span></span>
<span id="cb4-12"></span>
<span id="cb4-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove all spacing between plots</span></span>
<span id="cb4-14">    plt.subplots_adjust(wspace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, hspace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb4-15">    plt.tight_layout(pad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb4-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># return fig, axs</span></span>
<span id="cb4-17"></span>
<span id="cb4-18">plot_digits(X, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># post image</span></span>
<span id="cb4-19"></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/logistic-regression-with-pytorch/nn_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img" alt="grid of digit images"></p>
<figcaption>Some instances from the MNIST hand-written digit dataset</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="logistic-regression-with-scikit-learn" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-with-scikit-learn">Logistic Regression with scikit-learn</h2>
<p>We’ll start with the traditional logistic regression model implementation from scikit-learn.</p>
<div id="25a08d16" class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Classical Logistic Regression</span></span>
<span id="cb5-2"></span>
<span id="cb5-3">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sag"</span>, tol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>) </span>
<span id="cb5-4">clf.fit(X_train, y_train)</span>
<span id="cb5-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Test Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> clf<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>score(X_test, y_test)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test Accuracy: 89.39%</code></pre>
</div>
</div>
<p>Now to visualize what this model is doing, let’s have a look at its coefficients.</p>
<div id="38d06056" class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">clf.coef_.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>(10, 784)</code></pre>
</div>
</div>
<p>Recall the coefficients <img src="https://latex.codecogs.com/png.latex?W"> are in a <img src="https://latex.codecogs.com/png.latex?K%20%5Ctimes%20D"> array, so each of the <img src="https://latex.codecogs.com/png.latex?K"> rows contains the <img src="https://latex.codecogs.com/png.latex?D"> coefficients for the corresponding class. Let’s reshape each of the rows into a 28 by 28 image and plot them.</p>
<div id="513712bd" class="cell" data-execution_count="96">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_class_weights(weights: np.ndarray, title: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Classification Weights"</span>):</span>
<span id="cb9-2">    num_classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb9-3">    n_rows, n_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, (num_classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb9-4"></span>
<span id="cb9-5">    fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb9-6">    scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(weights).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>()</span>
<span id="cb9-7"></span>
<span id="cb9-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set up shared color scale</span></span>
<span id="cb9-9">    cmap <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.cm.viridis</span>
<span id="cb9-10">    norm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.Normalize(vmin<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span>scale, vmax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scale)</span>
<span id="cb9-11"></span>
<span id="cb9-12">    axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs.flatten()</span>
<span id="cb9-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_classes):</span>
<span id="cb9-14">        ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs[i]</span>
<span id="cb9-15">        im <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ax.imshow(</span>
<span id="cb9-16">            weights[i].reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>),</span>
<span id="cb9-17">            interpolation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nearest"</span>,</span>
<span id="cb9-18">            cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cmap,</span>
<span id="cb9-19">            norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>norm</span>
<span id="cb9-20">        )</span>
<span id="cb9-21">        ax.set_xticks(())</span>
<span id="cb9-22">        ax.set_yticks(())</span>
<span id="cb9-23">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ax.set_xlabel(f"Class {i}")</span></span>
<span id="cb9-24">        ax.set_xlabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Class </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, labelpad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb9-25">        ax.xaxis.set_label_position(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'top'</span>)</span>
<span id="cb9-26"></span>
<span id="cb9-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_classes, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(axs)):</span>
<span id="cb9-28">        axs[j].axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span>
<span id="cb9-29"></span>
<span id="cb9-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add colorbar BELOW the entire figure</span></span>
<span id="cb9-31">    cbar_ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fig.add_axes([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.03</span>])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [left, bottom, width, height]</span></span>
<span id="cb9-32">    fig.colorbar(</span>
<span id="cb9-33">        plt.cm.ScalarMappable(norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>norm, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cmap),</span>
<span id="cb9-34">        cax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cbar_ax,</span>
<span id="cb9-35">        orientation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"horizontal"</span></span>
<span id="cb9-36">    ).set_label(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"value"</span>)</span>
<span id="cb9-37"></span>
<span id="cb9-38">    fig.suptitle(title)</span>
<span id="cb9-39">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Make space for suptitle</span></span>
<span id="cb9-40">    plt.show()</span></code></pre></div>
</details>
</div>
<div id="d801fd5e" class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">plot_class_weights(clf.coef_, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Scikit-learn Logistic Regression Coefficients"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/logistic-regression-with-pytorch/nn_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img" alt="Traditional logistic regression coefficients for each class"></p>
<figcaption>Traditional logistic regression coefficients for each class</figcaption>
</figure>
</div>
</div>
</div>
<p>Intuitively, for a given digit, we’d expect the coefficients to be positive on pixels where the digit is typically located, and we’d expect the coefficients to be zero or perhaps even negative on pixels where other digits tend to be located. We can see that’s more or less what this model is doing.</p>
</section>
<section id="logistic-regression-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-in-pytorch">Logistic Regression in PyTorch</h2>
<p>I recommend checking out the PyTorch <a href="https://docs.pytorch.org/tutorials/beginner/basics/intro.html">Basic Tutorial</a> to get started with the library’s API. Our basic flow for creating PyTorch models will look like</p>
<ol type="1">
<li>Create <code>Dataset</code> and <code>DataLoader</code> objects.</li>
<li>Build the model.</li>
<li>Train the model.</li>
</ol>
<section id="create-pytorch-dataset-and-dataloader-objects" class="level3">
<h3 class="anchored" data-anchor-id="create-pytorch-dataset-and-dataloader-objects">Create PyTorch <code>Dataset</code> and <code>DataLoader</code> Objects</h3>
<div id="b50a8d9a" class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.utils.data <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TensorDataset, DataLoader</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.optim <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> optim</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert to torch tensors</span></span>
<span id="cb11-7">X_train_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(X_train, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb11-8">y_train_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(y_train.astype(np.int64), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">long</span>)</span>
<span id="cb11-9">X_test_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(X_test, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb11-10">y_test_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(y_test.astype(np.int64), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">long</span>)</span>
<span id="cb11-11"></span>
<span id="cb11-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Wrap in TensorDataset</span></span>
<span id="cb11-13">train_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TensorDataset(X_train_tensor, y_train_tensor)</span>
<span id="cb11-14">test_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TensorDataset(X_test_tensor, y_test_tensor)</span>
<span id="cb11-15"></span>
<span id="cb11-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create DataLoaders</span></span>
<span id="cb11-17">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span></span>
<span id="cb11-18">train_loader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(train_dataset, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batch_size, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb11-19">test_loader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(test_dataset, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batch_size, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
</section>
<section id="build-the-model" class="level3">
<h3 class="anchored" data-anchor-id="build-the-model">Build the Model</h3>
<div id="98b1afca" class="cell" data-execution_count="128">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Logistic Regression Model</span></span>
<span id="cb12-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> LogReg(nn.Module):</span>
<span id="cb12-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb12-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb12-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">784</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb12-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.activation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.LogSoftmax(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># returns log probabilities</span></span>
<span id="cb12-7"></span>
<span id="cb12-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb12-9">        z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear(x)</span>
<span id="cb12-10">        p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.activation(z)</span>
<span id="cb12-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> p</span>
<span id="cb12-12"></span>
<span id="cb12-13">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogReg()</span></code></pre></div>
</div>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">Train the Model</h3>
<div id="4dc0164b" class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hyperparameters</span></span>
<span id="cb13-2">num_epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb13-3">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb13-4"></span>
<span id="cb13-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Loss</span></span>
<span id="cb13-6">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.NLLLoss() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># expects log probabilities</span></span>
<span id="cb13-7"></span>
<span id="cb13-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Optimizer</span></span>
<span id="cb13-9">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.SGD(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learning_rate)</span>
<span id="cb13-10"></span>
<span id="cb13-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training loop</span></span>
<span id="cb13-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_epochs):</span>
<span id="cb13-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> train_loader:</span>
<span id="cb13-14">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Flatten</span></span>
<span id="cb13-15">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(images)</span>
<span id="cb13-16">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> criterion(outputs, labels)</span>
<span id="cb13-17"></span>
<span id="cb13-18">        optimizer.zero_grad()</span>
<span id="cb13-19">        loss.backward()</span>
<span id="cb13-20">        optimizer.step()</span>
<span id="cb13-21"></span>
<span id="cb13-22">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Epoch [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epoch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_epochs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">], Training Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/5], Training Loss: 0.4151
Epoch [2/5], Training Loss: 0.1858
Epoch [3/5], Training Loss: 0.1102
Epoch [4/5], Training Loss: 0.5600
Epoch [5/5], Training Loss: 0.4414</code></pre>
</div>
</div>
</section>
<section id="evaluate-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluate-the-model">Evaluate the Model</h3>
<div id="5c79af15" class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluate model</span></span>
<span id="cb15-2">model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb15-3">correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb15-4">total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb15-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb15-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> test_loader:</span>
<span id="cb15-7">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb15-8">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(images)</span>
<span id="cb15-9">        _, predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(outputs.data, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb15-10">        total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> labels.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb15-11">        correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> (predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> labels).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>().item()</span>
<span id="cb15-12"></span>
<span id="cb15-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Test Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> total<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test Accuracy: 90.71%</code></pre>
</div>
</div>
</section>
<section id="pytorch-model-weights" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-model-weights">PyTorch Model Weights</h3>
<p>Let’s take a look at the weights.</p>
<div id="2b787d80" class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.linear.weight.detach().numpy()</span>
<span id="cb17-2">weights.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="131">
<pre><code>(10, 784)</code></pre>
</div>
</div>
<div id="79baf6df" class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">plot_class_weights(weights, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PyTorch Logistic Regression Weights"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/logistic-regression-with-pytorch/nn_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="PyTorch logistic regression coefficients for each class"></p>
<figcaption>PyTorch logistic regression coefficients for each class</figcaption>
</figure>
</div>
</div>
</div>
<p>Nice! We can see that the weights from the neural network are qualitatively similar to the coefficients from the logistic regression model. Interestingly, the neural network weight patterns look a bit more noisy than the classical logistic regression coefficients, and yet, the models are performing similarly on the test data (89-90%). Likely there are a lot of logistic regression parameter solutions that yield similar performance, and these two models have found slightly different solutions in parameter space.</p>
</section>
</section>
<section id="multilayer-perceptron-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-perceptron-in-pytorch">Multilayer Perceptron in PyTorch</h2>
<p>Well, logistic regression is great and all, but of course, the reason for messing around with PyTorch is so that we can start building more interesting neural network architectures. The next obvious step is to build out a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a> (MLP). The MLP is a network with</p>
<ul>
<li>An input layer</li>
<li>One or more hidden layers comprised of a linear transformation passed to a non-linear activation function</li>
<li>An output layer, e.g.&nbsp;returning class probabilities</li>
</ul>
<p>When we add “hidden” layers between the input and output layers, the network earns the modifier “deep”, meaning that MLP’s are deep networks. We’ll build a model with two hidden layers that uses <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear unit activation functions</a> (ReLU). The ReLU just replaces negative inputs with zero and passes positive inputs through unchanged—a very simple form of non-linearity.</p>
<div id="a6038b9e" class="cell" data-execution_count="153">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MLP(nn.Module):</span>
<span id="cb20-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb20-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb20-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb20-5">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">784</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>),   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Input layer</span></span>
<span id="cb20-6">            nn.ReLU(),</span>
<span id="cb20-7">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>),   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hidden layer</span></span>
<span id="cb20-8">            nn.ReLU(),</span>
<span id="cb20-9">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb20-10">            nn.LogSoftmax(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># output layer - log probabilities</span></span>
<span id="cb20-11">        )</span>
<span id="cb20-12"></span>
<span id="cb20-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb20-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net(x)</span>
<span id="cb20-15"></span>
<span id="cb20-16">mlp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MLP()</span></code></pre></div>
</div>
<div id="1a69b7d4" class="cell" data-execution_count="154">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hyperparameters</span></span>
<span id="cb21-2">num_epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span></span>
<span id="cb21-3">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span></span>
<span id="cb21-4"></span>
<span id="cb21-5">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(mlp.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learning_rate)</span>
<span id="cb21-6">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.NLLLoss() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># expects log probability</span></span>
<span id="cb21-7"></span>
<span id="cb21-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training loop</span></span>
<span id="cb21-9">mlp.train()</span>
<span id="cb21-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_epochs):</span>
<span id="cb21-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> train_loader:</span>
<span id="cb21-12">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Flatten</span></span>
<span id="cb21-13">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mlp(images)</span>
<span id="cb21-14">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> criterion(outputs, labels)</span>
<span id="cb21-15"></span>
<span id="cb21-16">        optimizer.zero_grad()</span>
<span id="cb21-17">        loss.backward()</span>
<span id="cb21-18">        optimizer.step()</span>
<span id="cb21-19"></span>
<span id="cb21-20">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Epoch [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epoch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_epochs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">], Training Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/20], Training Loss: 0.4061
Epoch [2/20], Training Loss: 0.0821
Epoch [3/20], Training Loss: 0.0214
Epoch [4/20], Training Loss: 0.0585
Epoch [5/20], Training Loss: 0.0027
Epoch [6/20], Training Loss: 0.0086
Epoch [7/20], Training Loss: 0.0005
Epoch [8/20], Training Loss: 0.0018
Epoch [9/20], Training Loss: 0.0016
Epoch [10/20], Training Loss: 0.0809
Epoch [11/20], Training Loss: 0.0024
Epoch [12/20], Training Loss: 0.0000
Epoch [13/20], Training Loss: 0.0002
Epoch [14/20], Training Loss: 0.0001
Epoch [15/20], Training Loss: 0.0001
Epoch [16/20], Training Loss: 0.0004
Epoch [17/20], Training Loss: 0.0015
Epoch [18/20], Training Loss: 0.0011
Epoch [19/20], Training Loss: 0.0004
Epoch [20/20], Training Loss: 0.0000</code></pre>
</div>
</div>
<div id="1bc26754" class="cell" data-execution_count="155">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluate model</span></span>
<span id="cb23-2">mlp.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb23-3">correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb23-4">total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb23-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb23-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> test_loader:</span>
<span id="cb23-7">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb23-8">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mlp(images)</span>
<span id="cb23-9">        _, predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(outputs.data, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb23-10">        total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> labels.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb23-11">        correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> (predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> labels).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>().item()</span>
<span id="cb23-12"></span>
<span id="cb23-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Test Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> total<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test Accuracy: 95.83%</code></pre>
</div>
</div>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Nice! In this post we’ve bridged the gap between classic ML and deep learning by showing that logistic regression is a special case of a neural network, building a logistic regression model in PyTorch, and then adding hidden layers to that model to obtain a multilayer perceptron.</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>pytorch</category>
  <guid>https://randomrealizations.com/posts/logistic-regression-with-pytorch/</guid>
  <pubDate>Fri, 04 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Bayesian Modeling Primer</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/bayesian-modeling-primer/</link>
  <description><![CDATA[ 




<p>Well, dear reader, I know I haven’t been posting very much lately. That’s because I’ve been busy moving to a new city and working a new DS gig and learning some new things, including Bayesian modeling. In particular I’ve been reading Richard McElreath’s excellent book <a href="https://xcelab.net/rm/">Statistical Rethinking</a>, which I recommend to you as well. As a dedicated reader of this blog, I’m sure you’re perfectly capable of digesting a 600 page statistics textbook on your own, but just for fun, today I present to you my Bayesian statistics crash course.</p>
<p>My primary goal is to illuminate the major steps in the Bayesian workflow, that way you have a mental framework where you can store and contextualize new pieces of information as you learn. My secondary goal is to give you an intuitive understanding of Bayesian modeling from two interconnected perspectives: a mathematical formulation based primarily in probability theory and a probabilistic programming approach based on writing code to generate random data. Each perspective supports the other, and they are both necessary to grasp the full picture. I will attempt to weave these two perspectives throughout the description of the workflow, which is motivated by a toy example we’ll use throughout the post.</p>
<p>Let’s do this! ➡️</p>
<section id="the-rock-paper-scissors-pro" class="level2">
<h2 class="anchored" data-anchor-id="the-rock-paper-scissors-pro">🪨📄✂️ The Rock Paper Scissors Pro</h2>
<p>I spent a summer as an intern at RAND Corporation during my PhD. It was a fascinating place full of fascinating characters. One of the researchers, Fritz R, liked to take each cohort of interns out for drinks at some point in the summer. After picking up our first round himself, Fritz offered to buy a second drink for any of the interns who could beat him in a rock paper scissors (RPS) match, warning us that he was “pretty good at it.”</p>
<p>Let’s fact check his claim. We’d like to know something about his actual RPS win rate, but that is unobservable. We can’t observe it directly, but we could observe some match outcomes and make an inference about what his actual win rate might plausibly be.</p>
<p>Let’s say that after facing off with the 10 interns, Fritz racks up the following match outcomes.</p>
<div id="cell-3" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">observed_outcomes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<p>He won 7 out of 10 matches—not bad. But is his performance the result of skill or simply a lucky round? We’re going to address this question using Bayesian statistical analysis.</p>
</section>
<section id="the-bayesian-workflow-in-3-steps" class="level2">
<h2 class="anchored" data-anchor-id="the-bayesian-workflow-in-3-steps">🛠️ The Bayesian Workflow in 3 Steps</h2>
<p>I consider the Bayesian workflow to have 3 major steps:</p>
<ol type="1">
<li><strong>Modeling</strong> - specify the data generating process as a generative model</li>
<li><strong>Inference</strong> - use the model, the observed data, and some inference algorithm to infer the values of unknown model parameters</li>
<li><strong>Interpretation</strong> - summarize and interpret the inferred model parameters to answer your analysis questions</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/modeling-inference-interpretation.png" title="." class="img-fluid figure-img"></p>
<figcaption>The Bayesian Workflow: modeling, inference, interpretation.</figcaption>
</figure>
</div>
</section>
<section id="step-1.-modeling" class="level2">
<h2 class="anchored" data-anchor-id="step-1.-modeling">⚙️ Step 1. Modeling</h2>
<section id="modeling-the-data-generating-process" class="level3">
<h3 class="anchored" data-anchor-id="modeling-the-data-generating-process">Modeling the Data Generating Process</h3>
<p>In this step, we’re going to build a <em>generative model</em>, i.e.&nbsp;a model that can simulate data similar to our observed data. If you’re coming from ML, the key mental shift is to think about modeling the <em>data generating process (DGP)</em>, rather than curve-fitting the data itself. Practically this means our model is a set of random variables which relate to one another in some way and from which we can draw realizations… random realizations, that is. You can invent a DGP as follows:</p>
<ol type="1">
<li>Identify the key variables in the system.</li>
<li>Define each variable as a draw from some probability distribution, or in terms of the other variables.</li>
<li>Use unknown parameters as needed in the probability distributions or in the functional relationships among the key variables.</li>
</ol>
<p>In our RPS example, there is one key variable—Fritz’s match outcome. We can define the match outcome variable as a random draw from some distribution, e.g.&nbsp;a Bernoulli distribution. The Bernoulli distribution has one parameter—the success probability—which corresponds here to Fritz’s actual true win rate. Given some true win rate, we can simulate match outcomes by drawing realizations from the Bernoulli distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20y_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%200"> if Fritz loses to intern <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?y_i%20=%201"> if he wins, and <img src="https://latex.codecogs.com/png.latex?i=1,%5Cdots,N"> where <img src="https://latex.codecogs.com/png.latex?N=10">. In this DGP, the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> corresponds to Fritz’s true win rate.</p>
<p>This is a good start, but we can’t simulate data from this model yet because <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> has no particular value. So, what value should we use?</p>
</section>
<section id="probability-as-relative-plausibility" class="level3">
<h3 class="anchored" data-anchor-id="probability-as-relative-plausibility">Probability as Relative Plausibility</h3>
<p>One of the key ideas in Bayesian modeling is that we can represent the relative plausibility of potential values of any unobserved variable using a probability distribution. Highly plausible values get higher probability, and less plausible values get lower probability.</p>
<p><em>It is this view of probability as a measure of relative plausibility that distinguishes Bayesian statistics from Frequentist statistics, which views probability as the relative frequency of events.</em></p>
<p>We don’t know the true value of Fritz’s RPS win rate, but even before collecting any data, we might have some contextual knowledge about how the world works which can provide some prior information about the relative plausibility of its possible values. For me it’s easiest to think in terms of how surprising a given true value would be. I wouldn’t be surprised at all if his win rate was near 0.5, but I would be shocked if it was 0.9 or 0.1, hence 0.5 has higher relative plausibility than 0.9 or 0.1.</p>
<p>Let’s represent the prior relative plausibility of values of Fritz’s RPS win rate with a probability distribution. Below are a few different probability distributions defined over the possible values <img src="https://latex.codecogs.com/png.latex?0%20%5Cle%20%5Ctheta%20%5Cle%201">.</p>
<div id="cell-7" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> beta, bernoulli</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># set colors for later</span></span>
<span id="cb2-7">prior_color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C0"</span></span>
<span id="cb2-8">post_color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C1"</span></span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># prior beta parameters</span></span>
<span id="cb2-11">parameters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb2-12">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb2-13">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>),</span>
<span id="cb2-14">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-15">]</span>
<span id="cb2-16"></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plot the prior</span></span>
<span id="cb2-18">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb2-19">plt.figure()</span>
<span id="cb2-20"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, (alpha, beta_val) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(parameters):</span>
<span id="cb2-21">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> beta.pdf(x, alpha, beta_val)</span>
<span id="cb2-22">    label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Beta($</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">alpha$=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, $</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">beta$=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_val<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span></span>
<span id="cb2-23">    plt.plot(x, y, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>label)</span>
<span id="cb2-24"></span>
<span id="cb2-25">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"win rate (theta)"</span>)</span>
<span id="cb2-26">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Probability Density"</span>)</span>
<span id="cb2-27">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Relative Plausibility of RPS Win Rate'</span>)</span>
<span id="cb2-28">plt.legend()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img" alt="beta distribution priors"></p>
</figure>
</div>
</div>
</div>
<p>Each of these PDFs has a mode at <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5"> and decreases toward 0 and 1. They’re all aligned with the relative plausibilities we discussed earlier.</p>
<p>You can check the relative plausibility between two possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> implied by a given pdf by taking the ratio of the height of the pdf at one value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> versus the height at another value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p>For example, let’s compare <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5"> to <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.7"> for a Beta(10, 10) prior.</p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">beta.pdf(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> beta.pdf(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>np.float64(4.802710683776413)</code></pre>
</div>
</div>
<p>The Beta(10, 10) distribution implies that a 0.5 win rate is about 5 times more plausible than a 0.7 win rate, which sounds, ahem, plausible.</p>
</section>
<section id="priors" class="level3">
<h3 class="anchored" data-anchor-id="priors">Priors</h3>
<p>We can include this prior information about the relative plausibility of values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> in our model as follows.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%20%5Csim%20%5Ctext%7BBeta%7D(10,%2010)%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ay_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)%0A"></p>
<p>In Bayesian parlance, we call the probability distribution that represents the relative plausibilities of an unobserved parameter its <em>prior distribution</em>, or simply its <em>prior</em>. Notice that with the addition of the prior for <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, our model is now fully generative.</p>
</section>
<section id="implementing-the-generative-model" class="level3">
<h3 class="anchored" data-anchor-id="implementing-the-generative-model">Implementing the generative model</h3>
<p>Let’s implement the DGP using random variables from <code>scipy</code>.</p>
<div id="cell-11" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Implementing the DGP as a generative model</span></span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> draw_from_prior(alpha_param, beta_param):</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> beta.rvs(alpha_param, beta_param)</span>
<span id="cb5-5"></span>
<span id="cb5-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_one_outcome(theta, N):</span>
<span id="cb5-7">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bernoulli.rvs(theta, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb5-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>: theta, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span>: y, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>: np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(y)}</span>
<span id="cb5-9"></span>
<span id="cb5-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_outcomes(n_outcomes, alpha_param, beta_param, N):</span>
<span id="cb5-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame([</span>
<span id="cb5-12">        simulate_one_outcome(theta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>draw_from_prior(alpha_param, beta_param), N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb5-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_outcomes)</span>
<span id="cb5-14">    ])</span>
<span id="cb5-15"></span>
<span id="cb5-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># set DGP parameters</span></span>
<span id="cb5-17">alpha_param, beta_param <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb5-18">N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb5-19"></span>
<span id="cb5-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># simulate outcomes from the generative model</span></span>
<span id="cb5-21">outcome_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1_000</span>, alpha_param, beta_param, N)</span>
<span id="cb5-22">outcome_df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">theta</th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">sum_y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.451620</td>
<td>[0, 0, 1, 1, 1, 0, 0, 1, 0, 0]</td>
<td>4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.460305</td>
<td>[0, 1, 1, 0, 1, 1, 1, 0, 1, 1]</td>
<td>7</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.555594</td>
<td>[0, 0, 1, 1, 1, 1, 0, 1, 0, 0]</td>
<td>5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.518724</td>
<td>[1, 0, 0, 0, 1, 1, 1, 0, 1, 0]</td>
<td>5</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.569247</td>
<td>[1, 0, 1, 1, 0, 1, 1, 1, 1, 1]</td>
<td>8</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Each time you run this simulation, you first draw a new value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from its prior, then that value is used in the Bernoulli distribution to draw an array of binary match win/loss observations. To help us summarize the match observations in each simulated outcome, we also compute the sum of the match values, i.e.&nbsp;the number of wins.</p>
</section>
<section id="prior-predictive-check" class="level3">
<h3 class="anchored" data-anchor-id="prior-predictive-check">Prior Predictive Check</h3>
<p>But how do we know that the prior we chose is reasonable? There are two places we can look: (1) at the parameter itself and (2) at the downstream variables it influences. We already looked at the parameter itself by inspecting its pdf and thinking about the relative plausibilities it implies. To look at its impact on the downstream variables, we can simply run simulations from the model and inspect the outcome data it produces. If we see it’s generating lots of highly implausible outcomes, then we know something isn’t right. This process is called a <em>prior predictive check</em>, because we’re checking the simulated outcomes (a.k.a. predictions) implied by the prior. Let’s run our model simulation 1000 times and have a look at the distribution of the number of wins out of 10 matches that it predicts, i.e.&nbsp;the sum of the <code>y</code> variable from each simulation.</p>
<div id="cell-13" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">outcome_df.hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb6-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum(y)'</span>)</span>
<span id="cb6-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb6-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Prior Predictive Check'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img" alt="prior predictive check of number of match wins"></p>
</figure>
</div>
</div>
</div>
<p>The histogram shows most of the simulations yield between 3 and 7 wins, with very few outcomes less than 3 or greater than 7. That seems pretty reasonable.</p>
<p>Let’s look at what the prior predictive check might look like when things aren’t quite right.</p>
<div id="cell-15" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1_000</span>, alpha_param<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, beta_param<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>).hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb7-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum(y)'</span>)</span>
<span id="cb7-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb7-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Prior Predictive Check'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img" alt="prior predictive check of the win rate, with unexpected distribution shape"></p>
</figure>
</div>
</div>
</div>
<p>In this simulation, many of the outcomes are close to 0 or 10 wins out of 10. From our prior knowledge about RPS, we know it would be possible but very unusual for someone to win either 0/10 or 10/10 matches. This tips us off that something isn’t right with our priors. At this point we would iterate on our priors until we find something reasonable like our Beta(10, 10).</p>
<p>Once we’ve got our generative model and its priors nailed down, we’re ready to move from the modeling step to the inference step!</p>
</section>
</section>
<section id="step-2.-inference" class="level2">
<h2 class="anchored" data-anchor-id="step-2.-inference">🧮 Step 2. Inference</h2>
<section id="the-goal-of-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="the-goal-of-bayesian-inference">The Goal of Bayesian Inference</h3>
<p>In the inference step, we use observed outcome data to infer the plausible values of the unobserved parameters. Whereas simulation passes information forward from parameters to outcomes, inference passes it backwards from observed outcomes to parameters. It’s analogous to model fitting or training in machine learning; it’s the part where we use data to learn about the model parameters. The specific output of inference is the updated relative plausibility of the unknown model parameters. Whereas we represent the prior relative plausibilities with the prior distribution, we represent the posterior relative plausibilities (after incorporating information from the data) with the <em>posterior distribution</em>, or simply, the <em>posterior</em>. Like the prior, our model’s posterior distribution is a probability density defined over the possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, where larger values indicate higher relative plausibility.</p>
</section>
<section id="analytical-formulation-of-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="analytical-formulation-of-bayesian-inference">Analytical Formulation of Bayesian Inference</h3>
<p>Let’s nail down the mathematical formulation of Bayesian inference. We have data <img src="https://latex.codecogs.com/png.latex?y"> and parameter(s) <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. These have a joint probability density <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)">. This joint distribution of data and parameters is defined by our generative model of the system—simulating data from our DGP is equivalent to drawing realizations from the joint distribution <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)">. Using the definition of conditional probability,, we can write the joint distribution as:</p>
<p><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)%20=%20p(y%7C%5Ctheta)p(%5Ctheta)%20"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)"> is the <em>joint distribution</em> of parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and data <img src="https://latex.codecogs.com/png.latex?y"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)"> is the <em>prior </em> distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(y%7C%5Ctheta)"> is the <em>likelihood</em>—the conditional distribution of observed data <img src="https://latex.codecogs.com/png.latex?y"> given parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
</ul>
<p>When we do inference we are interested in the relative plausibility of unknown parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> given data <img src="https://latex.codecogs.com/png.latex?y">, which we quantify as the conditional distribution of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> given <img src="https://latex.codecogs.com/png.latex?y">. Using Baye’s Rule, we can write the posterior as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(%5Ctheta%20%7C%20y)%20=%20%5Cfrac%7B%20p(%5Ctheta,%20y)%20%7D%20%7B%20p(y)%20%7D%20=%20%5Cfrac%20%7B%20p(y%7C%5Ctheta)p(%5Ctheta)%20%20%7D%20%7B%20p(y)%20%7D%20%20"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%20%7C%20y)"> is the <em>posterior</em> distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(y)"> is the <em>marginal likelihood</em> of the data <img src="https://latex.codecogs.com/png.latex?y"> (to be explained soon)</li>
</ul>
<p>Technically, the joint distribution and the posterior are functions of <em>both</em> parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and data <img src="https://latex.codecogs.com/png.latex?y">. But in practice when we compute the posterior, we’ll have some actual observed data—say <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D">—so that <img src="https://latex.codecogs.com/png.latex?y"> is actually fixed at <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">. Substituting the fixed value <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D"> in the posterior, we get</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(%5Ctheta%20%7C%20y_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cfrac%7B%20p(%5Ctheta,%20y_%7B%5Ctext%7Bobs%7D%7D)%20%7D%20%7B%20p(y_%7B%5Ctext%7Bobs%7D%7D)%20%7D%20=%20%20%5Cfrac%20%7B%20p(y_%7B%5Ctext%7Bobs%7D%7D%7C%5Ctheta)p(%5Ctheta)%20%20%7D%20%7B%20%20%5Cint%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)p(%5Ctheta)%20d%20%5Ctheta%20%20%7D%20%20"></p>
<p>If we view <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D"> as fixed, then the posterior can be interpreted as just the slice of the joint distribution where <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">. To get a proper conditional probability distribution, we just need to divide the sliced joint density function by the area under <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,y_%7B%5Ctext%7Bobs%7D%7D)"> along the <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> axis. And guess what? That’s exactly what the marginal likelihood is doing; <img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cint%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)p(%5Ctheta)%20d%20%5Ctheta"> is just the area under the sliced joint density, and it’s there in the denominator to normalize the sliced joint density so that we get a proper conditional distribution for the posterior.</p>
</section>
<section id="computing-the-posterior-using-grid-approximation" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-posterior-using-grid-approximation">Computing the Posterior using Grid Approximation</h3>
<p>Let’s compute the posterior using the formulas we cooked up in the previous section. Earlier when we wrote down our generative model, we already identified all the pieces we need:</p>
<ul>
<li><strong>the prior</strong>—since <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Csim%20%5Ctext%7BBeta%7D(10,%2010)">, <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)"> is the probability density function of a <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BBeta%7D(10,10)"> random variable.</li>
<li><strong>the likelihood</strong>—since <img src="https://latex.codecogs.com/png.latex?y_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)">, the likelihood is the probability mass function of a Bernoulli random variable with parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">… well, almost.</li>
</ul>
<p>The one remaining detail to iron out is that our observed data <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D=%5By_1,%5Cdots,y_N%5D"> consists of <img src="https://latex.codecogs.com/png.latex?N=10"> observations of the binary match outcomes. Our likelihood needs to reflect the conditional probability of the entire dataset given <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, not just a single observation. We know from probability theory that the joint probability of two independent events is the product of their individual probabilities. Therefore, assuming independence among our observations, the joint likelihood of the full dataset is the product of the likelihood of each observation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(y_%7B%5Ctext%7Bobs%7D%7D%7C%5Ctheta)%20=%20p(y_1,%5Cdots,y_N%7C%5Ctheta)%20=%20%20%5Cprod_%7Bi=1%7D%5EN%20p(y_i%7C%5Ctheta)%20"></p>
<p>Let’s implement the prior, the likelihood, the joint distribution, and the posterior in python and plot out the prior and the posterior distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<div id="cell-18" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0. Defining the observed data</span></span>
<span id="cb8-2"></span>
<span id="cb8-3">y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(observed_outcomes)</span>
<span id="cb8-4">sum_y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(y_obs)</span>
<span id="cb8-5"></span>
<span id="cb8-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Functions for Prior and Likelihood</span></span>
<span id="cb8-7"></span>
<span id="cb8-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> prior(theta):</span>
<span id="cb8-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> beta.pdf(theta, alpha_param, beta_param)</span>
<span id="cb8-10"></span>
<span id="cb8-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> likelihood(theta, y):</span>
<span id="cb8-12">    product_of_likelihoods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span></span>
<span id="cb8-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> y_i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> y:</span>
<span id="cb8-14">        product_of_likelihoods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> bernoulli.pmf(y_i, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>theta)</span>
<span id="cb8-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> product_of_likelihoods</span>
<span id="cb8-16"></span>
<span id="cb8-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Function for Joint Density p(y, theta)</span></span>
<span id="cb8-18"></span>
<span id="cb8-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> joint_density(theta, y):</span>
<span id="cb8-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> likelihood(theta, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> prior(theta)</span>
<span id="cb8-21"></span>
<span id="cb8-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Computing the Posterior by "Slicing" and Normalizing</span></span>
<span id="cb8-23"></span>
<span id="cb8-24"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> posterior_from_joint_slice(theta_values, y_observed_data):</span>
<span id="cb8-25">    </span>
<span id="cb8-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute grid of p(theta, y_obs) over values of theta and fixed y_obs</span></span>
<span id="cb8-27">    unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([</span>
<span id="cb8-28">        joint_density(theta, y_observed_data) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_values</span>
<span id="cb8-29">    ])</span>
<span id="cb8-30">    </span>
<span id="cb8-31">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># numerical integration  to get marginal likelihood p(y_obs</span></span>
<span id="cb8-32">    delta_theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> theta_values[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> theta_values[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] </span>
<span id="cb8-33">    marginal_likelihood_approx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> delta_theta)</span>
<span id="cb8-34">    </span>
<span id="cb8-35">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># p(theta | y_obs) = p(theta, y_obs) / p(y_obs)</span></span>
<span id="cb8-36">    normalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> marginal_likelihood_approx</span>
<span id="cb8-37">    </span>
<span id="cb8-38">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> normalized_posterior_values</span></code></pre></div>
</div>
<div id="cell-19" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define a grid of theta values</span></span>
<span id="cb9-2">theta_grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>) </span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate prior over the grid of theta values</span></span>
<span id="cb9-5">prior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([prior(theta) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_grid])</span>
<span id="cb9-6"></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate likelihood values over the grid of theta values and fixed y_obs</span></span>
<span id="cb9-8">likelihood_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([likelihood(theta, y_obs) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_grid])</span>
<span id="cb9-9"></span>
<span id="cb9-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate the posterior over the grid of theta values and fixed y_obs</span></span>
<span id="cb9-11">posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> posterior_from_joint_slice(theta_grid, y_obs)</span></code></pre></div>
</div>
<div id="cell-20" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plotting</span></span>
<span id="cb10-2">plt.plot(theta_grid, prior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Prior p(theta) ~ Beta(</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb10-3">plt.plot(theta_grid, posterior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Posterior p(theta|y_obs)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb10-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian Inference: Prior, Likelihood, and Posterior'</span>)</span>
<span id="cb10-5">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'$\theta$ (Probability of Match Win)'</span>)</span>
<span id="cb10-6">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb10-7">plt.legend()</span>
<span id="cb10-8">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb10-9">plt.xlim(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb10-10">plt.ylim(bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb10-11">plt.tight_layout()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img" alt="plot of prior and posterior computed with  grid approximation"></p>
</figure>
</div>
</div>
</div>
<p>From the figure we can see that while the prior is centered at <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5">, the posterior is actually pulled slightly toward larger values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> by the observed data, indicating increased relative plausibility on win rates greater than 0.5.</p>
<p>It’s nice to see that the math works and that we can successfully implement it in code, but grid approximation is a pedagogical endeavor. In practice, when models start to get complicated, we’ll need a more flexible approach for finding the posterior.</p>
</section>
<section id="sampling-from-the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-posterior">Sampling from the Posterior</h3>
<p>It turns out that we can do inference using our generative model and our observed data without having to compute the likelihood directly. But while the forward problem of simulating data from the model is quite straightforward, it’s less obvious how to approach the inverse problem of doing inference. There is no silver bullet here. In fact there are tons of different algorithms for doing inference on probabilistic models, but luckily, since virtually all the important inference algorithms use sampling-based approaches, e.g.&nbsp;Hamiltonian Monte Carlo, Metropolis-Hastings, and Variational Inference, no matter which algorithm we use under the hood, we’ll end up with the same kind of output at the end—samples of the unknown parameters which have been drawn from the posterior.</p>
<p>To get some intuition for how we can do inference by sampling from the generative model,, we’re going to implement a very simple inference algorithm called <em>rejection sampling</em>. The key idea is based on the insight we discussed earlier that the posterior is essentially a slice through the joint distribution where the data is fixed to what we actually observed. Since simulating from the generative model is equivalent to drawing samples from the joint distribution, isolating simulation outcomes where the simmulated data is equal to the observed data is equivalent to sampling from the joint distribution along the slice, and hence equivalent to sampling from the posterior.</p>
<p>The algorithm for doing Bayesian inference via rejection sampling is as follows</p>
<ol type="1">
<li>Generate a sample <img src="https://latex.codecogs.com/png.latex?(%5Ctheta%5E*,y%5E*)"> from the generative model.</li>
<li>Keep the sample if <img src="https://latex.codecogs.com/png.latex?y%5E*=y_%7B%5Ctext%7Bobs%7D%7D">, otherwise discard it.</li>
<li>Repeat 1 and 2 until the desired number of retained samples is collected.</li>
<li>The retained samples of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> can be interpreted as samples from the posterior.</li>
</ol>
<p>Let’s do this in python. In our example, the order of the wins and losses over the <img src="https://latex.codecogs.com/png.latex?N=10"> match observations doesn’t matter, so we’ll focus on the number of wins. Since <img src="https://latex.codecogs.com/png.latex?%5Csum_iy_i=7"> in our observed data, we’ll isolate the simulation outcomes where <code>sum_y</code> equals 7.</p>
<div id="cell-22" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb11-2">outcome_df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y != @sum_y_obs'</span>).plot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>, kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scatter"</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"outcomes where $y^* </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">ne y_{</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">text</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{obs}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}$"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb11-3">outcome_df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y == @sum_y_obs'</span>).plot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>, kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scatter"</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"outcomes where $y^* = y_{</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">text</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{obs}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}$"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb11-4">ax.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sum_y_obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb11-5">ax.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sum_y_obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb11-6">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Samples from the Generative Model"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img" alt="scatter plot of samples from joint distribution with posterior slice highlighted"></p>
</figure>
</div>
</div>
</div>
<p>Boom! By isolating the outcomes where <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">, we effectively have samples from the posterior. Let’s draw a larger number of samples so we get an adequate sample from the posterior.</p>
<div id="cell-24" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># drawing a large number of samples and isolating outcomes where y = y_obs</span></span>
<span id="cb12-2">rejection_sampling_outcome_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span>, alpha_param, beta_param, N).query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y == @sum_y_obs'</span>)</span>
<span id="cb12-3"></span>
<span id="cb12-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># posterior samples from rejection sampling</span></span>
<span id="cb12-5">posterior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rejection_sampling_outcome_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>]</span></code></pre></div>
</div>
<div id="cell-25" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">posterior_samples.hist(bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color)</span>
<span id="cb13-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>)</span>
<span id="cb13-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Samples from the Posterior"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img" alt="histogram of posterior"></p>
</figure>
</div>
</div>
</div>
<p>Now let’s put all the pieces together.</p>
<div id="cell-27" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb14-2">prior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outcome_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>]</span>
<span id="cb14-3">prior_samples.hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prior samples"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb14-4">posterior_samples.hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"posterior samples"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb14-5">ax.plot(theta_grid, prior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Prior p(theta) ~ Beta(</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb14-6">ax.plot(theta_grid, posterior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Posterior p(theta|y_obs)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb14-7">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian Inference: Prior and Posterior'</span>)</span>
<span id="cb14-8">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'$\theta$ (Probability of Match Win)'</span>)</span>
<span id="cb14-9">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb14-10">plt.legend()</span>
<span id="cb14-11">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb14-12">plt.xlim(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb14-13">plt.ylim(bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb14-14">plt.tight_layout()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="plot of prior and posterior as line plot from grid approximation and histogram from sampling"></p>
</figure>
</div>
</div>
</div>
<p>In this figure we have:</p>
<ol type="1">
<li>the functional form of the prior</li>
<li>the functional form of the posterior from grid approximation</li>
<li>samples from the prior drawn directly from the generative model</li>
<li>and samples of the posterior obtained by applying rejection sampling to the generative model.</li>
</ol>
<p>Great! Our sampling algorithms are generating samples from the prior and the posterior which are consistent with the functional forms we computed earlier!</p>
<p>While we used rejection sampling here, regardless of what sampling algorithm we choose, we’ll end up with the same thing after inference—a set of samples from the posterior distribution for each unknown parameter. Once we have those samples, we’re ready to move to the interpretation and analysis step.</p>
</section>
</section>
<section id="step-3.-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="step-3.-interpretation">🔬 Step 3. Interpretation</h2>
<p>So how do we get insight into our analysis questions from this <code>posterior_samples</code> array? Well we’ve got samples from the posterior distribution of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> which represents our updated beliefs about the relative plausibility of different values of Fritz’s actual underlying RPS win rate. We can use them just like any other dataset to answer questions about his win rate.</p>
<p>Let’s start with getting a point estimate of his true win rate. We can simply take the mean of the samples.</p>
<div id="cell-31" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'point estimate of theta: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_samples)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>point estimate of theta: 0.5655783348185507</code></pre>
</div>
</div>
<p>To get a confidence interval, often called a <em>credible interval</em> in the Bayesian context, we can just pull the quantiles of the posterior distribution. Note there are fancier ways to do this, e.g.&nbsp;computing highest posterior density intervals (HPDIs), but conceptually we’re basically just looking at the quantiles of the sample.</p>
<div id="cell-33" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'89% credible interval of theta:: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>quantile(posterior_samples, [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.055</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.945</span>])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>89% credible interval of theta:: [0.42005907 0.70367915]</code></pre>
</div>
</div>
<p>What’s the probability that Fritz’s win rate is actually really good, say greater than 75%? We can just check the samples directly for the proportion greater than 0.75.</p>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'P[theta &gt; 0.75]: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>P[theta &gt; 0.75]: 0.011933174224343675</code></pre>
</div>
</div>
<p>This means our analysis implies theres only a 1.5% chance that his true win rate is larger than 75%.</p>
<p>Now that we’ve taken a look at interpreting the posterior samples to get some insight into the unknown parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> representing Fritz’s actual RPS win rate, we can take it one step further and make some predictions.</p>
<section id="posterior-predictive-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-distribution">Posterior Predictive Distribution</h3>
<p>We have one more character to meet in this cast of Bayesian players—the <em>posterior predictive</em> distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)"></p>
<p>It represents the most plausible distribution of <em>new</em> data <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bnew%7D%7D"> given that we observed data $y_{}, i.e.&nbsp;based on the posterior rather than the prior. Mathematically it is obtained by integrating the product of the likelihood for the new data and the posterior distribution over the parameter space.</p>
<p><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cint%20p(y_%7B%5Ctext%7Bnew%7D%7D%7C%5Ctheta)%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)%20d%5Ctheta"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7C%5Ctheta)"> is the likelihood of the new data, assuming a specific parameter value <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. It’s the same likelihood function we used before, evaluated on <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bnew%7D%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)"> is just the posterior distribution</li>
<li>the integral <img src="https://latex.codecogs.com/png.latex?%5Cint%20%5Cdots%20d%20%5Ctheta"> effectively “averages” the likelihood of the new data over all possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, weighted by how plausible each value is according to the posterior.</li>
</ul>
<p>In terms of the DGP, we can generate samples from the posterior predictive by</p>
<ol type="1">
<li>Drawing a sample of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from the posterior distribution.</li>
<li>Using this value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to generate a value of <img src="https://latex.codecogs.com/png.latex?y"> from the generative model.</li>
</ol>
<p>Let’s implement this in python.</p>
<div id="cell-37" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_posterior_predictive(posterior_samples, N):</span>
<span id="cb21-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame([</span>
<span id="cb21-3">        simulate_one_outcome(theta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>theta, N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb21-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> posterior_samples</span>
<span id="cb21-5">    ])</span>
<span id="cb21-6"></span>
<span id="cb21-7">posterior_predictive_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_posterior_predictive(posterior_samples, N)</span></code></pre></div>
</div>
<p>Here again, we can inspect the distribution using any familiar techniques for working with samples of data. Here’s a histogram of the posterior predictive.</p>
<div id="cell-39" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>].hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color)</span>
<span id="cb22-2">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Posterior Predictive Distribution"</span>)</span>
<span id="cb22-3">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum(y)"</span>)</span>
<span id="cb22-4">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"probability mass"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img" alt="posterior predictive of number of wins"></p>
</figure>
</div>
</div>
</div>
<p>We can use this for forecasting, e.g.&nbsp;what’s the probability that Fritz wins at least 7 games in his next round?</p>
<div id="cell-41" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Probability of winning &gt;= 7 in next round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability of winning &gt;= 7 in next round: 0.32537788385043753</code></pre>
</div>
</div>
<p>Here’s where things can get interesting. In addition to forecasting the outcome itself, we can also compute the probabilities of events that depend on the outcome. For example, let’s say Fritz has only $50 left in his wallet, and he wants to know the probability that he can cover his bill after the next round. Let’s assume each drink costs $12. We can compute that probability as follows.</p>
<div id="cell-43" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># probability that Fritz's next round bill is less than or equal to $50</span></span>
<span id="cb25-2"></span>
<span id="cb25-3">cost_per_drink <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">12.0</span></span>
<span id="cb25-4">posterior_predictive_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb25-5">    posterior_predictive_df</span>
<span id="cb25-6">    .assign(losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> x.sum_y)</span>
<span id="cb25-7">    .assign(bill <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: cost_per_drink <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x.losses)</span>
<span id="cb25-8">)</span>
<span id="cb25-9"></span>
<span id="cb25-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Probability next round bill &lt;= $50: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bill"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability next round bill &lt;= $50: 0.5369928400954654</code></pre>
</div>
</div>
</section>
</section>
<section id="summary-of-the-bayesian-workflow" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-bayesian-workflow">Summary of the Bayesian Workflow</h2>
<p>Wow, we covered a lot of ground today. Let’s summarize the key points.</p>
<p>The Bayesian Analysis Workflow has three major steps:</p>
<ol type="1">
<li>Modeling
<ul>
<li>We build a <em>generative model</em> that describes the relationships among key variables and unknown parameters to represent the data generating process.</li>
<li>We encode our prior knowledge about the relative plausibility of different parameter values in the <em>prior distribution</em> of the parameters.</li>
<li>We can use <em>prior predictive checks</em> to simulate outcome data from the generative model to sanity check our modeling assumptions.</li>
</ul></li>
<li>Inference
<ul>
<li>Based on our modeling assumptions, we can use observed data to infer the <em>posterior distribution</em>, which quantifies the relative plausibility of different values of the unknown parameters after observing data.</li>
<li>We can view simulations from the generative model as sampling from the joint distribution of data and parameters, and we can view the posterior as the result of conditioning the joint distribution on the data we actually observed.</li>
<li>We can do inference by using sampling-based inference algorithms like rejection sampling, which uses logic on top of our generative model to isolate samples from the posterior distribution.</li>
</ul></li>
<li>Interpretation
<ul>
<li>After inference we can use data analysis tools to summarize the samples from the posterior distribution to compute point estimates, intervals, and probabilities of interest.</li>
<li>If we simulate data from our generative model using the posterior rather than the prior, we get samples from the <em>posterior predictive </em>distribution, which predicts future outcomes given observed data.</li>
<li>Again, we can analyze these posterior predictive samples to compute point estimates, intervals, or probabilities of future outcomes.</li>
</ul></li>
</ol>
<p>Phew! Hopefully that’s a helpful introduction to the Bayesian workflow and the major ideas behind it. The techniques we looked at here are mostly for pedagogical purposes; if you want to apply Bayesian methods to practical problems, you’ll want to use a probabilistic programming language like PyMC, pyro, or stan. Maybe we’ll get into some of those tools in future posts. See you then!</p>
<section id="resources" class="level3">
<h3 class="anchored" data-anchor-id="resources">Resources</h3>
<ul>
<li><a href="https://xcelab.net/rm/">Statistical Rethinking</a> - This page links to where you can obtain the book and also to a number of repos where folks have ported the R and Stan code examples to python libraries like PyMC and pyro.</li>
</ul>
</section>
<section id="reader-exercises" class="level3">
<h3 class="anchored" data-anchor-id="reader-exercises">Reader Exercises</h3>
<p>You didn’t think you’d get away without homework did you? Here are a couple suggestions for exercises.</p>
<ul>
<li>compute the posterior predictive distribution <img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)"> for the RPS example using grid approximation.</li>
<li>Suppose that each RPS match was played as best out of three. Rewrite the generative model to generate both sub-match and match outcomes. Do inference with rejection sampling. Use your model to find the probability that Fritz wins his next match by winning two submatches in a row.</li>
</ul>
</section>
</section>

 ]]></description>
  <category>bayesian</category>
  <category>python</category>
  <guid>https://randomrealizations.com/posts/bayesian-modeling-primer/</guid>
  <pubDate>Wed, 04 Jun 2025 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/bayesian-modeling-primer/hist.png" medium="image" type="image/png" height="107" width="144"/>
</item>
<item>
  <title>Analyzing After Tax Retirement Income: Roth vs. Traditional 401(k)</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/traditional-vs-roth-401k/</link>
  <description><![CDATA[ 




<p>Today we’re taking a break from our typical hard hitting algorithm deep dives for a quick foray into the world of personal finance. We’ll take on a question I recently encountered while setting up my retirement account with my new employer—which is more efficient, the traditional 401(k) or the Roth 401(k)? US-based readers will recognize these as the two main types of employer-sponsored retirement accounts. When I searched for traditional vs Roth 401(k), the articles I found gave only very hand-wavy guidance on which is better in a given situation. So, today I’ll share my quantitative analysis of which account type provides superior performance for a given set of personal circumstances. We’ll implement the analysis in python, so you can run the numbers for your own situation and determine which employer-sponsored account type is better for you.</p>
<section id="traditional-401k-vs-roth-401k" class="level2">
<h2 class="anchored" data-anchor-id="traditional-401k-vs-roth-401k">Traditional 401(k) vs Roth 401(k)</h2>
<p>I’ll let JLCollins explain the <a href="https://jlcollinsnh.com/2015/06/02/stocks-part-viii-the-401k-403b-tsp-ira-roth-buckets/">background on 401(k)s</a>; read that post first if you’re not already familiar with the concepts of taxable accounts, IRAs, 401(k)s, and the basic rules of Roth vs traditional. The key distinction is * In a traditional 401(k), money you contribute now is deducted from your taxable income, meaning you’ll pay less in income tax now. During retirement however, withdrawals from the account will count toward your taxable income, so you’ll pay tax then. * In a Roth 401(k), money you contribute now does count toward your taxable income, meaning you’ll pay income tax on any contributions now. During retirement however, withdrawals do not count toward your taxable income and are therefore tax free.</p>
<p>Essentially you can either pay tax now (Roth) or pay tax later (traditional). The hand-wavy advice points out that which account is better for you depends on your income tax rate now versus your income tax rate during retirement. High tax rate now and low tax rate during retirement could favor traditional, while low tax rate now and high tax rate during retirement could favor Roth. Let’s put some numbers on this advice.</p>
<p>I’ll assume that you’re following the <a href="https://www.mrmoneymustache.com/2011/04/10/post-4-what-am-i-supposed-to-do-with-all-this-money/">sage advice of Mr.&nbsp;Money Mustache</a> and (after paying off any high-interest debt) maxing out your 401(k) contribution for the year. In 2024, the IRS has set a maximum combined contribution of $23,000; i.e.&nbsp;the sum of your Roth and traditional contributions cannot exceed this limit. Also, once you contribute to these accounts, you may not begin withdrawals (without penalty) until the age of 59.5.</p>
</section>
<section id="analysis-formulation" class="level2">
<h2 class="anchored" data-anchor-id="analysis-formulation">Analysis Formulation</h2>
<p>Let’s state the question precisely—which account type will yield me the most money during retirement after withdrawal and after all taxes are paid? Let’s think through the Roth vs traditional scenarios, setting aside the same amount of money today and liquidating the entire account at retirement; we’ll compare how much money we have at retirement after liquidating and settling any tax obligations.</p>
<p><strong>Roth</strong>: I contribute <code>contribution = 23_000</code> now, plus I pay income tax on this contribution in the amount of <code>current_income_tax_rate * contribution</code>. Over the years from now to retirement <code>retirement_age - current_age</code>, my contribution grows at some average long term yearly rate <code>investment_growth_rate</code>. At retirement, I liquidate the entire account, paying no income tax on the proceeds.</p>
<p><strong>Traditional</strong>: I contribute <code>contribution = 23_000</code> now. For fair comparison with the Roth, I invest an additional amount <code>current_income_tax_rate * contribution</code> (the extra income tax I would have paid had I chosen the Roth) in a normal taxable investment account as well. Over the time from now to retirement, the 401(k) and the taxable account both grow at the average long term rate <code>investment_growth_rate</code>. However, in the taxable account, I’ll also need to pay income tax every year on any dividends that I earn; the S&amp;P500 has recently paid out 1.5-2% in dividends each year, let’s call it <code>dividend_rate</code>. At retirement, I liquidate both accounts, paying income tax on the proceeds from the 401(k) at the rate of <code>retirement_income_tax_rate</code> and paying capital gains tax on the proceeds from the taxable account at the rate of <code>retirement_capital_gains_tax_rate</code>.</p>
<p>Let’s code up a function that takes in all our parameters and returns the total liquidation value after taxes of the Roth versus traditional 401(k)s as described above.</p>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span></code></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_401k_liquidation_value(</span>
<span id="cb2-2">    current_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">37</span>,</span>
<span id="cb2-3">    current_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.35</span>,</span>
<span id="cb2-4">    contribution <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">23_000</span>,</span>
<span id="cb2-5">    retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">59.5</span>,</span>
<span id="cb2-6">     investment_growth_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.07</span>,</span>
<span id="cb2-7">     dividend_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>,</span>
<span id="cb2-8">    retirement_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.24</span>,</span>
<span id="cb2-9">    retirement_capital_gains_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0%, 15%, 20%</span></span>
<span id="cb2-10">):</span>
<span id="cb2-11"></span>
<span id="cb2-12">    investment_growth_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> investment_growth_rate) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_age)</span>
<span id="cb2-13">    dividend_income_tax_drag_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> dividend_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> current_income_tax_rate) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_age)</span>
<span id="cb2-14"></span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Roth 401k</span></span>
<span id="cb2-16">    roth_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contribution </span>
<span id="cb2-17">    roth_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor</span>
<span id="cb2-18">    total_roth_401k_liquidation_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roth_401k_value</span>
<span id="cb2-19">    </span>
<span id="cb2-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># traditional  401k</span></span>
<span id="cb2-21">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contribution </span>
<span id="cb2-22">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> contribution </span>
<span id="cb2-23">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor</span>
<span id="cb2-24">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dividend_income_tax_drag_factor</span>
<span id="cb2-25">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> retirement_income_tax_rate)</span>
<span id="cb2-26">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> retirement_capital_gains_tax_rate)</span>
<span id="cb2-27">    total_traditional_401k_liquidation_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> taxable_account_value</span>
<span id="cb2-28"></span>
<span id="cb2-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb2-30">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'traditional'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(total_traditional_401k_liquidation_value), </span>
<span id="cb2-31">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'roth'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(total_roth_401k_liquidation_value)</span>
<span id="cb2-32">    }</span></code></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">get_401k_liquidation_value()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>{'traditional': 106882, 'roth': 105405}</code></pre>
</div>
</div>
<p>Somehow it’s not surprising that these two options seem to yield very similar after-tax performance—no arbitrage right?</p>
<p>Let’s write a function to perturb some of our parameter values to see under what conditions one option dominates the other.</p>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_liquidation_value_by_parameter_values(param, grid_values, func<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_401k_liquidation_value):</span>
<span id="cb5-2">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [func(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>{param: x}) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> grid_values]</span>
<span id="cb5-3">    df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(y, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.Series(grid_values, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param))</span>
<span id="cb5-4">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb5-5">    df.plot(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb5-6">    plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liquidation value'</span>)</span>
<span id="cb5-7">    plt.title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Liquidation Value at Retirement by </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb5-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> fig, ax</span></code></pre></div>
</div>
<section id="income-tax-rate-at-retirement" class="level3">
<h3 class="anchored" data-anchor-id="income-tax-rate-at-retirement">Income Tax Rate at Retirement</h3>
<p>It seems that income tax rate at retirement is by far the most important determining factor in whether traditional or Roth 401(k) is a better option.</p>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_income_tax_rate'</span>, np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.38</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So given the other parameters I’ve set, Roth outperforms traditional when our income tax rate in retirement exceeds about 25%. <a href="https://www.irs.gov/filing/federal-income-tax-rates-and-brackets">According to the IRS</a> in 2023, an individual tax payer is in the 24% bracket if their income is between about $95k and $180k. So, how much income do you expect to pull in retirement? If we’re really building FIRE wealth, the kind indicated by Mr.&nbsp;Money Mustache and JLCollins, our income in retirement could easily exceed $180k, which would push us into the 32% bracket where Roth is more efficient than traditional.</p>
</section>
<section id="capital-gains-tax-rate-at-retirement" class="level3">
<h3 class="anchored" data-anchor-id="capital-gains-tax-rate-at-retirement">Capital Gains Tax Rate at Retirement</h3>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_capital_gains_tax_rate'</span>, np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.20</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In 2023, <a href="https://www.irs.gov/filing/federal-income-tax-rates-and-brackets">according to the IRS</a> as a single tax payer, if your income is between $44k and $492k, you’ll pay 15% capital gains. Over $492k you’ll jump up to 20% where Roth dominates traditional.</p>
</section>
<section id="retirement-age" class="level3">
<h3 class="anchored" data-anchor-id="retirement-age">Retirement Age</h3>
<div id="cell-16" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_age'</span>, np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">59.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">75</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For retirement ages beyond 59.5, traditional’s edge over Roth grows slightly.</p>
</section>
</section>
<section id="bottom-line" class="level2">
<h2 class="anchored" data-anchor-id="bottom-line">Bottom Line</h2>
<p>When I plugged in my actual parameters, I found that because I was only employed for 6 months last year, my current income tax rate pushed me into the regime where Roth performs better than traditional. However for this next year, I expect to be in a higher income tax bracket where traditional will be a better deal than Roth.</p>
<p>That said, the most important factor is your income tax rate at the time of withdrawal during retirement, which is based on your taxable income at that time. But how, I hear you asking, am I supposed to know what to plug in for my post-retirement income? That quantity is unknown. This illuminates the fundamental limitation of this kind of analysis—what to do about uncertain inputs to the calculation? That’s a question that we might take on in a future post, so stay tuned!</p>
</section>

 ]]></description>
  <category>personal finance</category>
  <guid>https://randomrealizations.com/posts/traditional-vs-roth-401k/</guid>
  <pubDate>Sat, 14 Dec 2024 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/enso-thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>SHAP from Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/shap-from-scratch/</link>
  <description><![CDATA[ 




<p>Ahh, SHAP. As you know it’s become one of the leading frameworks for explaining ML model predictions. I’d guess it’s popularity is due to its appealing theoretical basis, its universal applicability to any type of ML model, and its easy-to-use python package. SHAP promises to turn your black box ML model into a nice friendly interpretable model. The hilarious irony is that, when I first started using it in my work, SHAP itself was a complete black box to me. In this post, we’ll change all that by diving into the SHAP paper, illuminating the key theoretical ideas behind its development step by step, and implementing it from scratch in python. If you aren’t already familiar with how to compute and interpret SHAP values in practice, I’d recommend that you go check out the <a href="https://shap.readthedocs.io/en/latest/index.html">documentation for the shap python package</a> before diving into this post.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_main.jpg" class="img-fluid figure-img"></p>
<figcaption>Snow, trees, and mountains overlook Lake Tahoe.</figcaption>
</figure>
</div>
<section id="what-is-shap" class="level2">
<h2 class="anchored" data-anchor-id="what-is-shap">What is SHAP?</h2>
<p>SHAP (SHapley Additive exPlanations) is a conceptual framework for creating explanations of ML model predictions. The term also refers to a set of computational methods for generating these explanations and a python library which implements them. The “SHAP” <a href="https://en.wikipedia.org/wiki/Backronym">backronym</a> was introduced in <a href="https://arxiv.org/abs/1705.07874">Lundberg and Lee 2017</a>, which I call the <em>SHAP paper</em>, that expanded on several previously existing ideas which we’ll build up in the following sections. The key concepts are:</p>
<ul>
<li><em>Shapley values</em>, a concept from cooperative game theory which originally had nothing to do with machine learning</li>
<li><em>Shapley regression values</em>, which showed how to use Shapley values to generate explanations of model predictions</li>
<li><em>Shapley sampling values</em>, which offered a computationally tractable way to compute Shapley regression values for any type of model.</li>
</ul>
<p>The SHAP paper tied Shapley regression values and several other existing model explanation methods together by showing they are all members of a class called “additive feature attribution methods.” Under the right conditions, these additive feature attribution methods can generate Shapley values, and when they do we can call them SHAP values.</p>
<p>After establishing this theoretical framework, the authors go on to discuss various computational methods for computing SHAP values; some are model-agnostic, meaning they work with any type of model, and others are model-specific, meaning they work for specific types of models. It turns out that the previously existing Shapley sampling values method is a model-agnostic approach, but while it’s the most intuitive, computationally speaking it’s relatively inefficient. Thus the authors propose a novel model-agnostic approach called Kernel SHAP, which is really just <a href="https://lime-ml.readthedocs.io/en/latest/">LIME</a> parameterized to yield SHAP values.</p>
<p>Model-specific approaches can be potentially much more efficient than model-agnostic ones by taking advantage of model idiosyncrasies. For example, there is an analytical solution for the SHAP values of linear models, so Linear SHAP is extremely efficient. Similarly, Deep SHAP (proposed in the SHAP paper) and Tree SHAP (proposed later in <a href="https://www.sciencedirect.com/science/article/pii/S2666827022000500#b20">Lundberg et al 2020</a>) take advantage of idiosyncrasies of deep learning and tree-based models to compute SHAP values efficiently.</p>
<p>The important thing about these different methods is that they provide computationally tractable ways to compute SHAP values, but ultimately, they are all based on the Shapley sampling values method—the original method to compute what we now call SHAP values. Thus, for the remainder of this post, we’ll focus on this method, building it up from Shapley values to Shapley regression values to Shapley sampling values and ultimately implementing it from scratch in python.</p>
</section>
<section id="shapley-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-values">Shapley Values</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Shapley_value">Shapley value</a> is named in honor of Nobel prize winning economist Loyd Shapley who introduced the idea in the field of coalitional game theory in the 1950’s. Shapley proposed a way to determine how a coalition of players can fairly share the payout they receive from a cooperative game. We’ll introduce the mathematical formalism in the next section, so for now let’s just touch on the intuition for the approach. Essentially, the method distributes the payout among the players according to the expected contribution of each player across all possible combinations of the players. The thought experiment works as follows:</p>
<ol type="1">
<li>Draw a random permutation (ordering) of the players.</li>
<li>Have the first player play alone, generating some payout. Then have the first two players play together, generating some payout. Then the first three, and so on.</li>
<li>As each new player is added, attribute the change in the payout to this new player.</li>
<li>Repeat this experiment for all permutations of the players. A player’s Shapley value is the average change in payout (across all permutations) when that player is added to the game.</li>
</ol>
<p>Next we’ll see how this idea can be applied to model explanations.</p>
</section>
<section id="shapley-regression-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-regression-values">Shapley Regression Values</h2>
<p>The next idea came from <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.446">Lipovetsky and Conklin 2001</a>, who proposed a way to use Shapley values to explain the predictions of a linear regression model. <em>Shapley regression values</em> assign an importance value to each feature that represents the effect on the model prediction of including that feature. The basic idea is to train a second model without the feature of interest, and then to compare the predictions from the model with the feature and the model without the feature. This procedure of training two models and comparing their predictions is repeated for all possible subsets of the other features; the average difference in predictions is the Shapley value for the feature of interest.</p>
<p>The Shapley value for feature <img src="https://latex.codecogs.com/png.latex?i"> on instance <img src="https://latex.codecogs.com/png.latex?x"> is given by equation 4 in the SHAP paper:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cphi_i%20=%20%5Csum_%7BS%20%5Csubseteq%20F%20%5Csetminus%20%5C%7Bi%5C%7D%7D%0A%5Cfrac%7B%7CS%7C!(%7CF%7C%20-%20%7CS%7C%20-%201)!%7D%7B%7CF%7C!%7D%0A%5Bf_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D(x_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D)%20-%20f_S(x_S)%20%5D%0A"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cphi_i"> is the Shapley value for feature of interest <img src="https://latex.codecogs.com/png.latex?i">,</li>
<li>the <img src="https://latex.codecogs.com/png.latex?%5Csubseteq"> symbol indicates the item on its left is a subset of the object on its right,</li>
<li><img src="https://latex.codecogs.com/png.latex?F"> is the set of all features,</li>
<li>the vertical bars indicate the number of elements in a set, e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?%7CF%7C"> is the total number of features,</li>
<li><img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D"> is the set of all features except the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?S"> is a particular subset of features not including the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a “subset model”—a model that uses only the features in <img src="https://latex.codecogs.com/png.latex?S"> for both training and prediction,</li>
<li>and <img src="https://latex.codecogs.com/png.latex?f_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D"> is asubset model using features in <img src="https://latex.codecogs.com/png.latex?S"> and the feature of interest.</li>
</ul>
<p>To reiterate, this is the most important equation when it comes to understanding SHAP, as it defines the Shapley value; let’s make sure we understand what’s going on by implementing it in python.</p>
<p>We start with the feature subsets. Notice that the sum is indexed over all subsets of <img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D">, which is the set of all features except the <img src="https://latex.codecogs.com/png.latex?i">th feature, the one we’re calculating the Shapley value for. Let’s write a function that takes a list of items and returns an iterable that yields all possible subsets of those items.</p>
<div id="cell-7" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations </span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_all_subsets(items):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span>  get_all_subsets([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]):</span>
<span id="cb1-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(2,)
(0, 1)
(0, 2)
(1, 2)
(0, 1, 2)</code></pre>
</div>
</div>
<p>To get all subsets of features, other than the feature of interest, we could do something like this.</p>
<div id="cell-9" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb3-2">    all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb3-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> get_all_subsets(all_other_features)</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb3-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(3,)
(0, 1)
(0, 3)
(1, 3)
(0, 1, 3)</code></pre>
</div>
</div>
<p>So for each of the feature subsets, we’ll need to calculate the summand, which is the product of a quotient with a bunch of factorials and the difference in predicted values between two subset models. Let’s start with those subset models. Subset model <img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a model trained only on the features in subset <img src="https://latex.codecogs.com/png.latex?S">. We can write a function that takes an untrained model, a training dataset, a feature subset to use, and a single instance to predict on; the function will then train a model using only features in the subset, and it will issue a prediction for the single instance we gave it.</p>
<div id="cell-11" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> subset_model(model, X_train, y_train, feature_subset, instance):</span>
<span id="cb5-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance.shape) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Instance must be a 1D array'</span></span>
<span id="cb5-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(feature_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y.mean() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a model with no features predicts E[y]</span></span>
<span id="cb5-5">    X_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.take(feature_subset, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-6">    model.fit(X_subset, y_train)</span>
<span id="cb5-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model.predict(instance.take(feature_subset).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
</div>
<p>Next let’s have a look at <img src="https://latex.codecogs.com/png.latex?%7CS%7C!(%7CF%7C-%7CS%7C-1)!/%7CF%7C!">. The keen reader will notice this factor kind of looks like the answers to those combinatorics questions like how many unique ways can you order the letters in the word MISSISSIPPI. The combinatorics connection is that Shapley values are defined in terms of all permutations of the players , where the included players come first, then the player of interest, followed by the excluded players. In ML models, the order of features doesn’t matter, so we can work with unordered subsets of features, scaling the prediction difference terms by the number of permutations that involve the same sets of included and excluded features. With that in mind, we can see that including the factor in each term of the sum gives us a weighted average over all feature combinations, where the numerator gives the number of permutations in which the included features come first, followed by the feature of interest, followed by the excluded features, and the denominator is the total number of feature permutations.</p>
<div id="cell-13" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb6-2"></span>
<span id="cb6-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> permutation_factor(n_features, n_subset):</span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> factorial(n_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features)</span></code></pre></div>
</div>
<p>Now we can put these pieces together to compute equation 4—a single Shapley regression value for a single instance and feature of interest.</p>
<div id="cell-15" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_single_shap_value(untrained_model,</span>
<span id="cb7-2">                              X_train,</span>
<span id="cb7-3">                              y_train,</span>
<span id="cb7-4">                              feature_of_interest,</span>
<span id="cb7-5">                              instance):</span>
<span id="cb7-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb7-7">    n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb7-8">    shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb7-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb7-10">        n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb7-11">        prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-12">            untrained_model,</span>
<span id="cb7-13">            X_train, y_train,</span>
<span id="cb7-14">            subset,</span>
<span id="cb7-15">            instance</span>
<span id="cb7-16">        )</span>
<span id="cb7-17">        prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-18">            untrained_model,</span>
<span id="cb7-19">            X_train, y_train,</span>
<span id="cb7-20">            subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature_of_interest,),</span>
<span id="cb7-21">            instance</span>
<span id="cb7-22">        )</span>
<span id="cb7-23">        factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_factor(n_features, n_subset)</span>
<span id="cb7-24">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb7-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_value</span></code></pre></div>
</div>
<p>Let’s use this function to compute a single Shapley regression value for a linear model and a small training dataset with 3 features.</p>
<div id="cell-17" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression </span>
<span id="cb8-3"></span>
<span id="cb8-4">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb8-5"></span>
<span id="cb8-6">compute_single_shap_value(untrained_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>LinearRegression(),</span>
<span id="cb8-7">                          X_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X, y_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y,</span>
<span id="cb8-8">                          feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,</span>
<span id="cb8-9">                          instance<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>-0.07477140629329351</code></pre>
</div>
</div>
<p>That gives us a single Shapley value corresponding to a single feature value in a single instance. To get useful model explanations, we’d need to compute Shapley values for each feature of each instance in some dataset of instances. You might notice there’s a big problem with the formulation above. Namely, we are going to have to train a whole bunch of new subset models—one for each subset of the features. If our model has <img src="https://latex.codecogs.com/png.latex?M"> features, we’ll have to train <img src="https://latex.codecogs.com/png.latex?2%5EM"> models, so this will get impractical in a hurry, especially if we’re trying to train anything other than linear models.</p>
</section>
<section id="shapley-sampling-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-sampling-values">Shapley Sampling Values</h2>
<p>Next, <a href="https://link.springer.com/article/10.1007/s10115-013-0679-x">Štrumbelj and Kononenko 2014</a> proposed <em>Shapley sampling values</em>, a method which provides a much more efficient way to approximate the subset models used to calculate Shapley regression values. In this approach, the effect of removing some features from the model is approximated by the conditional expectation of the model given the known features.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20f_S(x_S)%20%20:=%20E%5Bf(x)%20%7C%20x_S%5D%20%20"></p>
<p>This means we’re approximating the output of a subset model by averaging over outputs of the full model. That’s great because now we don’t have to train all those new subset models, we can just query our full model over some set of inputs and average over the outputs to compute these conditional expectation subset models.</p>
<p>Now how exactly do we compute that conditional expectation? First we rewrite the above conditional expectation (equation 10 in the SHAP paper)</p>
<p><img src="https://latex.codecogs.com/png.latex?%20E%5Bf(x)%20%7C%20x_S%5D%20%20=%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> is the set of excluded or missing features. Beside this equation in the paper they give the note “expectation over <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D%20%7C%20x_S">, which means we’re taking the expectation over the missing features given the known features. Then we get another step (equation 11)</p>
<p><img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D%20%5Capprox%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%20%5Bf(x)%5D"></p>
<p>Now it’s not an equality but an approximation. The authors give the note “assume feature independence”. The intuition here is that if the missing features are correlated with the known features, then their distribution depends on the particular values taken by the known features. But here the authors make the simplifying assumption that known and missing features are independent, which allows us to replace the conditional expectation with an unconditional expectation over the missing features.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>So is this assumption that features in <img src="https://latex.codecogs.com/png.latex?S"> are independent from features in <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> a problem? The short answer is… maybe 🤷‍♀️? It’s potentially problematic enough that people have worked out some ways to relax this assumption, e.g.&nbsp;<a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.PartitionExplainer.html">partition masking</a>, but that makes <em>Owen values</em> instead of Shapley values, so we’ll save it for another post.</p>
</div>
</div>
<p>Anyway, how do we compute this unconditional expectation over the missing features in practice? We’ll need to use a so-called <em>background dataset</em>, which is just some set of observations of our feature variables that represents their distribution. A good candidate is the training data we used to train our model. Štrumbelj and Kononenko 2014 propose a way to estimate this conditional expectation using resampling of the background dataset.</p>
<p>The idea is to notice that the instance of interest <img src="https://latex.codecogs.com/png.latex?x"> is a feature vector comprised of the set of “known” features <img src="https://latex.codecogs.com/png.latex?x_S"> and the set of excluded features <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D"> such that <img src="https://latex.codecogs.com/png.latex?x=%5C%7Bx_S,x_%7B%5Cbar%7BS%7D%7D%20%5C%7D">. Our resampling scheme will be based on constructing “masked” samples <img src="https://latex.codecogs.com/png.latex?x%5E*=%5C%7Bx_S,z_%7B%5Cbar%7BS%7D%7D%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D"> are values of the missing features drawn from some random observation in the background dataset. We can then compute an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)"> of the conditional expectation <img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%5Bf(x)%5D"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bk=1%7D%5En%20f(%5C%7Bx_S,%20z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D%20%5C%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D"> is the vector of values of the excluded features from the <img src="https://latex.codecogs.com/png.latex?k">-th row of the background dataset. Algorithmically, we can view this as first drawing a sample of observations from the background dataset, second “masking” features in <img src="https://latex.codecogs.com/png.latex?S"> in the sampled background dataset by replacing the observed values <img src="https://latex.codecogs.com/png.latex?z_S"> on each row with the values in the instance <img src="https://latex.codecogs.com/png.latex?x_S">, third using the full model <img src="https://latex.codecogs.com/png.latex?f"> to predict on each of these masked samples in the background dataset, and finally averaging over these predictions. We can implement a new subset model function that takes a fully trained model, a background dataset,a feature subset, and an instance for explanation and returns an approximation of the subset model prediction.</p>
<div id="cell-21" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> subset_model_approximation(trained_model, </span>
<span id="cb10-4">                               background_dataset,</span>
<span id="cb10-5">                               feature_subset,  </span>
<span id="cb10-6">                               instance):</span>
<span id="cb10-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">""" </span></span>
<span id="cb10-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Approximate subset model prediction  (Equation 11)</span></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    \hat{f}_S(x) = E_{x_{\hat{S</span><span class="re">}}}</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[f_S(x)]</span></span>
<span id="cb10-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    for feature subset S on single instance x</span></span>
<span id="cb10-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb10-12">    masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset.copy()</span>
<span id="cb10-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb10-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb10-15">            masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb10-16">    conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb10-17">        trained_model.predict(masked_background_dataset)</span>
<span id="cb10-18">    )</span>
<span id="cb10-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>If we replace our <code>subset_model</code> function with this new <code>subset_model_approximation</code> function in our <code>compute_single_shap_value</code> function from earlier, then we’ll be computing Shapley sampling values. And according to the SHAP paper: “if we assume feature independence when approximating conditional expectations (using Equation 11 to estimate subset model output) … then SHAP values can be estimated directly using the Shapley sampling values method.” That means we’ll be computing SHAP values!</p>
</section>
<section id="how-to-implement-shap-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="how-to-implement-shap-from-scratch">How to Implement SHAP from Scratch</h2>
<p>Let’s put the pieces together and implement a class for a model explainer that computes SHAP values via the Shapley sampling values method. We’ll talk through a couple of points after the code.</p>
<div id="cell-24" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Any, Callable, Iterable</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ShapFromScratchExplainer():</span>
<span id="cb11-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb11-8">                 model: Callable[[np.ndarray], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>], </span>
<span id="cb11-9">                 background_dataset: np.ndarray,</span>
<span id="cb11-10">                 max_samples: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb11-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb11-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> max_samples:</span>
<span id="cb11-13">            max_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(max_samples, background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]) </span>
<span id="cb11-14">            rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.default_rng()</span>
<span id="cb11-15">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.choice(background_dataset, </span>
<span id="cb11-16">                                                 size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_samples, </span>
<span id="cb11-17">                                                 replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb11-19">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset</span>
<span id="cb11-20"></span>
<span id="cb11-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> shap_values(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb11-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"SHAP Values for instances in DataFrame or 2D array"</span></span>
<span id="cb11-23">        shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty(X.shape)</span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb11-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-26">                shap_values[i, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._compute_single_shap_value(j, X[i, :])</span>
<span id="cb11-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_values</span>
<span id="cb11-28">       </span>
<span id="cb11-29">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _compute_single_shap_value(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-30">                                   feature: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb11-31">                                   instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb11-33">        n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance)</span>
<span id="cb11-34">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb11-35">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_other_feature_subsets(n_features, feature):</span>
<span id="cb11-36">            n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb11-37">            prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-38">                subset, </span>
<span id="cb11-39">                instance</span>
<span id="cb11-40">            )</span>
<span id="cb11-41">            prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-42">                subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature,), </span>
<span id="cb11-43">                instance</span>
<span id="cb11-44">            )</span>
<span id="cb11-45">            factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._permutation_factor(n_features, n_subset)</span>
<span id="cb11-46">            shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb11-47">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_value</span>
<span id="cb11-48">    </span>
<span id="cb11-49">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _get_all_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, items: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Iterable:</span>
<span id="cb11-50">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb11-51">    </span>
<span id="cb11-52">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _get_all_other_feature_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, feature_of_interest):</span>
<span id="cb11-53">        all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb11-54">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_subsets(all_other_features)</span>
<span id="cb11-55"></span>
<span id="cb11-56">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _permutation_factor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, n_subset):</span>
<span id="cb11-57">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (</span>
<span id="cb11-58">            factorial(n_subset) </span>
<span id="cb11-59">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb11-60">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features) </span>
<span id="cb11-61">        )</span>
<span id="cb11-62">    </span>
<span id="cb11-63">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _subset_model_approximation(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-64">                                    feature_subset: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, ...], </span>
<span id="cb11-65">                                    instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-66">        masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset.copy()</span>
<span id="cb11-67">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-68">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb11-69">                masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb11-70">        conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb11-71">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model(masked_background_dataset)</span>
<span id="cb11-72">        )</span>
<span id="cb11-73">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>The <code>SHAPExplainerFromScratch</code> API is similar to that of the <a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html"><code>KernelExplainer</code></a> from the python library, taking two required arguments during instantiation:</p>
<ul>
<li><code>model</code>: “User supplied function that takes a matrix of samples (# samples x # features) and computes the output of the model for those samples.” That means if our model is a scikit-learn model, we’ll need to pass in its predict method, not the model object itself.</li>
<li><code>background_dataset</code>: “The background dataset to use for integrating out features.” We know about this idea from the Shapley sampling values section above; a good choice for this data could be the training dataset we used to fit the model. By default, we’ll use all the rows of this background dataset, but we’ll also implement the ability to sample down to the desired number of rows with an argument called <code>max_samples</code>.</li>
</ul>
<p>Like the <code>KernelExplainer</code>, this class has a method called <code>shap_values</code> which estimates the SHAP values for a set of instances. It takes an argument <code>X</code> which is “a matrix of samples (# samples x # features) on which to explain the model’s output.” This <code>shap_values</code> method just loops through each feature value of each instance of the input samples <code>X</code> and calls an internal method named <code>_compute_single_shap_value</code> to compute each SHAP value. The <code>_compute_single_shap_value</code> method is the real workhorse of the class. It implements equation 4 from the SHAP paper as described in the Shapley regression values section above by calling a few other internal helper methods corresponding to functions that we’ve already written.</p>
</section>
<section id="testing-the-implementation" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-implementation">Testing the Implementation</h2>
<p>Let’s check our work by comparing SHAP values computed by our implementation with those from the SHAP python library. We’ll use our old friend the diabetes dataset, training a linear model, a random forest, and a gradient boosting machine.</p>
<div id="cell-27" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_diabetes</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb12-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GradientBoostingRegressor, RandomForestRegressor</span>
<span id="cb12-5"></span>
<span id="cb12-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_diabetes(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, return_X_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb12-7">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb12-8">                                                    random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb12-9"></span>
<span id="cb12-10">lin_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-11">rfr_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomForestRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-12">gbt_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>Here’s a little function to compare the SHAP values generated by our implementation and those from the library <code>KernelExplainer</code>.</p>
<div id="cell-29" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shap</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compare_methods(model, X_background, X_instances):</span>
<span id="cb13-4">        </span>
<span id="cb13-5">    library_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shap.KernelExplainer(model.predict, X_background)</span>
<span id="cb13-6">    library_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> library_explainer.shap_values(X_instances)</span>
<span id="cb13-7"></span>
<span id="cb13-8">    from_scratch_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ShapFromScratchExplainer(model.predict, X_background)</span>
<span id="cb13-9">    from_scratch_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> from_scratch_explainer.shap_values(X_instances)</span>
<span id="cb13-10"></span>
<span id="cb13-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.allclose(library_shap_values, from_scratch_shap_values)</span></code></pre></div>
</div>
<div id="cell-30" class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">compare_methods(lin_model, </span>
<span id="cb14-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb14-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"995f763174824efebecb5c2522c3f6f5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">compare_methods(rfr_model, </span>
<span id="cb16-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb16-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9468451774b4c758a7696ff10fabc74","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">compare_methods(gbt_model, </span>
<span id="cb18-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb18-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8bdea22fab86440db68c4a669beaf7df","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>True</code></pre>
</div>
</div>
<p>Beautiful! Our Implementation is consistent with the SHAP library explainer!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well I hope this one was helpful to you. The research phase actually took me a lot longer than I expected; it just took me a while to figure out what SHAP really is and how those different ideas and papers fit together. I thought the implementation itself was pretty fun and relatively easy. What do you think?</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1705.07874">The SHAP Paper (Lundberg and Lee, 2017)</a></li>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning by Christoph Molnar</a></li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>from scratch</category>
  <guid>https://randomrealizations.com/posts/shap-from-scratch/</guid>
  <pubDate>Sun, 04 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_thumb.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The Ultimate Guide to XGBoost Parameter Tuning</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/</link>
  <description><![CDATA[ 




<p>Ahh, the dark art of hyperparameter tuning. It’s a key step in the machine learning workflow, and it’s an activity that can easily be overlooked or be overkill. Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils. Today I’ll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework. I’ll give you some intuition for how to think about the key parameters in XGBoost, and I’ll show you an efficient strategy for parameter tuning GBTs. I’ll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like. You can download a notebook with this tuning workflow from my <a href="https://github.com/mcb00/ds-templates">data science templates repository</a>. Finally we’ll wrap up with the kind of cautionary tale data scientists tell their colleagues around the campfire about when all this fancy hyperparameter tuning can backfire catastrophically—ignore at your own peril.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_main.jpg" class="img-fluid figure-img"></p>
<figcaption>Lunar halo on a frosty night in Johnson City, TN</figcaption>
</figure>
</div>
<section id="xgboost-parameters" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-parameters">XGBoost Parameters</h2>
<p>Gradient boosting algorithms like XGBoost have two main types of hyperparameters: <em>tree parameters</em> which control the decision tree trained at each boosting round and <em>boosting parameters</em> which control the boosting procedure itself. Below I’ll highlight my favorite parameters, but you can see the full list in the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">documentation</a>.</p>
<section id="tree-parameters" class="level3">
<h3 class="anchored" data-anchor-id="tree-parameters">Tree Parameters</h3>
<p>In theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, <a href="../../posts/consider-the-decision-tree/">decision trees</a> are typically the best choice. In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.</p>
<section id="tree-construction-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="tree-construction-algorithm">Tree construction algorithm</h4>
<p>The tree construction algorithm boils down to split finding, and different algorithms have different ways of generating candidate splits to consider. In XGBoost we have the parameter:</p>
<ul>
<li><code>tree_method</code> - select tree construction algorithm: <code>exact</code>, <code>hist</code>, <code>approx</code>, or the horrifying default—<code>auto</code>—which outsources your choice of tree construction algo to XGBoost and which you should never ever use. I’ve been burned by this hidden <code>tree_method=auto</code> default multiple times before learning my lesson. Why is the online model worse than the offline model? Why is this model suddenly taking so much longer to train? Avoid these debugging nightmares and set <code>tree_method</code> explicitly; the exact method tends to be slow and ironically less accurate, so I use either approx or hist.</li>
</ul>
</section>
<section id="tree-complexity-parameters" class="level4">
<h4 class="anchored" data-anchor-id="tree-complexity-parameters">Tree complexity parameters</h4>
<p>Tree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be. I use these two parameters:</p>
<ul>
<li><code>max_depth</code> - maximum number of split levels allowed. Reasonable values are usually from 3-12.</li>
<li><code>min_child_weight</code> - minimum allowable sum of hessian values over data in a node. When using the default squared error objective, this is the minimum number of samples allowed in a leaf node (see <a href="https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm">this explanation</a> of why that’s true). For a squared error objective, values in [1, 200] usually work well.</li>
</ul>
<p>These two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing min child weight makes trees less expressive and therefore is a powerful way to counter overfitting. Note that <code>gamma</code> (a.k.a. <code>min_split_loss</code>) also limits node splitting, but I usually don’t use it because <code>min_child_weight</code> seems to work well enough on its own.</p>
</section>
<section id="sampling-parameters" class="level4">
<h4 class="anchored" data-anchor-id="sampling-parameters">Sampling parameters</h4>
<p>XGBoost can randomly sample rows and columns to be used for training each tree; you might think of this as <em>bagging</em>. We have a few parameters:</p>
<ul>
<li><code>subsample</code> - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.</li>
<li><code>colsample_bytree</code>, <code>colsample_bylevel</code>, <code>colsample_bynode</code> - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split. Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.</li>
</ul>
</section>
<section id="regularization-parameters" class="level4">
<h4 class="anchored" data-anchor-id="regularization-parameters">Regularization parameters</h4>
<p>In XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero. I usually use:</p>
<ul>
<li><code>reg_lambda</code> - L2 regularization of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. Valid values are in [0,<img src="https://latex.codecogs.com/png.latex?%5Cinfty">), but good values typically fall in [0,10].</li>
</ul>
<p>There is also an L1 regularization parameter called <code>reg_alpha</code>; feel free to use it instead. It seems that using one or the other is usually sufficient.</p>
</section>
</section>
<section id="boosting-parameters-and-early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="boosting-parameters-and-early-stopping">Boosting Parameters and Early Stopping</h3>
<p>Trained gradient boosting models take the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20F(%5Cmathbf%7Bx%7D)%20=%20b%20+%20%5Ceta%20%5Csum_%7Bk=1%7D%5E%7BK%7D%20f_k(%5Cmathbf%7Bx%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b"> is the constant base predicted value, <img src="https://latex.codecogs.com/png.latex?f_k(%5Ccdot)"> is the base learner for round <img src="https://latex.codecogs.com/png.latex?k">, parameter <img src="https://latex.codecogs.com/png.latex?K"> is the number of boosting rounds, and parameter <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is the learning rate. In XGBoost these parameters correspond with:</p>
<ul>
<li><code>num_boost_round</code> (<img src="https://latex.codecogs.com/png.latex?K">) - the number of boosting iterations</li>
<li><code>learning_rate</code> (<img src="https://latex.codecogs.com/png.latex?%5Ceta">) - the scaling or “shrinkage” factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. Fun fact: the <img src="https://latex.codecogs.com/png.latex?%5Ceta"> character is called “eta”, and <code>learning_rate</code> is aliased to <code>eta</code> in xgboost, so you can use parameter <code>eta</code> instead of <code>learning_rate</code> if you like.</li>
</ul>
<p>These two parameters are very closely linked; the optimal value of one depends on the value of the other. To illustrate their relationship, we can train two different XGBoost models on the same training dataset, where one model has a lower learning rate than the other.</p>
<div id="cell-4" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split </span>
<span id="cb1-5"></span>
<span id="cb1-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5000</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-7">X_train, X_valid, y_train, y_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">eta1, eta2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span></span>
<span id="cb1-10">reg1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta1, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-11">reg1.fit(X_train, y_train, </span>
<span id="cb1-12">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-13">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-14">reg2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta2, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-15">reg2.fit(X_train, y_train, </span>
<span id="cb1-16">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-17">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-18"></span>
<span id="cb1-19">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb1-20"></span>
<span id="cb1-21">best_round1, best_round2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.best_iteration, reg2.best_iteration</span>
<span id="cb1-22">obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-23">obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-24">best_obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj1[best_round1]</span>
<span id="cb1-25">best_obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj2[best_round1]</span>
<span id="cb1-26"></span>
<span id="cb1-27">plt.plot(reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-b'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-28">plt.plot(reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-r'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-29">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-30">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1), </span>
<span id="cb1-31">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-32">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-33">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-34">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-35">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2), </span>
<span id="cb1-36">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-37">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-38">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-39">plt.legend()</span>
<span id="cb1-40">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RMSE'</span>) </span>
<span id="cb1-41">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting round'</span>) </span>
<span id="cb1-42">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Validation Scores by Boosting Round'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img" alt="XGBoost objective curves for two models"></p>
<figcaption>Hold-out validation score (RMSE) by boosting round for two XGBoost models differing only by learning rate.</figcaption>
</figure>
</div>
</div>
</div>
<p>The above figure shows root mean squared error measured on a held-out validation dataset for two different XGBoost models: one with a higher learning rate and one with a lower learning rate. The figure demonstrates two key properties of the boosting parameters:</p>
<ol type="1">
<li>While training a model with a given learning rate, the evaluation score (computed on a hold-out set) tends to improve with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.</li>
<li>All else constant, a smaller learning rate leads to a model with more boosting rounds and better evaluation score.</li>
</ol>
<p>We can leverage the first property to make our tuning more efficient by using XGBoost’s <code>early_stopping_rounds: int</code> argument, which terminates training after observing the specified number of boosting rounds <a href="https://github.com/dmlc/xgboost/pull/6942">without sufficient improvement</a> to the evaluation metric. The models above were trained using <code>early_stopping_rounds=50</code>, which terminates training after 50 boosting rounds without improvement in RMSE on the validation data. For each model, the arrow indicates the boosting round with the best score.</p>
<p>The figure also exemplifies the second property, where the model with lower learning rate attains a better validation score but requires more boosting rounds to trigger early stopping. Note that smaller and smaller learning rates will provide diminishing improvements to the validation score.</p>
</section>
</section>
<section id="an-efficient-parameter-search-strategy-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="an-efficient-parameter-search-strategy-for-xgboost">An Efficient Parameter Search Strategy for XGBoost</h2>
<p>Efficiency is the key to effective parameter tuning, because wasting less time means searching more parameter values and finding better models in a given amount of time. But as we just saw, there is a tradeoff between accuracy and training time via the learning rate. Given infinite time and compute resources, we would just choose an arbitrarily tiny learning rate and search through tree parameter values while using early stopping to choose the number of boosting rounds. The problem is that tiny learning rates require tons of boosting rounds, which will make our ideal search prohibitively slow when confronted with the reality of finite time and resources. So what can we do?</p>
<p>My approach is based on the claim that <em>good tree parameters at one learning rate are also good tree parameters at other learning rates</em>. The intuition is that given two models—one with good tree parameters and one with bad tree parameters—the model with good tree parameters will score better, regardless of the learning rate. Thus, tree parameters are “independent” of boosting parameters—See <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> for justification of this claim.</p>
<p>Independence between tree parameters and boosting parameters suggests a two-stage procedure where we first find optimal tree parameters, then we maximize performance by pushing boosting parameters to the extreme. The procedure is:</p>
<ol type="1">
<li><strong>Tune tree parameters.</strong> Fix the learning rate at a relatively high value (like 0.3ish) and enable early stopping so that each model trains within a few seconds. Use your favorite hyperparameter tuning technique to find the optimal tree parameters.</li>
<li><strong>Tune boosting parameters.</strong> Using these optimal tree parameter values, fix the learning rate as low as you want and train your model, using early stopping to identify the optimal number of boosting rounds.</li>
</ol>
<p>Why is this a good idea? Because by starting with a high learning rate and early stopping enabled, you can burn through hundreds of model training trials and find some really good tree parameters in a few minutes. Then, with the confidence that your tree parameters are actually quite good, you can set a really low learning rate and boost a few thousand rounds to get a model with the best of both tree parameter and boosting parameter worlds.</p>
<blockquote class="blockquote">
<p>You can check out <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> where I justify this approach by running two parameter searches—one with high learning rate and one with low learning rate—showing that they recover the same optimal tree parameters.</p>
</blockquote>
</section>
<section id="tuning-xgboost-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="tuning-xgboost-parameters-with-optuna">Tuning XGBoost Parameters with Optuna</h2>
<p><a href="https://optuna.readthedocs.io/en/stable/">Optuna</a> is a model-agnostic python library for hyperparameter tuning. I like it because it has a flexible API that abstracts away the details of the search algorithm being used. That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more. Another massive benefit is that optuna provides a specific <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.XGBoostPruningCallback.html">XGBoost integration</a> which terminates training early on lousy parameter combinations.</p>
<p>You can install optuna with anaconda, e.g.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode .zsh code-with-copy"><code class="sourceCode zsh"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> conda install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge optuna</span></code></pre></div>
</section>
<section id="example-tuning-the-bluebook-for-bulldozers-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="example-tuning-the-bluebook-for-bulldozers-regression-model">Example: Tuning the Bluebook for Bulldozers Regression Model</h2>
<p>To illustrate the procedure, we’ll tune the parameters for the regression model we built back in the <a href="../../posts/xgboost-for-regression-in-python/">XGBoost for regression</a> post. First we’ll load up the bulldozer data and prepare the features and target just like we did before.</p>
<div id="cell-9" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time </span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb3-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_squared_error</span>
<span id="cb3-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb</span>
<span id="cb3-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> optuna </span>
<span id="cb3-8"></span>
<span id="cb3-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../xgboost-for-regression-in-python/Train.csv'</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-10"></span>
<span id="cb3-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encode_string_features(df):</span>
<span id="cb3-12">    out_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> feature, feature_type <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.dtypes.items():</span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> feature_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'object'</span>:</span>
<span id="cb3-15">            out_df[feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> out_df[feature].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb3-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out_df</span>
<span id="cb3-17"></span>
<span id="cb3-18">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encode_string_features(df)</span>
<span id="cb3-19"></span>
<span id="cb3-20">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-21">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pd.Timestamp(year<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1970</span>, month<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, day<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-22">    ).dt.days</span>
<span id="cb3-23"></span>
<span id="cb3-24">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>])</span>
<span id="cb3-25"></span>
<span id="cb3-26"></span>
<span id="cb3-27">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb3-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,</span>
<span id="cb3-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>,</span>
<span id="cb3-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ModelID'</span>,</span>
<span id="cb3-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datasource'</span>,</span>
<span id="cb3-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auctioneerID'</span>,</span>
<span id="cb3-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>,</span>
<span id="cb3-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineHoursCurrentMeter'</span>,</span>
<span id="cb3-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'UsageBand'</span>,</span>
<span id="cb3-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDesc'</span>,</span>
<span id="cb3-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>,</span>
<span id="cb3-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiSecondaryDesc'</span>,</span>
<span id="cb3-39">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelSeries'</span>,</span>
<span id="cb3-40">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>,</span>
<span id="cb3-41">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>,</span>
<span id="cb3-42">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiProductClassDesc'</span>,</span>
<span id="cb3-43">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'state'</span>,</span>
<span id="cb3-44">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroup'</span>,</span>
<span id="cb3-45">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>,</span>
<span id="cb3-46">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Drive_System'</span>,</span>
<span id="cb3-47">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span>,</span>
<span id="cb3-48">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Forks'</span>,</span>
<span id="cb3-49">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pad_Type'</span>,</span>
<span id="cb3-50">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ride_Control'</span>,</span>
<span id="cb3-51">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick'</span>,</span>
<span id="cb3-52">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Transmission'</span>,</span>
<span id="cb3-53">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Turbocharged'</span>,</span>
<span id="cb3-54">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Extension'</span>,</span>
<span id="cb3-55">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Width'</span>,</span>
<span id="cb3-56">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure_Type'</span>,</span>
<span id="cb3-57">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Engine_Horsepower'</span>,</span>
<span id="cb3-58">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics'</span>,</span>
<span id="cb3-59">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pushblock'</span>,</span>
<span id="cb3-60">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ripper'</span>,</span>
<span id="cb3-61">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Scarifier'</span>,</span>
<span id="cb3-62">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tip_Control'</span>,</span>
<span id="cb3-63">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tire_Size'</span>,</span>
<span id="cb3-64">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler'</span>,</span>
<span id="cb3-65">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler_System'</span>,</span>
<span id="cb3-66">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>,</span>
<span id="cb3-67">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics_Flow'</span>,</span>
<span id="cb3-68">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Track_Type'</span>,</span>
<span id="cb3-69">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Undercarriage_Pad_Width'</span>,</span>
<span id="cb3-70">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick_Length'</span>,</span>
<span id="cb3-71">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Thumb'</span>,</span>
<span id="cb3-72">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pattern_Changer'</span>,</span>
<span id="cb3-73">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Type'</span>,</span>
<span id="cb3-74">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Backhoe_Mounting'</span>,</span>
<span id="cb3-75">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Type'</span>,</span>
<span id="cb3-76">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Travel_Controls'</span>,</span>
<span id="cb3-77">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Differential_Type'</span>,</span>
<span id="cb3-78">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Steering_Controls'</span>,</span>
<span id="cb3-79">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span></span>
<span id="cb3-80"> ]</span>
<span id="cb3-81"></span>
<span id="cb3-82">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span></span></code></pre></div>
</details>
</div>
<p>But this time, since we’re going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes. We make four different <code>xgboost.DMatrix</code> datasets for this process: training, validation, training+validation, and test. Training and validation are for the parameter search, and training+validation and test are for the final model.</p>
<div id="cell-11" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-2">n_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">sorted_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>)</span>
<span id="cb4-5">train_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test)] </span>
<span id="cb4-6">valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test):<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test] </span>
<span id="cb4-7">test_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test:]</span>
<span id="cb4-8"></span>
<span id="cb4-9">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], </span>
<span id="cb4-10">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-11">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], </span>
<span id="cb4-12">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-13">dtest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[target], </span>
<span id="cb4-14">                    enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-15">dtrainvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[features], </span>
<span id="cb4-16">                          label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[target], </span>
<span id="cb4-17">                          enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
</section>
<section id="preliminaries-base-parameters-and-scoring-function" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries-base-parameters-and-scoring-function">Preliminaries: base parameters and scoring function</h2>
<p>We’ll go ahead and set a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping. We’ll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE.</p>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span></span>
<span id="cb5-2">base_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb5-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg:squarederror'</span>,</span>
<span id="cb5-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eval_metric'</span>: metric,</span>
<span id="cb5-5">}</span></code></pre></div>
</div>
<div id="cell-14" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> score_model(model: xgb.core.Booster, dmat: xgb.core.DMatrix) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb6-2">    y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dmat.get_label() </span>
<span id="cb6-3">    y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(dmat) </span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> mean_squared_error(y_true, y_pred, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
</section>
<section id="stage-1-tune-tree-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="stage-1-tune-tree-parameters-with-optuna">Stage 1: Tune Tree Parameters with Optuna</h2>
<p>Next we need to choose a fixed learning rate and tune the tree parameters. We want a learning rate that allows us to train within a few seconds, so we need to time model training. Start with a high learning rate (like 0.8) and work down until you find a rate that takes a few seconds. Below I end up landing at 0.3, which takes about 4 seconds to train on my little laptop.</p>
<div id="cell-16" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span></span>
<span id="cb7-2"></span>
<span id="cb7-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate</span>
<span id="cb7-6">}</span>
<span id="cb7-7">params.update(base_params)</span>
<span id="cb7-8">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb7-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain,</span>
<span id="cb7-10">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb7-11">                  num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb7-12">                  early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb7-13">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb7-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>time<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> seconds'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>4.5 seconds</code></pre>
</div>
</div>
<p>Then we implement our optuna objective, a function taking an optuna study <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html"><code>Trial</code></a> object and returning the score we want to optimize. We use the <code>suggest_categorical</code>, <code>suggest_float</code>, and <code>suggest_int</code> methods of the <code>Trial</code> object to define the search space for each parameter. Note the use of the pruning callback function which we pass into the <code>callback</code> argument of the XGBoost <code>train</code> function; this is a must, since it allows optuna to terminate training on lousy models after a few boosting rounds. After training a model with the selected parameter values, we stash the optimal number of boosting rounds from early stopping into an optuna user attribute using the <a href="https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/003_attributes.html"><code>trial.user_attrs()</code></a> method. Finally we return the score computed by our <code>model_score</code> function.</p>
<div id="cell-18" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective(trial):</span>
<span id="cb9-2">    params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb9-3">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: trial.suggest_categorical(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist'</span>]),</span>
<span id="cb9-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>),</span>
<span id="cb9-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">250</span>),</span>
<span id="cb9-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>, log<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb9-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate,</span>
<span id="cb9-10">    }</span>
<span id="cb9-11">    num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb9-12">    params.update(base_params)</span>
<span id="cb9-13">    pruning_callback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.integration.XGBoostPruningCallback(trial, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'valid-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>metric<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb9-14">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb9-15">                      evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb9-16">                      early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb9-17">                      verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb9-18">                      callbacks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[pruning_callback])</span>
<span id="cb9-19">    trial.set_user_attr(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best_iteration'</span>, model.best_iteration)</span>
<span id="cb9-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model.best_score</span></code></pre></div>
</div>
<p>To create a new optuna study and search through 50 parameter combinations, you could just run these two lines.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>)</span>
<span id="cb10-2">study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span></code></pre></div>
<p>But, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials—who knows how long 50 trials will take. I also want the results to be reproducible. So, to set the random seed and run the optimization for around 300 seconds (long enough to go make a nice cup of tea, stretch, and come back), I do something like this:</p>
<div id="cell-20" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">sampler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.samplers.TPESampler(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb11-2">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>, sampler<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sampler)</span>
<span id="cb11-3">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb11-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>:</span>
<span id="cb11-5">    study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-21" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 1 =============================='</span>)</span>
<span id="cb12-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>value<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb12-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>learning_rate<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>user_attrs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"best_iteration"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best tree params --------------------------'</span>)</span>
<span id="cb12-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> study.best_trial.params.items():</span>
<span id="cb12-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 1 ==============================
best score = 0.23107522766919256
boosting params ---------------------------
fixed learning rate: 0.3
best boosting round: 23
best tree params --------------------------
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469</code></pre>
</div>
</div>
<p>If we decide we want to tune the tree parameters a little more, we can just call <code>study.optimize(...)</code> again, adding as many trials as we want to the study. Once we’re happy with the tree parameters, we can proceed to stage 2.</p>
</section>
<section id="stage-2-intensify-the-boosting-parameters" class="level2">
<h2 class="anchored" data-anchor-id="stage-2-intensify-the-boosting-parameters">Stage 2: Intensify the Boosting Parameters</h2>
<p>Now we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate; here I use 0.01, but you could go lower. The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you’ll need to max out the evaluation metric on the validation data.</p>
<div id="cell-23" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">low_learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb14-2"></span>
<span id="cb14-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb14-4">params.update(base_params)</span>
<span id="cb14-5">params.update(study.best_trial.params)</span>
<span id="cb14-6">params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> low_learning_rate</span>
<span id="cb14-7">model_stage2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, </span>
<span id="cb14-8">                         num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb14-9">                         evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb14-10">                         early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb14-11">                         verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div id="cell-24" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 2 =============================='</span>)</span>
<span id="cb15-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_stage2, dvalid)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb15-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 2 ==============================
best score = 0.22172991931438446
boosting params ---------------------------
fixed learning rate: 0.01
best boosting round: 1446</code></pre>
</div>
</div>
</section>
<section id="train-and-evaluate-the-final-model" class="level2">
<h2 class="anchored" data-anchor-id="train-and-evaluate-the-final-model">Train and Evaluate the Final Model</h2>
<p>Now we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2. Then we evaluate on the held out test data.</p>
<div id="cell-26" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">model_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrainvalid, </span>
<span id="cb17-2">                        num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_stage2.best_iteration,</span>
<span id="cb17-3">                        verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Final Model =========================='</span>)</span>
<span id="cb18-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'test score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_final, dtest)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb18-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'parameters ---------------------------'</span>)</span>
<span id="cb18-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> params.items():</span>
<span id="cb18-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span>
<span id="cb18-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'num_boost_round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Final Model ==========================
test score = 0.21621863543987274
parameters ---------------------------
objective : reg:squarederror
eval_metric : rmse
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469
learning_rate : 0.01
num_boost_round: 1446</code></pre>
</div>
</div>
<p>Back in the <a href="../../posts/xgboost-for-regression-in-python/">regression post</a> we got an RMSE of about 0.231 just using default parameter values, which put us in 5th place on the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard">leaderboard for the Kagle dozers competition</a>. Now with about 10 minutes of hyperparameter tuning, our RMSE is down to 0.216 which puts us in 1st place by a huge margin. 🙌</p>
</section>
<section id="what-could-possibly-go-wrong" class="level2">
<h2 class="anchored" data-anchor-id="what-could-possibly-go-wrong">What could possibly go wrong?</h2>
<p>Hyperparameter tuning can easily be overlooked in the move-fast-and-break-everything hustle of building an ML product, but it can also easily become overkill or even downright harmful, depending on the application. There are three key questions to ask:</p>
<ol type="1">
<li>How much value is created by an incremental gain in model prediction accuracy?</li>
<li>What is the cost of increasing model prediction accuracy?</li>
<li>Is my model answering the right question?</li>
</ol>
<p>Sometimes a small gain in model prediction performance translates into millions of dollars of impact. The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org’s KPIs, and get mad respect, bonuses, and promoted. But the reality is that often additional model accuracy doesn’t really change business KPIs by very much. Try to figure out the actual value of improved model accuracy and proceed accordingly.</p>
<p>Remember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself. It can also lead us to larger and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.</p>
<p>Worst of all and quite counterintuitively, it’s possible that improving a model’s prediction accuracy can compromise overall business KPIs. I’ve seen this with my own eyes at work; offline testing shows that hyperparameter tuning significantly improves a model’s prediction accuracy, but when the model goes into production, an AB test shows that the business KPIs are actually worse. What happened? In this case, the model’s prediction was being used indirectly to infer the relationship between one of the features and the prediction target to inform automatic business decisions. Answering questions about how changing an input will affect an output requires causal reasoning, and <a href="https://arxiv.org/abs/1608.00060">traditional ML models are not the right tool for the job</a>. I’ll have a lot more to say about that soon; let this story foreshadow an epic new epoch on Random Realizations….</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna. If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/</guid>
  <pubDate>Tue, 26 Dec 2023 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
