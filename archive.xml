<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<atom:link href="https://randomrealizations.com/archive.xml" rel="self" type="application/rss+xml"/>
<description>A blog about data science, statistics, machine learning, and the scientific method</description>
<image>
<url>https://randomrealizations.com/opengraph.png</url>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<height>76</height>
<width>144</width>
</image>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Sun, 04 Aug 2024 07:00:00 GMT</lastBuildDate>
<item>
  <title>SHAP from Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/shap-from-scratch/index.html</link>
  <description><![CDATA[ 



<p>Ahh, SHAP. As you know it’s become one of the leading frameworks for explaining ML model predictions. I’d guess it’s popularity is due to its appealing theoretical basis, its universal applicability to any type of ML model, and its easy-to-use python package. SHAP promises to turn your black box ML model into a nice friendly interpretable model. The hilarious irony is that, when I first started using it in my work, SHAP itself was a complete black box to me. In this post, we’ll change all that by diving into the SHAP paper, illuminating the key theoretical ideas behind its development step by step, and implementing it from scratch in python. If you aren’t already familiar with how to compute and interpret SHAP values in practice, I’d recommend that you go check out the <a href="https://shap.readthedocs.io/en/latest/index.html">documentation for the shap python package</a> before diving into this post.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_main.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Snow, trees, and mountains overlook Lake Tahoe.</figcaption>
</figure>
</div>
<section id="what-is-shap" class="level2">
<h2 class="anchored" data-anchor-id="what-is-shap">What is SHAP?</h2>
<p>SHAP (SHapley Additive exPlanations) is a conceptual framework for creating explanations of ML model predictions. The term also refers to a set of computational methods for generating these explanations and a python library which implements them. The “SHAP” <a href="https://en.wikipedia.org/wiki/Backronym">backronym</a> was introduced in <a href="https://arxiv.org/abs/1705.07874">Lundberg and Lee 2017</a>, which I call the <em>SHAP paper</em>, that expanded on several previously existing ideas which we’ll build up in the following sections. The key concepts are:</p>
<ul>
<li><em>Shapley values</em>, a concept from cooperative game theory which originally had nothing to do with machine learning</li>
<li><em>Shapley regression values</em>, which showed how to use Shapley values to generate explanations of model predictions</li>
<li><em>Shapley sampling values</em>, which offered a computationally tractable way to compute Shapley regression values for any type of model.</li>
</ul>
<p>The SHAP paper tied Shapley regression values and several other existing model explanation methods together by showing they are all members of a class called “additive feature attribution methods.” Under the right conditions, these additive feature attribution methods can generate Shapley values, and when they do we can call them SHAP values.</p>
<p>After establishing this theoretical framework, the authors go on to discuss various computational methods for computing SHAP values; some are model-agnostic, meaning they work with any type of model, and others are model-specific, meaning they work for specific types of models. It turns out that the previously existing Shapley sampling values method is a model-agnostic approach, but while it’s the most intuitive, computationally speaking it’s relatively inefficient. Thus the authors propose a novel model-agnostic approach called Kernel SHAP, which is really just <a href="https://lime-ml.readthedocs.io/en/latest/">LIME</a> parameterized to yield SHAP values.</p>
<p>Model-specific approaches can be potentially much more efficient than model-agnostic ones by taking advantage of model idiosyncrasies. For example, there is an analytical solution for the SHAP values of linear models, so Linear SHAP is extremely efficient. Similarly, Deep SHAP (proposed in the SHAP paper) and Tree SHAP (proposed later in <a href="https://www.sciencedirect.com/science/article/pii/S2666827022000500#b20">Lundberg et al 2020</a>) take advantage of idiosyncrasies of deep learning and tree-based models to compute SHAP values efficiently.</p>
<p>The important thing about these different methods is that they provide computationally tractable ways to compute SHAP values, but ultimately, they are all based on the Shapley sampling values method—the original method to compute what we now call SHAP values. Thus, for the remainder of this post, we’ll focus on this method, building it up from Shapley values to Shapley regression values to Shapley sampling values and ultimately implementing it from scratch in python.</p>
</section>
<section id="shapley-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-values">Shapley Values</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Shapley_value">Shapley value</a> is named in honor of Nobel prize winning economist Loyd Shapley who introduced the idea in the field of coalitional game theory in the 1950’s. Shapley proposed a way to determine how a coalition of players can fairly share the payout they receive from a cooperative game. We’ll introduce the mathematical formalism in the next section, so for now let’s just touch on the intuition for the approach. Essentially, the method distributes the payout among the players according to the expected contribution of each player across all possible combinations of the players. The thought experiment works as follows:</p>
<ol type="1">
<li>Draw a random permutation (ordering) of the players.</li>
<li>Have the first player play alone, generating some payout. Then have the first two players play together, generating some payout. Then the first three, and so on.</li>
<li>As each new player is added, attribute the change in the payout to this new player.</li>
<li>Repeat this experiment for all permutations of the players. A player’s Shapley value is the average change in payout (across all permutations) when that player is added to the game.</li>
</ol>
<p>Next we’ll see how this idea can be applied to model explanations.</p>
</section>
<section id="shapley-regression-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-regression-values">Shapley Regression Values</h2>
<p>The next idea came from <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.446">Lipovetsky and Conklin 2001</a>, who proposed a way to use Shapley values to explain the predictions of a linear regression model. <em>Shapley regression values</em> assign an importance value to each feature that represents the effect on the model prediction of including that feature. The basic idea is to train a second model without the feature of interest, and then to compare the predictions from the model with the feature and the model without the feature. This procedure of training two models and comparing their predictions is repeated for all possible subsets of the other features; the average difference in predictions is the Shapley value for the feature of interest.</p>
<p>The Shapley value for feature <img src="https://latex.codecogs.com/png.latex?i"> on instance <img src="https://latex.codecogs.com/png.latex?x"> is given by equation 4 in the SHAP paper:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cphi_i%20=%20%5Csum_%7BS%20%5Csubseteq%20F%20%5Csetminus%20%5C%7Bi%5C%7D%7D%0A%5Cfrac%7B%7CS%7C!(%7CF%7C%20-%20%7CS%7C%20-%201)!%7D%7B%7CF%7C!%7D%0A%5Bf_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D(x_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D)%20-%20f_S(x_S)%20%5D%0A"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cphi_i"> is the Shapley value for feature of interest <img src="https://latex.codecogs.com/png.latex?i">,</li>
<li>the <img src="https://latex.codecogs.com/png.latex?%5Csubseteq"> symbol indicates the item on its left is a subset of the object on its right,</li>
<li><img src="https://latex.codecogs.com/png.latex?F"> is the set of all features,</li>
<li>the vertical bars indicate the number of elements in a set, e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?%7CF%7C"> is the total number of features,</li>
<li><img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D"> is the set of all features except the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?S"> is a particular subset of features not including the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a “subset model”—a model that uses only the features in <img src="https://latex.codecogs.com/png.latex?S"> for both training and prediction,</li>
<li>and <img src="https://latex.codecogs.com/png.latex?f_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D"> is asubset model using features in <img src="https://latex.codecogs.com/png.latex?S"> and the feature of interest.</li>
</ul>
<p>To reiterate, this is the most important equation when it comes to understanding SHAP, as it defines the Shapley value; let’s make sure we understand what’s going on by implementing it in python.</p>
<p>We start with the feature subsets. Notice that the sum is indexed over all subsets of <img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D">, which is the set of all features except the <img src="https://latex.codecogs.com/png.latex?i">th feature, the one we’re calculating the Shapley value for. Let’s write a function that takes a list of items and returns an iterable that yields all possible subsets of those items.</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations </span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_all_subsets(items):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span>  get_all_subsets([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]):</span>
<span id="cb1-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(2,)
(0, 1)
(0, 2)
(1, 2)
(0, 1, 2)</code></pre>
</div>
</div>
<p>To get all subsets of features, other than the feature of interest, we could do something like this.</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb3-2">    all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb3-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> get_all_subsets(all_other_features)</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb3-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(3,)
(0, 1)
(0, 3)
(1, 3)
(0, 1, 3)</code></pre>
</div>
</div>
<p>So for each of the feature subsets, we’ll need to calculate the summand, which is the product of a quotient with a bunch of factorials and the difference in predicted values between two subset models. Let’s start with those subset models. Subset model <img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a model trained only on the features in subset <img src="https://latex.codecogs.com/png.latex?S">. We can write a function that takes an untrained model, a training dataset, a feature subset to use, and a single instance to predict on; the function will then train a model using only features in the subset, and it will issue a prediction for the single instance we gave it.</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> subset_model(model, X_train, y_train, feature_subset, instance):</span>
<span id="cb5-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance.shape) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Instance must be a 1D array'</span></span>
<span id="cb5-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(feature_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y.mean() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a model with no features predicts E[y]</span></span>
<span id="cb5-5">    X_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.take(feature_subset, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-6">    model.fit(X_subset, y_train)</span>
<span id="cb5-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> model.predict(instance.take(feature_subset).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
</div>
<p>Next let’s have a look at <img src="https://latex.codecogs.com/png.latex?%7CS%7C!(%7CF%7C-%7CS%7C-1)!/%7CF%7C!">. The keen reader will notice this factor kind of looks like the answers to those combinatorics questions like how many unique ways can you order the letters in the word MISSISSIPPI. The combinatorics connection is that Shapley values are defined in terms of all permutations of the players , where the included players come first, then the player of interest, followed by the excluded players. In ML models, the order of features doesn’t matter, so we can work with unordered subsets of features, scaling the prediction difference terms by the number of permutations that involve the same sets of included and excluded features. With that in mind, we can see that including the factor in each term of the sum gives us a weighted average over all feature combinations, where the numerator gives the number of permutations in which the included features come first, followed by the feature of interest, followed by the excluded features, and the denominator is the total number of feature permutations.</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb6-2"></span>
<span id="cb6-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> permutation_factor(n_features, n_subset):</span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> factorial(n_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features)</span></code></pre></div>
</div>
<p>Now we can put these pieces together to compute equation 4—a single Shapley regression value for a single instance and feature of interest.</p>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> compute_single_shap_value(untrained_model,</span>
<span id="cb7-2">                              X_train,</span>
<span id="cb7-3">                              y_train,</span>
<span id="cb7-4">                              feature_of_interest,</span>
<span id="cb7-5">                              instance):</span>
<span id="cb7-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb7-7">    n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb7-8">    shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb7-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb7-10">        n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb7-11">        prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-12">            untrained_model,</span>
<span id="cb7-13">            X_train, y_train,</span>
<span id="cb7-14">            subset,</span>
<span id="cb7-15">            instance</span>
<span id="cb7-16">        )</span>
<span id="cb7-17">        prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-18">            untrained_model,</span>
<span id="cb7-19">            X_train, y_train,</span>
<span id="cb7-20">            subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature_of_interest,),</span>
<span id="cb7-21">            instance</span>
<span id="cb7-22">        )</span>
<span id="cb7-23">        factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_factor(n_features, n_subset)</span>
<span id="cb7-24">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb7-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> shap_value</span></code></pre></div>
</div>
<p>Let’s use this function to compute a single Shapley regression value for a linear model and a small training dataset with 3 features.</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression </span>
<span id="cb8-3"></span>
<span id="cb8-4">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb8-5"></span>
<span id="cb8-6">compute_single_shap_value(untrained_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>LinearRegression(),</span>
<span id="cb8-7">                          X_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X, y_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y,</span>
<span id="cb8-8">                          feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,</span>
<span id="cb8-9">                          instance<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>-0.07477140629329351</code></pre>
</div>
</div>
<p>That gives us a single Shapley value corresponding to a single feature value in a single instance. To get useful model explanations, we’d need to compute Shapley values for each feature of each instance in some dataset of instances. You might notice there’s a big problem with the formulation above. Namely, we are going to have to train a whole bunch of new subset models—one for each subset of the features. If our model has <img src="https://latex.codecogs.com/png.latex?M"> features, we’ll have to train <img src="https://latex.codecogs.com/png.latex?2%5EM"> models, so this will get impractical in a hurry, especially if we’re trying to train anything other than linear models.</p>
</section>
<section id="shapley-sampling-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-sampling-values">Shapley Sampling Values</h2>
<p>Next, <a href="https://link.springer.com/article/10.1007/s10115-013-0679-x">Štrumbelj and Kononenko 2014</a> proposed <em>Shapley sampling values</em>, a method which provides a much more efficient way to approximate the subset models used to calculate Shapley regression values. In this approach, the effect of removing some features from the model is approximated by the conditional expectation of the model given the known features.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20f_S(x_S)%20%20:=%20E%5Bf(x)%20%7C%20x_S%5D%20%20"></p>
<p>This means we’re approximating the output of a subset model by averaging over outputs of the full model. That’s great because now we don’t have to train all those new subset models, we can just query our full model over some set of inputs and average over the outputs to compute these conditional expectation subset models.</p>
<p>Now how exactly do we compute that conditional expectation? First we rewrite the above conditional expectation (equation 10 in the SHAP paper)</p>
<p><img src="https://latex.codecogs.com/png.latex?%20E%5Bf(x)%20%7C%20x_S%5D%20%20=%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> is the set of excluded or missing features. Beside this equation in the paper they give the note “expectation over <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D%20%7C%20x_S">, which means we’re taking the expectation over the missing features given the known features. Then we get another step (equation 11)</p>
<p><img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D%20%5Capprox%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%20%5Bf(x)%5D"></p>
<p>Now it’s not an equality but an approximation. The authors give the note “assume feature independence”. The intuition here is that if the missing features are correlated with the known features, then their distribution depends on the particular values taken by the known features. But here the authors make the simplifying assumption that known and missing features are independent, which allows us to replace the conditional expectation with an unconditional expectation over the missing features.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>So is this assumption that features in <img src="https://latex.codecogs.com/png.latex?S"> are independent from features in <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> a problem? The short answer is… maybe 🤷‍♀️? It’s potentially problematic enough that people have worked out some ways to relax this assumption, e.g.&nbsp;<a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.PartitionExplainer.html">partition masking</a>, but that makes <em>Owen values</em> instead of Shapley values, so we’ll save it for another post.</p>
</div>
</div>
<p>Anyway, how do we compute this unconditional expectation over the missing features in practice? We’ll need to use a so-called <em>background dataset</em>, which is just some set of observations of our feature variables that represents their distribution. A good candidate is the training data we used to train our model. Štrumbelj and Kononenko 2014 propose a way to estimate this conditional expectation using resampling of the background dataset.</p>
<p>The idea is to notice that the instance of interest <img src="https://latex.codecogs.com/png.latex?x"> is a feature vector comprised of the set of “known” features <img src="https://latex.codecogs.com/png.latex?x_S"> and the set of excluded features <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D"> such that <img src="https://latex.codecogs.com/png.latex?x=%5C%7Bx_S,x_%7B%5Cbar%7BS%7D%7D%20%5C%7D">. Our resampling scheme will be based on constructing “masked” samples <img src="https://latex.codecogs.com/png.latex?x%5E*=%5C%7Bx_S,z_%7B%5Cbar%7BS%7D%7D%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D"> are values of the missing features drawn from some random observation in the background dataset. We can then compute an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)"> of the conditional expectation <img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%5Bf(x)%5D"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bk=1%7D%5En%20f(%5C%7Bx_S,%20z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D%20%5C%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D"> is the vector of values of the excluded features from the <img src="https://latex.codecogs.com/png.latex?k">-th row of the background dataset. Algorithmically, we can view this as first drawing a sample of observations from the background dataset, second “masking” features in <img src="https://latex.codecogs.com/png.latex?S"> in the sampled background dataset by replacing the observed values <img src="https://latex.codecogs.com/png.latex?z_S"> on each row with the values in the instance <img src="https://latex.codecogs.com/png.latex?x_S">, third using the full model <img src="https://latex.codecogs.com/png.latex?f"> to predict on each of these masked samples in the background dataset, and finally averaging over these predictions. We can implement a new subset model function that takes a fully trained model, a background dataset,a feature subset, and an instance for explanation and returns an approximation of the subset model prediction.</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> subset_model_approximation(trained_model, </span>
<span id="cb10-4">                               background_dataset,</span>
<span id="cb10-5">                               feature_subset,  </span>
<span id="cb10-6">                               instance):</span>
<span id="cb10-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">""" </span></span>
<span id="cb10-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Approximate subset model prediction  (Equation 11)</span></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    \hat{f}_S(x) = E_{x_{\hat{S</span><span class="re">}}}</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[f_S(x)]</span></span>
<span id="cb10-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    for feature subset S on single instance x</span></span>
<span id="cb10-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb10-12">    masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset.copy()</span>
<span id="cb10-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb10-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb10-15">            masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb10-16">    conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb10-17">        trained_model.predict(masked_background_dataset)</span>
<span id="cb10-18">    )</span>
<span id="cb10-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>If we replace our <code>subset_model</code> function with this new <code>subset_model_approximation</code> function in our <code>compute_single_shap_value</code> function from earlier, then we’ll be computing Shapley sampling values. And according to the SHAP paper: “if we assume feature independence when approximating conditional expectations (using Equation 11 to estimate subset model output) … then SHAP values can be estimated directly using the Shapley sampling values method.” That means we’ll be computing SHAP values!</p>
</section>
<section id="how-to-implement-shap-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="how-to-implement-shap-from-scratch">How to Implement SHAP from Scratch</h2>
<p>Let’s put the pieces together and implement a class for a model explainer that computes SHAP values via the Shapley sampling values method. We’ll talk through a couple of points after the code.</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Any, Callable, Iterable</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> ShapFromScratchExplainer():</span>
<span id="cb11-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb11-8">                 model: Callable[[np.ndarray], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>], </span>
<span id="cb11-9">                 background_dataset: np.ndarray,</span>
<span id="cb11-10">                 max_samples: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb11-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb11-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> max_samples:</span>
<span id="cb11-13">            max_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(max_samples, background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]) </span>
<span id="cb11-14">            rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.default_rng()</span>
<span id="cb11-15">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.choice(background_dataset, </span>
<span id="cb11-16">                                                 size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_samples, </span>
<span id="cb11-17">                                                 replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span>:</span>
<span id="cb11-19">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset</span>
<span id="cb11-20"></span>
<span id="cb11-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> shap_values(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb11-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"SHAP Values for instances in DataFrame or 2D array"</span></span>
<span id="cb11-23">        shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty(X.shape)</span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb11-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-26">                shap_values[i, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._compute_single_shap_value(j, X[i, :])</span>
<span id="cb11-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> shap_values</span>
<span id="cb11-28">       </span>
<span id="cb11-29">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _compute_single_shap_value(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-30">                                   feature: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb11-31">                                   instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb11-33">        n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance)</span>
<span id="cb11-34">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb11-35">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_other_feature_subsets(n_features, feature):</span>
<span id="cb11-36">            n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb11-37">            prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-38">                subset, </span>
<span id="cb11-39">                instance</span>
<span id="cb11-40">            )</span>
<span id="cb11-41">            prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-42">                subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature,), </span>
<span id="cb11-43">                instance</span>
<span id="cb11-44">            )</span>
<span id="cb11-45">            factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._permutation_factor(n_features, n_subset)</span>
<span id="cb11-46">            shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb11-47">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> shap_value</span>
<span id="cb11-48">    </span>
<span id="cb11-49">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _get_all_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, items: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Iterable:</span>
<span id="cb11-50">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb11-51">    </span>
<span id="cb11-52">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _get_all_other_feature_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, feature_of_interest):</span>
<span id="cb11-53">        all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb11-54">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_subsets(all_other_features)</span>
<span id="cb11-55"></span>
<span id="cb11-56">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _permutation_factor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, n_subset):</span>
<span id="cb11-57">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> (</span>
<span id="cb11-58">            factorial(n_subset) </span>
<span id="cb11-59">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb11-60">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features) </span>
<span id="cb11-61">        )</span>
<span id="cb11-62">    </span>
<span id="cb11-63">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _subset_model_approximation(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-64">                                    feature_subset: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, ...], </span>
<span id="cb11-65">                                    instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-66">        masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset.copy()</span>
<span id="cb11-67">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-68">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb11-69">                masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb11-70">        conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb11-71">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model(masked_background_dataset)</span>
<span id="cb11-72">        )</span>
<span id="cb11-73">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>The <code>SHAPExplainerFromScratch</code> API is similar to that of the <a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html"><code>KernelExplainer</code></a> from the python library, taking two required arguments during instantiation:</p>
<ul>
<li><code>model</code>: “User supplied function that takes a matrix of samples (# samples x # features) and computes the output of the model for those samples.” That means if our model is a scikit-learn model, we’ll need to pass in its predict method, not the model object itself.</li>
<li><code>background_dataset</code>: “The background dataset to use for integrating out features.” We know about this idea from the Shapley sampling values section above; a good choice for this data could be the training dataset we used to fit the model. By default, we’ll use all the rows of this background dataset, but we’ll also implement the ability to sample down to the desired number of rows with an argument called <code>max_samples</code>.</li>
</ul>
<p>Like the <code>KernelExplainer</code>, this class has a method called <code>shap_values</code> which estimates the SHAP values for a set of instances. It takes an argument <code>X</code> which is “a matrix of samples (# samples x # features) on which to explain the model’s output.” This <code>shap_values</code> method just loops through each feature value of each instance of the input samples <code>X</code> and calls an internal method named <code>_compute_single_shap_value</code> to compute each SHAP value. The <code>_compute_single_shap_value</code> method is the real workhorse of the class. It implements equation 4 from the SHAP paper as described in the Shapley regression values section above by calling a few other internal helper methods corresponding to functions that we’ve already written.</p>
</section>
<section id="testing-the-implementation" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-implementation">Testing the Implementation</h2>
<p>Let’s check our work by comparing SHAP values computed by our implementation with those from the SHAP python library. We’ll use our old friend the diabetes dataset, training a linear model, a random forest, and a gradient boosting machine.</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_diabetes</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb12-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GradientBoostingRegressor, RandomForestRegressor</span>
<span id="cb12-5"></span>
<span id="cb12-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_diabetes(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, return_X_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb12-7">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb12-8">                                                    random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb12-9"></span>
<span id="cb12-10">lin_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-11">rfr_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomForestRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-12">gbt_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>Here’s a little function to compare the SHAP values generated by our implementation and those from the library <code>KernelExplainer</code>.</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shap</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> compare_methods(model, X_background, X_instances):</span>
<span id="cb13-4">        </span>
<span id="cb13-5">    library_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shap.KernelExplainer(model.predict, X_background)</span>
<span id="cb13-6">    library_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> library_explainer.shap_values(X_instances)</span>
<span id="cb13-7"></span>
<span id="cb13-8">    from_scratch_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ShapFromScratchExplainer(model.predict, X_background)</span>
<span id="cb13-9">    from_scratch_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> from_scratch_explainer.shap_values(X_instances)</span>
<span id="cb13-10"></span>
<span id="cb13-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.allclose(library_shap_values, from_scratch_shap_values)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">compare_methods(lin_model, </span>
<span id="cb14-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb14-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"995f763174824efebecb5c2522c3f6f5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>True</code></pre>
</div>
</div>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">compare_methods(rfr_model, </span>
<span id="cb16-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb16-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9468451774b4c758a7696ff10fabc74","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>True</code></pre>
</div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">compare_methods(gbt_model, </span>
<span id="cb18-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb18-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8bdea22fab86440db68c4a669beaf7df","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>True</code></pre>
</div>
</div>
<p>Beautiful! Our Implementation is consistent with the SHAP library explainer!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well I hope this one was helpful to you. The research phase actually took me a lot longer than I expected; it just took me a while to figure out what SHAP really is and how those different ideas and papers fit together. I thought the implementation itself was pretty fun and relatively easy. What do you think?</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1705.07874">The SHAP Paper (Lundberg and Lee, 2017)</a></li>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning by Christoph Molnar</a></li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>from scratch</category>
  <guid>https://randomrealizations.com/posts/shap-from-scratch/index.html</guid>
  <pubDate>Sun, 04 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_thumb.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The Ultimate Guide to XGBoost Parameter Tuning</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index.html</link>
  <description><![CDATA[ 



<p>Ahh, the dark art of hyperparameter tuning. It’s a key step in the machine learning workflow, and it’s an activity that can easily be overlooked or be overkill. Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils. Today I’ll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework. I’ll give you some intuition for how to think about the key parameters in XGBoost, and I’ll show you an efficient strategy for parameter tuning GBTs. I’ll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like. You can download a notebook with this tuning workflow from my <a href="https://github.com/mcb00/ds-templates">data science templates repository</a>. Finally we’ll wrap up with the kind of cautionary tale data scientists tell their colleagues around the campfire about when all this fancy hyperparameter tuning can backfire catastrophically—ignore at your own peril.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_main.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Lunar halo on a frosty night in Johnson City, TN</figcaption>
</figure>
</div>
<section id="xgboost-parameters" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-parameters">XGBoost Parameters</h2>
<p>Gradient boosting algorithms like XGBoost have two main types of hyperparameters: <em>tree parameters</em> which control the decision tree trained at each boosting round and <em>boosting parameters</em> which control the boosting procedure itself. Below I’ll highlight my favorite parameters, but you can see the full list in the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">documentation</a>.</p>
<section id="tree-parameters" class="level3">
<h3 class="anchored" data-anchor-id="tree-parameters">Tree Parameters</h3>
<p>In theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, <a href="../../posts/consider-the-decision-tree/">decision trees</a> are typically the best choice. In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.</p>
<section id="tree-construction-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="tree-construction-algorithm">Tree construction algorithm</h4>
<p>The tree construction algorithm boils down to split finding, and different algorithms have different ways of generating candidate splits to consider. In XGBoost we have the parameter:</p>
<ul>
<li><code>tree_method</code> - select tree construction algorithm: <code>exact</code>, <code>hist</code>, <code>approx</code>, or the horrifying default—<code>auto</code>—which outsources your choice of tree construction algo to XGBoost and which you should never ever use. I’ve been burned by this hidden <code>tree_method=auto</code> default multiple times before learning my lesson. Why is the online model worse than the offline model? Why is this model suddenly taking so much longer to train? Avoid these debugging nightmares and set <code>tree_method</code> explicitly; the exact method tends to be slow and ironically less accurate, so I use either approx or hist.</li>
</ul>
</section>
<section id="tree-complexity-parameters" class="level4">
<h4 class="anchored" data-anchor-id="tree-complexity-parameters">Tree complexity parameters</h4>
<p>Tree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be. I use these two parameters:</p>
<ul>
<li><code>max_depth</code> - maximum number of split levels allowed. Reasonable values are usually from 3-12.</li>
<li><code>min_child_weight</code> - minimum allowable sum of hessian values over data in a node. When using the default squared error objective, this is the minimum number of samples allowed in a leaf node (see <a href="https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm">this explanation</a> of why that’s true). For a squared error objective, values in [1, 200] usually work well.</li>
</ul>
<p>These two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing min child weight makes trees less expressive and therefore is a powerful way to counter overfitting. Note that <code>gamma</code> (a.k.a. <code>min_split_loss</code>) also limits node splitting, but I usually don’t use it because <code>min_child_weight</code> seems to work well enough on its own.</p>
</section>
<section id="sampling-parameters" class="level4">
<h4 class="anchored" data-anchor-id="sampling-parameters">Sampling parameters</h4>
<p>XGBoost can randomly sample rows and columns to be used for training each tree; you might think of this as <em>bagging</em>. We have a few parameters:</p>
<ul>
<li><code>subsample</code> - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.</li>
<li><code>colsample_bytree</code>, <code>colsample_bylevel</code>, <code>colsample_bynode</code> - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split. Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.</li>
</ul>
</section>
<section id="regularization-parameters" class="level4">
<h4 class="anchored" data-anchor-id="regularization-parameters">Regularization parameters</h4>
<p>In XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero. I usually use:</p>
<ul>
<li><code>reg_lambda</code> - L2 regularization of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. Valid values are in [0,<img src="https://latex.codecogs.com/png.latex?%5Cinfty">), but good values typically fall in [0,10].</li>
</ul>
<p>There is also an L1 regularization parameter called <code>reg_alpha</code>; feel free to use it instead. It seems that using one or the other is usually sufficient.</p>
</section>
</section>
<section id="boosting-parameters-and-early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="boosting-parameters-and-early-stopping">Boosting Parameters and Early Stopping</h3>
<p>Trained gradient boosting models take the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20F(%5Cmathbf%7Bx%7D)%20=%20b%20+%20%5Ceta%20%5Csum_%7Bk=1%7D%5E%7BK%7D%20f_k(%5Cmathbf%7Bx%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b"> is the constant base predicted value, <img src="https://latex.codecogs.com/png.latex?f_k(%5Ccdot)"> is the base learner for round <img src="https://latex.codecogs.com/png.latex?k">, parameter <img src="https://latex.codecogs.com/png.latex?K"> is the number of boosting rounds, and parameter <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is the learning rate. In XGBoost these parameters correspond with:</p>
<ul>
<li><code>num_boost_round</code> (<img src="https://latex.codecogs.com/png.latex?K">) - the number of boosting iterations</li>
<li><code>learning_rate</code> (<img src="https://latex.codecogs.com/png.latex?%5Ceta">) - the scaling or “shrinkage” factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. Fun fact: the <img src="https://latex.codecogs.com/png.latex?%5Ceta"> character is called “eta”, and <code>learning_rate</code> is aliased to <code>eta</code> in xgboost, so you can use parameter <code>eta</code> instead of <code>learning_rate</code> if you like.</li>
</ul>
<p>These two parameters are very closely linked; the optimal value of one depends on the value of the other. To illustrate their relationship, we can train two different XGBoost models on the same training dataset, where one model has a lower learning rate than the other.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split </span>
<span id="cb1-5"></span>
<span id="cb1-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5000</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-7">X_train, X_valid, y_train, y_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">eta1, eta2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span></span>
<span id="cb1-10">reg1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta1, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-11">reg1.fit(X_train, y_train, </span>
<span id="cb1-12">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-13">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-14">reg2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta2, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-15">reg2.fit(X_train, y_train, </span>
<span id="cb1-16">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-17">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-18"></span>
<span id="cb1-19">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb1-20"></span>
<span id="cb1-21">best_round1, best_round2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.best_iteration, reg2.best_iteration</span>
<span id="cb1-22">obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-23">obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-24">best_obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj1[best_round1]</span>
<span id="cb1-25">best_obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj2[best_round1]</span>
<span id="cb1-26"></span>
<span id="cb1-27">plt.plot(reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-b'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-28">plt.plot(reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-r'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-29">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-30">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1), </span>
<span id="cb1-31">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-32">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-33">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-34">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-35">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2), </span>
<span id="cb1-36">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-37">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-38">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-39">plt.legend()</span>
<span id="cb1-40">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RMSE'</span>) </span>
<span id="cb1-41">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting round'</span>) </span>
<span id="cb1-42">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Validation Scores by Boosting Round'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img" alt="XGBoost objective curves for two models"></p>
<figcaption class="figure-caption">Hold-out validation score (RMSE) by boosting round for two XGBoost models differing only by learning rate.</figcaption>
</figure>
</div>
</div>
</div>
<p>The above figure shows root mean squared error measured on a held-out validation dataset for two different XGBoost models: one with a higher learning rate and one with a lower learning rate. The figure demonstrates two key properties of the boosting parameters:</p>
<ol type="1">
<li>While training a model with a given learning rate, the evaluation score (computed on a hold-out set) tends to improve with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.</li>
<li>All else constant, a smaller learning rate leads to a model with more boosting rounds and better evaluation score.</li>
</ol>
<p>We can leverage the first property to make our tuning more efficient by using XGBoost’s <code>early_stopping_rounds: int</code> argument, which terminates training after observing the specified number of boosting rounds <a href="https://github.com/dmlc/xgboost/pull/6942">without sufficient improvement</a> to the evaluation metric. The models above were trained using <code>early_stopping_rounds=50</code>, which terminates training after 50 boosting rounds without improvement in RMSE on the validation data. For each model, the arrow indicates the boosting round with the best score.</p>
<p>The figure also exemplifies the second property, where the model with lower learning rate attains a better validation score but requires more boosting rounds to trigger early stopping. Note that smaller and smaller learning rates will provide diminishing improvements to the validation score.</p>
</section>
</section>
<section id="an-efficient-parameter-search-strategy-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="an-efficient-parameter-search-strategy-for-xgboost">An Efficient Parameter Search Strategy for XGBoost</h2>
<p>Efficiency is the key to effective parameter tuning, because wasting less time means searching more parameter values and finding better models in a given amount of time. But as we just saw, there is a tradeoff between accuracy and training time via the learning rate. Given infinite time and compute resources, we would just choose an arbitrarily tiny learning rate and search through tree parameter values while using early stopping to choose the number of boosting rounds. The problem is that tiny learning rates require tons of boosting rounds, which will make our ideal search prohibitively slow when confronted with the reality of finite time and resources. So what can we do?</p>
<p>My approach is based on the claim that <em>good tree parameters at one learning rate are also good tree parameters at other learning rates</em>. The intuition is that given two models—one with good tree parameters and one with bad tree parameters—the model with good tree parameters will score better, regardless of the learning rate. Thus, tree parameters are “independent” of boosting parameters—See <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> for justification of this claim.</p>
<p>Independence between tree parameters and boosting parameters suggests a two-stage procedure where we first find optimal tree parameters, then we maximize performance by pushing boosting parameters to the extreme. The procedure is:</p>
<ol type="1">
<li><strong>Tune tree parameters.</strong> Fix the learning rate at a relatively high value (like 0.3ish) and enable early stopping so that each model trains within a few seconds. Use your favorite hyperparameter tuning technique to find the optimal tree parameters.</li>
<li><strong>Tune boosting parameters.</strong> Using these optimal tree parameter values, fix the learning rate as low as you want and train your model, using early stopping to identify the optimal number of boosting rounds.</li>
</ol>
<p>Why is this a good idea? Because by starting with a high learning rate and early stopping enabled, you can burn through hundreds of model training trials and find some really good tree parameters in a few minutes. Then, with the confidence that your tree parameters are actually quite good, you can set a really low learning rate and boost a few thousand rounds to get a model with the best of both tree parameter and boosting parameter worlds.</p>
<blockquote class="blockquote">
<p>You can check out <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> where I justify this approach by running two parameter searches—one with high learning rate and one with low learning rate—showing that they recover the same optimal tree parameters.</p>
</blockquote>
</section>
<section id="tuning-xgboost-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="tuning-xgboost-parameters-with-optuna">Tuning XGBoost Parameters with Optuna</h2>
<p><a href="https://optuna.readthedocs.io/en/stable/">Optuna</a> is a model-agnostic python library for hyperparameter tuning. I like it because it has a flexible API that abstracts away the details of the search algorithm being used. That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more. Another massive benefit is that optuna provides a specific <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.XGBoostPruningCallback.html">XGBoost integration</a> which terminates training early on lousy parameter combinations.</p>
<p>You can install optuna with anaconda, e.g.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode .zsh code-with-copy"><code class="sourceCode zsh"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> conda install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge optuna</span></code></pre></div>
</section>
<section id="example-tuning-the-bluebook-for-bulldozers-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="example-tuning-the-bluebook-for-bulldozers-regression-model">Example: Tuning the Bluebook for Bulldozers Regression Model</h2>
<p>To illustrate the procedure, we’ll tune the parameters for the regression model we built back in the <a href="../../posts/xgboost-for-regression-in-python/">XGBoost for regression</a> post. First we’ll load up the bulldozer data and prepare the features and target just like we did before.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time </span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb3-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_squared_error</span>
<span id="cb3-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb</span>
<span id="cb3-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> optuna </span>
<span id="cb3-8"></span>
<span id="cb3-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../xgboost-for-regression-in-python/Train.csv'</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-10"></span>
<span id="cb3-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> encode_string_features(df):</span>
<span id="cb3-12">    out_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> feature, feature_type <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> df.dtypes.items():</span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> feature_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'object'</span>:</span>
<span id="cb3-15">            out_df[feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> out_df[feature].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb3-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> out_df</span>
<span id="cb3-17"></span>
<span id="cb3-18">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encode_string_features(df)</span>
<span id="cb3-19"></span>
<span id="cb3-20">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-21">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pd.Timestamp(year<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1970</span>, month<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, day<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-22">    ).dt.days</span>
<span id="cb3-23"></span>
<span id="cb3-24">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>])</span>
<span id="cb3-25"></span>
<span id="cb3-26"></span>
<span id="cb3-27">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb3-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,</span>
<span id="cb3-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>,</span>
<span id="cb3-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ModelID'</span>,</span>
<span id="cb3-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datasource'</span>,</span>
<span id="cb3-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auctioneerID'</span>,</span>
<span id="cb3-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>,</span>
<span id="cb3-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineHoursCurrentMeter'</span>,</span>
<span id="cb3-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'UsageBand'</span>,</span>
<span id="cb3-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDesc'</span>,</span>
<span id="cb3-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>,</span>
<span id="cb3-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiSecondaryDesc'</span>,</span>
<span id="cb3-39">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelSeries'</span>,</span>
<span id="cb3-40">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>,</span>
<span id="cb3-41">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>,</span>
<span id="cb3-42">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiProductClassDesc'</span>,</span>
<span id="cb3-43">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'state'</span>,</span>
<span id="cb3-44">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroup'</span>,</span>
<span id="cb3-45">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>,</span>
<span id="cb3-46">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Drive_System'</span>,</span>
<span id="cb3-47">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span>,</span>
<span id="cb3-48">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Forks'</span>,</span>
<span id="cb3-49">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pad_Type'</span>,</span>
<span id="cb3-50">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ride_Control'</span>,</span>
<span id="cb3-51">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick'</span>,</span>
<span id="cb3-52">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Transmission'</span>,</span>
<span id="cb3-53">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Turbocharged'</span>,</span>
<span id="cb3-54">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Extension'</span>,</span>
<span id="cb3-55">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Width'</span>,</span>
<span id="cb3-56">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure_Type'</span>,</span>
<span id="cb3-57">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Engine_Horsepower'</span>,</span>
<span id="cb3-58">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics'</span>,</span>
<span id="cb3-59">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pushblock'</span>,</span>
<span id="cb3-60">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ripper'</span>,</span>
<span id="cb3-61">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Scarifier'</span>,</span>
<span id="cb3-62">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tip_Control'</span>,</span>
<span id="cb3-63">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tire_Size'</span>,</span>
<span id="cb3-64">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler'</span>,</span>
<span id="cb3-65">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler_System'</span>,</span>
<span id="cb3-66">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>,</span>
<span id="cb3-67">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics_Flow'</span>,</span>
<span id="cb3-68">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Track_Type'</span>,</span>
<span id="cb3-69">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Undercarriage_Pad_Width'</span>,</span>
<span id="cb3-70">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick_Length'</span>,</span>
<span id="cb3-71">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Thumb'</span>,</span>
<span id="cb3-72">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pattern_Changer'</span>,</span>
<span id="cb3-73">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Type'</span>,</span>
<span id="cb3-74">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Backhoe_Mounting'</span>,</span>
<span id="cb3-75">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Type'</span>,</span>
<span id="cb3-76">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Travel_Controls'</span>,</span>
<span id="cb3-77">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Differential_Type'</span>,</span>
<span id="cb3-78">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Steering_Controls'</span>,</span>
<span id="cb3-79">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span></span>
<span id="cb3-80"> ]</span>
<span id="cb3-81"></span>
<span id="cb3-82">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span></span></code></pre></div>
</details>
</div>
<p>But this time, since we’re going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes. We make four different <code>xgboost.DMatrix</code> datasets for this process: training, validation, training+validation, and test. Training and validation are for the parameter search, and training+validation and test are for the final model.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-2">n_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">sorted_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>)</span>
<span id="cb4-5">train_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test)] </span>
<span id="cb4-6">valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test):<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test] </span>
<span id="cb4-7">test_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test:]</span>
<span id="cb4-8"></span>
<span id="cb4-9">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], </span>
<span id="cb4-10">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-11">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], </span>
<span id="cb4-12">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-13">dtest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[target], </span>
<span id="cb4-14">                    enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-15">dtrainvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[features], </span>
<span id="cb4-16">                          label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[target], </span>
<span id="cb4-17">                          enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
</section>
<section id="preliminaries-base-parameters-and-scoring-function" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries-base-parameters-and-scoring-function">Preliminaries: base parameters and scoring function</h2>
<p>We’ll go ahead and set a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping. We’ll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span></span>
<span id="cb5-2">base_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb5-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg:squarederror'</span>,</span>
<span id="cb5-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eval_metric'</span>: metric,</span>
<span id="cb5-5">}</span></code></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> score_model(model: xgb.core.Booster, dmat: xgb.core.DMatrix) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb6-2">    y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dmat.get_label() </span>
<span id="cb6-3">    y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(dmat) </span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> mean_squared_error(y_true, y_pred, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
</section>
<section id="stage-1-tune-tree-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="stage-1-tune-tree-parameters-with-optuna">Stage 1: Tune Tree Parameters with Optuna</h2>
<p>Next we need to choose a fixed learning rate and tune the tree parameters. We want a learning rate that allows us to train within a few seconds, so we need to time model training. Start with a high learning rate (like 0.8) and work down until you find a rate that takes a few seconds. Below I end up landing at 0.3, which takes about 4 seconds to train on my little laptop.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span></span>
<span id="cb7-2"></span>
<span id="cb7-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate</span>
<span id="cb7-6">}</span>
<span id="cb7-7">params.update(base_params)</span>
<span id="cb7-8">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb7-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain,</span>
<span id="cb7-10">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb7-11">                  num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb7-12">                  early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb7-13">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb7-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>time<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> seconds'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>4.5 seconds</code></pre>
</div>
</div>
<p>Then we implement our optuna objective, a function taking an optuna study <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html"><code>Trial</code></a> object and returning the score we want to optimize. We use the <code>suggest_categorical</code>, <code>suggest_float</code>, and <code>suggest_int</code> methods of the <code>Trial</code> object to define the search space for each parameter. Note the use of the pruning callback function which we pass into the <code>callback</code> argument of the XGBoost <code>train</code> function; this is a must, since it allows optuna to terminate training on lousy models after a few boosting rounds. After training a model with the selected parameter values, we stash the optimal number of boosting rounds from early stopping into an optuna user attribute using the <a href="https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/003_attributes.html"><code>trial.user_attrs()</code></a> method. Finally we return the score computed by our <code>model_score</code> function.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> objective(trial):</span>
<span id="cb9-2">    params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb9-3">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: trial.suggest_categorical(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist'</span>]),</span>
<span id="cb9-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>),</span>
<span id="cb9-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">250</span>),</span>
<span id="cb9-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>, log<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb9-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate,</span>
<span id="cb9-10">    }</span>
<span id="cb9-11">    num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb9-12">    params.update(base_params)</span>
<span id="cb9-13">    pruning_callback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.integration.XGBoostPruningCallback(trial, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'valid-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>metric<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb9-14">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb9-15">                      evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb9-16">                      early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb9-17">                      verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb9-18">                      callbacks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[pruning_callback])</span>
<span id="cb9-19">    trial.set_user_attr(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best_iteration'</span>, model.best_iteration)</span>
<span id="cb9-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> model.best_score</span></code></pre></div>
</div>
<p>To create a new optuna study and search through 50 parameter combinations, you could just run these two lines.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>)</span>
<span id="cb10-2">study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span></code></pre></div>
<p>But, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials—who knows how long 50 trials will take. I also want the results to be reproducible. So, to set the random seed and run the optimization for around 300 seconds (long enough to go make a nice cup of tea, stretch, and come back), I do something like this:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">sampler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.samplers.TPESampler(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb11-2">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>, sampler<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sampler)</span>
<span id="cb11-3">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb11-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>:</span>
<span id="cb11-5">    study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 1 =============================='</span>)</span>
<span id="cb12-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>value<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb12-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>learning_rate<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>user_attrs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"best_iteration"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best tree params --------------------------'</span>)</span>
<span id="cb12-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> study.best_trial.params.items():</span>
<span id="cb12-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 1 ==============================
best score = 0.23107522766919256
boosting params ---------------------------
fixed learning rate: 0.3
best boosting round: 23
best tree params --------------------------
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469</code></pre>
</div>
</div>
<p>If we decide we want to tune the tree parameters a little more, we can just call <code>study.optimize(...)</code> again, adding as many trials as we want to the study. Once we’re happy with the tree parameters, we can proceed to stage 2.</p>
</section>
<section id="stage-2-intensify-the-boosting-parameters" class="level2">
<h2 class="anchored" data-anchor-id="stage-2-intensify-the-boosting-parameters">Stage 2: Intensify the Boosting Parameters</h2>
<p>Now we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate; here I use 0.01, but you could go lower. The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you’ll need to max out the evaluation metric on the validation data.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">low_learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb14-2"></span>
<span id="cb14-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb14-4">params.update(base_params)</span>
<span id="cb14-5">params.update(study.best_trial.params)</span>
<span id="cb14-6">params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> low_learning_rate</span>
<span id="cb14-7">model_stage2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, </span>
<span id="cb14-8">                         num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb14-9">                         evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb14-10">                         early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb14-11">                         verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 2 =============================='</span>)</span>
<span id="cb15-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_stage2, dvalid)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb15-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 2 ==============================
best score = 0.22172991931438446
boosting params ---------------------------
fixed learning rate: 0.01
best boosting round: 1446</code></pre>
</div>
</div>
</section>
<section id="train-and-evaluate-the-final-model" class="level2">
<h2 class="anchored" data-anchor-id="train-and-evaluate-the-final-model">Train and Evaluate the Final Model</h2>
<p>Now we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2. Then we evaluate on the held out test data.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">model_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrainvalid, </span>
<span id="cb17-2">                        num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_stage2.best_iteration,</span>
<span id="cb17-3">                        verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Final Model =========================='</span>)</span>
<span id="cb18-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'test score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_final, dtest)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb18-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'parameters ---------------------------'</span>)</span>
<span id="cb18-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> params.items():</span>
<span id="cb18-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span>
<span id="cb18-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'num_boost_round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Final Model ==========================
test score = 0.21621863543987274
parameters ---------------------------
objective : reg:squarederror
eval_metric : rmse
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469
learning_rate : 0.01
num_boost_round: 1446</code></pre>
</div>
</div>
<p>Back in the <a href="../../posts/xgboost-for-regression-in-python/">regression post</a> we got an RMSE of about 0.231 just using default parameter values, which put us in 5th place on the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard">leaderboard for the Kagle dozers competition</a>. Now with about 10 minutes of hyperparameter tuning, our RMSE is down to 0.216 which puts us in 1st place by a huge margin. 🙌</p>
</section>
<section id="what-could-possibly-go-wrong" class="level2">
<h2 class="anchored" data-anchor-id="what-could-possibly-go-wrong">What could possibly go wrong?</h2>
<p>Hyperparameter tuning can easily be overlooked in the move-fast-and-break-everything hustle of building an ML product, but it can also easily become overkill or even downright harmful, depending on the application. There are three key questions to ask:</p>
<ol type="1">
<li>How much value is created by an incremental gain in model prediction accuracy?</li>
<li>What is the cost of increasing model prediction accuracy?</li>
<li>Is my model answering the right question?</li>
</ol>
<p>Sometimes a small gain in model prediction performance translates into millions of dollars of impact. The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org’s KPIs, and get mad respect, bonuses, and promoted. But the reality is that often additional model accuracy doesn’t really change business KPIs by very much. Try to figure out the actual value of improved model accuracy and proceed accordingly.</p>
<p>Remember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself. It can also lead us to larger and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.</p>
<p>Worst of all and quite counterintuitively, it’s possible that improving a model’s prediction accuracy can compromise overall business KPIs. I’ve seen this with my own eyes at work; offline testing shows that hyperparameter tuning significantly improves a model’s prediction accuracy, but when the model goes into production, an AB test shows that the business KPIs are actually worse. What happened? In this case, the model’s prediction was being used indirectly to infer the relationship between one of the features and the prediction target to inform automatic business decisions. Answering questions about how changing an input will affect an output requires causal reasoning, and <a href="https://arxiv.org/abs/1608.00060">traditional ML models are not the right tool for the job</a>. I’ll have a lot more to say about that soon; let this story foreshadow an epic new epoch on Random Realizations….</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna. If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index.html</guid>
  <pubDate>Tue, 26 Dec 2023 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>XGBoost for Binary and Multi-Class Classification in Python</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-for-classification-in-python/index.html</link>
  <description><![CDATA[ 



<p>Today we continue the <a href="../../gradient-boosting-series.html">saga on gradient boosting</a> with a down-to-Earth tutorial on the essentials of solving classification problems with XGBoost. We’ll run through two examples: one for binary classification and another for multi-class classification. In both cases I’ll show you how to train XGBoost models using either the scikit-learn interface or the native xgboost training API. Once trained, we’ll evaluate the models with validation data then inspect them with feature importance and partial dependence plots. You can use the XGBoost classification notebook in my <a href="https://github.com/mcb00/ds-templates">ds-templates repository</a> to follow along with your own dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-classification-main.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Afternoon in the Mara</figcaption>
</figure>
</div>
<section id="preparing-data-for-xgboost-classifier" class="level2">
<h2 class="anchored" data-anchor-id="preparing-data-for-xgboost-classifier">Preparing Data for XGBoost Classifier</h2>
<p>Our dataset must satisfy two requirements to be used in an XGBoost classifier. First all feature data must be numeric—no strings and no datetimes; if you have non-numeric features, you need to <a href="../../posts/xgboost-for-regression-in-python/#prepare-raw-data-for-xgboost">transform your feature data</a>. Second, the target must be integer encoded using <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D"> for binary targets and <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1,%5Cdots,K%5C%7D"> for multiclass targets. Note that if your data is encoded to positive integers (no 0 class) XGBoost will throw potentially cryptic errors. You can use the scikit-learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"><code>LabelEncoder</code></a> (which we’ll do below) to generate a valid target encoding.</p>
</section>
<section id="xgboost-training-apis" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-training-apis">XGBoost Training APIs</h2>
<p>The <code>xgboost</code> python library offers two API’s for training classification models: the native <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training"><code>train</code></a> function and a wrapper class called <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"><code>XGBClassifier</code></a>, which offers an API consistent with the scikit-learn universe. I’ll show you how to use both approaches in the examples below, but if you’re planning to use other utilities from scikit-learn, you might find the <code>XGBClassifier</code> approach to be more convenient, since the trained model object will generally play nice with sklearn functionality.</p>
</section>
<section id="binary-classification-example" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification-example">Binary Classification Example</h2>
<section id="breast-cancer-wisconsin-dataset" class="level3">
<h3 class="anchored" data-anchor-id="breast-cancer-wisconsin-dataset">Breast Cancer Wisconsin Dataset</h3>
<p>We’ll demonstrate binary classification in XGBoost using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer">breast cancer wisconsin data</a>, one of scikit-learn’s built-in toy datasets. This is a tiny dataset with 569 observations of 30 features and a binary target representing whether samples are malignant or benign..</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datasets</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb </span>
<span id="cb1-6"></span>
<span id="cb1-7">dbunch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.load_breast_cancer(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.frame</span>
<span id="cb1-9">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.feature_names </span>
<span id="cb1-10">target_names <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.target_names </span>
<span id="cb1-11">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span> </span>
<span id="cb1-12">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 569 entries, 0 to 568
Data columns (total 31 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   mean radius              569 non-null    float64
 1   mean texture             569 non-null    float64
 2   mean perimeter           569 non-null    float64
 3   mean area                569 non-null    float64
 4   mean smoothness          569 non-null    float64
 5   mean compactness         569 non-null    float64
 6   mean concavity           569 non-null    float64
 7   mean concave points      569 non-null    float64
 8   mean symmetry            569 non-null    float64
 9   mean fractal dimension   569 non-null    float64
 10  radius error             569 non-null    float64
 11  texture error            569 non-null    float64
 12  perimeter error          569 non-null    float64
 13  area error               569 non-null    float64
 14  smoothness error         569 non-null    float64
 15  compactness error        569 non-null    float64
 16  concavity error          569 non-null    float64
 17  concave points error     569 non-null    float64
 18  symmetry error           569 non-null    float64
 19  fractal dimension error  569 non-null    float64
 20  worst radius             569 non-null    float64
 21  worst texture            569 non-null    float64
 22  worst perimeter          569 non-null    float64
 23  worst area               569 non-null    float64
 24  worst smoothness         569 non-null    float64
 25  worst compactness        569 non-null    float64
 26  worst concavity          569 non-null    float64
 27  worst concave points     569 non-null    float64
 28  worst symmetry           569 non-null    float64
 29  worst fractal dimension  569 non-null    float64
 30  target                   569 non-null    int64  
dtypes: float64(30), int64(1)
memory usage: 137.9 KB</code></pre>
</div>
</div>
<p>In this dataset, the features are all numeric, so no need to do preprocessing before passing to XGBoost. Below we’ll have a look at the target to ensure it’s encoded in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D"> and to check the class balance.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(df[target].unique())</span>
<span id="cb3-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(target_names)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1]
['malignant' 'benign']</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df.target.value_counts().sort_index().plot.bar()</span>
<span id="cb5-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>) </span>
<span id="cb5-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img" alt="bar plot showing the count of observations in each class"></p>
<figcaption class="figure-caption">class counts for the breast cancer dataset</figcaption>
</figure>
</div>
</div>
</div>
<p>Next We randomly split data into train and validation sets.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb6-2"></span>
<span id="cb6-3">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span> </span>
<span id="cb6-4"></span>
<span id="cb6-5">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_valid, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb6-6">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>((519, 31), (50, 31))</code></pre>
</div>
</div>
</section>
<section id="training-with-the-train-function" class="level3">
<h3 class="anchored" data-anchor-id="training-with-the-train-function">Training with the <code>train</code> function</h3>
<p>We need to set a couple of <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">model parameters</a>, most notably <code>objective</code>, which should be set to <code>binary:logistic</code> for binary classification. I also prefer to explicitly set <code>tree_method</code> to something other than its default of <code>auto</code>; usually I’ll start with <code>exact</code> on small datasets or <code>approx</code> on larger ones. Note also that The <code>train</code> function expects to receive data as <code>DMatrix</code> objects, not pandas dataframes, so we need to create dense matrix objects as well.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb8-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'exact'</span>,</span>
<span id="cb8-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary:logistic'</span>,</span>
<span id="cb8-4">}</span>
<span id="cb8-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb8-6"></span>
<span id="cb8-7">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features])</span>
<span id="cb8-8">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features])</span>
<span id="cb8-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb8-10">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb8-11">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-logloss:0.46232   valid-logloss:0.49033
[10]    train-logloss:0.04394   valid-logloss:0.13434
[20]    train-logloss:0.01515   valid-logloss:0.12193
[30]    train-logloss:0.00995   valid-logloss:0.11988
[40]    train-logloss:0.00766   valid-logloss:0.12416
[49]    train-logloss:0.00657   valid-logloss:0.12799</code></pre>
</div>
</div>
</section>
<section id="training-with-xgbclassifier" class="level3">
<h3 class="anchored" data-anchor-id="training-with-xgbclassifier">Training with <code>XGBClassifier</code></h3>
<p>The <code>XGBClassifier</code> takes dataframes or numpy arrays as input, so this time we don’t need to create those dense matrix objects.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb10-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'exact'</span>,</span>
<span id="cb10-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary:logistic'</span>,</span>
<span id="cb10-4">}</span>
<span id="cb10-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb10-6"></span>
<span id="cb10-7">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb10-8">clf.fit(train_df[features], train_df[target], </span>
<span id="cb10-9">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb10-10">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-logloss:0.46232    validation_1-logloss:0.49033
[10]    validation_0-logloss:0.04394    validation_1-logloss:0.13434
[20]    validation_0-logloss:0.01515    validation_1-logloss:0.12193
[30]    validation_0-logloss:0.00995    validation_1-logloss:0.11988
[40]    validation_0-logloss:0.00766    validation_1-logloss:0.12416
[49]    validation_0-logloss:0.00657    validation_1-logloss:0.12799</code></pre>
</div>
</div>
</section>
<section id="evaluating-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model">Evaluating the Model</h3>
<p>We’ll use the <code>sklearn.metrics</code> module to evaluate model performance on the held-out validation set. Have a look at the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics">scikit-learn metrics for classification</a> for examples of other metrics to use.</p>
<p>One thing to watch out for when computing metrics is the difference between the actual labels (usually called <code>y_true</code>), the model’s predicted labels (usually called <code>y_pred</code>), and the models predicted probabilities (usually called <code>y_score</code>). If you’re using the <code>XGBClassifier</code> wrapper, you can get predicted labels with the <code>predict</code> method and predicted probabilities with the <code>predict_proba</code> method. Also note that whereas <code>predict</code> returns a vector of size (num data), <code>predict_proba</code> returns a vector of size (num data, num classes); thus for binary classification, we’ll take just the second column of the array which gives the probability of class 1.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_df[target]</span>
<span id="cb12-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict(valid_df[features])</span>
<span id="cb12-3">y_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict_proba(valid_df[features])[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<p>Probably the simplest classification metric is accuracy, the proportion of labels we predicted correctly.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> metrics </span>
<span id="cb13-2"></span>
<span id="cb13-3">metrics.accuracy_score(y_true, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>0.96</code></pre>
</div>
</div>
<p>We can generate a classification report with several different metrics at once.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(metrics.classification_report(y_true, y_pred, target_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>target_names))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

   malignant       0.93      0.93      0.93        15
      benign       0.97      0.97      0.97        35

    accuracy                           0.96        50
   macro avg       0.95      0.95      0.95        50
weighted avg       0.96      0.96      0.96        50
</code></pre>
</div>
</div>
<p>And we can compute the AUC, a popular classification metric based on the ROC curve, which depends on the predicted probability rather than the predicted labels.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">metrics.roc_auc_score(y_true, y_score)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>0.9885714285714287</code></pre>
</div>
</div>
</section>
<section id="feature-importance" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance">Feature Importance</h3>
<p>Because of the <a href="../../posts/xgboost-for-regression-in-python/#feature-importance-for-xgboost">limitations of the built-in XGBoost feature importance metrics</a> I recommend that you use either <a href="https://scikit-learn.org/stable/modules/permutation_importance.html">permutation feature importance</a> or perhaps <a href="https://shap.readthedocs.io/en/latest/index.html">SHAP feature importance</a>.</p>
<p>Here we’ll compute the permutation feature importance, which tells us by how much the model’s performance changes when we scramble a particular feature’s values at prediction time. This reflects how much the model relies on each feature when making predictions.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> permutation_importance</span>
<span id="cb19-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_scorer</span>
<span id="cb19-3"></span>
<span id="cb19-4">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(metrics.log_loss, greater_is_better<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, needs_proba<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-5">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(clf, valid_df[features], valid_df[target], </span>
<span id="cb19-6">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb20-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb20-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb20-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'change in log likelihood'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption class="figure-caption">top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="partial-dependence" class="level2">
<h2 class="anchored" data-anchor-id="partial-dependence">Partial Dependence</h2>
<p>A <a href="https://scikit-learn.org/stable/modules/partial_dependence.html">partial dependence plot (PDP)</a> is a representation of the dependence between the model output and one or more feature variables. In binary classification, the model output is the probability of the so-called positive class, i.e.&nbsp;the class with encoded label 1, which corresponds to probability of “benign” in this example.. We can loosely interpret the partial dependence as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say “loosely” because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PartialDependenceDisplay</span>
<span id="cb21-2"></span>
<span id="cb21-3">PartialDependenceDisplay.from_estimator(clf, </span>
<span id="cb21-4">                                        valid_df[features], </span>
<span id="cb21-5">                                        [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'worst area'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'area error'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean area'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="line plots showing partial dependence of probability of benign"></p>
<figcaption class="figure-caption">PDP of target probability of benign vs three features</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="multi-class-classification-example" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification-example">Multi-Class Classification Example</h2>
<section id="forest-cover-type-dataset" class="level3">
<h3 class="anchored" data-anchor-id="forest-cover-type-dataset">Forest Cover Type Dataset</h3>
<p>We’ll illustrate multi-class classification using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype">scikit-learn forest cover type dataset</a>, which has around 580k observations of 54 features and a target with 7 classes.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">dbunch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.fetch_covtype(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb22-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.frame</span>
<span id="cb22-3">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.feature_names </span>
<span id="cb22-4">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 581012 entries, 0 to 581011
Data columns (total 55 columns):
 #   Column                              Non-Null Count   Dtype  
---  ------                              --------------   -----  
 0   Elevation                           581012 non-null  float64
 1   Aspect                              581012 non-null  float64
 2   Slope                               581012 non-null  float64
 3   Horizontal_Distance_To_Hydrology    581012 non-null  float64
 4   Vertical_Distance_To_Hydrology      581012 non-null  float64
 5   Horizontal_Distance_To_Roadways     581012 non-null  float64
 6   Hillshade_9am                       581012 non-null  float64
 7   Hillshade_Noon                      581012 non-null  float64
 8   Hillshade_3pm                       581012 non-null  float64
 9   Horizontal_Distance_To_Fire_Points  581012 non-null  float64
 10  Wilderness_Area_0                   581012 non-null  float64
 11  Wilderness_Area_1                   581012 non-null  float64
 12  Wilderness_Area_2                   581012 non-null  float64
 13  Wilderness_Area_3                   581012 non-null  float64
 14  Soil_Type_0                         581012 non-null  float64
 15  Soil_Type_1                         581012 non-null  float64
 16  Soil_Type_2                         581012 non-null  float64
 17  Soil_Type_3                         581012 non-null  float64
 18  Soil_Type_4                         581012 non-null  float64
 19  Soil_Type_5                         581012 non-null  float64
 20  Soil_Type_6                         581012 non-null  float64
 21  Soil_Type_7                         581012 non-null  float64
 22  Soil_Type_8                         581012 non-null  float64
 23  Soil_Type_9                         581012 non-null  float64
 24  Soil_Type_10                        581012 non-null  float64
 25  Soil_Type_11                        581012 non-null  float64
 26  Soil_Type_12                        581012 non-null  float64
 27  Soil_Type_13                        581012 non-null  float64
 28  Soil_Type_14                        581012 non-null  float64
 29  Soil_Type_15                        581012 non-null  float64
 30  Soil_Type_16                        581012 non-null  float64
 31  Soil_Type_17                        581012 non-null  float64
 32  Soil_Type_18                        581012 non-null  float64
 33  Soil_Type_19                        581012 non-null  float64
 34  Soil_Type_20                        581012 non-null  float64
 35  Soil_Type_21                        581012 non-null  float64
 36  Soil_Type_22                        581012 non-null  float64
 37  Soil_Type_23                        581012 non-null  float64
 38  Soil_Type_24                        581012 non-null  float64
 39  Soil_Type_25                        581012 non-null  float64
 40  Soil_Type_26                        581012 non-null  float64
 41  Soil_Type_27                        581012 non-null  float64
 42  Soil_Type_28                        581012 non-null  float64
 43  Soil_Type_29                        581012 non-null  float64
 44  Soil_Type_30                        581012 non-null  float64
 45  Soil_Type_31                        581012 non-null  float64
 46  Soil_Type_32                        581012 non-null  float64
 47  Soil_Type_33                        581012 non-null  float64
 48  Soil_Type_34                        581012 non-null  float64
 49  Soil_Type_35                        581012 non-null  float64
 50  Soil_Type_36                        581012 non-null  float64
 51  Soil_Type_37                        581012 non-null  float64
 52  Soil_Type_38                        581012 non-null  float64
 53  Soil_Type_39                        581012 non-null  float64
 54  Cover_Type                          581012 non-null  int32  
dtypes: float64(54), int32(1)
memory usage: 241.6 MB</code></pre>
</div>
</div>
<p>Here again the features are all numeric, so we don’t need to further preprocess them. Let’s have a look at the target.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cover_Type'</span>].value_counts().sort_index().plot.bar()</span>
<span id="cb24-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cover type'</span>) </span>
<span id="cb24-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img" alt="bar plot showing the count of observations in each class"></p>
<figcaption class="figure-caption">class counts for the forest cover type dataset</figcaption>
</figure>
</div>
</div>
</div>
<p>For multi-class classification, our target variable must take values in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1,%5Cdots,K%5C%7D">. However, from the histogram of the cover type above, we see that it takes values in <img src="https://latex.codecogs.com/png.latex?%5C%7B1,2,%5Cdots,7%5C%7D">. To fix this we can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">scikit-learn label encoder</a> to create a valid target column.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LabelEncoder </span>
<span id="cb25-2"></span>
<span id="cb25-3">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'encoded'</span></span>
<span id="cb25-4">enc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LabelEncoder()</span>
<span id="cb25-5">df[target] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> enc.fit_transform(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cover_Type'</span>])</span>
<span id="cb25-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(np.sort(df[target].unique()))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 2 3 4 5 6]</code></pre>
</div>
</div>
<p>Then we can create training and validation sets.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span></span>
<span id="cb27-2"></span>
<span id="cb27-3">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_valid, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb27-4">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>((561012, 56), (20000, 56))</code></pre>
</div>
</div>
</section>
<section id="training-with-the-train-function-1" class="level3">
<h3 class="anchored" data-anchor-id="training-with-the-train-function-1">Training with the <code>train</code> function</h3>
<p>If you’re training with the <code>train</code> function, multi-class classification can be done with two objectives: <code>multi:softmax</code> and <code>multi:softprob</code>. Both use the same loss function—negative multinomial log likelihood—but the softmax option produces a trained <code>Booster</code> object whose predict method returns a 1d array of predicted labels, whereas the softprob option produces a trained <code>Booster</code> object whose predict method returns a 2d array of predicted probabilities. In either case, you also need to explicitly tell XGBoost how many classes the target has with the <code>num_class</code> parameter.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb29-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb29-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'multi:softprob'</span>,</span>
<span id="cb29-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_class'</span>: df[target].nunique()</span>
<span id="cb29-5">}</span>
<span id="cb29-6">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb29-7"></span>
<span id="cb29-8">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features])</span>
<span id="cb29-9">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features])</span>
<span id="cb29-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb29-11">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb29-12">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-mlogloss:1.42032  valid-mlogloss:1.42366
[2] train-mlogloss:1.00541  valid-mlogloss:1.00963
[4] train-mlogloss:0.80557  valid-mlogloss:0.81109
[6] train-mlogloss:0.69432  valid-mlogloss:0.70085
[8] train-mlogloss:0.62653  valid-mlogloss:0.63350
[9] train-mlogloss:0.60111  valid-mlogloss:0.60794</code></pre>
</div>
</div>
</section>
<section id="training-with-xgbclassifier-1" class="level3">
<h3 class="anchored" data-anchor-id="training-with-xgbclassifier-1">Training with <code>XGBClassifier</code></h3>
<p>In multi-class classification, I think the scikit-learn <code>XGBClassifier</code> wrapper is quite a bit more convenient than the native <code>train</code> function. You can set the <code>objective</code> parameter to <code>multi:softprob</code>, and <code>XGBClassifier.fit</code> will produce a model having both <code>predict</code> and <code>predict_proba</code> methods. Also there is no need to explicitly set the number of classes in the target and no need to create the <code>DMatrix</code> objects.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb31-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb31-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'multi:softprob'</span>,</span>
<span id="cb31-4">}</span>
<span id="cb31-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb31-6"></span>
<span id="cb31-7">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb31-8">clf.fit(train_df[features], train_df[target], </span>
<span id="cb31-9">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb31-10">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-mlogloss:1.42032   validation_1-mlogloss:1.42366
[2] validation_0-mlogloss:1.00541   validation_1-mlogloss:1.00963
[4] validation_0-mlogloss:0.80557   validation_1-mlogloss:0.81109
[6] validation_0-mlogloss:0.69432   validation_1-mlogloss:0.70085
[8] validation_0-mlogloss:0.62653   validation_1-mlogloss:0.63350
[9] validation_0-mlogloss:0.60111   validation_1-mlogloss:0.60794</code></pre>
</div>
</div>
</section>
<section id="evaluating-the-model-1" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model-1">Evaluating the Model</h3>
<p>This time, we’ll keep the entire 2d array of predicted probabilities in <code>y_score</code>.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_df[target]</span>
<span id="cb33-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict(valid_df[features])</span>
<span id="cb33-3">y_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict_proba(valid_df[features])</span>
<span id="cb33-4">y_true.shape, y_pred.shape, y_score.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>((20000,), (20000,), (20000, 7))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">metrics.accuracy_score(y_true, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0.77425</code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(metrics.classification_report(y_true, y_pred))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

           0       0.77      0.74      0.75      7365
           1       0.78      0.84      0.81      9725
           2       0.75      0.85      0.80      1207
           3       0.82      0.78      0.80        85
           4       0.93      0.26      0.40       317
           5       0.76      0.31      0.44       627
           6       0.88      0.68      0.77       674

    accuracy                           0.77     20000
   macro avg       0.81      0.64      0.68     20000
weighted avg       0.78      0.77      0.77     20000
</code></pre>
</div>
</div>
<p>Some binary classification metrics, like AUC, can be extended to the multi-class setting by computing the metric for each class, then averaging in some way to get an overall score. The details are controlled by the <code>average</code> and <code>multi_class</code> parameters, which are described in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">documentation</a>.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">metrics.roc_auc_score(y_true, y_score, average<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'weighted'</span>, multi_class<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ovr'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>0.9129422094408693</code></pre>
</div>
</div>
</section>
<section id="feature-importance-1" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-1">Feature Importance</h3>
<p>We can compute permutation feature importance with exactly the same code that we used for the binary classifier.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(metrics.log_loss, greater_is_better<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, needs_proba<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb41-2">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(clf, valid_df[features], valid_df[target], </span>
<span id="cb41-3">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb42-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb42-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb42-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'change in multivariate log likelihood'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption class="figure-caption">top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="partial-dependence-1" class="level3">
<h3 class="anchored" data-anchor-id="partial-dependence-1">Partial Dependence</h3>
<p>Recall that partial dependence reflects how the expected model output changes with a particular feature. In the multi-class setting, the model has multiple outputs—one probability for each class—so we need to choose which class probability to show in the plots. We choose the target class with the <code>target</code> parameter; be sure to pass in the encoded value, e.g.&nbsp;we need to use the label encoder to transform a raw class label back into the encoded value. Here we’ll examine partial dependence for the probability of cover type 3.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">PartialDependenceDisplay.from_estimator(clf, </span>
<span id="cb43-2">                                        X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], </span>
<span id="cb43-3">                                        features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Elevation'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Horizontal_Distance_To_Roadways'</span>], </span>
<span id="cb43-4">                                        target<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>enc.transform([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img" alt="line plots showing partial dependence of probability of cover type 3 vs two features"></p>
<figcaption class="figure-caption">PDP of target probability of cover type == 3 vs elevation and distance to roadway</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, for me, those are really the minimal nuts and bolts one needs to get XGBoost models working on classification problems. If you dig this tutorial, or if you have additional insights into using XGBoost to solve classification problems, let me know about it down in the comments!</p>
</section>
<section id="go-deeper" class="level2">
<h2 class="anchored" data-anchor-id="go-deeper">Go Deeper</h2>
<p>If you’re feeling like Alice, and you want to go tumbling down the rabbit hole, might I recommend checking out some of the following:</p>
<ul>
<li><a href="../../posts/xgboost-explained/">XGBoost Explained</a> - for a deep dive into the math</li>
<li><a href="../../posts/xgboost-from-scratch/">XGBoost from Scratch</a> - to see how to implement all those equations in code</li>
<li><a href="../../posts/gradient-boosting-multi-class-classification-from-scratch/">Multi-Class Gradient Boosting from Scratch</a> - to fully grok the multi-class gradient boosting algorithm</li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-for-classification-in-python/index.html</guid>
  <pubDate>Tue, 28 Nov 2023 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-classification-thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Gradient Boosting Multi-Class Classification from Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/index.html</link>
  <description><![CDATA[ 



<p>Tell me dear reader, who among us, while gazing in wonder at the improbably verdant aloe vera clinging to the windswept rock at Cape Point near the southern tip of Africa, hasn’t wondered: how the heck do gradient boosting trees implement multi-class classification? Today, we’ll unravel this mystery by reviewing the theory and implementing the algorithm for ourselves in python. Specifically, we’ll review the multi-class gradient boosting model originally described in <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Friedman’s classic Greedy Function Approximation paper</a>, and we’ll implement components of the algorithm as we go along. Once we have all the pieces, we’ll write a python class for multi-class gradient boosting with a similar API to the scikit-learn <code>GradientBoostingClassifier</code>.</p>
<p>If you need a refresher on gradient boosting before diving in here, then start with my original <a href="../../posts/gradient-boosting-machine-from-scratch/">gradient boosting from scratch post</a>, which is the first installment in my ongoing <a href="../../gradient-boosting-series/">series on gradient boosting</a>.</p>
<section id="the-multi-class-gradient-boosting-classification-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-multi-class-gradient-boosting-classification-algorithm">The multi-class gradient boosting classification algorithm</h2>
<p>Friedman describes the algorithm for training a multi-class classification gradient boosting model in Algorithm 6 of the <a href="(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)">classic Greedy Function Approximation paper</a>. If you want a step-by-step walkthrough of the ideas in the paper, have a look at my post on <a href="../../posts/gradient-boosting-machine-with-any-loss-function/">the generalized gradient boosting algorithm</a>. In high-level terms, the algorithm for multi-class gradient boosting is:</p>
<ol type="1">
<li><p>Set the initial model predictions.</p></li>
<li><p>Repeat the following for each boosting round.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; Repeat the following for each class.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Compute the pseudo residuals.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Train a regression tree to predict the pseudo residuals.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Adjust the tree’s predicted values to optimize the objective function.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Add the new tree to the current composite model.</p></li>
</ol>
<p>Let’s take a look at the details for each of these steps.</p>
</section>
<section id="target-variable-encoding" class="level2">
<h2 class="anchored" data-anchor-id="target-variable-encoding">Target variable encoding</h2>
<p>Following the convention in scikit-learn, when training a multi-class classifier, the target variable in the training dataset should be integer encoded so that the <img src="https://latex.codecogs.com/png.latex?K"> distinct classes are mapped to the integers <img src="https://latex.codecogs.com/png.latex?0,1,%5Cdots,K-1">. In the code for model training, however, it’s going to be more convenient to work with a one hot encoded representation of the target. Therefore we’ll start by writing an internal method to transform the target variable from integer encoding to one hot encoding. Remember that eventually we’ll write a class for our multi-class gradient boosting model, so I’ll write this function like a class method with a leading argument called <code>self</code>.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OneHotEncoder</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _one_hot_encode_labels(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y):</span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.values</span>
<span id="cb1-7">    ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OneHotEncoder()</span>
<span id="cb1-8">    y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ohe.fit_transform(y.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).toarray()</span>
<span id="cb1-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe</span></code></pre></div>
<p>This code takes the integer-encoded target variable, makes sure it’s a numpy array, then uses cikit-learn’s one hot encoder to encode it as a 2D array with observations along the first axis and classes along the second axis. I tend to think of the one hot encoded output as a matrix with <img src="https://latex.codecogs.com/png.latex?n"> rows (the number of observations in the training data) and <img src="https://latex.codecogs.com/png.latex?K"> columns (the number of classes), although it’s technically not a matrix but rather a 2D array.</p>
</section>
<section id="model-predictions-in-raw-space-and-probability-space" class="level2">
<h2 class="anchored" data-anchor-id="model-predictions-in-raw-space-and-probability-space">Model predictions in raw space and probability space</h2>
<p>In amulti-class classification problem with <img src="https://latex.codecogs.com/png.latex?K"> classes, the model prediction for a particular observation returns a list of <img src="https://latex.codecogs.com/png.latex?K"> probabilities, one for each class. Essentially the model prediction is a conditional probability mass function for the discrete target variable, conditioned on the feature values.</p>
<p>So, we need a way to ensure that the model output is a valid probability mass function, i.e.&nbsp;each probability is in (0, 1) and the <img src="https://latex.codecogs.com/png.latex?K"> class probabilities sum to 1. Analogous to logistic regression, we can accomplish this by using the model to first make a raw prediction which can be any real number, then using something like the inverse logit function to transform the raw model prediction into a number between 0 and 1 that can be interpreted as a probability. Again analogous to logistic regression, in the multi-class setting we use <img src="https://latex.codecogs.com/png.latex?K"> different models, one for each class, to generate the raw predictions, then we transform the raw model predictions into probabilities using the softmax function,, which takes a length-<img src="https://latex.codecogs.com/png.latex?K"> vector of real numbers as input and returns a probability mass function over <img src="https://latex.codecogs.com/png.latex?K"> discrete classes.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5C%7BF_1(%5Cmathbf%7Bx%7D),%5Cdots,F_K(%5Cmathbf%7Bx%7D)%5C%7D=%5C%7BF_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> be the list of <img src="https://latex.codecogs.com/png.latex?K"> raw model outputs, and let <img src="https://latex.codecogs.com/png.latex?%5C%7Bp_1(%5Cmathbf%7Bx%7D),%5Cdots,p_K(%5Cmathbf%7Bx%7D)%5C%7D=%5C%7Bp_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> be the corresponding probability mass function over the <img src="https://latex.codecogs.com/png.latex?K"> classes, then the softmax function is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p_k(%5Cmathbf%7Bx%7D)%20=%20%5Ctext%7Bsoftmax%7D_k(%5C%7BF_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK)%0A%20%20%20%20=%20%5Cfrac%7Be%5E%7BF_k(%5Cmathbf%7Bx%7D)%7D%7D%7B%5Csum_%7Bl=1%7D%5EK%20e%5E%7BF_l(%5Cmathbf%7Bx%7D)%7D%7D"></p>
<pre><code>Let's implement an internal softmax method that transforms the raw predictions into probabilities.</code></pre>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _softmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, raw_predictions):</span>
<span id="cb3-2">    numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(raw_predictions) </span>
<span id="cb3-3">    denominator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(raw_predictions), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> denominator</span></code></pre></div>
</section>
<section id="initial-model-predictions" class="level2">
<h2 class="anchored" data-anchor-id="initial-model-predictions">Initial model predictions</h2>
<p>We’re now ready to implement model training, starting with line 1 of the algorithm which sets the initial model predictions. In our code, we’ll keep the raw model predictions <img src="https://latex.codecogs.com/png.latex?%5C%7BF_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> for the <img src="https://latex.codecogs.com/png.latex?n"> observations in the training dataset in a size <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20K"> array called <code>raw_predictions</code>, and we’ll keep the corresponding probabilities <img src="https://latex.codecogs.com/png.latex?%5C%7Bp_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> in another <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20K"> array called <code>probabilities</code>. Perhaps the simplest reasonable initialization is to set the probabilities to <img src="https://latex.codecogs.com/png.latex?1/K">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?p_k(%5Cmathbf%7Bx%7D)=1/K">, which implies <img src="https://latex.codecogs.com/png.latex?F_k(%5Cmathbf%7Bx%7D)=0">.</p>
<p>We’ll go ahead and create that one hot encoded representation of the target, then use it to set the right size for the model prediction arrays.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._one_hot_encode_labels(y)</span>
<span id="cb4-2">raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_ohe.shape)</span>
<span id="cb4-3">probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span></code></pre></div>
</section>
<section id="boosting" class="level2">
<h2 class="anchored" data-anchor-id="boosting">Boosting</h2>
<p>Line 2 of the algorithm kicks off a loop to iteratively perform boosting rounds. Within each round, line 3 specifies that we iterate through each of the <img src="https://latex.codecogs.com/png.latex?K"> classes, adding a new booster model for each class at each boosting round. We’ll keep all the boosters in a list called <code>boosters</code>, where each element is itself a list which we’ll call <code>class_trees</code> that contains the <img src="https://latex.codecogs.com/png.latex?K"> trees we trained in a given boosting round. For each round and each class, we compute the pseudo residuals (negative gradients), train a decision tree to predict them, update the tree’s predicted values to optimize the overall objective function, then update the current raw and probability predictions before storing the new tree in that round’s list of class trees.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> m <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators):</span>
<span id="cb5-3">    class_trees <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb5-5">        negative_gradients <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._negative_gradients(y_ohe[:, k], probabilities[:, k])</span>
<span id="cb5-6">        hessians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._hessians(probabilities[:, k])</span>
<span id="cb5-7">        tree <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_depth)</span>
<span id="cb5-8">        tree.fit(X, negative_gradients)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb5-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._update_terminal_nodes(tree, X, negative_gradients, hessians)</span>
<span id="cb5-10">        raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> tree.predict(X)</span>
<span id="cb5-11">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb5-12">        class_trees.append(tree)</span>
<span id="cb5-13">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters.append(class_trees)</span></code></pre></div>
<p>Next we’ll dive into the details of the pseudo residual computation and the adjustment to the tree booster predicted values.</p>
</section>
<section id="pseudo-residuals" class="level2">
<h2 class="anchored" data-anchor-id="pseudo-residuals">Pseudo Residuals</h2>
<p>For each observation in the training dataset, the pseudo residual is the negative gradient of the objective function with respect to the corresponding model prediction. The objective function for multi-class classification is the Multinomial Negative Log Likelihood. For a single observation, the objective is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J(%5C%7B%20y_k,%20p_k(%5Cmathbf%7Bx%7D)%20%5C%7D_1%5EK)%20=%20-%5Csum_%7Bk=1%7D%5EK%20y_k%20%5Clog%20p_k(%5Cmathbf%7Bx%7D)%20"></p>
<p>We can rewrite the objective in terms of our raw model output <img src="https://latex.codecogs.com/png.latex?F"> like this.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J(%5C%7B%20y_k,%20F_k(%5Cmathbf%7Bx%7D)%20%5C%7D_1%5EK)%20=%20-%5Csum_%7Bk=1%7D%5EK%20y_k%20%5Clog%20%5Cfrac%7Be%5E%7BF_k(%5Cmathbf%7Bx%7D)%7D%7D%7B%5Csum_%7Bl=1%7D%5EK%20e%5E%7BF_l(%5Cmathbf%7Bx%7D)%7D%7D"></p>
<p>The negative gradient of the objective with respect to raw model prediction <img src="https://latex.codecogs.com/png.latex?F_k(%5Cmathbf%7Bx%7D_i)"> for training example <img src="https://latex.codecogs.com/png.latex?i"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%20r_%7Bik%7D%20=%20-J'(F_k(%5Cmathbf%7Bx%7D_i))%20=%20-%5Cleft%5B%20%5Cfrac%7B%5Cpartial%20J(%5C%7B%20y_%7Bil%7D,%20F_l(%5Cmathbf%7Bx_i%7D)%5C%7D_%7Bl=1%7D%5EK)%7D%7B%5Cpartial%20F_k(%5Cmathbf%7Bx%7D_i)%20%7D%20%5Cright%5D%0A=y_%7Bik%7D%20-%20p_%7Bk%7D(%5Cmathbf%7Bx%7D_i)"></p>
<p>You can take a look at the <a href="https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1">derivation</a> if you’re curious how to work it out yourself. Note that this formula has a nice intuition. When <img src="https://latex.codecogs.com/png.latex?y_%7Bik%7D=1">, if predicted probability <img src="https://latex.codecogs.com/png.latex?p_k(%5Cmathbf%7Bx%7D_i)"> is terrible and close to 0, then the pseudo residual will be positive, and the next boosting round will try to increase the predicted probability. Otherwise if the predicted probability is already good and close to 1, the pseudo residual will be close to 0 and the next boosting round won’t change the predicted probability very much.</p>
<p>We can easily implement an internal method to compute the negative gradients over the training dataset as follows.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _negative_gradients(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y_ohe, probabilities):</span>
<span id="cb6-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities</span></code></pre></div>
</section>
<section id="adjusting-the-trees-predicted-values" class="level2">
<h2 class="anchored" data-anchor-id="adjusting-the-trees-predicted-values">Adjusting the trees’ predicted values</h2>
<p>After training a regression tree to predict the pseudo residuals, we need to adjust the predicted values in its terminal nodes to optimize the overall objective function. In the Greedy Function Approximation paper, Friedman actually specifies finding the optimal value using a numerical optimization routine like line search. We could express that like</p>
<p><img src="https://latex.codecogs.com/png.latex?%20v%20=%20%5Ctext%7Bargmin%7D_v%20%5Csum_%7Bi%20%5Cin%20t%7D%20J(y_%7Bi%7D,%20F(%5Cmathbf%7Bx%7D_i)%20+%20v)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?t"> is the set of samples falling into this terminal node.</p>
<p>In the scikit-learn implementation of gradient boosting classification, the authors instead use the approach from <a href="https://www.researchgate.net/publication/228776646_Additive_Logistic_Regression_A_Statistical_View_of_Boosting">FHT00</a> which uses a single Newton descent step to approximate the optimal predicted value for each terminal node. See code and comments for the function <code>_update_terminal_regions</code> in the scikit-learn gradient boosting module. The updated value is computed like</p>
<p><img src="https://latex.codecogs.com/png.latex?%20v%20=%20-%5Cfrac%7B%5Csum_%7Bi%20%5Cin%20t%7D%20J'(F(%5Cmathbf%7Bx%7D_i))%7D%7B%5Csum_%7Bi%20%5Cin%20t%7D%20J''(F(%5Cmathbf%7Bx%7D_i))%7D%20"></p>
<p>We already found the first derivative of the objective, so we just need to calculate the second derivative.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J''(F_k(%5Cmathbf%7Bx%7D_i))%20=%0A%5Cleft%5B%20%5Cfrac%7B%5Cpartial%20J(%5C%7B%20y_%7Bil%7D,%20F_l(%5Cmathbf%7Bx_i%7D)%5C%7D_%7Bl=1%7D%5EK)%7D%7B%5Cpartial%20%5E2%20F_k(%5Cmathbf%7Bx%7D_i)%20%7D%20%5Cright%5D%0A=%20p_k(%5Cmathbf%7Bx%7D_i)%20(1%20-%20p_k(%5Cmathbf%7Bx%7D_i))%0A"></p>
<p>Here’s the internal method to compute the second derivative .</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _hessians(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, probabilities): </span>
<span id="cb7-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities)</span></code></pre></div>
<p>Then we can implement the internal method for updating the tree predicted values. I give more details about how to manually set scikit-learn’s decision tree predicted values in <a href="../../posts/gradient-boosting-machine-with-any-loss-function/">the post on gradient boosting with any loss function</a>.</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _update_terminal_nodes(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, tree, X, negative_gradients, hessians):</span>
<span id="cb8-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Update the terminal node predicted values'''</span></span>
<span id="cb8-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminal node id's</span></span>
<span id="cb8-4">    leaf_nodes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.nonzero(tree.tree_.children_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb8-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute leaf for each sample in ``X``.</span></span>
<span id="cb8-6">    leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tree.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(X)</span>
<span id="cb8-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> leaf <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> leaf_nodes:</span>
<span id="cb8-8">        samples_in_this_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> leaf)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb8-9">        negative_gradients_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> negative_gradients.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb8-10">        hessians_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hessians.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb8-11">        val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(negative_gradients_in_leaf) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(hessians_in_leaf)</span>
<span id="cb8-12">        tree.tree_.value[leaf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val</span></code></pre></div>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>At inference time, the user supplies an <code>X</code> with multiple observations of the feature variables, and our model needs to issue a prediction for each observation. We’ll start by implementing the <code>predict_proba</code> method, which takes <code>X</code> as input and returns a length-<img src="https://latex.codecogs.com/png.latex?K"> probability mass function for each observation in <code>X</code>. To do this, we’ll initialize the raw predictions with zeros, just as we did in training, and then for each class, we’ll loop through all the boosters, collecting their predictions on <code>X</code>, scaling by the learning rate, and summing them up. Finally, we use the softmax to transform raw predictions into the probabilities.</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict_proba(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb9-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate probability predictions for the given input data.'''</span></span>
<span id="cb9-3">    raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes))</span>
<span id="cb9-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb9-5">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> booster <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters:</span>
<span id="cb9-6">            raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> booster[k].predict(X)</span>
<span id="cb9-7">    probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb9-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities</span></code></pre></div>
<p>Then to get the predicted labels, we can use the <code>predict_proba</code> method to generate probabilities, simply returning the integer-encoded class label of the largest probability for each observation in <code>X</code>.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb10-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate predicted labels (as integer-encoded array)'''</span></span>
<span id="cb10-3">    probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.predict_proba(X)</span>
<span id="cb10-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.argmax(probabilities, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</section>
<section id="the-complete-multi-class-gradient-boosting-classification-model-implementation" class="level2">
<h2 class="anchored" data-anchor-id="the-complete-multi-class-gradient-boosting-classification-model-implementation">The complete multi-class gradient boosting classification model implementation</h2>
<p>Now we’re ready to implement a multi-class classification gradient boosting model class with public <code>fit</code>, <code>predict_proba</code>, and <code>predict</code> methods. We combine the components above into a <code>fit</code> method for model training, and we add the two prediction methods to complete the model’s functionality.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.tree <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DecisionTreeRegressor </span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OneHotEncoder</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> GradientBoostingClassifierFromScratch():</span>
<span id="cb11-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Gradient Boosting Classifier from Scratch.</span></span>
<span id="cb11-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb11-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Parameters</span></span>
<span id="cb11-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    ----------</span></span>
<span id="cb11-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_estimators : int</span></span>
<span id="cb11-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        number of boosting rounds</span></span>
<span id="cb11-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb11-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    learning_rate : float</span></span>
<span id="cb11-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        learning rate hyperparameter</span></span>
<span id="cb11-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb11-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    max_depth : int</span></span>
<span id="cb11-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        maximum tree depth</span></span>
<span id="cb11-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    '''</span></span>
<span id="cb11-20">    </span>
<span id="cb11-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_estimators, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb11-22">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> </span>
<span id="cb11-23">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learning_rate</span>
<span id="cb11-24">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb11-25">    </span>
<span id="cb11-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y):</span>
<span id="cb11-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Fit the GBM</span></span>
<span id="cb11-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb11-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Parameters</span></span>
<span id="cb11-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        ----------</span></span>
<span id="cb11-31"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        X : ndarray of size (number observations, number features)</span></span>
<span id="cb11-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            design matrix</span></span>
<span id="cb11-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            </span></span>
<span id="cb11-34"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        y : ndarray of size (number observations,)</span></span>
<span id="cb11-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            integer-encoded target labels in {0,1,...,k-1}</span></span>
<span id="cb11-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        '''</span></span>
<span id="cb11-37">        </span>
<span id="cb11-38">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(y).nunique()</span>
<span id="cb11-39">        y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._one_hot_encode_labels(y)</span>
<span id="cb11-40"></span>
<span id="cb11-41">        raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_ohe.shape)</span>
<span id="cb11-42">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb11-43">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-44">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> m <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators):</span>
<span id="cb11-45">            class_trees <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-46">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb11-47">                negative_gradients <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._negative_gradients(y_ohe[:, k], probabilities[:, k])</span>
<span id="cb11-48">                hessians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._hessians(probabilities[:, k])</span>
<span id="cb11-49">                tree <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_depth)</span>
<span id="cb11-50">                tree.fit(X, negative_gradients)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb11-51">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._update_terminal_nodes(tree, X, negative_gradients, hessians)</span>
<span id="cb11-52">                raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> tree.predict(X)</span>
<span id="cb11-53">                probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb11-54">                class_trees.append(tree)</span>
<span id="cb11-55">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters.append(class_trees)</span>
<span id="cb11-56">    </span>
<span id="cb11-57">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _one_hot_encode_labels(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y):</span>
<span id="cb11-58">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.values</span>
<span id="cb11-59">        ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OneHotEncoder()</span>
<span id="cb11-60">        y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ohe.fit_transform(y.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).toarray()</span>
<span id="cb11-61">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe</span>
<span id="cb11-62">        </span>
<span id="cb11-63">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _negative_gradients(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y_ohe, probabilities):</span>
<span id="cb11-64">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities</span>
<span id="cb11-65">    </span>
<span id="cb11-66">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _hessians(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, probabilities): </span>
<span id="cb11-67">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities)</span>
<span id="cb11-68"></span>
<span id="cb11-69">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _softmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, raw_predictions):</span>
<span id="cb11-70">        numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(raw_predictions) </span>
<span id="cb11-71">        denominator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(raw_predictions), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb11-72">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> denominator</span>
<span id="cb11-73">        </span>
<span id="cb11-74">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _update_terminal_nodes(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, tree, X, negative_gradients, hessians):</span>
<span id="cb11-75">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Update the terminal node predicted values'''</span></span>
<span id="cb11-76">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminal node id's</span></span>
<span id="cb11-77">        leaf_nodes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.nonzero(tree.tree_.children_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb11-78">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute leaf for each sample in ``X``.</span></span>
<span id="cb11-79">        leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tree.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(X)</span>
<span id="cb11-80">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> leaf <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> leaf_nodes:</span>
<span id="cb11-81">            samples_in_this_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> leaf)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb11-82">            negative_gradients_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> negative_gradients.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-83">            hessians_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hessians.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-84">            val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(negative_gradients_in_leaf) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(hessians_in_leaf)</span>
<span id="cb11-85">            tree.tree_.value[leaf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val</span>
<span id="cb11-86">          </span>
<span id="cb11-87">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict_proba(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb11-88">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate probability predictions for the given input data.'''</span></span>
<span id="cb11-89">        raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes))</span>
<span id="cb11-90">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb11-91">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> booster <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters:</span>
<span id="cb11-92">                raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> booster[k].predict(X)</span>
<span id="cb11-93">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb11-94">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities</span>
<span id="cb11-95">        </span>
<span id="cb11-96">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb11-97">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate predicted labels (as 1-d array)'''</span></span>
<span id="cb11-98">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.predict_proba(X)</span>
<span id="cb11-99">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.argmax(probabilities, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
</section>
<section id="testing-our-implementation" class="level2">
<h2 class="anchored" data-anchor-id="testing-our-implementation">Testing our implementation</h2>
<p>Let’s test our implementation alongside the scikit-learn <code>GradientBoostingClassifier</code> to ensure it works as expected.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_classification</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy_score</span>
<span id="cb12-4"></span>
<span id="cb12-5">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_classification(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>, </span>
<span id="cb12-6">                           n_classes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, </span>
<span id="cb12-7">                           n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>,</span>
<span id="cb12-8">                           n_informative<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb12-9">                           random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb12-10"></span>
<span id="cb12-11">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GradientBoostingClassifier</span>
<span id="cb13-2"></span>
<span id="cb13-3">gbc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, </span>
<span id="cb13-4">                                 learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb13-5">                                 max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb13-6">gbc.fit(X_train, y_train)</span>
<span id="cb13-7">accuracy_score(y_test, gbc.predict(X_test))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>0.7756</code></pre>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">gbcfs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingClassifierFromScratch(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, </span>
<span id="cb15-2">                                              learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb15-3">                                              max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb15-4">gbcfs.fit(X_train, y_train)</span>
<span id="cb15-5">accuracy_score(y_test, gbcfs.predict(X_test))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>0.7768</code></pre>
</div>
</div>
<p>Beautiful. Our implementation is performing comparably to the sklearn gradient boosting classifier!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, there you have it, another epic scratch build for the books. I think the most interesting thing about the multi-class gradient boosting algorithm is that it generates multi-dimensional predictions based on a single objective function by training multiple decision trees in each boosting round. That’s a very interesting extension of the classic gradient boosting machine! If you have questions about the implementation, or if you found this post helpful, please leave a comment below to tell me about it.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)">Friedman’s Greedy Function Approximation paper</a></li>
<li><a href="https://www.researchgate.net/publication/228776646_Additive_Logistic_Regression_A_Statistical_View_of_Boosting">Friedman, Hastie, and Tibshirani 2000: paper on additive logistic regression</a></li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/index.html</guid>
  <pubDate>Sun, 15 Oct 2023 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/multi-class-classification-from-scratch.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>XGBoost for Regression in Python</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-for-regression-in-python/index.html</link>
  <description><![CDATA[ 



<p>In this post I’m going to show you my process for solving regression problems with XGBoost in python, using either the native <code>xgboost</code> API or the scikit-learn interface. This is a powerful methodology that can produce world class results in a short time with minimal thought or effort. While we’ll be working on an old Kagle competition for predicting the sale prices of bulldozers and other heavy machinery, you can use this flow to solve whatever tabular data regression problem you’re working on.</p>
<p>This post serves as the explanation and documentation for the XGBoost regression jupyter notebook from my <a href="https://github.com/mcb00/ds-templates">ds-templates repo</a> on GitHub, so go ahead and download the notebook and follow along with your own data.</p>
<p>If you’re not already comfortable with the ideas behind gradient boosting and XGBoost, you’ll find it helpful to read some of my previous posts to get up to speed. I’d start with this <a href="../../posts/gradient-boosting-machine-from-scratch/">introduction to gradient boosting</a>, and then read this <a href="../../posts/xgboost-explained/">explanation of how XGBoost works</a>.</p>
<p>Let’s get into it! 🚀</p>
<section id="install-and-import-the-xgboost-library" class="level2">
<h2 class="anchored" data-anchor-id="install-and-import-the-xgboost-library">Install and import the <code>xgboost</code> library</h2>
<p>If you don’t already have it, go ahead and <a href="https://anaconda.org/conda-forge/xgboost">use conda to install the xgboost library</a>, e.g.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode .zsh code-with-copy"><code class="sourceCode zsh"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> conda install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge xgboost</span></code></pre></div>
<p>Then import it along with the usual suspects.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb</span></code></pre></div>
</div>
</section>
<section id="read-dataset-into-python" class="level2">
<h2 class="anchored" data-anchor-id="read-dataset-into-python">Read dataset into python</h2>
<p>In this example we’ll work on the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/overview">Kagle Bluebook for Bulldozers</a> competition, which asks us to build a regression model to predict the sale price of heavy equipment. Amazingly, you can solve your own regression problem by swapping this data out with your organization’s data before proceeding with the tutorial.</p>
<p>Go ahead and download the <code>Train.zip</code> file from Kagle and extract it into <code>Train.csv</code>. Then read the data into a pandas dataframe.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Train.csv'</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>Notice I cheated a little bit, checking the columns ahead of time and telling pandas to treat the <code>saledate</code> column as a date. In general it will make life easier to read in any date-like columns as dates.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 401125 entries, 0 to 401124
Data columns (total 53 columns):
 #   Column                    Non-Null Count   Dtype         
---  ------                    --------------   -----         
 0   SalesID                   401125 non-null  int64         
 1   SalePrice                 401125 non-null  int64         
 2   MachineID                 401125 non-null  int64         
 3   ModelID                   401125 non-null  int64         
 4   datasource                401125 non-null  int64         
 5   auctioneerID              380989 non-null  float64       
 6   YearMade                  401125 non-null  int64         
 7   MachineHoursCurrentMeter  142765 non-null  float64       
 8   UsageBand                 69639 non-null   object        
 9   saledate                  401125 non-null  datetime64[ns]
 10  fiModelDesc               401125 non-null  object        
 11  fiBaseModel               401125 non-null  object        
 12  fiSecondaryDesc           263934 non-null  object        
 13  fiModelSeries             56908 non-null   object        
 14  fiModelDescriptor         71919 non-null   object        
 15  ProductSize               190350 non-null  object        
 16  fiProductClassDesc        401125 non-null  object        
 17  state                     401125 non-null  object        
 18  ProductGroup              401125 non-null  object        
 19  ProductGroupDesc          401125 non-null  object        
 20  Drive_System              104361 non-null  object        
 21  Enclosure                 400800 non-null  object        
 22  Forks                     192077 non-null  object        
 23  Pad_Type                  79134 non-null   object        
 24  Ride_Control              148606 non-null  object        
 25  Stick                     79134 non-null   object        
 26  Transmission              183230 non-null  object        
 27  Turbocharged              79134 non-null   object        
 28  Blade_Extension           25219 non-null   object        
 29  Blade_Width               25219 non-null   object        
 30  Enclosure_Type            25219 non-null   object        
 31  Engine_Horsepower         25219 non-null   object        
 32  Hydraulics                320570 non-null  object        
 33  Pushblock                 25219 non-null   object        
 34  Ripper                    104137 non-null  object        
 35  Scarifier                 25230 non-null   object        
 36  Tip_Control               25219 non-null   object        
 37  Tire_Size                 94718 non-null   object        
 38  Coupler                   213952 non-null  object        
 39  Coupler_System            43458 non-null   object        
 40  Grouser_Tracks            43362 non-null   object        
 41  Hydraulics_Flow           43362 non-null   object        
 42  Track_Type                99153 non-null   object        
 43  Undercarriage_Pad_Width   99872 non-null   object        
 44  Stick_Length              99218 non-null   object        
 45  Thumb                     99288 non-null   object        
 46  Pattern_Changer           99218 non-null   object        
 47  Grouser_Type              99153 non-null   object        
 48  Backhoe_Mounting          78672 non-null   object        
 49  Blade_Type                79833 non-null   object        
 50  Travel_Controls           79834 non-null   object        
 51  Differential_Type         69411 non-null   object        
 52  Steering_Controls         69369 non-null   object        
dtypes: datetime64[ns](1), float64(2), int64(6), object(44)
memory usage: 162.2+ MB</code></pre>
</div>
</div>
</section>
<section id="prepare-raw-data-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="prepare-raw-data-for-xgboost">Prepare raw data for XGBoost</h2>
<p>When faced with a new tabular dataset for modeling, we have two format considerations: data types and missingness. From the call to <code>df.info()</code> above, we can see we have both mixed types and missing values.</p>
<p>When it comes to missing values, some models like the gradient booster or random forest in scikit-learn require purely non-missing inputs. One of the great strengths of XGBoost is that it relaxes this requirement, allowing us to pass in missing feature values, so we don’t have to worry about them.</p>
<p>Regarding data types, all ML models for tabular data require inputs to be numeric, either integers or floats, so we’re going to have to deal with those <code>object</code> columns.</p>
<section id="encode-string-features" class="level3">
<h3 class="anchored" data-anchor-id="encode-string-features">Encode string features</h3>
<p>The simplest way to encode string variables is to map each unique string value to an integer; this is called <em>integer encoding</em>.</p>
<p>We can easily accomplish this by using the <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical data type in pandas</a>. The category type is a bit like the factor type in R; pandas stores the underlying data as integers, and it keeps a mapping from the integers back to the original string values. XGBoost is able to access the numeric data underlying the categorical features for model training and prediction. This is a nice way to encode string features because it’s easy to implement and it preserves the original category levels in the data frame. If you prefer to generate your own integer mappings, you can also do it with the scikit-learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html">OrdinalEncoder</a>.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> encode_string_features(df):</span>
<span id="cb6-2">    out_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb6-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> feature, feature_type <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> df.dtypes.items():</span>
<span id="cb6-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> feature_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'object'</span>:</span>
<span id="cb6-5">            out_df[feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> out_df[feature].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb6-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> out_df</span>
<span id="cb6-7"></span>
<span id="cb6-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encode_string_features(df)</span></code></pre></div>
</div>
</section>
<section id="encode-date-and-timestamp-features" class="level3">
<h3 class="anchored" data-anchor-id="encode-date-and-timestamp-features">Encode date and timestamp features</h3>
<p>While dates feel sort of numeric, they are not quite numbers, so we need to transform them into numeric columns that XGBoost can understand. Unfortunately, encoding timestamps isn’t as straightforward as encoding strings, so we actually might need to engage in a little bit of feature engineering. A single date has many different attributes, e.g.&nbsp;days since epoch, year, quarter, month, day, day of year, day of week, is holiday, etc. Often a simple time index is the most useful information in a date column, so here we’ll just start by adding a feature that gives the number of days since some epoch date.</p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb7-2">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pd.Timestamp(year<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1970</span>, month<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, day<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-3">    ).dt.days</span></code></pre></div>
</div>
</section>
<section id="transform-the-target-if-necessary" class="level3">
<h3 class="anchored" data-anchor-id="transform-the-target-if-necessary">Transform the target if necessary</h3>
<p>In the interest of speed and efficiency, we didn’t bother doing any EDA with the feature data. Part of my justification for this is that trees are incredibly robust to outliers, colinearity, missingness, and other assorted nonsense in the feature data. However, they are not necessarily robust to nonsense in the target variable, so it’s worth having a look at it before proceeding any further.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">df.SalePrice.hist()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-7-output-1.png" class="img-fluid" alt="histogram of sale price showing right-skewed data"></p>
</div>
</div>
<p>Often when predicting prices it makes sense to use log price, especially when they span multiple orders of magnitude or have a strong right skew. These data look pretty friendly, lacking outliers and exhibiting only a mild positive skew; we could probably get away without doing any transformation. But checking the evaluation metric used to score the Kagle competition, we see they’re using root mean squared log error. That’s equivalent to using RMSE on log-transformed target data, so let’s go ahead and work with log prices.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>])</span>
<span id="cb9-2">df.logSalePrice.hist()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-8-output-1.png" class="img-fluid" alt="histogram of log sale price showing a more symetric distribution"></p>
</div>
</div>
</section>
</section>
<section id="train-and-evaluate-the-xgboost-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="train-and-evaluate-the-xgboost-regression-model">Train and Evaluate the XGBoost regression model</h2>
<p>Having prepared our dataset, we are now ready to train an XGBoost model. Let’s walk through the flow step-by-step.</p>
<section id="split-the-data-into-training-and-validation-sets" class="level3">
<h3 class="anchored" data-anchor-id="split-the-data-into-training-and-validation-sets">Split the data into training and validation sets</h3>
<p>First we split the dataset into a training set and a validation set. Of course since we’re going to evaluate against the validation set a number of times as we iterate, it’s best practice to keep a separate test set reserved to check our final model to ensure it generalizes well. Assuming that final test set is hidden away, we can use the rest of the data for training and validation.</p>
<p>There are two main ways we might want to select the validation set. If there isn’t a temporal ordering of the observations, we might be able to randomly sample. In practice, it’s common that observations have a temporal ordering, and that models are trained on observations up to a certain time and used to predict on observations occuring after that time. Since this data is temporal, we don’t want to split randomly; instead we’ll split on observation date, reserving the latest observations for the validation set.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Temporal Validation Set</span></span>
<span id="cb10-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> train_test_split_temporal(df, datetime_column, n_test):</span>
<span id="cb10-3">    idx_sort <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argsort(df[datetime_column])</span>
<span id="cb10-4">    idx_train, idx_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> idx_sort[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_valid], idx_sort[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_valid:]</span>
<span id="cb10-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> df.iloc[idx_train, :], df.iloc[idx_test, :]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb11-2"></span>
<span id="cb11-3">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split_temporal(df, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>, n_valid)</span>
<span id="cb11-4">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>((389125, 55), (12000, 55))</code></pre>
</div>
</div>
</section>
<section id="specify-target-and-feature-columns" class="level3">
<h3 class="anchored" data-anchor-id="specify-target-and-feature-columns">Specify target and feature columns</h3>
<p>Next we’ll put together a list of our features and define the target column. I like to have an actual list defined in the code so it’s easy to explicitly see everything we’re puting into the model and easier to add or remove features as we iterate. Just run something like <code>list(df.columns)</code> in a cel to get a copy-pasteable list of columns, then edit it down to the full list of features, i.e.&nbsp;remove the target, date columns, and other non-feature columns..</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  list(df.columns)</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb14-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,</span>
<span id="cb14-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>,</span>
<span id="cb14-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ModelID'</span>,</span>
<span id="cb14-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datasource'</span>,</span>
<span id="cb14-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auctioneerID'</span>,</span>
<span id="cb14-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>,</span>
<span id="cb14-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineHoursCurrentMeter'</span>,</span>
<span id="cb14-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'UsageBand'</span>,</span>
<span id="cb14-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDesc'</span>,</span>
<span id="cb14-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>,</span>
<span id="cb14-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiSecondaryDesc'</span>,</span>
<span id="cb14-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelSeries'</span>,</span>
<span id="cb14-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>,</span>
<span id="cb14-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>,</span>
<span id="cb14-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiProductClassDesc'</span>,</span>
<span id="cb14-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'state'</span>,</span>
<span id="cb14-18">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroup'</span>,</span>
<span id="cb14-19">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>,</span>
<span id="cb14-20">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Drive_System'</span>,</span>
<span id="cb14-21">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span>,</span>
<span id="cb14-22">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Forks'</span>,</span>
<span id="cb14-23">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pad_Type'</span>,</span>
<span id="cb14-24">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ride_Control'</span>,</span>
<span id="cb14-25">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick'</span>,</span>
<span id="cb14-26">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Transmission'</span>,</span>
<span id="cb14-27">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Turbocharged'</span>,</span>
<span id="cb14-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Extension'</span>,</span>
<span id="cb14-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Width'</span>,</span>
<span id="cb14-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure_Type'</span>,</span>
<span id="cb14-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Engine_Horsepower'</span>,</span>
<span id="cb14-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics'</span>,</span>
<span id="cb14-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pushblock'</span>,</span>
<span id="cb14-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ripper'</span>,</span>
<span id="cb14-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Scarifier'</span>,</span>
<span id="cb14-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tip_Control'</span>,</span>
<span id="cb14-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tire_Size'</span>,</span>
<span id="cb14-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler'</span>,</span>
<span id="cb14-39">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler_System'</span>,</span>
<span id="cb14-40">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>,</span>
<span id="cb14-41">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics_Flow'</span>,</span>
<span id="cb14-42">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Track_Type'</span>,</span>
<span id="cb14-43">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Undercarriage_Pad_Width'</span>,</span>
<span id="cb14-44">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick_Length'</span>,</span>
<span id="cb14-45">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Thumb'</span>,</span>
<span id="cb14-46">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pattern_Changer'</span>,</span>
<span id="cb14-47">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Type'</span>,</span>
<span id="cb14-48">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Backhoe_Mounting'</span>,</span>
<span id="cb14-49">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Type'</span>,</span>
<span id="cb14-50">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Travel_Controls'</span>,</span>
<span id="cb14-51">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Differential_Type'</span>,</span>
<span id="cb14-52">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Steering_Controls'</span>,</span>
<span id="cb14-53">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span></span>
<span id="cb14-54"> ]</span>
<span id="cb14-55"></span>
<span id="cb14-56">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span></span></code></pre></div>
</div>
</section>
<section id="create-dmatrix-data-objects" class="level3">
<h3 class="anchored" data-anchor-id="create-dmatrix-data-objects">Create <code>DMatrix</code> data objects</h3>
<p>XGBoost uses a data type called dense matrix for efficient training and prediction, so next we need to create <code>DMatrix</code> objects for our training and validation datasets. Remember how we decided to encode our string columns by casting them as pandas categorical types? For this to work, we need to set the <code>enable_categoricals</code> argument to <code>True</code>.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], </span>
<span id="cb15-2">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb15-3">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], </span>
<span id="cb15-4">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
</section>
<section id="set-the-xgboost-parameters" class="level3">
<h3 class="anchored" data-anchor-id="set-the-xgboost-parameters">Set the XGBoost parameters</h3>
<p>XGBoost has <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">numerous hyperparameters</a>. Fortunately, just a handful of them tend to be the most influential; furthermore, the default values are not bad in most situations. I like to start out with a dictionary containing the default values for the parameters I’m most likely to adjust later, with one exception. I dislike the default value of <code>auto</code> for the <code>tree_method</code> parameter, which tells XGBoost to choose a tree method on it’s own. I’ve been burned by this ambiguity in the past, so now I prefer to set it to <code>approx</code>. For training there is one required boosting parameter called <code>num_boost_round</code> which I set to 50 as a starting point.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># default values for important parameters</span></span>
<span id="cb16-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb16-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb16-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>,</span>
<span id="cb16-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,</span>
<span id="cb16-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb16-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb16-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb16-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg:squarederror'</span>,</span>
<span id="cb16-10">}</span>
<span id="cb16-11">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span></code></pre></div>
</div>
</section>
<section id="train-the-xgboost-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-xgboost-model">Train the XGBoost model</h3>
<p>The <code>xgb.train()</code> function takes our training dataset and parameters, and it returns a trained XGBoost model, which is an object of class <code>xgb.core.Booster</code>. Check out the <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training">documentation on the learning API</a> to see all the training options. During training, I like to have XGBoost print out the evaluation metric on the train and validation set after every few boosting rounds and again at the end of training; that can be done by setting <code>evals</code> and <code>verbose_eval</code>.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb17-2">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb17-3">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-rmse:6.74240  valid-rmse:6.80825
[10]    train-rmse:0.31071  valid-rmse:0.34532
[20]    train-rmse:0.21950  valid-rmse:0.24364
[30]    train-rmse:0.20878  valid-rmse:0.23669
[40]    train-rmse:0.20164  valid-rmse:0.23254
[49]    train-rmse:0.19705  valid-rmse:0.23125</code></pre>
</div>
</div>
</section>
<section id="train-the-xgboost-model-using-the-sklearn-interface" class="level3">
<h3 class="anchored" data-anchor-id="train-the-xgboost-model-using-the-sklearn-interface">Train the XGBoost model using the sklearn interface</h3>
<p>If you prefer scikit-learn-like syntax, you can use the <a href="https://xgboost.readthedocs.io/en/latest/python/sklearn_estimator.html">sklearn estimator interface</a> to create and train XGBoost models. The <code>XGBRegressor</code> class, which is available in the <code>xgboost</code> library that we already imported, constructs an <code>XGBRegressor</code> object with <code>fit</code> and <code>predict</code> methods like you’re used to using in scikit-learn. The <code>fit</code> and <code>predict</code> methods take pandas dataframes, so you don’t need to create <code>DMatrix</code> data objects yourself; however, since these methods still have to transform input data into <code>DMatrix</code> objects internally, training and prediction seem to be slower via the sklearn interface.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scikit-learn interface</span></span>
<span id="cb19-2">reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb19-3">reg.fit(train_df[features], train_df[target], </span>
<span id="cb19-4">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb19-5">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-rmse:6.74240   validation_1-rmse:6.80825
[10]    validation_0-rmse:0.31071   validation_1-rmse:0.34532
[20]    validation_0-rmse:0.21950   validation_1-rmse:0.24364
[30]    validation_0-rmse:0.20878   validation_1-rmse:0.23669
[40]    validation_0-rmse:0.20164   validation_1-rmse:0.23254
[49]    validation_0-rmse:0.19705   validation_1-rmse:0.23125</code></pre>
</div>
</div>
<p>Since not all features of XGBoost are available through the scikit-learn estimator interface, you might want to get the native <code>xgb.core.Booster</code> object back out of the sklearn wrapper.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">booster <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg.get_booster()</span></code></pre></div>
</div>
</section>
<section id="evaluate-the-xgboost-model-and-check-for-overfitting" class="level3">
<h3 class="anchored" data-anchor-id="evaluate-the-xgboost-model-and-check-for-overfitting">Evaluate the XGBoost model and check for overfitting</h3>
<p>We get the model evaluation metrics on the training and validation sets printed to stdout when we use the <code>evals</code> argument to the training API. Typically I just look at those printed metrics, but sometimes it’s helpful to retain them in a variable for further inspection via, e.g.&nbsp;plotting. To do that we need to train again, passing an empty dictionary to the <code>evals_result</code> argument. In the objective curves, I’m looking for signs of overfitting, which could include validation scores staying the same or getting worse over later iterations or huge gaps between training and validation scores.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">evals_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb22-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb22-3">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb22-4">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb22-5">                  evals_result<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals_result)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-rmse:6.74240  valid-rmse:6.80825
[10]    train-rmse:0.31071  valid-rmse:0.34532
[20]    train-rmse:0.21950  valid-rmse:0.24364
[30]    train-rmse:0.20878  valid-rmse:0.23669
[40]    train-rmse:0.20164  valid-rmse:0.23254
[49]    train-rmse:0.19705  valid-rmse:0.23125</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">pd.DataFrame({</span>
<span id="cb24-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>: evals_result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>],</span>
<span id="cb24-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>: evals_result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb24-4">}).plot()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting round'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img" alt="line plot showing objective function versus training iteration for training and validation sets"></p>
<figcaption class="figure-caption">These objective curves look pretty good–no obvious signs of trouble.</figcaption>
</figure>
</div>
</div>
</div>
<p>While we could just look at the validation RMSE in the printed output from model training, let’s go ahead and compute it by hand, just to be sure.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_squared_error</span>
<span id="cb25-2"></span>
<span id="cb25-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># squared=False returns RMSE</span></span>
<span id="cb25-4">mean_squared_error(y_true<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dvalid.get_label(), </span>
<span id="cb25-5">                   y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.predict(dvalid), </span>
<span id="cb25-6">                   squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>0.23124987</code></pre>
</div>
</div>
<p>So, how good is that RMSLE of 0.231? Well, checking the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard">Kagle leaderboard</a> for this competition, we would have come in around 5th out of 474. That’s not bad for 10 minutes of work doing the bare minimum necessary to transform the raw data into a format consumable by XGBoost and then training a model using default hyperparameter values. To improve our model from here we would want to explore some feature engineering and some hyperparameter tuning, which we’ll save for another post.</p>
<blockquote class="blockquote">
<p>Wait, why was that so easy? Since XGBoost made it’s big Kagle debut in the <a href="https://www.kaggle.com/c/higgs-boson">2014 Higgs Boson competition</a>, presumably no one in this 2013 competition was using it yet. A second potential reason is that we’re using a different validation set from that used for the final leaderboard (which is long closed), but our score is likely still a decent approximation for how we would have done in the competition.</p>
</blockquote>
</section>
</section>
<section id="xgboost-model-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-model-interpretation">XGBoost Model Interpretation</h2>
<p>Next let’s have a look at how to apply a couple of the most common model interpretation techniques, feature importance and partial dependence, to XGBoost.</p>
<blockquote class="blockquote">
<p>Remember we have two trained models floating around: one called <code>model</code> of class <code>xgb.core.Booster</code> which is compatible with xgboost library utilities and another called <code>reg</code> of class <code>XGBRegressor</code> which is compatible with scikit-learn utilities. We need to be sure to use the model that’s compatible with whatever utility we’re using.</p>
</blockquote>
<blockquote class="blockquote">
<p>While these interpretation tools are still very common, there’s a newer, more comprehensive, and self-consistent model interpretation framework called <a href="https://shap.readthedocs.io/en/latest/">SHAP</a> that’s worth checking out.</p>
</blockquote>
<section id="feature-importance-for-xgboost" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-for-xgboost">Feature Importance for XGBoost</h3>
<p>While XGBoost automatically computes feature importance by three different metrics during training, you should only use them with great care and skepticism. The three metrics are</p>
<ul>
<li><strong>weight</strong>: the number of splits that use the feature</li>
<li><strong>gain</strong>: the average gain in the objective function from splits which use the feature</li>
<li><strong>cover</strong>: the average number of training samples affected by splits that use the feature</li>
</ul>
<p>The first problem with these metrics is that they are computed using only the training dataset, which means they don’t reflect how useful a feature is when predicting on out-of-sample data. If your model is overfit on some nonsense feature, it will still have a high importance. Secondly, I think they are difficult to interpret; all three are specific to decision trees and reflect domain-irrelevant idiosyncrasies like whether a feature is used nearer the root or the leaves of a tree. Anyway let’s see what these metrics have to say about our features.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">fig, (ax1, ax2, ax3) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb27-2">xgb.plot_importance(model, importance_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'weight'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importance_type=weight'</span>, </span>
<span id="cb27-3">                    max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, show_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax1, )</span>
<span id="cb27-4">xgb.plot_importance(model, importance_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cover'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importance_type=cover'</span>, </span>
<span id="cb27-5">                    max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, show_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax2)</span>
<span id="cb27-6">xgb.plot_importance(model, importance_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gain'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importance_type=gain'</span>, </span>
<span id="cb27-7">                    max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, show_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax3)</span>
<span id="cb27-8">plt.tight_layout()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img" alt="three horizontal bar plots showing feature importance for weight, cover, and gain metrics"></p>
<figcaption class="figure-caption">top 10 features according to each built-in XGBoost feature importance metric</figcaption>
</figure>
</div>
</div>
</div>
<p>Wow, notice that the top 10 features by weight and by cover are completely different. This should forever cause you to feel skeptical whenever you see a feature importance plot.</p>
<p>Luckily, there is a better way. IMHO, <a href="https://scikit-learn.org/stable/modules/permutation_importance.html">permutation feature importance</a> is better aligned with our intuition about what feature importance should mean. It tells us by how much the model performance decreases when the values of a particular feature are randomly shuffled during prediction. This effectively breaks the relationship between the feature and the target, thus revealing how much the model relies on that feature for prediction. It also has the benefit that it can be computed using either training data or out-of-sample data.</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> permutation_importance</span>
<span id="cb28-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_scorer</span>
<span id="cb28-3"></span>
<span id="cb28-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># make a scorer for RMSE</span></span>
<span id="cb28-5">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(mean_squared_error, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb28-6">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(reg, valid_df[features], valid_df[target], </span>
<span id="cb28-7">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb29-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb29-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb29-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'drop in RMSE'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption class="figure-caption">top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we can see which features the model relies on most for out-of-sample predictions. These are good candidate features to dig into with some EDA and conversations with any domain expert collaborators.</p>
</section>
</section>
<section id="partial-dependence-plots-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="partial-dependence-plots-for-xgboost">Partial Dependence Plots for XGBoost</h2>
<p>A <a href="https://scikit-learn.org/stable/modules/partial_dependence.html">partial dependence plot (PDP)</a> is a representation of the dependence between the target variable and one or more feature variables. We can loosely interpret it as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say “loosely” because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PartialDependenceDisplay</span>
<span id="cb30-2"></span>
<span id="cb30-3">PartialDependenceDisplay.from_estimator(reg, </span>
<span id="cb30-4">                                        valid_df[features].query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade &gt;= 1960'</span>), </span>
<span id="cb30-5">                                        [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img" alt="line plot showing partial dependence of logSalePrice on YearMade"></p>
<figcaption class="figure-caption">PDP of target logSalePrice on feature YearMade</figcaption>
</figure>
</div>
</div>
</div>
<p>It looks like the log sale price tends to increase in a non-linear way with year made.</p>
<div class="cell" data-execution_count="71">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The PDPs for categorical features expect numeric data, not pandas categorical types,</span></span>
<span id="cb31-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># so the sklearn API for partial dependence won't work directly with the dataframe we've been using.</span></span>
<span id="cb31-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The workaround is to create a new dataframe where categorical columns are encoded numerically,</span></span>
<span id="cb31-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># retrain the XGBoost model using the sklearn interface, create the PDPs,</span></span>
<span id="cb31-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># then add the category levels as the tick labels for the PDP.</span></span>
<span id="cb31-6"></span>
<span id="cb31-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> cat_pdp():</span>
<span id="cb31-8">    cat_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span></span>
<span id="cb31-9">    modified_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb31-10">    cat_codes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> modified_df[cat_feature].cat.codes</span>
<span id="cb31-11">    cat_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(modified_df[cat_feature].cat.categories)</span>
<span id="cb31-12">    cat_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NaN'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> cat_labels <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> cat_codes.unique() <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> cat_labels</span>
<span id="cb31-13">    modified_df[cat_feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cat_codes</span>
<span id="cb31-14"></span>
<span id="cb31-15">    n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb31-16">    train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split_temporal(modified_df, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>, n_valid)</span>
<span id="cb31-17">    train_df.shape, valid_df.shape</span>
<span id="cb31-18"></span>
<span id="cb31-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scikit-learn interface</span></span>
<span id="cb31-20">    reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb31-21">    reg.fit(train_df[features], train_df[target], </span>
<span id="cb31-22">            eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb31-23">            verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb31-24">    PartialDependenceDisplay.from_estimator(reg, valid_df[features], [cat_feature], categorical_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[cat_feature])</span>
<span id="cb31-25">    plt.xticks(ticks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cat_codes.unique(), labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cat_labels)</span>
<span id="cb31-26">cat_pdp()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img" alt="bar plot showing partial dependence of logSalePrice on Enclosure"></p>
<figcaption class="figure-caption">PDP of target logSalePrice on categorical feature Enclosure</figcaption>
</figure>
</div>
</div>
</div>
<p>You can imagine how useful these model interpretation tools can be, both for understanding data and for improving your models.</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There you have it, a simple flow for solving regression problems with XGBoost in python. Remember you can use the XGBoost regression notebook from my <a href="https://github.com/mcb00/ds-templates">ds-templates repo</a> to make it easy to follow this flow on your own problems. If you found this helpful, or if you have additional ideas about solving regression problems with XGBoost, let me know down in the comments.</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-for-regression-in-python/index.html</guid>
  <pubDate>Mon, 18 Sep 2023 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-for-regression-in-python/kigali-branches.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
