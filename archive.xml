<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<atom:link href="https://randomrealizations.com/archive.xml" rel="self" type="application/rss+xml"/>
<description>A blog about data science, statistics, machine learning, and the scientific method</description>
<image>
<url>https://randomrealizations.com/opengraph.png</url>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<height>76</height>
<width>144</width>
</image>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Wed, 04 Jun 2025 07:00:00 GMT</lastBuildDate>
<item>
  <title>Bayesian Modeling Primer</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/bayesian-modeling-primer/</link>
  <description><![CDATA[ 




<p>Well, dear reader, I know I haven‚Äôt been posting very much lately. That‚Äôs because I‚Äôve been busy moving to a new city and working a new DS gig and learning some new things, including Bayesian modeling. In particular I‚Äôve been reading Richard McElreath‚Äôs excellent book <a href="https://xcelab.net/rm/">Statistical Rethinking</a>, which I recommend to you as well. As a dedicated reader of this blog, I‚Äôm sure you‚Äôre perfectly capable of digesting a 600 page statistics textbook on your own, but just for fun, today I present to you my Bayesian statistics crash course.</p>
<p>My primary goal is to illuminate the major steps in the Bayesian workflow, that way you have a mental framework where you can store and contextualize new pieces of information as you learn. My secondary goal is to give you an intuitive understanding of Bayesian modeling from two interconnected perspectives: a mathematical formulation based primarily in probability theory and a probabilistic programming approach based on writing code to generate random data. Each perspective supports the other, and they are both necessary to grasp the full picture. I will attempt to weave these two perspectives throughout the description of the workflow, which is motivated by a toy example we‚Äôll use throughout the post.</p>
<p>Let‚Äôs do this! ‚û°Ô∏è</p>
<section id="the-rock-paper-scissors-pro" class="level2">
<h2 class="anchored" data-anchor-id="the-rock-paper-scissors-pro">ü™®üìÑ‚úÇÔ∏è The Rock Paper Scissors Pro</h2>
<p>I spent a summer as an intern at RAND Corporation during my PhD. It was a fascinating place full of fascinating characters. One of the researchers, Fritz R, liked to take each cohort of interns out for drinks at some point in the summer. After picking up our first round himself, Fritz offered to buy a second drink for any of the interns who could beat him in a rock paper scissors (RPS) match, warning us that he was ‚Äúpretty good at it.‚Äù</p>
<p>Let‚Äôs fact check his claim. We‚Äôd like to know something about his actual RPS win rate, but that is unobservable. We can‚Äôt observe it directly, but we could observe some match outcomes and make an inference about what his actual win rate might plausibly be.</p>
<p>Let‚Äôs say that after facing off with the 10 interns, Fritz racks up the following match outcomes.</p>
<div id="cell-3" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">observed_outcomes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<p>He won 7 out of 10 matches‚Äînot bad. But is his performance the result of skill or simply a lucky round? We‚Äôre going to address this question using Bayesian statistical analysis.</p>
</section>
<section id="the-bayesian-workflow-in-3-steps" class="level2">
<h2 class="anchored" data-anchor-id="the-bayesian-workflow-in-3-steps">üõ†Ô∏è The Bayesian Workflow in 3 Steps</h2>
<p>I consider the Bayesian workflow to have 3 major steps:</p>
<ol type="1">
<li><strong>Modeling</strong> - specify the data generating process as a generative model</li>
<li><strong>Inference</strong> - use the model, the observed data, and some inference algorithm to infer the values of unknown model parameters</li>
<li><strong>Interpretation</strong> - summarize and interpret the inferred model parameters to answer your analysis questions</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/modeling-inference-interpretation.png" title="." class="img-fluid figure-img"></p>
<figcaption>The Bayesian Workflow: modeling, inference, interpretation.</figcaption>
</figure>
</div>
</section>
<section id="step-1.-modeling" class="level2">
<h2 class="anchored" data-anchor-id="step-1.-modeling">‚öôÔ∏è Step 1. Modeling</h2>
<section id="modeling-the-data-generating-process" class="level3">
<h3 class="anchored" data-anchor-id="modeling-the-data-generating-process">Modeling the Data Generating Process</h3>
<p>In this step, we‚Äôre going to build a <em>generative model</em>, i.e.&nbsp;a model that can simulate data similar to our observed data. If you‚Äôre coming from ML, the key mental shift is to think about modeling the <em>data generating process (DGP)</em>, rather than curve-fitting the data itself. Practically this means our model is a set of random variables which relate to one another in some way and from which we can draw realizations‚Ä¶ random realizations, that is. You can invent a DGP as follows:</p>
<ol type="1">
<li>Identify the key variables in the system.</li>
<li>Define each variable as a draw from some probability distribution, or in terms of the other variables.</li>
<li>Use unknown parameters as needed in the probability distributions or in the functional relationships among the key variables.</li>
</ol>
<p>In our RPS example, there is one key variable‚ÄîFritz‚Äôs match outcome. We can define the match outcome variable as a random draw from some distribution, e.g.&nbsp;a Bernoulli distribution. The Bernoulli distribution has one parameter‚Äîthe success probability‚Äîwhich corresponds here to Fritz‚Äôs actual true win rate. Given some true win rate, we can simulate match outcomes by drawing realizations from the Bernoulli distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20y_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%200"> if Fritz loses to intern <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?y_i%20=%201"> if he wins, and <img src="https://latex.codecogs.com/png.latex?i=1,%5Cdots,N"> where <img src="https://latex.codecogs.com/png.latex?N=10">. In this DGP, the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> corresponds to Fritz‚Äôs true win rate.</p>
<p>This is a good start, but we can‚Äôt simulate data from this model yet because <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> has no particular value. So, what value should we use?</p>
</section>
<section id="probability-as-relative-plausibility" class="level3">
<h3 class="anchored" data-anchor-id="probability-as-relative-plausibility">Probability as Relative Plausibility</h3>
<p>One of the key ideas in Bayesian modeling is that we can represent the relative plausibility of potential values of any unobserved variable using a probability distribution. Highly plausible values get higher probability, and less plausible values get lower probability.</p>
<p><em>It is this view of probability as a measure of relative plausibility that distinguishes Bayesian statistics from Frequentist statistics, which views probability as the relative frequency of events.</em></p>
<p>We don‚Äôt know the true value of Fritz‚Äôs RPS win rate, but even before collecting any data, we might have some contextual knowledge about how the world works which can provide some prior information about the relative plausibility of its possible values. For me it‚Äôs easiest to think in terms of how surprising a given true value would be. I wouldn‚Äôt be surprised at all if his win rate was near 0.5, but I would be shocked if it was 0.9 or 0.1, hence 0.5 has higher relative plausibility than 0.9 or 0.1.</p>
<p>Let‚Äôs represent the prior relative plausibility of values of Fritz‚Äôs RPS win rate with a probability distribution. Below are a few different probability distributions defined over the possible values <img src="https://latex.codecogs.com/png.latex?0%20%5Cle%20%5Ctheta%20%5Cle%201">.</p>
<div id="cell-7" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> beta, bernoulli</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># set colors for later</span></span>
<span id="cb2-7">prior_color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C0"</span></span>
<span id="cb2-8">post_color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C1"</span></span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># prior beta parameters</span></span>
<span id="cb2-11">parameters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb2-12">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb2-13">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>),</span>
<span id="cb2-14">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-15">]</span>
<span id="cb2-16"></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plot the prior</span></span>
<span id="cb2-18">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb2-19">plt.figure()</span>
<span id="cb2-20"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, (alpha, beta_val) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(parameters):</span>
<span id="cb2-21">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> beta.pdf(x, alpha, beta_val)</span>
<span id="cb2-22">    label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Beta($</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">alpha$=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, $</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">beta$=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_val<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span></span>
<span id="cb2-23">    plt.plot(x, y, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>label)</span>
<span id="cb2-24"></span>
<span id="cb2-25">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"win rate (theta)"</span>)</span>
<span id="cb2-26">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Probability Density"</span>)</span>
<span id="cb2-27">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Relative Plausibility of RPS Win Rate'</span>)</span>
<span id="cb2-28">plt.legend()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img" alt="beta distribution priors"></p>
</figure>
</div>
</div>
</div>
<p>Each of these PDFs has a mode at <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5"> and decreases toward 0 and 1. They‚Äôre all aligned with the relative plausibilities we discussed earlier.</p>
<p>You can check the relative plausibility between two possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> implied by a given pdf by taking the ratio of the height of the pdf at one value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> versus the height at another value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p>For example, let‚Äôs compare <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5"> to <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.7"> for a Beta(10, 10) prior.</p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">beta.pdf(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> beta.pdf(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>np.float64(4.802710683776413)</code></pre>
</div>
</div>
<p>The Beta(10, 10) distribution implies that a 0.5 win rate is about 5 times more plausible than a 0.7 win rate, which sounds, ahem, plausible.</p>
</section>
<section id="priors" class="level3">
<h3 class="anchored" data-anchor-id="priors">Priors</h3>
<p>We can include this prior information about the relative plausibility of values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> in our model as follows.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%20%5Csim%20%5Ctext%7BBeta%7D(10,%2010)%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ay_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)%0A"></p>
<p>In Bayesian parlance, we call the probability distribution that represents the relative plausibilities of an unobserved parameter its <em>prior distribution</em>, or simply its <em>prior</em>. Notice that with the addition of the prior for <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, our model is now fully generative.</p>
</section>
<section id="implementing-the-generative-model" class="level3">
<h3 class="anchored" data-anchor-id="implementing-the-generative-model">Implementing the generative model</h3>
<p>Let‚Äôs implement the DGP using random variables from <code>scipy</code>.</p>
<div id="cell-11" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Implementing the DGP as a generative model</span></span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> draw_from_prior(alpha_param, beta_param):</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> beta.rvs(alpha_param, beta_param)</span>
<span id="cb5-5"></span>
<span id="cb5-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_one_outcome(theta, N):</span>
<span id="cb5-7">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bernoulli.rvs(theta, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb5-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>: theta, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span>: y, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>: np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(y)}</span>
<span id="cb5-9"></span>
<span id="cb5-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_outcomes(n_outcomes, alpha_param, beta_param, N):</span>
<span id="cb5-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame([</span>
<span id="cb5-12">        simulate_one_outcome(theta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>draw_from_prior(alpha_param, beta_param), N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb5-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_outcomes)</span>
<span id="cb5-14">    ])</span>
<span id="cb5-15"></span>
<span id="cb5-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># set DGP parameters</span></span>
<span id="cb5-17">alpha_param, beta_param <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb5-18">N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb5-19"></span>
<span id="cb5-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># simulate outcomes from the generative model</span></span>
<span id="cb5-21">outcome_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1_000</span>, alpha_param, beta_param, N)</span>
<span id="cb5-22">outcome_df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">theta</th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">sum_y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.451620</td>
<td>[0, 0, 1, 1, 1, 0, 0, 1, 0, 0]</td>
<td>4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.460305</td>
<td>[0, 1, 1, 0, 1, 1, 1, 0, 1, 1]</td>
<td>7</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.555594</td>
<td>[0, 0, 1, 1, 1, 1, 0, 1, 0, 0]</td>
<td>5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.518724</td>
<td>[1, 0, 0, 0, 1, 1, 1, 0, 1, 0]</td>
<td>5</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.569247</td>
<td>[1, 0, 1, 1, 0, 1, 1, 1, 1, 1]</td>
<td>8</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Each time you run this simulation, you first draw a new value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from its prior, then that value is used in the Bernoulli distribution to draw an array of binary match win/loss observations. To help us summarize the match observations in each simulated outcome, we also compute the sum of the match values, i.e.&nbsp;the number of wins.</p>
</section>
<section id="prior-predictive-check" class="level3">
<h3 class="anchored" data-anchor-id="prior-predictive-check">Prior Predictive Check</h3>
<p>But how do we know that the prior we chose is reasonable? There are two places we can look: (1) at the parameter itself and (2) at the downstream variables it influences. We already looked at the parameter itself by inspecting its pdf and thinking about the relative plausibilities it implies. To look at its impact on the downstream variables, we can simply run simulations from the model and inspect the outcome data it produces. If we see it‚Äôs generating lots of highly implausible outcomes, then we know something isn‚Äôt right. This process is called a <em>prior predictive check</em>, because we‚Äôre checking the simulated outcomes (a.k.a. predictions) implied by the prior. Let‚Äôs run our model simulation 1000 times and have a look at the distribution of the number of wins out of 10 matches that it predicts, i.e.&nbsp;the sum of the <code>y</code> variable from each simulation.</p>
<div id="cell-13" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">outcome_df.hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb6-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum(y)'</span>)</span>
<span id="cb6-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb6-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Prior Predictive Check'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img" alt="prior predictive check of number of match wins"></p>
</figure>
</div>
</div>
</div>
<p>The histogram shows most of the simulations yield between 3 and 7 wins, with very few outcomes less than 3 or greater than 7. That seems pretty reasonable.</p>
<p>Let‚Äôs look at what the prior predictive check might look like when things aren‚Äôt quite right.</p>
<div id="cell-15" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1_000</span>, alpha_param<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, beta_param<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>).hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb7-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum(y)'</span>)</span>
<span id="cb7-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb7-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Prior Predictive Check'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img" alt="prior predictive check of the win rate, with unexpected distribution shape"></p>
</figure>
</div>
</div>
</div>
<p>In this simulation, many of the outcomes are close to 0 or 10 wins out of 10. From our prior knowledge about RPS, we know it would be possible but very unusual for someone to win either 0/10 or 10/10 matches. This tips us off that something isn‚Äôt right with our priors. At this point we would iterate on our priors until we find something reasonable like our Beta(10, 10).</p>
<p>Once we‚Äôve got our generative model and its priors nailed down, we‚Äôre ready to move from the modeling step to the inference step!</p>
</section>
</section>
<section id="step-2.-inference" class="level2">
<h2 class="anchored" data-anchor-id="step-2.-inference">üßÆ Step 2. Inference</h2>
<section id="the-goal-of-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="the-goal-of-bayesian-inference">The Goal of Bayesian Inference</h3>
<p>In the inference step, we use observed outcome data to infer the plausible values of the unobserved parameters. Whereas simulation passes information forward from parameters to outcomes, inference passes it backwards from observed outcomes to parameters. It‚Äôs analogous to model fitting or training in machine learning; it‚Äôs the part where we use data to learn about the model parameters. The specific output of inference is the updated relative plausibility of the unknown model parameters. Whereas we represent the prior relative plausibilities with the prior distribution, we represent the posterior relative plausibilities (after incorporating information from the data) with the <em>posterior distribution</em>, or simply, the <em>posterior</em>. Like the prior, our model‚Äôs posterior distribution is a probability density defined over the possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, where larger values indicate higher relative plausibility.</p>
</section>
<section id="analytical-formulation-of-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="analytical-formulation-of-bayesian-inference">Analytical Formulation of Bayesian Inference</h3>
<p>Let‚Äôs nail down the mathematical formulation of Bayesian inference. We have data <img src="https://latex.codecogs.com/png.latex?y"> and parameter(s) <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. These have a joint probability density <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)">. This joint distribution of data and parameters is defined by our generative model of the system‚Äîsimulating data from our DGP is equivalent to drawing realizations from the joint distribution <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)">. Using the definition of conditional probability,, we can write the joint distribution as:</p>
<p><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)%20=%20p(y%7C%5Ctheta)p(%5Ctheta)%20"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)"> is the <em>joint distribution</em> of parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and data <img src="https://latex.codecogs.com/png.latex?y"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)"> is the <em>prior </em> distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(y%7C%5Ctheta)"> is the <em>likelihood</em>‚Äîthe conditional distribution of observed data <img src="https://latex.codecogs.com/png.latex?y"> given parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
</ul>
<p>When we do inference we are interested in the relative plausibility of unknown parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> given data <img src="https://latex.codecogs.com/png.latex?y">, which we quantify as the conditional distribution of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> given <img src="https://latex.codecogs.com/png.latex?y">. Using Baye‚Äôs Rule, we can write the posterior as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(%5Ctheta%20%7C%20y)%20=%20%5Cfrac%7B%20p(%5Ctheta,%20y)%20%7D%20%7B%20p(y)%20%7D%20=%20%5Cfrac%20%7B%20p(y%7C%5Ctheta)p(%5Ctheta)%20%20%7D%20%7B%20p(y)%20%7D%20%20"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%20%7C%20y)"> is the <em>posterior</em> distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(y)"> is the <em>marginal likelihood</em> of the data <img src="https://latex.codecogs.com/png.latex?y"> (to be explained soon)</li>
</ul>
<p>Technically, the joint distribution and the posterior are functions of <em>both</em> parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and data <img src="https://latex.codecogs.com/png.latex?y">. But in practice when we compute the posterior, we‚Äôll have some actual observed data‚Äîsay <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D">‚Äîso that <img src="https://latex.codecogs.com/png.latex?y"> is actually fixed at <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">. Substituting the fixed value <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D"> in the posterior, we get</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(%5Ctheta%20%7C%20y_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cfrac%7B%20p(%5Ctheta,%20y_%7B%5Ctext%7Bobs%7D%7D)%20%7D%20%7B%20p(y_%7B%5Ctext%7Bobs%7D%7D)%20%7D%20=%20%20%5Cfrac%20%7B%20p(y_%7B%5Ctext%7Bobs%7D%7D%7C%5Ctheta)p(%5Ctheta)%20%20%7D%20%7B%20%20%5Cint%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)p(%5Ctheta)%20d%20%5Ctheta%20%20%7D%20%20"></p>
<p>If we view <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D"> as fixed, then the posterior can be interpreted as just the slice of the joint distribution where <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">. To get a proper conditional probability distribution, we just need to divide the sliced joint density function by the area under <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,y_%7B%5Ctext%7Bobs%7D%7D)"> along the <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> axis. And guess what? That‚Äôs exactly what the marginal likelihood is doing; <img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cint%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)p(%5Ctheta)%20d%20%5Ctheta"> is just the area under the sliced joint density, and it‚Äôs there in the denominator to normalize the sliced joint density so that we get a proper conditional distribution for the posterior.</p>
</section>
<section id="computing-the-posterior-using-grid-approximation" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-posterior-using-grid-approximation">Computing the Posterior using Grid Approximation</h3>
<p>Let‚Äôs compute the posterior using the formulas we cooked up in the previous section. Earlier when we wrote down our generative model, we already identified all the pieces we need:</p>
<ul>
<li><strong>the prior</strong>‚Äîsince <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Csim%20%5Ctext%7BBeta%7D(10,%2010)">, <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)"> is the probability density function of a <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BBeta%7D(10,10)"> random variable.</li>
<li><strong>the likelihood</strong>‚Äîsince <img src="https://latex.codecogs.com/png.latex?y_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)">, the likelihood is the probability mass function of a Bernoulli random variable with parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">‚Ä¶ well, almost.</li>
</ul>
<p>The one remaining detail to iron out is that our observed data <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D=%5By_1,%5Cdots,y_N%5D"> consists of <img src="https://latex.codecogs.com/png.latex?N=10"> observations of the binary match outcomes. Our likelihood needs to reflect the conditional probability of the entire dataset given <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, not just a single observation. We know from probability theory that the joint probability of two independent events is the product of their individual probabilities. Therefore, assuming independence among our observations, the joint likelihood of the full dataset is the product of the likelihood of each observation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(y_%7B%5Ctext%7Bobs%7D%7D%7C%5Ctheta)%20=%20p(y_1,%5Cdots,y_N%7C%5Ctheta)%20=%20%20%5Cprod_%7Bi=1%7D%5EN%20p(y_i%7C%5Ctheta)%20"></p>
<p>Let‚Äôs implement the prior, the likelihood, the joint distribution, and the posterior in python and plot out the prior and the posterior distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<div id="cell-18" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0. Defining the observed data</span></span>
<span id="cb8-2"></span>
<span id="cb8-3">y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(observed_outcomes)</span>
<span id="cb8-4">sum_y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(y_obs)</span>
<span id="cb8-5"></span>
<span id="cb8-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Functions for Prior and Likelihood</span></span>
<span id="cb8-7"></span>
<span id="cb8-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> prior(theta):</span>
<span id="cb8-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> beta.pdf(theta, alpha_param, beta_param)</span>
<span id="cb8-10"></span>
<span id="cb8-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> likelihood(theta, y):</span>
<span id="cb8-12">    product_of_likelihoods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span></span>
<span id="cb8-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> y_i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> y:</span>
<span id="cb8-14">        product_of_likelihoods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> bernoulli.pmf(y_i, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>theta)</span>
<span id="cb8-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> product_of_likelihoods</span>
<span id="cb8-16"></span>
<span id="cb8-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Function for Joint Density p(y, theta)</span></span>
<span id="cb8-18"></span>
<span id="cb8-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> joint_density(theta, y):</span>
<span id="cb8-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> likelihood(theta, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> prior(theta)</span>
<span id="cb8-21"></span>
<span id="cb8-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Computing the Posterior by "Slicing" and Normalizing</span></span>
<span id="cb8-23"></span>
<span id="cb8-24"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> posterior_from_joint_slice(theta_values, y_observed_data):</span>
<span id="cb8-25">    </span>
<span id="cb8-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute grid of p(theta, y_obs) over values of theta and fixed y_obs</span></span>
<span id="cb8-27">    unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([</span>
<span id="cb8-28">        joint_density(theta, y_observed_data) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_values</span>
<span id="cb8-29">    ])</span>
<span id="cb8-30">    </span>
<span id="cb8-31">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># numerical integration  to get marginal likelihood p(y_obs</span></span>
<span id="cb8-32">    delta_theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> theta_values[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> theta_values[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] </span>
<span id="cb8-33">    marginal_likelihood_approx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> delta_theta)</span>
<span id="cb8-34">    </span>
<span id="cb8-35">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># p(theta | y_obs) = p(theta, y_obs) / p(y_obs)</span></span>
<span id="cb8-36">    normalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> marginal_likelihood_approx</span>
<span id="cb8-37">    </span>
<span id="cb8-38">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> normalized_posterior_values</span></code></pre></div>
</div>
<div id="cell-19" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define a grid of theta values</span></span>
<span id="cb9-2">theta_grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>) </span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate prior over the grid of theta values</span></span>
<span id="cb9-5">prior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([prior(theta) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_grid])</span>
<span id="cb9-6"></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate likelihood values over the grid of theta values and fixed y_obs</span></span>
<span id="cb9-8">likelihood_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([likelihood(theta, y_obs) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_grid])</span>
<span id="cb9-9"></span>
<span id="cb9-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate the posterior over the grid of theta values and fixed y_obs</span></span>
<span id="cb9-11">posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> posterior_from_joint_slice(theta_grid, y_obs)</span></code></pre></div>
</div>
<div id="cell-20" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plotting</span></span>
<span id="cb10-2">plt.plot(theta_grid, prior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Prior p(theta) ~ Beta(</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb10-3">plt.plot(theta_grid, posterior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Posterior p(theta|y_obs)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb10-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian Inference: Prior, Likelihood, and Posterior'</span>)</span>
<span id="cb10-5">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'$\theta$ (Probability of Match Win)'</span>)</span>
<span id="cb10-6">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb10-7">plt.legend()</span>
<span id="cb10-8">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb10-9">plt.xlim(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb10-10">plt.ylim(bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb10-11">plt.tight_layout()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img" alt="plot of prior and posterior computed with  grid approximation"></p>
</figure>
</div>
</div>
</div>
<p>From the figure we can see that while the prior is centered at <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5">, the posterior is actually pulled slightly toward larger values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> by the observed data, indicating increased relative plausibility on win rates greater than 0.5.</p>
<p>It‚Äôs nice to see that the math works and that we can successfully implement it in code, but grid approximation is a pedagogical endeavor. In practice, when models start to get complicated, we‚Äôll need a more flexible approach for finding the posterior.</p>
</section>
<section id="sampling-from-the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-posterior">Sampling from the Posterior</h3>
<p>It turns out that we can do inference using our generative model and our observed data without having to compute the likelihood directly. But while the forward problem of simulating data from the model is quite straightforward, it‚Äôs less obvious how to approach the inverse problem of doing inference. There is no silver bullet here. In fact there are tons of different algorithms for doing inference on probabilistic models, but luckily, since virtually all the important inference algorithms use sampling-based approaches, e.g.&nbsp;Hamiltonian Monte Carlo, Metropolis-Hastings, and Variational Inference, no matter which algorithm we use under the hood, we‚Äôll end up with the same kind of output at the end‚Äîsamples of the unknown parameters which have been drawn from the posterior.</p>
<p>To get some intuition for how we can do inference by sampling from the generative model,, we‚Äôre going to implement a very simple inference algorithm called <em>rejection sampling</em>. The key idea is based on the insight we discussed earlier that the posterior is essentially a slice through the joint distribution where the data is fixed to what we actually observed. Since simulating from the generative model is equivalent to drawing samples from the joint distribution, isolating simulation outcomes where the simmulated data is equal to the observed data is equivalent to sampling from the joint distribution along the slice, and hence equivalent to sampling from the posterior.</p>
<p>The algorithm for doing Bayesian inference via rejection sampling is as follows</p>
<ol type="1">
<li>Generate a sample <img src="https://latex.codecogs.com/png.latex?(%5Ctheta%5E*,y%5E*)"> from the generative model.</li>
<li>Keep the sample if <img src="https://latex.codecogs.com/png.latex?y%5E*=y_%7B%5Ctext%7Bobs%7D%7D">, otherwise discard it.</li>
<li>Repeat 1 and 2 until the desired number of retained samples is collected.</li>
<li>The retained samples of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> can be interpreted as samples from the posterior.</li>
</ol>
<p>Let‚Äôs do this in python. In our example, the order of the wins and losses over the <img src="https://latex.codecogs.com/png.latex?N=10"> match observations doesn‚Äôt matter, so we‚Äôll focus on the number of wins. Since <img src="https://latex.codecogs.com/png.latex?%5Csum_iy_i=7"> in our observed data, we‚Äôll isolate the simulation outcomes where <code>sum_y</code> equals 7.</p>
<div id="cell-22" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb11-2">outcome_df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y != @sum_y_obs'</span>).plot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>, kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scatter"</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"outcomes where $y^* </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">ne y_{</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">text</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{obs}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}$"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb11-3">outcome_df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y == @sum_y_obs'</span>).plot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>, kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scatter"</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"outcomes where $y^* = y_{</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">text</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{obs}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}$"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb11-4">ax.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sum_y_obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb11-5">ax.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sum_y_obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb11-6">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Samples from the Generative Model"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img" alt="scatter plot of samples from joint distribution with posterior slice highlighted"></p>
</figure>
</div>
</div>
</div>
<p>Boom! By isolating the outcomes where <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">, we effectively have samples from the posterior. Let‚Äôs draw a larger number of samples so we get an adequate sample from the posterior.</p>
<div id="cell-24" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># drawing a large number of samples and isolating outcomes where y = y_obs</span></span>
<span id="cb12-2">rejection_sampling_outcome_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span>, alpha_param, beta_param, N).query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y == @sum_y_obs'</span>)</span>
<span id="cb12-3"></span>
<span id="cb12-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># posterior samples from rejection sampling</span></span>
<span id="cb12-5">posterior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rejection_sampling_outcome_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>]</span></code></pre></div>
</div>
<div id="cell-25" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">posterior_samples.hist(bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color)</span>
<span id="cb13-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>)</span>
<span id="cb13-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Samples from the Posterior"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img" alt="histogram of posterior"></p>
</figure>
</div>
</div>
</div>
<p>Now let‚Äôs put all the pieces together.</p>
<div id="cell-27" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb14-2">prior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outcome_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>]</span>
<span id="cb14-3">prior_samples.hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prior samples"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb14-4">posterior_samples.hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"posterior samples"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb14-5">ax.plot(theta_grid, prior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Prior p(theta) ~ Beta(</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb14-6">ax.plot(theta_grid, posterior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Posterior p(theta|y_obs)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb14-7">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian Inference: Prior and Posterior'</span>)</span>
<span id="cb14-8">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'$\theta$ (Probability of Match Win)'</span>)</span>
<span id="cb14-9">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb14-10">plt.legend()</span>
<span id="cb14-11">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb14-12">plt.xlim(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb14-13">plt.ylim(bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb14-14">plt.tight_layout()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="plot of prior and posterior as line plot from grid approximation and histogram from sampling"></p>
</figure>
</div>
</div>
</div>
<p>In this figure we have:</p>
<ol type="1">
<li>the functional form of the prior</li>
<li>the functional form of the posterior from grid approximation</li>
<li>samples from the prior drawn directly from the generative model</li>
<li>and samples of the posterior obtained by applying rejection sampling to the generative model.</li>
</ol>
<p>Great! Our sampling algorithms are generating samples from the prior and the posterior which are consistent with the functional forms we computed earlier!</p>
<p>While we used rejection sampling here, regardless of what sampling algorithm we choose, we‚Äôll end up with the same thing after inference‚Äîa set of samples from the posterior distribution for each unknown parameter. Once we have those samples, we‚Äôre ready to move to the interpretation and analysis step.</p>
</section>
</section>
<section id="step-3.-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="step-3.-interpretation">üî¨ Step 3. Interpretation</h2>
<p>So how do we get insight into our analysis questions from this <code>posterior_samples</code> array? Well we‚Äôve got samples from the posterior distribution of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> which represents our updated beliefs about the relative plausibility of different values of Fritz‚Äôs actual underlying RPS win rate. We can use them just like any other dataset to answer questions about his win rate.</p>
<p>Let‚Äôs start with getting a point estimate of his true win rate. We can simply take the mean of the samples.</p>
<div id="cell-31" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'point estimate of theta: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_samples)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>point estimate of theta: 0.5655783348185507</code></pre>
</div>
</div>
<p>To get a confidence interval, often called a <em>credible interval</em> in the Bayesian context, we can just pull the quantiles of the posterior distribution. Note there are fancier ways to do this, e.g.&nbsp;computing highest posterior density intervals (HPDIs), but conceptually we‚Äôre basically just looking at the quantiles of the sample.</p>
<div id="cell-33" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'89% credible interval of theta:: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>quantile(posterior_samples, [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.055</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.945</span>])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>89% credible interval of theta:: [0.42005907 0.70367915]</code></pre>
</div>
</div>
<p>What‚Äôs the probability that Fritz‚Äôs win rate is actually really good, say greater than 75%? We can just check the samples directly for the proportion greater than 0.75.</p>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'P[theta &gt; 0.75]: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>P[theta &gt; 0.75]: 0.011933174224343675</code></pre>
</div>
</div>
<p>This means our analysis implies theres only a 1.5% chance that his true win rate is larger than 75%.</p>
<p>Now that we‚Äôve taken a look at interpreting the posterior samples to get some insight into the unknown parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> representing Fritz‚Äôs actual RPS win rate, we can take it one step further and make some predictions.</p>
<section id="posterior-predictive-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-distribution">Posterior Predictive Distribution</h3>
<p>We have one more character to meet in this cast of Bayesian players‚Äîthe <em>posterior predictive</em> distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)"></p>
<p>It represents the most plausible distribution of <em>new</em> data <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bnew%7D%7D"> given that we observed data $y_{}, i.e.&nbsp;based on the posterior rather than the prior. Mathematically it is obtained by integrating the product of the likelihood for the new data and the posterior distribution over the parameter space.</p>
<p><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cint%20p(y_%7B%5Ctext%7Bnew%7D%7D%7C%5Ctheta)%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)%20d%5Ctheta"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7C%5Ctheta)"> is the likelihood of the new data, assuming a specific parameter value <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. It‚Äôs the same likelihood function we used before, evaluated on <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bnew%7D%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)"> is just the posterior distribution</li>
<li>the integral <img src="https://latex.codecogs.com/png.latex?%5Cint%20%5Cdots%20d%20%5Ctheta"> effectively ‚Äúaverages‚Äù the likelihood of the new data over all possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, weighted by how plausible each value is according to the posterior.</li>
</ul>
<p>In terms of the DGP, we can generate samples from the posterior predictive by</p>
<ol type="1">
<li>Drawing a sample of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from the posterior distribution.</li>
<li>Using this value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to generate a value of <img src="https://latex.codecogs.com/png.latex?y"> from the generative model.</li>
</ol>
<p>Let‚Äôs implement this in python.</p>
<div id="cell-37" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_posterior_predictive(posterior_samples, N):</span>
<span id="cb21-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame([</span>
<span id="cb21-3">        simulate_one_outcome(theta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>theta, N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb21-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> posterior_samples</span>
<span id="cb21-5">    ])</span>
<span id="cb21-6"></span>
<span id="cb21-7">posterior_predictive_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_posterior_predictive(posterior_samples, N)</span></code></pre></div>
</div>
<p>Here again, we can inspect the distribution using any familiar techniques for working with samples of data. Here‚Äôs a histogram of the posterior predictive.</p>
<div id="cell-39" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>].hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color)</span>
<span id="cb22-2">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Posterior Predictive Distribution"</span>)</span>
<span id="cb22-3">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum(y)"</span>)</span>
<span id="cb22-4">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"probability mass"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img" alt="posterior predictive of number of wins"></p>
</figure>
</div>
</div>
</div>
<p>We can use this for forecasting, e.g.&nbsp;what‚Äôs the probability that Fritz wins at least 7 games in his next round?</p>
<div id="cell-41" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Probability of winning &gt;= 7 in next round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability of winning &gt;= 7 in next round: 0.32537788385043753</code></pre>
</div>
</div>
<p>Here‚Äôs where things can get interesting. In addition to forecasting the outcome itself, we can also compute the probabilities of events that depend on the outcome. For example, let‚Äôs say Fritz has only $50 left in his wallet, and he wants to know the probability that he can cover his bill after the next round. Let‚Äôs assume each drink costs $12. We can compute that probability as follows.</p>
<div id="cell-43" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># probability that Fritz's next round bill is less than or equal to $50</span></span>
<span id="cb25-2"></span>
<span id="cb25-3">cost_per_drink <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">12.0</span></span>
<span id="cb25-4">posterior_predictive_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb25-5">    posterior_predictive_df</span>
<span id="cb25-6">    .assign(losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> x.sum_y)</span>
<span id="cb25-7">    .assign(bill <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: cost_per_drink <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x.losses)</span>
<span id="cb25-8">)</span>
<span id="cb25-9"></span>
<span id="cb25-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Probability next round bill &lt;= $50: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bill"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability next round bill &lt;= $50: 0.5369928400954654</code></pre>
</div>
</div>
</section>
</section>
<section id="summary-of-the-bayesian-workflow" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-bayesian-workflow">Summary of the Bayesian Workflow</h2>
<p>Wow, we covered a lot of ground today. Let‚Äôs summarize the key points.</p>
<p>The Bayesian Analysis Workflow has three major steps:</p>
<ol type="1">
<li>Modeling
<ul>
<li>We build a <em>generative model</em> that describes the relationships among key variables and unknown parameters to represent the data generating process.</li>
<li>We encode our prior knowledge about the relative plausibility of different parameter values in the <em>prior distribution</em> of the parameters.</li>
<li>We can use <em>prior predictive checks</em> to simulate outcome data from the generative model to sanity check our modeling assumptions.</li>
</ul></li>
<li>Inference
<ul>
<li>Based on our modeling assumptions, we can use observed data to infer the <em>posterior distribution</em>, which quantifies the relative plausibility of different values of the unknown parameters after observing data.</li>
<li>We can view simulations from the generative model as sampling from the joint distribution of data and parameters, and we can view the posterior as the result of conditioning the joint distribution on the data we actually observed.</li>
<li>We can do inference by using sampling-based inference algorithms like rejection sampling, which uses logic on top of our generative model to isolate samples from the posterior distribution.</li>
</ul></li>
<li>Interpretation
<ul>
<li>After inference we can use data analysis tools to summarize the samples from the posterior distribution to compute point estimates, intervals, and probabilities of interest.</li>
<li>If we simulate data from our generative model using the posterior rather than the prior, we get samples from the <em>posterior predictive </em>distribution, which predicts future outcomes given observed data.</li>
<li>Again, we can analyze these posterior predictive samples to compute point estimates, intervals, or probabilities of future outcomes.</li>
</ul></li>
</ol>
<p>Phew! Hopefully that‚Äôs a helpful introduction to the Bayesian workflow and the major ideas behind it. The techniques we looked at here are mostly for pedagogical purposes; if you want to apply Bayesian methods to practical problems, you‚Äôll want to use a probabilistic programming language like PyMC, pyro, or stan. Maybe we‚Äôll get into some of those tools in future posts. See you then!</p>
<section id="resources" class="level3">
<h3 class="anchored" data-anchor-id="resources">Resources</h3>
<ul>
<li><a href="https://xcelab.net/rm/">Statistical Rethinking</a> - This page links to where you can obtain the book and also to a number of repos where folks have ported the R and Stan code examples to python libraries like PyMC and pyro.</li>
</ul>
</section>
<section id="reader-exercises" class="level3">
<h3 class="anchored" data-anchor-id="reader-exercises">Reader Exercises</h3>
<p>You didn‚Äôt think you‚Äôd get away without homework did you? Here are a couple suggestions for exercises.</p>
<ul>
<li>compute the posterior predictive distribution <img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)"> for the RPS example using grid approximation.</li>
<li>Suppose that each RPS match was played as best out of three. Rewrite the generative model to generate both sub-match and match outcomes. Do inference with rejection sampling. Use your model to find the probability that Fritz wins his next match by winning two submatches in a row.</li>
</ul>
</section>
</section>

 ]]></description>
  <category>bayesian</category>
  <category>python</category>
  <guid>https://randomrealizations.com/posts/bayesian-modeling-primer/</guid>
  <pubDate>Wed, 04 Jun 2025 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/bayesian-modeling-primer/hist.png" medium="image" type="image/png" height="107" width="144"/>
</item>
<item>
  <title>Analyzing After Tax Retirement Income: Roth vs.¬†Traditional 401(k)</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/traditional-vs-roth-401k/</link>
  <description><![CDATA[ 




<p>Today we‚Äôre taking a break from our typical hard hitting algorithm deep dives for a quick foray into the world of personal finance. We‚Äôll take on a question I recently encountered while setting up my retirement account with my new employer‚Äîwhich is more efficient, the traditional 401(k) or the Roth 401(k)? US-based readers will recognize these as the two main types of employer-sponsored retirement accounts. When I searched for traditional vs Roth 401(k), the articles I found gave only very hand-wavy guidance on which is better in a given situation. So, today I‚Äôll share my quantitative analysis of which account type provides superior performance for a given set of personal circumstances. We‚Äôll implement the analysis in python, so you can run the numbers for your own situation and determine which employer-sponsored account type is better for you.</p>
<section id="traditional-401k-vs-roth-401k" class="level2">
<h2 class="anchored" data-anchor-id="traditional-401k-vs-roth-401k">Traditional 401(k) vs Roth 401(k)</h2>
<p>I‚Äôll let JLCollins explain the <a href="https://jlcollinsnh.com/2015/06/02/stocks-part-viii-the-401k-403b-tsp-ira-roth-buckets/">background on 401(k)s</a>; read that post first if you‚Äôre not already familiar with the concepts of taxable accounts, IRAs, 401(k)s, and the basic rules of Roth vs traditional. The key distinction is * In a traditional 401(k), money you contribute now is deducted from your taxable income, meaning you‚Äôll pay less in income tax now. During retirement however, withdrawals from the account will count toward your taxable income, so you‚Äôll pay tax then. * In a Roth 401(k), money you contribute now does count toward your taxable income, meaning you‚Äôll pay income tax on any contributions now. During retirement however, withdrawals do not count toward your taxable income and are therefore tax free.</p>
<p>Essentially you can either pay tax now (Roth) or pay tax later (traditional). The hand-wavy advice points out that which account is better for you depends on your income tax rate now versus your income tax rate during retirement. High tax rate now and low tax rate during retirement could favor traditional, while low tax rate now and high tax rate during retirement could favor Roth. Let‚Äôs put some numbers on this advice.</p>
<p>I‚Äôll assume that you‚Äôre following the <a href="https://www.mrmoneymustache.com/2011/04/10/post-4-what-am-i-supposed-to-do-with-all-this-money/">sage advice of Mr.&nbsp;Money Mustache</a> and (after paying off any high-interest debt) maxing out your 401(k) contribution for the year. In 2024, the IRS has set a maximum combined contribution of $23,000; i.e.&nbsp;the sum of your Roth and traditional contributions cannot exceed this limit. Also, once you contribute to these accounts, you may not begin withdrawals (without penalty) until the age of 59.5.</p>
</section>
<section id="analysis-formulation" class="level2">
<h2 class="anchored" data-anchor-id="analysis-formulation">Analysis Formulation</h2>
<p>Let‚Äôs state the question precisely‚Äîwhich account type will yield me the most money during retirement after withdrawal and after all taxes are paid? Let‚Äôs think through the Roth vs traditional scenarios, setting aside the same amount of money today and liquidating the entire account at retirement; we‚Äôll compare how much money we have at retirement after liquidating and settling any tax obligations.</p>
<p><strong>Roth</strong>: I contribute <code>contribution = 23_000</code> now, plus I pay income tax on this contribution in the amount of <code>current_income_tax_rate * contribution</code>. Over the years from now to retirement <code>retirement_age - current_age</code>, my contribution grows at some average long term yearly rate <code>investment_growth_rate</code>. At retirement, I liquidate the entire account, paying no income tax on the proceeds.</p>
<p><strong>Traditional</strong>: I contribute <code>contribution = 23_000</code> now. For fair comparison with the Roth, I invest an additional amount <code>current_income_tax_rate * contribution</code> (the extra income tax I would have paid had I chosen the Roth) in a normal taxable investment account as well. Over the time from now to retirement, the 401(k) and the taxable account both grow at the average long term rate <code>investment_growth_rate</code>. However, in the taxable account, I‚Äôll also need to pay income tax every year on any dividends that I earn; the S&amp;P500 has recently paid out 1.5-2% in dividends each year, let‚Äôs call it <code>dividend_rate</code>. At retirement, I liquidate both accounts, paying income tax on the proceeds from the 401(k) at the rate of <code>retirement_income_tax_rate</code> and paying capital gains tax on the proceeds from the taxable account at the rate of <code>retirement_capital_gains_tax_rate</code>.</p>
<p>Let‚Äôs code up a function that takes in all our parameters and returns the total liquidation value after taxes of the Roth versus traditional 401(k)s as described above.</p>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span></code></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_401k_liquidation_value(</span>
<span id="cb2-2">    current_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">37</span>,</span>
<span id="cb2-3">    current_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.35</span>,</span>
<span id="cb2-4">    contribution <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">23_000</span>,</span>
<span id="cb2-5">    retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">59.5</span>,</span>
<span id="cb2-6">     investment_growth_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.07</span>,</span>
<span id="cb2-7">     dividend_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>,</span>
<span id="cb2-8">    retirement_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.24</span>,</span>
<span id="cb2-9">    retirement_capital_gains_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0%, 15%, 20%</span></span>
<span id="cb2-10">):</span>
<span id="cb2-11"></span>
<span id="cb2-12">    investment_growth_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> investment_growth_rate) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_age)</span>
<span id="cb2-13">    dividend_income_tax_drag_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> dividend_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> current_income_tax_rate) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_age)</span>
<span id="cb2-14"></span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Roth 401k</span></span>
<span id="cb2-16">    roth_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contribution </span>
<span id="cb2-17">    roth_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor</span>
<span id="cb2-18">    total_roth_401k_liquidation_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roth_401k_value</span>
<span id="cb2-19">    </span>
<span id="cb2-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># traditional  401k</span></span>
<span id="cb2-21">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contribution </span>
<span id="cb2-22">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> contribution </span>
<span id="cb2-23">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor</span>
<span id="cb2-24">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dividend_income_tax_drag_factor</span>
<span id="cb2-25">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> retirement_income_tax_rate)</span>
<span id="cb2-26">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> retirement_capital_gains_tax_rate)</span>
<span id="cb2-27">    total_traditional_401k_liquidation_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> taxable_account_value</span>
<span id="cb2-28"></span>
<span id="cb2-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb2-30">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'traditional'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(total_traditional_401k_liquidation_value), </span>
<span id="cb2-31">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'roth'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(total_roth_401k_liquidation_value)</span>
<span id="cb2-32">    }</span></code></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">get_401k_liquidation_value()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>{'traditional': 106882, 'roth': 105405}</code></pre>
</div>
</div>
<p>Somehow it‚Äôs not surprising that these two options seem to yield very similar after-tax performance‚Äîno arbitrage right?</p>
<p>Let‚Äôs write a function to perturb some of our parameter values to see under what conditions one option dominates the other.</p>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_liquidation_value_by_parameter_values(param, grid_values, func<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_401k_liquidation_value):</span>
<span id="cb5-2">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [func(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>{param: x}) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> grid_values]</span>
<span id="cb5-3">    df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(y, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.Series(grid_values, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param))</span>
<span id="cb5-4">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb5-5">    df.plot(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb5-6">    plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liquidation value'</span>)</span>
<span id="cb5-7">    plt.title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Liquidation Value at Retirement by </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb5-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> fig, ax</span></code></pre></div>
</div>
<section id="income-tax-rate-at-retirement" class="level3">
<h3 class="anchored" data-anchor-id="income-tax-rate-at-retirement">Income Tax Rate at Retirement</h3>
<p>It seems that income tax rate at retirement is by far the most important determining factor in whether traditional or Roth 401(k) is a better option.</p>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_income_tax_rate'</span>, np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.38</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So given the other parameters I‚Äôve set, Roth outperforms traditional when our income tax rate in retirement exceeds about 25%. <a href="https://www.irs.gov/filing/federal-income-tax-rates-and-brackets">According to the IRS</a> in 2023, an individual tax payer is in the 24% bracket if their income is between about $95k and $180k. So, how much income do you expect to pull in retirement? If we‚Äôre really building FIRE wealth, the kind indicated by Mr.&nbsp;Money Mustache and JLCollins, our income in retirement could easily exceed $180k, which would push us into the 32% bracket where Roth is more efficient than traditional.</p>
</section>
<section id="capital-gains-tax-rate-at-retirement" class="level3">
<h3 class="anchored" data-anchor-id="capital-gains-tax-rate-at-retirement">Capital Gains Tax Rate at Retirement</h3>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_capital_gains_tax_rate'</span>, np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.20</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In 2023, <a href="https://www.irs.gov/filing/federal-income-tax-rates-and-brackets">according to the IRS</a> as a single tax payer, if your income is between $44k and $492k, you‚Äôll pay 15% capital gains. Over $492k you‚Äôll jump up to 20% where Roth dominates traditional.</p>
</section>
<section id="retirement-age" class="level3">
<h3 class="anchored" data-anchor-id="retirement-age">Retirement Age</h3>
<div id="cell-16" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_age'</span>, np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">59.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">75</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For retirement ages beyond 59.5, traditional‚Äôs edge over Roth grows slightly.</p>
</section>
</section>
<section id="bottom-line" class="level2">
<h2 class="anchored" data-anchor-id="bottom-line">Bottom Line</h2>
<p>When I plugged in my actual parameters, I found that because I was only employed for 6 months last year, my current income tax rate pushed me into the regime where Roth performs better than traditional. However for this next year, I expect to be in a higher income tax bracket where traditional will be a better deal than Roth.</p>
<p>That said, the most important factor is your income tax rate at the time of withdrawal during retirement, which is based on your taxable income at that time. But how, I hear you asking, am I supposed to know what to plug in for my post-retirement income? That quantity is unknown. This illuminates the fundamental limitation of this kind of analysis‚Äîwhat to do about uncertain inputs to the calculation? That‚Äôs a question that we might take on in a future post, so stay tuned!</p>
</section>

 ]]></description>
  <category>personal finance</category>
  <guid>https://randomrealizations.com/posts/traditional-vs-roth-401k/</guid>
  <pubDate>Sat, 14 Dec 2024 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/enso-thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>SHAP from Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/shap-from-scratch/</link>
  <description><![CDATA[ 




<p>Ahh, SHAP. As you know it‚Äôs become one of the leading frameworks for explaining ML model predictions. I‚Äôd guess it‚Äôs popularity is due to its appealing theoretical basis, its universal applicability to any type of ML model, and its easy-to-use python package. SHAP promises to turn your black box ML model into a nice friendly interpretable model. The hilarious irony is that, when I first started using it in my work, SHAP itself was a complete black box to me. In this post, we‚Äôll change all that by diving into the SHAP paper, illuminating the key theoretical ideas behind its development step by step, and implementing it from scratch in python. If you aren‚Äôt already familiar with how to compute and interpret SHAP values in practice, I‚Äôd recommend that you go check out the <a href="https://shap.readthedocs.io/en/latest/index.html">documentation for the shap python package</a> before diving into this post.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_main.jpg" class="img-fluid figure-img"></p>
<figcaption>Snow, trees, and mountains overlook Lake Tahoe.</figcaption>
</figure>
</div>
<section id="what-is-shap" class="level2">
<h2 class="anchored" data-anchor-id="what-is-shap">What is SHAP?</h2>
<p>SHAP (SHapley Additive exPlanations) is a conceptual framework for creating explanations of ML model predictions. The term also refers to a set of computational methods for generating these explanations and a python library which implements them. The ‚ÄúSHAP‚Äù <a href="https://en.wikipedia.org/wiki/Backronym">backronym</a> was introduced in <a href="https://arxiv.org/abs/1705.07874">Lundberg and Lee 2017</a>, which I call the <em>SHAP paper</em>, that expanded on several previously existing ideas which we‚Äôll build up in the following sections. The key concepts are:</p>
<ul>
<li><em>Shapley values</em>, a concept from cooperative game theory which originally had nothing to do with machine learning</li>
<li><em>Shapley regression values</em>, which showed how to use Shapley values to generate explanations of model predictions</li>
<li><em>Shapley sampling values</em>, which offered a computationally tractable way to compute Shapley regression values for any type of model.</li>
</ul>
<p>The SHAP paper tied Shapley regression values and several other existing model explanation methods together by showing they are all members of a class called ‚Äúadditive feature attribution methods.‚Äù Under the right conditions, these additive feature attribution methods can generate Shapley values, and when they do we can call them SHAP values.</p>
<p>After establishing this theoretical framework, the authors go on to discuss various computational methods for computing SHAP values; some are model-agnostic, meaning they work with any type of model, and others are model-specific, meaning they work for specific types of models. It turns out that the previously existing Shapley sampling values method is a model-agnostic approach, but while it‚Äôs the most intuitive, computationally speaking it‚Äôs relatively inefficient. Thus the authors propose a novel model-agnostic approach called Kernel SHAP, which is really just <a href="https://lime-ml.readthedocs.io/en/latest/">LIME</a> parameterized to yield SHAP values.</p>
<p>Model-specific approaches can be potentially much more efficient than model-agnostic ones by taking advantage of model idiosyncrasies. For example, there is an analytical solution for the SHAP values of linear models, so Linear SHAP is extremely efficient. Similarly, Deep SHAP (proposed in the SHAP paper) and Tree SHAP (proposed later in <a href="https://www.sciencedirect.com/science/article/pii/S2666827022000500#b20">Lundberg et al 2020</a>) take advantage of idiosyncrasies of deep learning and tree-based models to compute SHAP values efficiently.</p>
<p>The important thing about these different methods is that they provide computationally tractable ways to compute SHAP values, but ultimately, they are all based on the Shapley sampling values method‚Äîthe original method to compute what we now call SHAP values. Thus, for the remainder of this post, we‚Äôll focus on this method, building it up from Shapley values to Shapley regression values to Shapley sampling values and ultimately implementing it from scratch in python.</p>
</section>
<section id="shapley-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-values">Shapley Values</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Shapley_value">Shapley value</a> is named in honor of Nobel prize winning economist Loyd Shapley who introduced the idea in the field of coalitional game theory in the 1950‚Äôs. Shapley proposed a way to determine how a coalition of players can fairly share the payout they receive from a cooperative game. We‚Äôll introduce the mathematical formalism in the next section, so for now let‚Äôs just touch on the intuition for the approach. Essentially, the method distributes the payout among the players according to the expected contribution of each player across all possible combinations of the players. The thought experiment works as follows:</p>
<ol type="1">
<li>Draw a random permutation (ordering) of the players.</li>
<li>Have the first player play alone, generating some payout. Then have the first two players play together, generating some payout. Then the first three, and so on.</li>
<li>As each new player is added, attribute the change in the payout to this new player.</li>
<li>Repeat this experiment for all permutations of the players. A player‚Äôs Shapley value is the average change in payout (across all permutations) when that player is added to the game.</li>
</ol>
<p>Next we‚Äôll see how this idea can be applied to model explanations.</p>
</section>
<section id="shapley-regression-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-regression-values">Shapley Regression Values</h2>
<p>The next idea came from <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.446">Lipovetsky and Conklin 2001</a>, who proposed a way to use Shapley values to explain the predictions of a linear regression model. <em>Shapley regression values</em> assign an importance value to each feature that represents the effect on the model prediction of including that feature. The basic idea is to train a second model without the feature of interest, and then to compare the predictions from the model with the feature and the model without the feature. This procedure of training two models and comparing their predictions is repeated for all possible subsets of the other features; the average difference in predictions is the Shapley value for the feature of interest.</p>
<p>The Shapley value for feature <img src="https://latex.codecogs.com/png.latex?i"> on instance <img src="https://latex.codecogs.com/png.latex?x"> is given by equation 4 in the SHAP paper:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cphi_i%20=%20%5Csum_%7BS%20%5Csubseteq%20F%20%5Csetminus%20%5C%7Bi%5C%7D%7D%0A%5Cfrac%7B%7CS%7C!(%7CF%7C%20-%20%7CS%7C%20-%201)!%7D%7B%7CF%7C!%7D%0A%5Bf_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D(x_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D)%20-%20f_S(x_S)%20%5D%0A"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cphi_i"> is the Shapley value for feature of interest <img src="https://latex.codecogs.com/png.latex?i">,</li>
<li>the <img src="https://latex.codecogs.com/png.latex?%5Csubseteq"> symbol indicates the item on its left is a subset of the object on its right,</li>
<li><img src="https://latex.codecogs.com/png.latex?F"> is the set of all features,</li>
<li>the vertical bars indicate the number of elements in a set, e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?%7CF%7C"> is the total number of features,</li>
<li><img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D"> is the set of all features except the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?S"> is a particular subset of features not including the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a ‚Äúsubset model‚Äù‚Äîa model that uses only the features in <img src="https://latex.codecogs.com/png.latex?S"> for both training and prediction,</li>
<li>and <img src="https://latex.codecogs.com/png.latex?f_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D"> is asubset model using features in <img src="https://latex.codecogs.com/png.latex?S"> and the feature of interest.</li>
</ul>
<p>To reiterate, this is the most important equation when it comes to understanding SHAP, as it defines the Shapley value; let‚Äôs make sure we understand what‚Äôs going on by implementing it in python.</p>
<p>We start with the feature subsets. Notice that the sum is indexed over all subsets of <img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D">, which is the set of all features except the <img src="https://latex.codecogs.com/png.latex?i">th feature, the one we‚Äôre calculating the Shapley value for. Let‚Äôs write a function that takes a list of items and returns an iterable that yields all possible subsets of those items.</p>
<div id="cell-7" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations </span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_all_subsets(items):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span>  get_all_subsets([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]):</span>
<span id="cb1-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(2,)
(0, 1)
(0, 2)
(1, 2)
(0, 1, 2)</code></pre>
</div>
</div>
<p>To get all subsets of features, other than the feature of interest, we could do something like this.</p>
<div id="cell-9" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb3-2">    all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb3-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> get_all_subsets(all_other_features)</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb3-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(3,)
(0, 1)
(0, 3)
(1, 3)
(0, 1, 3)</code></pre>
</div>
</div>
<p>So for each of the feature subsets, we‚Äôll need to calculate the summand, which is the product of a quotient with a bunch of factorials and the difference in predicted values between two subset models. Let‚Äôs start with those subset models. Subset model <img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a model trained only on the features in subset <img src="https://latex.codecogs.com/png.latex?S">. We can write a function that takes an untrained model, a training dataset, a feature subset to use, and a single instance to predict on; the function will then train a model using only features in the subset, and it will issue a prediction for the single instance we gave it.</p>
<div id="cell-11" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> subset_model(model, X_train, y_train, feature_subset, instance):</span>
<span id="cb5-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance.shape) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Instance must be a 1D array'</span></span>
<span id="cb5-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(feature_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y.mean() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a model with no features predicts E[y]</span></span>
<span id="cb5-5">    X_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.take(feature_subset, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-6">    model.fit(X_subset, y_train)</span>
<span id="cb5-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model.predict(instance.take(feature_subset).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
</div>
<p>Next let‚Äôs have a look at <img src="https://latex.codecogs.com/png.latex?%7CS%7C!(%7CF%7C-%7CS%7C-1)!/%7CF%7C!">. The keen reader will notice this factor kind of looks like the answers to those combinatorics questions like how many unique ways can you order the letters in the word MISSISSIPPI. The combinatorics connection is that Shapley values are defined in terms of all permutations of the players , where the included players come first, then the player of interest, followed by the excluded players. In ML models, the order of features doesn‚Äôt matter, so we can work with unordered subsets of features, scaling the prediction difference terms by the number of permutations that involve the same sets of included and excluded features. With that in mind, we can see that including the factor in each term of the sum gives us a weighted average over all feature combinations, where the numerator gives the number of permutations in which the included features come first, followed by the feature of interest, followed by the excluded features, and the denominator is the total number of feature permutations.</p>
<div id="cell-13" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb6-2"></span>
<span id="cb6-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> permutation_factor(n_features, n_subset):</span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> factorial(n_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features)</span></code></pre></div>
</div>
<p>Now we can put these pieces together to compute equation 4‚Äîa single Shapley regression value for a single instance and feature of interest.</p>
<div id="cell-15" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_single_shap_value(untrained_model,</span>
<span id="cb7-2">                              X_train,</span>
<span id="cb7-3">                              y_train,</span>
<span id="cb7-4">                              feature_of_interest,</span>
<span id="cb7-5">                              instance):</span>
<span id="cb7-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb7-7">    n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb7-8">    shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb7-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb7-10">        n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb7-11">        prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-12">            untrained_model,</span>
<span id="cb7-13">            X_train, y_train,</span>
<span id="cb7-14">            subset,</span>
<span id="cb7-15">            instance</span>
<span id="cb7-16">        )</span>
<span id="cb7-17">        prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-18">            untrained_model,</span>
<span id="cb7-19">            X_train, y_train,</span>
<span id="cb7-20">            subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature_of_interest,),</span>
<span id="cb7-21">            instance</span>
<span id="cb7-22">        )</span>
<span id="cb7-23">        factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_factor(n_features, n_subset)</span>
<span id="cb7-24">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb7-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_value</span></code></pre></div>
</div>
<p>Let‚Äôs use this function to compute a single Shapley regression value for a linear model and a small training dataset with 3 features.</p>
<div id="cell-17" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression </span>
<span id="cb8-3"></span>
<span id="cb8-4">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb8-5"></span>
<span id="cb8-6">compute_single_shap_value(untrained_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>LinearRegression(),</span>
<span id="cb8-7">                          X_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X, y_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y,</span>
<span id="cb8-8">                          feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,</span>
<span id="cb8-9">                          instance<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>-0.07477140629329351</code></pre>
</div>
</div>
<p>That gives us a single Shapley value corresponding to a single feature value in a single instance. To get useful model explanations, we‚Äôd need to compute Shapley values for each feature of each instance in some dataset of instances. You might notice there‚Äôs a big problem with the formulation above. Namely, we are going to have to train a whole bunch of new subset models‚Äîone for each subset of the features. If our model has <img src="https://latex.codecogs.com/png.latex?M"> features, we‚Äôll have to train <img src="https://latex.codecogs.com/png.latex?2%5EM"> models, so this will get impractical in a hurry, especially if we‚Äôre trying to train anything other than linear models.</p>
</section>
<section id="shapley-sampling-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-sampling-values">Shapley Sampling Values</h2>
<p>Next, <a href="https://link.springer.com/article/10.1007/s10115-013-0679-x">≈†trumbelj and Kononenko 2014</a> proposed <em>Shapley sampling values</em>, a method which provides a much more efficient way to approximate the subset models used to calculate Shapley regression values. In this approach, the effect of removing some features from the model is approximated by the conditional expectation of the model given the known features.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20f_S(x_S)%20%20:=%20E%5Bf(x)%20%7C%20x_S%5D%20%20"></p>
<p>This means we‚Äôre approximating the output of a subset model by averaging over outputs of the full model. That‚Äôs great because now we don‚Äôt have to train all those new subset models, we can just query our full model over some set of inputs and average over the outputs to compute these conditional expectation subset models.</p>
<p>Now how exactly do we compute that conditional expectation? First we rewrite the above conditional expectation (equation 10 in the SHAP paper)</p>
<p><img src="https://latex.codecogs.com/png.latex?%20E%5Bf(x)%20%7C%20x_S%5D%20%20=%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> is the set of excluded or missing features. Beside this equation in the paper they give the note ‚Äúexpectation over <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D%20%7C%20x_S">, which means we‚Äôre taking the expectation over the missing features given the known features. Then we get another step (equation 11)</p>
<p><img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D%20%5Capprox%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%20%5Bf(x)%5D"></p>
<p>Now it‚Äôs not an equality but an approximation. The authors give the note ‚Äúassume feature independence‚Äù. The intuition here is that if the missing features are correlated with the known features, then their distribution depends on the particular values taken by the known features. But here the authors make the simplifying assumption that known and missing features are independent, which allows us to replace the conditional expectation with an unconditional expectation over the missing features.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>So is this assumption that features in <img src="https://latex.codecogs.com/png.latex?S"> are independent from features in <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> a problem? The short answer is‚Ä¶ maybe ü§∑‚Äç‚ôÄÔ∏è? It‚Äôs potentially problematic enough that people have worked out some ways to relax this assumption, e.g.&nbsp;<a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.PartitionExplainer.html">partition masking</a>, but that makes <em>Owen values</em> instead of Shapley values, so we‚Äôll save it for another post.</p>
</div>
</div>
<p>Anyway, how do we compute this unconditional expectation over the missing features in practice? We‚Äôll need to use a so-called <em>background dataset</em>, which is just some set of observations of our feature variables that represents their distribution. A good candidate is the training data we used to train our model. ≈†trumbelj and Kononenko 2014 propose a way to estimate this conditional expectation using resampling of the background dataset.</p>
<p>The idea is to notice that the instance of interest <img src="https://latex.codecogs.com/png.latex?x"> is a feature vector comprised of the set of ‚Äúknown‚Äù features <img src="https://latex.codecogs.com/png.latex?x_S"> and the set of excluded features <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D"> such that <img src="https://latex.codecogs.com/png.latex?x=%5C%7Bx_S,x_%7B%5Cbar%7BS%7D%7D%20%5C%7D">. Our resampling scheme will be based on constructing ‚Äúmasked‚Äù samples <img src="https://latex.codecogs.com/png.latex?x%5E*=%5C%7Bx_S,z_%7B%5Cbar%7BS%7D%7D%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D"> are values of the missing features drawn from some random observation in the background dataset. We can then compute an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)"> of the conditional expectation <img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%5Bf(x)%5D"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bk=1%7D%5En%20f(%5C%7Bx_S,%20z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D%20%5C%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D"> is the vector of values of the excluded features from the <img src="https://latex.codecogs.com/png.latex?k">-th row of the background dataset. Algorithmically, we can view this as first drawing a sample of observations from the background dataset, second ‚Äúmasking‚Äù features in <img src="https://latex.codecogs.com/png.latex?S"> in the sampled background dataset by replacing the observed values <img src="https://latex.codecogs.com/png.latex?z_S"> on each row with the values in the instance <img src="https://latex.codecogs.com/png.latex?x_S">, third using the full model <img src="https://latex.codecogs.com/png.latex?f"> to predict on each of these masked samples in the background dataset, and finally averaging over these predictions. We can implement a new subset model function that takes a fully trained model, a background dataset,a feature subset, and an instance for explanation and returns an approximation of the subset model prediction.</p>
<div id="cell-21" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> subset_model_approximation(trained_model, </span>
<span id="cb10-4">                               background_dataset,</span>
<span id="cb10-5">                               feature_subset,  </span>
<span id="cb10-6">                               instance):</span>
<span id="cb10-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">""" </span></span>
<span id="cb10-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Approximate subset model prediction  (Equation 11)</span></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    \hat{f}_S(x) = E_{x_{\hat{S</span><span class="re">}}}</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[f_S(x)]</span></span>
<span id="cb10-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    for feature subset S on single instance x</span></span>
<span id="cb10-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb10-12">    masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset.copy()</span>
<span id="cb10-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb10-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb10-15">            masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb10-16">    conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb10-17">        trained_model.predict(masked_background_dataset)</span>
<span id="cb10-18">    )</span>
<span id="cb10-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>If we replace our <code>subset_model</code> function with this new <code>subset_model_approximation</code> function in our <code>compute_single_shap_value</code> function from earlier, then we‚Äôll be computing Shapley sampling values. And according to the SHAP paper: ‚Äúif we assume feature independence when approximating conditional expectations (using Equation 11 to estimate subset model output) ‚Ä¶ then SHAP values can be estimated directly using the Shapley sampling values method.‚Äù That means we‚Äôll be computing SHAP values!</p>
</section>
<section id="how-to-implement-shap-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="how-to-implement-shap-from-scratch">How to Implement SHAP from Scratch</h2>
<p>Let‚Äôs put the pieces together and implement a class for a model explainer that computes SHAP values via the Shapley sampling values method. We‚Äôll talk through a couple of points after the code.</p>
<div id="cell-24" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Any, Callable, Iterable</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ShapFromScratchExplainer():</span>
<span id="cb11-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb11-8">                 model: Callable[[np.ndarray], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>], </span>
<span id="cb11-9">                 background_dataset: np.ndarray,</span>
<span id="cb11-10">                 max_samples: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb11-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb11-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> max_samples:</span>
<span id="cb11-13">            max_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(max_samples, background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]) </span>
<span id="cb11-14">            rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.default_rng()</span>
<span id="cb11-15">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.choice(background_dataset, </span>
<span id="cb11-16">                                                 size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_samples, </span>
<span id="cb11-17">                                                 replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb11-19">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset</span>
<span id="cb11-20"></span>
<span id="cb11-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> shap_values(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb11-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"SHAP Values for instances in DataFrame or 2D array"</span></span>
<span id="cb11-23">        shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty(X.shape)</span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb11-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-26">                shap_values[i, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._compute_single_shap_value(j, X[i, :])</span>
<span id="cb11-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_values</span>
<span id="cb11-28">       </span>
<span id="cb11-29">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _compute_single_shap_value(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-30">                                   feature: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb11-31">                                   instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb11-33">        n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance)</span>
<span id="cb11-34">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb11-35">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_other_feature_subsets(n_features, feature):</span>
<span id="cb11-36">            n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb11-37">            prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-38">                subset, </span>
<span id="cb11-39">                instance</span>
<span id="cb11-40">            )</span>
<span id="cb11-41">            prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-42">                subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature,), </span>
<span id="cb11-43">                instance</span>
<span id="cb11-44">            )</span>
<span id="cb11-45">            factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._permutation_factor(n_features, n_subset)</span>
<span id="cb11-46">            shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb11-47">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_value</span>
<span id="cb11-48">    </span>
<span id="cb11-49">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _get_all_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, items: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Iterable:</span>
<span id="cb11-50">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb11-51">    </span>
<span id="cb11-52">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _get_all_other_feature_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, feature_of_interest):</span>
<span id="cb11-53">        all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb11-54">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_subsets(all_other_features)</span>
<span id="cb11-55"></span>
<span id="cb11-56">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _permutation_factor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, n_subset):</span>
<span id="cb11-57">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (</span>
<span id="cb11-58">            factorial(n_subset) </span>
<span id="cb11-59">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb11-60">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features) </span>
<span id="cb11-61">        )</span>
<span id="cb11-62">    </span>
<span id="cb11-63">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _subset_model_approximation(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-64">                                    feature_subset: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, ...], </span>
<span id="cb11-65">                                    instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-66">        masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset.copy()</span>
<span id="cb11-67">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-68">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb11-69">                masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb11-70">        conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb11-71">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model(masked_background_dataset)</span>
<span id="cb11-72">        )</span>
<span id="cb11-73">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>The <code>SHAPExplainerFromScratch</code> API is similar to that of the <a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html"><code>KernelExplainer</code></a> from the python library, taking two required arguments during instantiation:</p>
<ul>
<li><code>model</code>: ‚ÄúUser supplied function that takes a matrix of samples (# samples x # features) and computes the output of the model for those samples.‚Äù That means if our model is a scikit-learn model, we‚Äôll need to pass in its predict method, not the model object itself.</li>
<li><code>background_dataset</code>: ‚ÄúThe background dataset to use for integrating out features.‚Äù We know about this idea from the Shapley sampling values section above; a good choice for this data could be the training dataset we used to fit the model. By default, we‚Äôll use all the rows of this background dataset, but we‚Äôll also implement the ability to sample down to the desired number of rows with an argument called <code>max_samples</code>.</li>
</ul>
<p>Like the <code>KernelExplainer</code>, this class has a method called <code>shap_values</code> which estimates the SHAP values for a set of instances. It takes an argument <code>X</code> which is ‚Äúa matrix of samples (# samples x # features) on which to explain the model‚Äôs output.‚Äù This <code>shap_values</code> method just loops through each feature value of each instance of the input samples <code>X</code> and calls an internal method named <code>_compute_single_shap_value</code> to compute each SHAP value. The <code>_compute_single_shap_value</code> method is the real workhorse of the class. It implements equation 4 from the SHAP paper as described in the Shapley regression values section above by calling a few other internal helper methods corresponding to functions that we‚Äôve already written.</p>
</section>
<section id="testing-the-implementation" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-implementation">Testing the Implementation</h2>
<p>Let‚Äôs check our work by comparing SHAP values computed by our implementation with those from the SHAP python library. We‚Äôll use our old friend the diabetes dataset, training a linear model, a random forest, and a gradient boosting machine.</p>
<div id="cell-27" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_diabetes</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb12-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GradientBoostingRegressor, RandomForestRegressor</span>
<span id="cb12-5"></span>
<span id="cb12-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_diabetes(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, return_X_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb12-7">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb12-8">                                                    random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb12-9"></span>
<span id="cb12-10">lin_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-11">rfr_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomForestRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-12">gbt_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>Here‚Äôs a little function to compare the SHAP values generated by our implementation and those from the library <code>KernelExplainer</code>.</p>
<div id="cell-29" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shap</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compare_methods(model, X_background, X_instances):</span>
<span id="cb13-4">        </span>
<span id="cb13-5">    library_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shap.KernelExplainer(model.predict, X_background)</span>
<span id="cb13-6">    library_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> library_explainer.shap_values(X_instances)</span>
<span id="cb13-7"></span>
<span id="cb13-8">    from_scratch_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ShapFromScratchExplainer(model.predict, X_background)</span>
<span id="cb13-9">    from_scratch_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> from_scratch_explainer.shap_values(X_instances)</span>
<span id="cb13-10"></span>
<span id="cb13-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.allclose(library_shap_values, from_scratch_shap_values)</span></code></pre></div>
</div>
<div id="cell-30" class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">compare_methods(lin_model, </span>
<span id="cb14-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb14-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"995f763174824efebecb5c2522c3f6f5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">compare_methods(rfr_model, </span>
<span id="cb16-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb16-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9468451774b4c758a7696ff10fabc74","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">compare_methods(gbt_model, </span>
<span id="cb18-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb18-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8bdea22fab86440db68c4a669beaf7df","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>True</code></pre>
</div>
</div>
<p>Beautiful! Our Implementation is consistent with the SHAP library explainer!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well I hope this one was helpful to you. The research phase actually took me a lot longer than I expected; it just took me a while to figure out what SHAP really is and how those different ideas and papers fit together. I thought the implementation itself was pretty fun and relatively easy. What do you think?</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1705.07874">The SHAP Paper (Lundberg and Lee, 2017)</a></li>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning by Christoph Molnar</a></li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>from scratch</category>
  <guid>https://randomrealizations.com/posts/shap-from-scratch/</guid>
  <pubDate>Sun, 04 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_thumb.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The Ultimate Guide to XGBoost Parameter Tuning</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/</link>
  <description><![CDATA[ 




<p>Ahh, the dark art of hyperparameter tuning. It‚Äôs a key step in the machine learning workflow, and it‚Äôs an activity that can easily be overlooked or be overkill. Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils. Today I‚Äôll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework. I‚Äôll give you some intuition for how to think about the key parameters in XGBoost, and I‚Äôll show you an efficient strategy for parameter tuning GBTs. I‚Äôll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like. You can download a notebook with this tuning workflow from my <a href="https://github.com/mcb00/ds-templates">data science templates repository</a>. Finally we‚Äôll wrap up with the kind of cautionary tale data scientists tell their colleagues around the campfire about when all this fancy hyperparameter tuning can backfire catastrophically‚Äîignore at your own peril.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_main.jpg" class="img-fluid figure-img"></p>
<figcaption>Lunar halo on a frosty night in Johnson City, TN</figcaption>
</figure>
</div>
<section id="xgboost-parameters" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-parameters">XGBoost Parameters</h2>
<p>Gradient boosting algorithms like XGBoost have two main types of hyperparameters: <em>tree parameters</em> which control the decision tree trained at each boosting round and <em>boosting parameters</em> which control the boosting procedure itself. Below I‚Äôll highlight my favorite parameters, but you can see the full list in the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">documentation</a>.</p>
<section id="tree-parameters" class="level3">
<h3 class="anchored" data-anchor-id="tree-parameters">Tree Parameters</h3>
<p>In theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, <a href="../../posts/consider-the-decision-tree/">decision trees</a> are typically the best choice. In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.</p>
<section id="tree-construction-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="tree-construction-algorithm">Tree construction algorithm</h4>
<p>The tree construction algorithm boils down to split finding, and different algorithms have different ways of generating candidate splits to consider. In XGBoost we have the parameter:</p>
<ul>
<li><code>tree_method</code> - select tree construction algorithm: <code>exact</code>, <code>hist</code>, <code>approx</code>, or the horrifying default‚Äî<code>auto</code>‚Äîwhich outsources your choice of tree construction algo to XGBoost and which you should never ever use. I‚Äôve been burned by this hidden <code>tree_method=auto</code> default multiple times before learning my lesson. Why is the online model worse than the offline model? Why is this model suddenly taking so much longer to train? Avoid these debugging nightmares and set <code>tree_method</code> explicitly; the exact method tends to be slow and ironically less accurate, so I use either approx or hist.</li>
</ul>
</section>
<section id="tree-complexity-parameters" class="level4">
<h4 class="anchored" data-anchor-id="tree-complexity-parameters">Tree complexity parameters</h4>
<p>Tree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be. I use these two parameters:</p>
<ul>
<li><code>max_depth</code> - maximum number of split levels allowed. Reasonable values are usually from 3-12.</li>
<li><code>min_child_weight</code> - minimum allowable sum of hessian values over data in a node. When using the default squared error objective, this is the minimum number of samples allowed in a leaf node (see <a href="https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm">this explanation</a> of why that‚Äôs true). For a squared error objective, values in [1, 200] usually work well.</li>
</ul>
<p>These two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing min child weight makes trees less expressive and therefore is a powerful way to counter overfitting. Note that <code>gamma</code> (a.k.a. <code>min_split_loss</code>) also limits node splitting, but I usually don‚Äôt use it because <code>min_child_weight</code> seems to work well enough on its own.</p>
</section>
<section id="sampling-parameters" class="level4">
<h4 class="anchored" data-anchor-id="sampling-parameters">Sampling parameters</h4>
<p>XGBoost can randomly sample rows and columns to be used for training each tree; you might think of this as <em>bagging</em>. We have a few parameters:</p>
<ul>
<li><code>subsample</code> - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.</li>
<li><code>colsample_bytree</code>, <code>colsample_bylevel</code>, <code>colsample_bynode</code> - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split. Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.</li>
</ul>
</section>
<section id="regularization-parameters" class="level4">
<h4 class="anchored" data-anchor-id="regularization-parameters">Regularization parameters</h4>
<p>In XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero. I usually use:</p>
<ul>
<li><code>reg_lambda</code> - L2 regularization of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. Valid values are in [0,<img src="https://latex.codecogs.com/png.latex?%5Cinfty">), but good values typically fall in [0,10].</li>
</ul>
<p>There is also an L1 regularization parameter called <code>reg_alpha</code>; feel free to use it instead. It seems that using one or the other is usually sufficient.</p>
</section>
</section>
<section id="boosting-parameters-and-early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="boosting-parameters-and-early-stopping">Boosting Parameters and Early Stopping</h3>
<p>Trained gradient boosting models take the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20F(%5Cmathbf%7Bx%7D)%20=%20b%20+%20%5Ceta%20%5Csum_%7Bk=1%7D%5E%7BK%7D%20f_k(%5Cmathbf%7Bx%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b"> is the constant base predicted value, <img src="https://latex.codecogs.com/png.latex?f_k(%5Ccdot)"> is the base learner for round <img src="https://latex.codecogs.com/png.latex?k">, parameter <img src="https://latex.codecogs.com/png.latex?K"> is the number of boosting rounds, and parameter <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is the learning rate. In XGBoost these parameters correspond with:</p>
<ul>
<li><code>num_boost_round</code> (<img src="https://latex.codecogs.com/png.latex?K">) - the number of boosting iterations</li>
<li><code>learning_rate</code> (<img src="https://latex.codecogs.com/png.latex?%5Ceta">) - the scaling or ‚Äúshrinkage‚Äù factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. Fun fact: the <img src="https://latex.codecogs.com/png.latex?%5Ceta"> character is called ‚Äúeta‚Äù, and <code>learning_rate</code> is aliased to <code>eta</code> in xgboost, so you can use parameter <code>eta</code> instead of <code>learning_rate</code> if you like.</li>
</ul>
<p>These two parameters are very closely linked; the optimal value of one depends on the value of the other. To illustrate their relationship, we can train two different XGBoost models on the same training dataset, where one model has a lower learning rate than the other.</p>
<div id="cell-4" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split </span>
<span id="cb1-5"></span>
<span id="cb1-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5000</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-7">X_train, X_valid, y_train, y_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">eta1, eta2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span></span>
<span id="cb1-10">reg1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta1, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-11">reg1.fit(X_train, y_train, </span>
<span id="cb1-12">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-13">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-14">reg2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta2, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-15">reg2.fit(X_train, y_train, </span>
<span id="cb1-16">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-17">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-18"></span>
<span id="cb1-19">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb1-20"></span>
<span id="cb1-21">best_round1, best_round2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.best_iteration, reg2.best_iteration</span>
<span id="cb1-22">obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-23">obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-24">best_obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj1[best_round1]</span>
<span id="cb1-25">best_obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj2[best_round1]</span>
<span id="cb1-26"></span>
<span id="cb1-27">plt.plot(reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-b'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-28">plt.plot(reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-r'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-29">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-30">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1), </span>
<span id="cb1-31">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-32">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-33">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-34">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-35">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2), </span>
<span id="cb1-36">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-37">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-38">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-39">plt.legend()</span>
<span id="cb1-40">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RMSE'</span>) </span>
<span id="cb1-41">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting round'</span>) </span>
<span id="cb1-42">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Validation Scores by Boosting Round'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img" alt="XGBoost objective curves for two models"></p>
<figcaption>Hold-out validation score (RMSE) by boosting round for two XGBoost models differing only by learning rate.</figcaption>
</figure>
</div>
</div>
</div>
<p>The above figure shows root mean squared error measured on a held-out validation dataset for two different XGBoost models: one with a higher learning rate and one with a lower learning rate. The figure demonstrates two key properties of the boosting parameters:</p>
<ol type="1">
<li>While training a model with a given learning rate, the evaluation score (computed on a hold-out set) tends to improve with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.</li>
<li>All else constant, a smaller learning rate leads to a model with more boosting rounds and better evaluation score.</li>
</ol>
<p>We can leverage the first property to make our tuning more efficient by using XGBoost‚Äôs <code>early_stopping_rounds: int</code> argument, which terminates training after observing the specified number of boosting rounds <a href="https://github.com/dmlc/xgboost/pull/6942">without sufficient improvement</a> to the evaluation metric. The models above were trained using <code>early_stopping_rounds=50</code>, which terminates training after 50 boosting rounds without improvement in RMSE on the validation data. For each model, the arrow indicates the boosting round with the best score.</p>
<p>The figure also exemplifies the second property, where the model with lower learning rate attains a better validation score but requires more boosting rounds to trigger early stopping. Note that smaller and smaller learning rates will provide diminishing improvements to the validation score.</p>
</section>
</section>
<section id="an-efficient-parameter-search-strategy-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="an-efficient-parameter-search-strategy-for-xgboost">An Efficient Parameter Search Strategy for XGBoost</h2>
<p>Efficiency is the key to effective parameter tuning, because wasting less time means searching more parameter values and finding better models in a given amount of time. But as we just saw, there is a tradeoff between accuracy and training time via the learning rate. Given infinite time and compute resources, we would just choose an arbitrarily tiny learning rate and search through tree parameter values while using early stopping to choose the number of boosting rounds. The problem is that tiny learning rates require tons of boosting rounds, which will make our ideal search prohibitively slow when confronted with the reality of finite time and resources. So what can we do?</p>
<p>My approach is based on the claim that <em>good tree parameters at one learning rate are also good tree parameters at other learning rates</em>. The intuition is that given two models‚Äîone with good tree parameters and one with bad tree parameters‚Äîthe model with good tree parameters will score better, regardless of the learning rate. Thus, tree parameters are ‚Äúindependent‚Äù of boosting parameters‚ÄîSee <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> for justification of this claim.</p>
<p>Independence between tree parameters and boosting parameters suggests a two-stage procedure where we first find optimal tree parameters, then we maximize performance by pushing boosting parameters to the extreme. The procedure is:</p>
<ol type="1">
<li><strong>Tune tree parameters.</strong> Fix the learning rate at a relatively high value (like 0.3ish) and enable early stopping so that each model trains within a few seconds. Use your favorite hyperparameter tuning technique to find the optimal tree parameters.</li>
<li><strong>Tune boosting parameters.</strong> Using these optimal tree parameter values, fix the learning rate as low as you want and train your model, using early stopping to identify the optimal number of boosting rounds.</li>
</ol>
<p>Why is this a good idea? Because by starting with a high learning rate and early stopping enabled, you can burn through hundreds of model training trials and find some really good tree parameters in a few minutes. Then, with the confidence that your tree parameters are actually quite good, you can set a really low learning rate and boost a few thousand rounds to get a model with the best of both tree parameter and boosting parameter worlds.</p>
<blockquote class="blockquote">
<p>You can check out <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> where I justify this approach by running two parameter searches‚Äîone with high learning rate and one with low learning rate‚Äîshowing that they recover the same optimal tree parameters.</p>
</blockquote>
</section>
<section id="tuning-xgboost-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="tuning-xgboost-parameters-with-optuna">Tuning XGBoost Parameters with Optuna</h2>
<p><a href="https://optuna.readthedocs.io/en/stable/">Optuna</a> is a model-agnostic python library for hyperparameter tuning. I like it because it has a flexible API that abstracts away the details of the search algorithm being used. That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more. Another massive benefit is that optuna provides a specific <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.XGBoostPruningCallback.html">XGBoost integration</a> which terminates training early on lousy parameter combinations.</p>
<p>You can install optuna with anaconda, e.g.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode .zsh code-with-copy"><code class="sourceCode zsh"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> conda install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge optuna</span></code></pre></div>
</section>
<section id="example-tuning-the-bluebook-for-bulldozers-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="example-tuning-the-bluebook-for-bulldozers-regression-model">Example: Tuning the Bluebook for Bulldozers Regression Model</h2>
<p>To illustrate the procedure, we‚Äôll tune the parameters for the regression model we built back in the <a href="../../posts/xgboost-for-regression-in-python/">XGBoost for regression</a> post. First we‚Äôll load up the bulldozer data and prepare the features and target just like we did before.</p>
<div id="cell-9" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time </span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb3-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_squared_error</span>
<span id="cb3-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb</span>
<span id="cb3-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> optuna </span>
<span id="cb3-8"></span>
<span id="cb3-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../xgboost-for-regression-in-python/Train.csv'</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-10"></span>
<span id="cb3-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encode_string_features(df):</span>
<span id="cb3-12">    out_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> feature, feature_type <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> df.dtypes.items():</span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> feature_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'object'</span>:</span>
<span id="cb3-15">            out_df[feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> out_df[feature].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb3-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out_df</span>
<span id="cb3-17"></span>
<span id="cb3-18">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encode_string_features(df)</span>
<span id="cb3-19"></span>
<span id="cb3-20">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-21">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pd.Timestamp(year<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1970</span>, month<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, day<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-22">    ).dt.days</span>
<span id="cb3-23"></span>
<span id="cb3-24">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>])</span>
<span id="cb3-25"></span>
<span id="cb3-26"></span>
<span id="cb3-27">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb3-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,</span>
<span id="cb3-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>,</span>
<span id="cb3-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ModelID'</span>,</span>
<span id="cb3-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datasource'</span>,</span>
<span id="cb3-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auctioneerID'</span>,</span>
<span id="cb3-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>,</span>
<span id="cb3-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineHoursCurrentMeter'</span>,</span>
<span id="cb3-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'UsageBand'</span>,</span>
<span id="cb3-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDesc'</span>,</span>
<span id="cb3-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>,</span>
<span id="cb3-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiSecondaryDesc'</span>,</span>
<span id="cb3-39">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelSeries'</span>,</span>
<span id="cb3-40">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>,</span>
<span id="cb3-41">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>,</span>
<span id="cb3-42">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiProductClassDesc'</span>,</span>
<span id="cb3-43">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'state'</span>,</span>
<span id="cb3-44">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroup'</span>,</span>
<span id="cb3-45">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>,</span>
<span id="cb3-46">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Drive_System'</span>,</span>
<span id="cb3-47">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span>,</span>
<span id="cb3-48">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Forks'</span>,</span>
<span id="cb3-49">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pad_Type'</span>,</span>
<span id="cb3-50">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ride_Control'</span>,</span>
<span id="cb3-51">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick'</span>,</span>
<span id="cb3-52">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Transmission'</span>,</span>
<span id="cb3-53">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Turbocharged'</span>,</span>
<span id="cb3-54">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Extension'</span>,</span>
<span id="cb3-55">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Width'</span>,</span>
<span id="cb3-56">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure_Type'</span>,</span>
<span id="cb3-57">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Engine_Horsepower'</span>,</span>
<span id="cb3-58">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics'</span>,</span>
<span id="cb3-59">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pushblock'</span>,</span>
<span id="cb3-60">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ripper'</span>,</span>
<span id="cb3-61">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Scarifier'</span>,</span>
<span id="cb3-62">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tip_Control'</span>,</span>
<span id="cb3-63">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tire_Size'</span>,</span>
<span id="cb3-64">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler'</span>,</span>
<span id="cb3-65">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler_System'</span>,</span>
<span id="cb3-66">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>,</span>
<span id="cb3-67">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics_Flow'</span>,</span>
<span id="cb3-68">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Track_Type'</span>,</span>
<span id="cb3-69">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Undercarriage_Pad_Width'</span>,</span>
<span id="cb3-70">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick_Length'</span>,</span>
<span id="cb3-71">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Thumb'</span>,</span>
<span id="cb3-72">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pattern_Changer'</span>,</span>
<span id="cb3-73">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Type'</span>,</span>
<span id="cb3-74">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Backhoe_Mounting'</span>,</span>
<span id="cb3-75">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Type'</span>,</span>
<span id="cb3-76">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Travel_Controls'</span>,</span>
<span id="cb3-77">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Differential_Type'</span>,</span>
<span id="cb3-78">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Steering_Controls'</span>,</span>
<span id="cb3-79">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span></span>
<span id="cb3-80"> ]</span>
<span id="cb3-81"></span>
<span id="cb3-82">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span></span></code></pre></div>
</details>
</div>
<p>But this time, since we‚Äôre going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes. We make four different <code>xgboost.DMatrix</code> datasets for this process: training, validation, training+validation, and test. Training and validation are for the parameter search, and training+validation and test are for the final model.</p>
<div id="cell-11" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-2">n_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">sorted_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>)</span>
<span id="cb4-5">train_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test)] </span>
<span id="cb4-6">valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test):<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test] </span>
<span id="cb4-7">test_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test:]</span>
<span id="cb4-8"></span>
<span id="cb4-9">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], </span>
<span id="cb4-10">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-11">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], </span>
<span id="cb4-12">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-13">dtest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[target], </span>
<span id="cb4-14">                    enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-15">dtrainvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[features], </span>
<span id="cb4-16">                          label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[target], </span>
<span id="cb4-17">                          enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
</section>
<section id="preliminaries-base-parameters-and-scoring-function" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries-base-parameters-and-scoring-function">Preliminaries: base parameters and scoring function</h2>
<p>We‚Äôll go ahead and set a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping. We‚Äôll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE.</p>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span></span>
<span id="cb5-2">base_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb5-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg:squarederror'</span>,</span>
<span id="cb5-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eval_metric'</span>: metric,</span>
<span id="cb5-5">}</span></code></pre></div>
</div>
<div id="cell-14" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> score_model(model: xgb.core.Booster, dmat: xgb.core.DMatrix) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb6-2">    y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dmat.get_label() </span>
<span id="cb6-3">    y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(dmat) </span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> mean_squared_error(y_true, y_pred, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
</section>
<section id="stage-1-tune-tree-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="stage-1-tune-tree-parameters-with-optuna">Stage 1: Tune Tree Parameters with Optuna</h2>
<p>Next we need to choose a fixed learning rate and tune the tree parameters. We want a learning rate that allows us to train within a few seconds, so we need to time model training. Start with a high learning rate (like 0.8) and work down until you find a rate that takes a few seconds. Below I end up landing at 0.3, which takes about 4 seconds to train on my little laptop.</p>
<div id="cell-16" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span></span>
<span id="cb7-2"></span>
<span id="cb7-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate</span>
<span id="cb7-6">}</span>
<span id="cb7-7">params.update(base_params)</span>
<span id="cb7-8">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb7-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain,</span>
<span id="cb7-10">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb7-11">                  num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb7-12">                  early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb7-13">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb7-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>time<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> seconds'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>4.5 seconds</code></pre>
</div>
</div>
<p>Then we implement our optuna objective, a function taking an optuna study <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html"><code>Trial</code></a> object and returning the score we want to optimize. We use the <code>suggest_categorical</code>, <code>suggest_float</code>, and <code>suggest_int</code> methods of the <code>Trial</code> object to define the search space for each parameter. Note the use of the pruning callback function which we pass into the <code>callback</code> argument of the XGBoost <code>train</code> function; this is a must, since it allows optuna to terminate training on lousy models after a few boosting rounds. After training a model with the selected parameter values, we stash the optimal number of boosting rounds from early stopping into an optuna user attribute using the <a href="https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/003_attributes.html"><code>trial.user_attrs()</code></a> method. Finally we return the score computed by our <code>model_score</code> function.</p>
<div id="cell-18" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> objective(trial):</span>
<span id="cb9-2">    params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb9-3">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: trial.suggest_categorical(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist'</span>]),</span>
<span id="cb9-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>),</span>
<span id="cb9-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">250</span>),</span>
<span id="cb9-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>, log<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb9-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate,</span>
<span id="cb9-10">    }</span>
<span id="cb9-11">    num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb9-12">    params.update(base_params)</span>
<span id="cb9-13">    pruning_callback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.integration.XGBoostPruningCallback(trial, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'valid-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>metric<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb9-14">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb9-15">                      evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb9-16">                      early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb9-17">                      verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb9-18">                      callbacks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[pruning_callback])</span>
<span id="cb9-19">    trial.set_user_attr(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best_iteration'</span>, model.best_iteration)</span>
<span id="cb9-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model.best_score</span></code></pre></div>
</div>
<p>To create a new optuna study and search through 50 parameter combinations, you could just run these two lines.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>)</span>
<span id="cb10-2">study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span></code></pre></div>
<p>But, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials‚Äîwho knows how long 50 trials will take. I also want the results to be reproducible. So, to set the random seed and run the optimization for around 300 seconds (long enough to go make a nice cup of tea, stretch, and come back), I do something like this:</p>
<div id="cell-20" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">sampler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.samplers.TPESampler(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb11-2">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>, sampler<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sampler)</span>
<span id="cb11-3">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb11-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>:</span>
<span id="cb11-5">    study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="cell-21" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 1 =============================='</span>)</span>
<span id="cb12-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>value<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb12-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>learning_rate<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>user_attrs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"best_iteration"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best tree params --------------------------'</span>)</span>
<span id="cb12-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> study.best_trial.params.items():</span>
<span id="cb12-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 1 ==============================
best score = 0.23107522766919256
boosting params ---------------------------
fixed learning rate: 0.3
best boosting round: 23
best tree params --------------------------
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469</code></pre>
</div>
</div>
<p>If we decide we want to tune the tree parameters a little more, we can just call <code>study.optimize(...)</code> again, adding as many trials as we want to the study. Once we‚Äôre happy with the tree parameters, we can proceed to stage 2.</p>
</section>
<section id="stage-2-intensify-the-boosting-parameters" class="level2">
<h2 class="anchored" data-anchor-id="stage-2-intensify-the-boosting-parameters">Stage 2: Intensify the Boosting Parameters</h2>
<p>Now we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate; here I use 0.01, but you could go lower. The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you‚Äôll need to max out the evaluation metric on the validation data.</p>
<div id="cell-23" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">low_learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb14-2"></span>
<span id="cb14-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb14-4">params.update(base_params)</span>
<span id="cb14-5">params.update(study.best_trial.params)</span>
<span id="cb14-6">params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> low_learning_rate</span>
<span id="cb14-7">model_stage2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, </span>
<span id="cb14-8">                         num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb14-9">                         evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb14-10">                         early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb14-11">                         verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div id="cell-24" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 2 =============================='</span>)</span>
<span id="cb15-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_stage2, dvalid)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb15-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 2 ==============================
best score = 0.22172991931438446
boosting params ---------------------------
fixed learning rate: 0.01
best boosting round: 1446</code></pre>
</div>
</div>
</section>
<section id="train-and-evaluate-the-final-model" class="level2">
<h2 class="anchored" data-anchor-id="train-and-evaluate-the-final-model">Train and Evaluate the Final Model</h2>
<p>Now we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2. Then we evaluate on the held out test data.</p>
<div id="cell-26" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">model_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrainvalid, </span>
<span id="cb17-2">                        num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_stage2.best_iteration,</span>
<span id="cb17-3">                        verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Final Model =========================='</span>)</span>
<span id="cb18-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'test score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_final, dtest)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb18-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'parameters ---------------------------'</span>)</span>
<span id="cb18-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> params.items():</span>
<span id="cb18-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span>
<span id="cb18-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'num_boost_round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Final Model ==========================
test score = 0.21621863543987274
parameters ---------------------------
objective : reg:squarederror
eval_metric : rmse
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469
learning_rate : 0.01
num_boost_round: 1446</code></pre>
</div>
</div>
<p>Back in the <a href="../../posts/xgboost-for-regression-in-python/">regression post</a> we got an RMSE of about 0.231 just using default parameter values, which put us in 5th place on the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard">leaderboard for the Kagle dozers competition</a>. Now with about 10 minutes of hyperparameter tuning, our RMSE is down to 0.216 which puts us in 1st place by a huge margin. üôå</p>
</section>
<section id="what-could-possibly-go-wrong" class="level2">
<h2 class="anchored" data-anchor-id="what-could-possibly-go-wrong">What could possibly go wrong?</h2>
<p>Hyperparameter tuning can easily be overlooked in the move-fast-and-break-everything hustle of building an ML product, but it can also easily become overkill or even downright harmful, depending on the application. There are three key questions to ask:</p>
<ol type="1">
<li>How much value is created by an incremental gain in model prediction accuracy?</li>
<li>What is the cost of increasing model prediction accuracy?</li>
<li>Is my model answering the right question?</li>
</ol>
<p>Sometimes a small gain in model prediction performance translates into millions of dollars of impact. The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org‚Äôs KPIs, and get mad respect, bonuses, and promoted. But the reality is that often additional model accuracy doesn‚Äôt really change business KPIs by very much. Try to figure out the actual value of improved model accuracy and proceed accordingly.</p>
<p>Remember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself. It can also lead us to larger and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.</p>
<p>Worst of all and quite counterintuitively, it‚Äôs possible that improving a model‚Äôs prediction accuracy can compromise overall business KPIs. I‚Äôve seen this with my own eyes at work; offline testing shows that hyperparameter tuning significantly improves a model‚Äôs prediction accuracy, but when the model goes into production, an AB test shows that the business KPIs are actually worse. What happened? In this case, the model‚Äôs prediction was being used indirectly to infer the relationship between one of the features and the prediction target to inform automatic business decisions. Answering questions about how changing an input will affect an output requires causal reasoning, and <a href="https://arxiv.org/abs/1608.00060">traditional ML models are not the right tool for the job</a>. I‚Äôll have a lot more to say about that soon; let this story foreshadow an epic new epoch on Random Realizations‚Ä¶.</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna. If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/</guid>
  <pubDate>Tue, 26 Dec 2023 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>XGBoost for Binary and Multi-Class Classification in Python</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-for-classification-in-python/</link>
  <description><![CDATA[ 




<p>Today we continue the <a href="../../gradient-boosting-series.html">saga on gradient boosting</a> with a down-to-Earth tutorial on the essentials of solving classification problems with XGBoost. We‚Äôll run through two examples: one for binary classification and another for multi-class classification. In both cases I‚Äôll show you how to train XGBoost models using either the scikit-learn interface or the native xgboost training API. Once trained, we‚Äôll evaluate the models with validation data then inspect them with feature importance and partial dependence plots. You can use the XGBoost classification notebook in my <a href="https://github.com/mcb00/ds-templates">ds-templates repository</a> to follow along with your own dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-classification-main.jpg" class="img-fluid figure-img"></p>
<figcaption>Afternoon in the Mara</figcaption>
</figure>
</div>
<section id="preparing-data-for-xgboost-classifier" class="level2">
<h2 class="anchored" data-anchor-id="preparing-data-for-xgboost-classifier">Preparing Data for XGBoost Classifier</h2>
<p>Our dataset must satisfy two requirements to be used in an XGBoost classifier. First all feature data must be numeric‚Äîno strings and no datetimes; if you have non-numeric features, you need to <a href="../../posts/xgboost-for-regression-in-python/#prepare-raw-data-for-xgboost">transform your feature data</a>. Second, the target must be integer encoded using <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D"> for binary targets and <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1,%5Cdots,K%5C%7D"> for multiclass targets. Note that if your data is encoded to positive integers (no 0 class) XGBoost will throw potentially cryptic errors. You can use the scikit-learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"><code>LabelEncoder</code></a> (which we‚Äôll do below) to generate a valid target encoding.</p>
</section>
<section id="xgboost-training-apis" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-training-apis">XGBoost Training APIs</h2>
<p>The <code>xgboost</code> python library offers two API‚Äôs for training classification models: the native <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training"><code>train</code></a> function and a wrapper class called <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"><code>XGBClassifier</code></a>, which offers an API consistent with the scikit-learn universe. I‚Äôll show you how to use both approaches in the examples below, but if you‚Äôre planning to use other utilities from scikit-learn, you might find the <code>XGBClassifier</code> approach to be more convenient, since the trained model object will generally play nice with sklearn functionality.</p>
</section>
<section id="binary-classification-example" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification-example">Binary Classification Example</h2>
<section id="breast-cancer-wisconsin-dataset" class="level3">
<h3 class="anchored" data-anchor-id="breast-cancer-wisconsin-dataset">Breast Cancer Wisconsin Dataset</h3>
<p>We‚Äôll demonstrate binary classification in XGBoost using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer">breast cancer wisconsin data</a>, one of scikit-learn‚Äôs built-in toy datasets. This is a tiny dataset with 569 observations of 30 features and a binary target representing whether samples are malignant or benign..</p>
<div id="cell-6" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datasets</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb </span>
<span id="cb1-6"></span>
<span id="cb1-7">dbunch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.load_breast_cancer(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.frame</span>
<span id="cb1-9">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.feature_names </span>
<span id="cb1-10">target_names <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.target_names </span>
<span id="cb1-11">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span> </span>
<span id="cb1-12">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 569 entries, 0 to 568
Data columns (total 31 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   mean radius              569 non-null    float64
 1   mean texture             569 non-null    float64
 2   mean perimeter           569 non-null    float64
 3   mean area                569 non-null    float64
 4   mean smoothness          569 non-null    float64
 5   mean compactness         569 non-null    float64
 6   mean concavity           569 non-null    float64
 7   mean concave points      569 non-null    float64
 8   mean symmetry            569 non-null    float64
 9   mean fractal dimension   569 non-null    float64
 10  radius error             569 non-null    float64
 11  texture error            569 non-null    float64
 12  perimeter error          569 non-null    float64
 13  area error               569 non-null    float64
 14  smoothness error         569 non-null    float64
 15  compactness error        569 non-null    float64
 16  concavity error          569 non-null    float64
 17  concave points error     569 non-null    float64
 18  symmetry error           569 non-null    float64
 19  fractal dimension error  569 non-null    float64
 20  worst radius             569 non-null    float64
 21  worst texture            569 non-null    float64
 22  worst perimeter          569 non-null    float64
 23  worst area               569 non-null    float64
 24  worst smoothness         569 non-null    float64
 25  worst compactness        569 non-null    float64
 26  worst concavity          569 non-null    float64
 27  worst concave points     569 non-null    float64
 28  worst symmetry           569 non-null    float64
 29  worst fractal dimension  569 non-null    float64
 30  target                   569 non-null    int64  
dtypes: float64(30), int64(1)
memory usage: 137.9 KB</code></pre>
</div>
</div>
<p>In this dataset, the features are all numeric, so no need to do preprocessing before passing to XGBoost. Below we‚Äôll have a look at the target to ensure it‚Äôs encoded in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D"> and to check the class balance.</p>
<div id="cell-8" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(df[target].unique())</span>
<span id="cb3-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(target_names)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1]
['malignant' 'benign']</code></pre>
</div>
</div>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df.target.value_counts().sort_index().plot.bar()</span>
<span id="cb5-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>) </span>
<span id="cb5-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img" alt="bar plot showing the count of observations in each class"></p>
<figcaption>class counts for the breast cancer dataset</figcaption>
</figure>
</div>
</div>
</div>
<p>Next We randomly split data into train and validation sets.</p>
<div id="cell-11" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb6-2"></span>
<span id="cb6-3">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span> </span>
<span id="cb6-4"></span>
<span id="cb6-5">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_valid, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb6-6">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>((519, 31), (50, 31))</code></pre>
</div>
</div>
</section>
<section id="training-with-the-train-function" class="level3">
<h3 class="anchored" data-anchor-id="training-with-the-train-function">Training with the <code>train</code> function</h3>
<p>We need to set a couple of <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">model parameters</a>, most notably <code>objective</code>, which should be set to <code>binary:logistic</code> for binary classification. I also prefer to explicitly set <code>tree_method</code> to something other than its default of <code>auto</code>; usually I‚Äôll start with <code>exact</code> on small datasets or <code>approx</code> on larger ones. Note also that The <code>train</code> function expects to receive data as <code>DMatrix</code> objects, not pandas dataframes, so we need to create dense matrix objects as well.</p>
<div id="cell-13" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb8-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'exact'</span>,</span>
<span id="cb8-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary:logistic'</span>,</span>
<span id="cb8-4">}</span>
<span id="cb8-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb8-6"></span>
<span id="cb8-7">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features])</span>
<span id="cb8-8">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features])</span>
<span id="cb8-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb8-10">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb8-11">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-logloss:0.46232   valid-logloss:0.49033
[10]    train-logloss:0.04394   valid-logloss:0.13434
[20]    train-logloss:0.01515   valid-logloss:0.12193
[30]    train-logloss:0.00995   valid-logloss:0.11988
[40]    train-logloss:0.00766   valid-logloss:0.12416
[49]    train-logloss:0.00657   valid-logloss:0.12799</code></pre>
</div>
</div>
</section>
<section id="training-with-xgbclassifier" class="level3">
<h3 class="anchored" data-anchor-id="training-with-xgbclassifier">Training with <code>XGBClassifier</code></h3>
<p>The <code>XGBClassifier</code> takes dataframes or numpy arrays as input, so this time we don‚Äôt need to create those dense matrix objects.</p>
<div id="cell-15" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb10-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'exact'</span>,</span>
<span id="cb10-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary:logistic'</span>,</span>
<span id="cb10-4">}</span>
<span id="cb10-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb10-6"></span>
<span id="cb10-7">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb10-8">clf.fit(train_df[features], train_df[target], </span>
<span id="cb10-9">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb10-10">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-logloss:0.46232    validation_1-logloss:0.49033
[10]    validation_0-logloss:0.04394    validation_1-logloss:0.13434
[20]    validation_0-logloss:0.01515    validation_1-logloss:0.12193
[30]    validation_0-logloss:0.00995    validation_1-logloss:0.11988
[40]    validation_0-logloss:0.00766    validation_1-logloss:0.12416
[49]    validation_0-logloss:0.00657    validation_1-logloss:0.12799</code></pre>
</div>
</div>
</section>
<section id="evaluating-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model">Evaluating the Model</h3>
<p>We‚Äôll use the <code>sklearn.metrics</code> module to evaluate model performance on the held-out validation set. Have a look at the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics">scikit-learn metrics for classification</a> for examples of other metrics to use.</p>
<p>One thing to watch out for when computing metrics is the difference between the actual labels (usually called <code>y_true</code>), the model‚Äôs predicted labels (usually called <code>y_pred</code>), and the models predicted probabilities (usually called <code>y_score</code>). If you‚Äôre using the <code>XGBClassifier</code> wrapper, you can get predicted labels with the <code>predict</code> method and predicted probabilities with the <code>predict_proba</code> method. Also note that whereas <code>predict</code> returns a vector of size (num data), <code>predict_proba</code> returns a vector of size (num data, num classes); thus for binary classification, we‚Äôll take just the second column of the array which gives the probability of class 1.</p>
<div id="cell-17" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_df[target]</span>
<span id="cb12-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict(valid_df[features])</span>
<span id="cb12-3">y_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict_proba(valid_df[features])[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<p>Probably the simplest classification metric is accuracy, the proportion of labels we predicted correctly.</p>
<div id="cell-19" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> metrics </span>
<span id="cb13-2"></span>
<span id="cb13-3">metrics.accuracy_score(y_true, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>0.96</code></pre>
</div>
</div>
<p>We can generate a classification report with several different metrics at once.</p>
<div id="cell-21" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(metrics.classification_report(y_true, y_pred, target_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>target_names))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

   malignant       0.93      0.93      0.93        15
      benign       0.97      0.97      0.97        35

    accuracy                           0.96        50
   macro avg       0.95      0.95      0.95        50
weighted avg       0.96      0.96      0.96        50
</code></pre>
</div>
</div>
<p>And we can compute the AUC, a popular classification metric based on the ROC curve, which depends on the predicted probability rather than the predicted labels.</p>
<div id="cell-23" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">metrics.roc_auc_score(y_true, y_score)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>0.9885714285714287</code></pre>
</div>
</div>
</section>
<section id="feature-importance" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance">Feature Importance</h3>
<p>Because of the <a href="../../posts/xgboost-for-regression-in-python/#feature-importance-for-xgboost">limitations of the built-in XGBoost feature importance metrics</a> I recommend that you use either <a href="https://scikit-learn.org/stable/modules/permutation_importance.html">permutation feature importance</a> or perhaps <a href="https://shap.readthedocs.io/en/latest/index.html">SHAP feature importance</a>.</p>
<p>Here we‚Äôll compute the permutation feature importance, which tells us by how much the model‚Äôs performance changes when we scramble a particular feature‚Äôs values at prediction time. This reflects how much the model relies on each feature when making predictions.</p>
<div id="cell-25" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> permutation_importance</span>
<span id="cb19-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_scorer</span>
<span id="cb19-3"></span>
<span id="cb19-4">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(metrics.log_loss, greater_is_better<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, needs_proba<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-5">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(clf, valid_df[features], valid_df[target], </span>
<span id="cb19-6">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div id="cell-26" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb20-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb20-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb20-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'change in log likelihood'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption>top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="partial-dependence" class="level2">
<h2 class="anchored" data-anchor-id="partial-dependence">Partial Dependence</h2>
<p>A <a href="https://scikit-learn.org/stable/modules/partial_dependence.html">partial dependence plot (PDP)</a> is a representation of the dependence between the model output and one or more feature variables. In binary classification, the model output is the probability of the so-called positive class, i.e.&nbsp;the class with encoded label 1, which corresponds to probability of ‚Äúbenign‚Äù in this example.. We can loosely interpret the partial dependence as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say ‚Äúloosely‚Äù because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.</p>
<div id="cell-28" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PartialDependenceDisplay</span>
<span id="cb21-2"></span>
<span id="cb21-3">PartialDependenceDisplay.from_estimator(clf, </span>
<span id="cb21-4">                                        valid_df[features], </span>
<span id="cb21-5">                                        [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'worst area'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'area error'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean area'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="line plots showing partial dependence of probability of benign"></p>
<figcaption>PDP of target probability of benign vs three features</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="multi-class-classification-example" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification-example">Multi-Class Classification Example</h2>
<section id="forest-cover-type-dataset" class="level3">
<h3 class="anchored" data-anchor-id="forest-cover-type-dataset">Forest Cover Type Dataset</h3>
<p>We‚Äôll illustrate multi-class classification using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype">scikit-learn forest cover type dataset</a>, which has around 580k observations of 54 features and a target with 7 classes.</p>
<div id="cell-30" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">dbunch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.fetch_covtype(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb22-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.frame</span>
<span id="cb22-3">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.feature_names </span>
<span id="cb22-4">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 581012 entries, 0 to 581011
Data columns (total 55 columns):
 #   Column                              Non-Null Count   Dtype  
---  ------                              --------------   -----  
 0   Elevation                           581012 non-null  float64
 1   Aspect                              581012 non-null  float64
 2   Slope                               581012 non-null  float64
 3   Horizontal_Distance_To_Hydrology    581012 non-null  float64
 4   Vertical_Distance_To_Hydrology      581012 non-null  float64
 5   Horizontal_Distance_To_Roadways     581012 non-null  float64
 6   Hillshade_9am                       581012 non-null  float64
 7   Hillshade_Noon                      581012 non-null  float64
 8   Hillshade_3pm                       581012 non-null  float64
 9   Horizontal_Distance_To_Fire_Points  581012 non-null  float64
 10  Wilderness_Area_0                   581012 non-null  float64
 11  Wilderness_Area_1                   581012 non-null  float64
 12  Wilderness_Area_2                   581012 non-null  float64
 13  Wilderness_Area_3                   581012 non-null  float64
 14  Soil_Type_0                         581012 non-null  float64
 15  Soil_Type_1                         581012 non-null  float64
 16  Soil_Type_2                         581012 non-null  float64
 17  Soil_Type_3                         581012 non-null  float64
 18  Soil_Type_4                         581012 non-null  float64
 19  Soil_Type_5                         581012 non-null  float64
 20  Soil_Type_6                         581012 non-null  float64
 21  Soil_Type_7                         581012 non-null  float64
 22  Soil_Type_8                         581012 non-null  float64
 23  Soil_Type_9                         581012 non-null  float64
 24  Soil_Type_10                        581012 non-null  float64
 25  Soil_Type_11                        581012 non-null  float64
 26  Soil_Type_12                        581012 non-null  float64
 27  Soil_Type_13                        581012 non-null  float64
 28  Soil_Type_14                        581012 non-null  float64
 29  Soil_Type_15                        581012 non-null  float64
 30  Soil_Type_16                        581012 non-null  float64
 31  Soil_Type_17                        581012 non-null  float64
 32  Soil_Type_18                        581012 non-null  float64
 33  Soil_Type_19                        581012 non-null  float64
 34  Soil_Type_20                        581012 non-null  float64
 35  Soil_Type_21                        581012 non-null  float64
 36  Soil_Type_22                        581012 non-null  float64
 37  Soil_Type_23                        581012 non-null  float64
 38  Soil_Type_24                        581012 non-null  float64
 39  Soil_Type_25                        581012 non-null  float64
 40  Soil_Type_26                        581012 non-null  float64
 41  Soil_Type_27                        581012 non-null  float64
 42  Soil_Type_28                        581012 non-null  float64
 43  Soil_Type_29                        581012 non-null  float64
 44  Soil_Type_30                        581012 non-null  float64
 45  Soil_Type_31                        581012 non-null  float64
 46  Soil_Type_32                        581012 non-null  float64
 47  Soil_Type_33                        581012 non-null  float64
 48  Soil_Type_34                        581012 non-null  float64
 49  Soil_Type_35                        581012 non-null  float64
 50  Soil_Type_36                        581012 non-null  float64
 51  Soil_Type_37                        581012 non-null  float64
 52  Soil_Type_38                        581012 non-null  float64
 53  Soil_Type_39                        581012 non-null  float64
 54  Cover_Type                          581012 non-null  int32  
dtypes: float64(54), int32(1)
memory usage: 241.6 MB</code></pre>
</div>
</div>
<p>Here again the features are all numeric, so we don‚Äôt need to further preprocess them. Let‚Äôs have a look at the target.</p>
<div id="cell-32" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cover_Type'</span>].value_counts().sort_index().plot.bar()</span>
<span id="cb24-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cover type'</span>) </span>
<span id="cb24-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img" alt="bar plot showing the count of observations in each class"></p>
<figcaption>class counts for the forest cover type dataset</figcaption>
</figure>
</div>
</div>
</div>
<p>For multi-class classification, our target variable must take values in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1,%5Cdots,K%5C%7D">. However, from the histogram of the cover type above, we see that it takes values in <img src="https://latex.codecogs.com/png.latex?%5C%7B1,2,%5Cdots,7%5C%7D">. To fix this we can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">scikit-learn label encoder</a> to create a valid target column.</p>
<div id="cell-34" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LabelEncoder </span>
<span id="cb25-2"></span>
<span id="cb25-3">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'encoded'</span></span>
<span id="cb25-4">enc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LabelEncoder()</span>
<span id="cb25-5">df[target] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> enc.fit_transform(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cover_Type'</span>])</span>
<span id="cb25-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(np.sort(df[target].unique()))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 2 3 4 5 6]</code></pre>
</div>
</div>
<p>Then we can create training and validation sets.</p>
<div id="cell-36" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span></span>
<span id="cb27-2"></span>
<span id="cb27-3">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_valid, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb27-4">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>((561012, 56), (20000, 56))</code></pre>
</div>
</div>
</section>
<section id="training-with-the-train-function-1" class="level3">
<h3 class="anchored" data-anchor-id="training-with-the-train-function-1">Training with the <code>train</code> function</h3>
<p>If you‚Äôre training with the <code>train</code> function, multi-class classification can be done with two objectives: <code>multi:softmax</code> and <code>multi:softprob</code>. Both use the same loss function‚Äînegative multinomial log likelihood‚Äîbut the softmax option produces a trained <code>Booster</code> object whose predict method returns a 1d array of predicted labels, whereas the softprob option produces a trained <code>Booster</code> object whose predict method returns a 2d array of predicted probabilities. In either case, you also need to explicitly tell XGBoost how many classes the target has with the <code>num_class</code> parameter.</p>
<div id="cell-38" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb29-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb29-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'multi:softprob'</span>,</span>
<span id="cb29-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_class'</span>: df[target].nunique()</span>
<span id="cb29-5">}</span>
<span id="cb29-6">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb29-7"></span>
<span id="cb29-8">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features])</span>
<span id="cb29-9">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features])</span>
<span id="cb29-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb29-11">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb29-12">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-mlogloss:1.42032  valid-mlogloss:1.42366
[2] train-mlogloss:1.00541  valid-mlogloss:1.00963
[4] train-mlogloss:0.80557  valid-mlogloss:0.81109
[6] train-mlogloss:0.69432  valid-mlogloss:0.70085
[8] train-mlogloss:0.62653  valid-mlogloss:0.63350
[9] train-mlogloss:0.60111  valid-mlogloss:0.60794</code></pre>
</div>
</div>
</section>
<section id="training-with-xgbclassifier-1" class="level3">
<h3 class="anchored" data-anchor-id="training-with-xgbclassifier-1">Training with <code>XGBClassifier</code></h3>
<p>In multi-class classification, I think the scikit-learn <code>XGBClassifier</code> wrapper is quite a bit more convenient than the native <code>train</code> function. You can set the <code>objective</code> parameter to <code>multi:softprob</code>, and <code>XGBClassifier.fit</code> will produce a model having both <code>predict</code> and <code>predict_proba</code> methods. Also there is no need to explicitly set the number of classes in the target and no need to create the <code>DMatrix</code> objects.</p>
<div id="cell-40" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb31-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb31-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'multi:softprob'</span>,</span>
<span id="cb31-4">}</span>
<span id="cb31-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb31-6"></span>
<span id="cb31-7">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb31-8">clf.fit(train_df[features], train_df[target], </span>
<span id="cb31-9">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb31-10">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-mlogloss:1.42032   validation_1-mlogloss:1.42366
[2] validation_0-mlogloss:1.00541   validation_1-mlogloss:1.00963
[4] validation_0-mlogloss:0.80557   validation_1-mlogloss:0.81109
[6] validation_0-mlogloss:0.69432   validation_1-mlogloss:0.70085
[8] validation_0-mlogloss:0.62653   validation_1-mlogloss:0.63350
[9] validation_0-mlogloss:0.60111   validation_1-mlogloss:0.60794</code></pre>
</div>
</div>
</section>
<section id="evaluating-the-model-1" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model-1">Evaluating the Model</h3>
<p>This time, we‚Äôll keep the entire 2d array of predicted probabilities in <code>y_score</code>.</p>
<div id="cell-42" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_df[target]</span>
<span id="cb33-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict(valid_df[features])</span>
<span id="cb33-3">y_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict_proba(valid_df[features])</span>
<span id="cb33-4">y_true.shape, y_pred.shape, y_score.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>((20000,), (20000,), (20000, 7))</code></pre>
</div>
</div>
<div id="cell-43" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">metrics.accuracy_score(y_true, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0.77425</code></pre>
</div>
</div>
<div id="cell-44" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(metrics.classification_report(y_true, y_pred))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

           0       0.77      0.74      0.75      7365
           1       0.78      0.84      0.81      9725
           2       0.75      0.85      0.80      1207
           3       0.82      0.78      0.80        85
           4       0.93      0.26      0.40       317
           5       0.76      0.31      0.44       627
           6       0.88      0.68      0.77       674

    accuracy                           0.77     20000
   macro avg       0.81      0.64      0.68     20000
weighted avg       0.78      0.77      0.77     20000
</code></pre>
</div>
</div>
<p>Some binary classification metrics, like AUC, can be extended to the multi-class setting by computing the metric for each class, then averaging in some way to get an overall score. The details are controlled by the <code>average</code> and <code>multi_class</code> parameters, which are described in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">documentation</a>.</p>
<div id="cell-46" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">metrics.roc_auc_score(y_true, y_score, average<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'weighted'</span>, multi_class<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ovr'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>0.9129422094408693</code></pre>
</div>
</div>
</section>
<section id="feature-importance-1" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-1">Feature Importance</h3>
<p>We can compute permutation feature importance with exactly the same code that we used for the binary classifier.</p>
<div id="cell-48" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(metrics.log_loss, greater_is_better<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, needs_proba<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb41-2">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(clf, valid_df[features], valid_df[target], </span>
<span id="cb41-3">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div id="cell-49" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb42-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb42-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb42-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'change in multivariate log likelihood'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption>top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="partial-dependence-1" class="level3">
<h3 class="anchored" data-anchor-id="partial-dependence-1">Partial Dependence</h3>
<p>Recall that partial dependence reflects how the expected model output changes with a particular feature. In the multi-class setting, the model has multiple outputs‚Äîone probability for each class‚Äîso we need to choose which class probability to show in the plots. We choose the target class with the <code>target</code> parameter; be sure to pass in the encoded value, e.g.&nbsp;we need to use the label encoder to transform a raw class label back into the encoded value. Here we‚Äôll examine partial dependence for the probability of cover type 3.</p>
<div id="cell-51" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">PartialDependenceDisplay.from_estimator(clf, </span>
<span id="cb43-2">                                        X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], </span>
<span id="cb43-3">                                        features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Elevation'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Horizontal_Distance_To_Roadways'</span>], </span>
<span id="cb43-4">                                        target<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>enc.transform([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img" alt="line plots showing partial dependence of probability of cover type 3 vs two features"></p>
<figcaption>PDP of target probability of cover type == 3 vs elevation and distance to roadway</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, for me, those are really the minimal nuts and bolts one needs to get XGBoost models working on classification problems. If you dig this tutorial, or if you have additional insights into using XGBoost to solve classification problems, let me know about it down in the comments!</p>
</section>
<section id="go-deeper" class="level2">
<h2 class="anchored" data-anchor-id="go-deeper">Go Deeper</h2>
<p>If you‚Äôre feeling like Alice, and you want to go tumbling down the rabbit hole, might I recommend checking out some of the following:</p>
<ul>
<li><a href="../../posts/xgboost-explained/">XGBoost Explained</a> - for a deep dive into the math</li>
<li><a href="../../posts/xgboost-from-scratch/">XGBoost from Scratch</a> - to see how to implement all those equations in code</li>
<li><a href="../../posts/gradient-boosting-multi-class-classification-from-scratch/">Multi-Class Gradient Boosting from Scratch</a> - to fully grok the multi-class gradient boosting algorithm</li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-for-classification-in-python/</guid>
  <pubDate>Tue, 28 Nov 2023 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-classification-thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
