<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<atom:link href="https://randomrealizations.com/archive.xml" rel="self" type="application/rss+xml"/>
<description>A blog about data science, statistics, machine learning, and the scientific method</description>
<image>
<url>https://randomrealizations.com/opengraph.png</url>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html</link>
<height>76</height>
<width>144</width>
</image>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Mon, 19 Jan 2026 08:00:00 GMT</lastBuildDate>
<item>
  <title>Decoding Statistical Testing with a Model-Based Perspective</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/model-based-stat-testing/</link>
  <description><![CDATA[ 




<p>Ahh, statistical hypothesis testing—it’s a cornerstone of classical statistical inference. It can also seem a bit daunting when presented as a list of <a href="https://en.wikipedia.org/wiki/Category:Statistical_tests">over 100 different methods</a> from which the user is expected to choose the right procedure based on their application domain, data, and question. But it doesn’t have to be that way—there is a unifying perspective that can help us cut right through all that complexity.</p>
<p><em>It turns out that many traditional frequentist tests can be viewed as special cases of regression models.</em></p>
<p>This is a powerful and liberating idea, because it can help us transition from choosing from the zoo of canned methods to building our own bespoke analyses, tailored to our exact use case. Adopting this perspective will allow us to</p>
<ul>
<li>transcend rote memorization of which test should be applied where and instead reason from first principles</li>
<li>make our assumptions clear and explicit</li>
<li>make extensions like incorporating covariates and interactions more natural</li>
</ul>
<p>So the plan for this post is to take a look at three of the most common statistical tests—the two-sample t-test, the ANOVA, and the chi-squared test for independence. For each test, we’ll simulate some data and implement it using the traditional test approach as well as a regression-based approach. I’m going to do a detailed analytical breakdown for the two-sample t-test to show the equivalence between the two approaches, leaving the analytical breakdowns of the other tests to you, dear reader, as exercises.</p>
<p>Let’s roll!</p>
<section id="two-sample-t-test" class="level2">
<h2 class="anchored" data-anchor-id="two-sample-t-test">Two-Sample t-test</h2>
<p>We’ll look at the t-test from two perspectives—the classical setup and a linear regression reformulation. In each case we’ll break the approach down into these items: data generating process, estimator, expectation and variance of the estimator, test statistic, and sampling distribution of the test statistic. You can use this kind of breakdown to understand pretty much any classical statistical test. In this case, the point is to clearly show that the classical t-test and the linear regression formulation yield identical tests.</p>
<section id="the-classical-t-test-approach" class="level3">
<h3 class="anchored" data-anchor-id="the-classical-t-test-approach">The Classical t-test Approach</h3>
<p><strong>The data generating process</strong></p>
<p>You have two populations or processes <img src="https://latex.codecogs.com/png.latex?Y_0"> and <img src="https://latex.codecogs.com/png.latex?Y_1">, and you want to know whether their true means <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?%5Cmu_1"> are equal. We assume that both processes are Gaussian with equal but unknown variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Y_0%20%5Csim%20N(%5Cmu_0,%20%5Csigma%5E2),%20%5Cquad%20Y_1%20%5Csim%20N(%5Cmu_1,%20%5Csigma%5E2)%20"></p>
<p><strong>The estimator</strong></p>
<p>You draw <img src="https://latex.codecogs.com/png.latex?n_0"> samples from group 0 and <img src="https://latex.codecogs.com/png.latex?n_1"> samples from group 1 for a total of <img src="https://latex.codecogs.com/png.latex?n=n_0+n_1"> samples, and compute the sample means <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_0"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_1">. Your estimator for the difference in means is simply:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cdelta%7D%20=%20%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0"></p>
<p><strong>Expectation of the estimator</strong></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?E%5B%5Cbar%7BY%7D_0%5D%20=%20%5Cmu_0"> and <img src="https://latex.codecogs.com/png.latex?E%5B%5Cbar%7BY%7D_1%5D%20=%20%5Cmu_1">, we have:</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7B%5Cdelta%7D%5D%20=%20E%5B%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0%5D%20=%20%5Cmu_1%20-%20%5Cmu_0"></p>
<p>So <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cdelta%7D"> is an unbiased estimator of the true difference in means.</p>
<p><strong>Standard error of the estimator</strong></p>
<p>The sample means are independent, so:</p>
<p><img src="https://latex.codecogs.com/png.latex?Var%5B%5Chat%7B%5Cdelta%7D%5D%20=%20Var%5B%5Cbar%7BY%7D_1%5D%20+%20Var%5B%5Cbar%7BY%7D_0%5D%20=%20%5Cfrac%7B%5Csigma%5E2%7D%7Bn_1%7D%20+%20%5Cfrac%7B%5Csigma%5E2%7D%7Bn_0%7D%20=%20%5Csigma%5E2%5Cleft(%5Cfrac%7B1%7D%7Bn_1%7D%20+%20%5Cfrac%7B1%7D%7Bn_0%7D%5Cright)"></p>
<p>Since we don’t know <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">, we estimate it with the pooled sample variance:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2%20=%20%5Cfrac%7B(n_0-1)s_0%5E2%20+%20(n_1-1)s_1%5E2%7D%7Bn_0%20+%20n_1%20-%202%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?s_0%5E2"> and <img src="https://latex.codecogs.com/png.latex?s_1%5E2"> are the sample variances for each group. This gives us the estimated standard error:</p>
<p><img src="https://latex.codecogs.com/png.latex?SE(%5Chat%7B%5Cdelta%7D)%20=%20%5Csqrt%7B%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2%5Cleft(%5Cfrac%7B1%7D%7Bn_0%7D%20+%20%5Cfrac%7B1%7D%7Bn_1%7D%5Cright)%7D"></p>
<p><strong>The test statistic</strong></p>
<p>We form the test statistic by dividing our estimator by its standard error:</p>
<p><img src="https://latex.codecogs.com/png.latex?t%20=%20%5Cfrac%7B%5Chat%7B%5Cdelta%7D%7D%7BSE(%5Chat%7B%5Cdelta%7D)%7D%20=%20%5Cfrac%7B%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0%7D%7B%5Csqrt%7B%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2%20(1/n_0%20+%201/n_1)%7D%7D"></p>
<p><strong>Sampling distribution</strong></p>
<p>Under the null hypothesis <img src="https://latex.codecogs.com/png.latex?H_0:%20%5Cmu_1%20=%20%5Cmu_0">, this test statistic follows a Student’s t-distribution with <img src="https://latex.codecogs.com/png.latex?n_0%20+%20n_1%20-%202"> degrees of freedom.</p>
<p>Having horrifying flashbacks to your intro to stats class yet? No worries. Let’s look at it from a new perspective.</p>
</section>
<section id="the-regression-approach" class="level3">
<h3 class="anchored" data-anchor-id="the-regression-approach">The Regression Approach</h3>
<p><strong>The data generating process</strong></p>
<p>We can express the exact same data generating process as a linear regression model. Stack all observations into a single length-<img src="https://latex.codecogs.com/png.latex?n"> vector <img src="https://latex.codecogs.com/png.latex?Y"> and create a dummy variable <img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5C%7B0,1%5C%7D"> indexing which group each observation came from:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Y%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20X%20+%20%5Cepsilon%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%5Coverset%7Biid%7D%7B%5Csim%7D%20N(0,%20%5Csigma%5E2)">.</p>
<p>Taking conditional expectations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20E%5BY%7CX=0%5D%20=%20%5Cbeta_0%20=%20%5Cmu_0%20"> <img src="https://latex.codecogs.com/png.latex?%20E%5BY%7CX=1%5D%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20=%20%5Cmu_1%20"></p>
<p>So we can see that <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1%20=%20%5Cmu_1%20-%20%5Cmu_0">, meaning the regression coefficient <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"> directly represents the difference in population means.</p>
<p><strong>The estimator</strong></p>
<p>The ordinary least squares estimator for <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"> is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_1%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(X_i%20-%20%5Cbar%7BX%7D)(Y_i%20-%20%5Cbar%7BY%7D)%7D%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(X_i%20-%20%5Cbar%7BX%7D)%5E2%7D"></p>
<p>For our dummy variable where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%20=%20n_1/(n_0%20+%20n_1)">, after some algebra that you can crank through on your own this simplifies to:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_1%20=%20%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0"></p>
<p>Well look at that—the regression coefficient estimate is exactly the difference in sample means!</p>
<p><strong>Expectation of the estimator</strong></p>
<p>By the properties of OLS under our model assumptions:</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7B%5Cbeta%7D_1%5D%20=%20%5Cbeta_1%20=%20%5Cmu_1%20-%20%5Cmu_0"></p>
<p>So <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_1"> is also an unbiased estimator of the difference in means.</p>
<p><strong>Standard error of the estimator</strong></p>
<p>The standard error formula for an OLS coefficient is:</p>
<p><img src="https://latex.codecogs.com/png.latex?SE(%5Chat%7B%5Cbeta%7D_1)%20=%20%5Csqrt%7B%5Chat%7B%5Csigma%7D%5E2%20%5Ccdot%20%5Cfrac%7B1%7D%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D(X_i%20-%20%5Cbar%7BX%7D)%5E2%7D%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%5E2"> is the residual variance from the regression:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%5E2%20=%20%5Cfrac%7B1%7D%7Bn_0%20+%20n_1%20-%202%7D%5Csum_%7Bi=1%7D%5E%7Bn%7D(Y_i%20-%20%5Chat%7BY%7D_i)%5E2"></p>
<p>For our dummy variable, it turns out that: - The residual variance <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D%5E2"> equals the pooled variance <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2"> - The sum <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5E%7Bn%7D(X_i%20-%20%5Cbar%7BX%7D)%5E2%20=%20%5Cfrac%7Bn_0%20n_1%7D%7Bn_0%20+%20n_1%7D"></p>
<p>Substituting these:</p>
<p><img src="https://latex.codecogs.com/png.latex?SE(%5Chat%7B%5Cbeta%7D_1)%20=%20%5Csqrt%7B%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2%20%5Ccdot%20%5Cfrac%7Bn_0%20+%20n_1%7D%7Bn_0%20n_1%7D%7D%20=%20%5Csqrt%7B%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2%5Cleft(%5Cfrac%7B1%7D%7Bn_0%7D%20+%20%5Cfrac%7B1%7D%7Bn_1%7D%5Cright)%7D"></p>
<p>This is exactly the same standard error we got from the classical approach.</p>
<p><strong>The test statistic</strong></p>
<p>We form the test statistic by dividing our coefficient estimate by its standard error:</p>
<p><img src="https://latex.codecogs.com/png.latex?t%20=%20%5Cfrac%7B%5Chat%7B%5Cbeta%7D_1%7D%7BSE(%5Chat%7B%5Cbeta%7D_1)%7D%20=%20%5Cfrac%7B%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0%7D%7B%5Csqrt%7B%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2%20(1/n_0%20+%201/n_1)%7D%7D"></p>
<p><strong>Sampling distribution</strong></p>
<p>Under the null hypothesis <img src="https://latex.codecogs.com/png.latex?H_0:%20%5Cbeta_1%20=%200">, this test statistic follows a Student’s t-distribution with <img src="https://latex.codecogs.com/png.latex?n_0%20+%20n_1%20-%202"> degrees of freedom (the residual degrees of freedom from the regression).</p>
</section>
<section id="the-punchline" class="level3">
<h3 class="anchored" data-anchor-id="the-punchline">The Punchline</h3>
<p>See what just happened? The two approaches give us:</p>
<ul>
<li>The same point estimate: <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cdelta%7D%20=%20%5Chat%7B%5Cbeta%7D_1%20=%20%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0"></li>
<li>The same standard error: <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7B%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2(1/n_0%20+%201/n_1)%7D"></li>
<li>The same test statistic: <img src="https://latex.codecogs.com/png.latex?t%20=%20%5Cfrac%7B%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0%7D%7B%5Csqrt%7B%5Chat%7B%5Csigma%7D_%7B%5Ctext%7Bpooled%7D%7D%5E2%20(1/n_0%20+%201/n_1)%7D%7D"></li>
<li>The same sampling distribution: <img src="https://latex.codecogs.com/png.latex?t_%7Bn_0+n_1-2%7D"></li>
<li>Therefore, the same p-value</li>
</ul>
<p>In other words these approaches are mathematically equivalent.</p>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p>Let’s simulate some data and implement both testing approaches.</p>
<div id="4230263f" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulate data</span></span>
<span id="cb1-6">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb1-7">n0, n1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span></span>
<span id="cb1-8">mu0, mu1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span></span>
<span id="cb1-9">sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-10">group0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(mu0, sigma, n0)</span>
<span id="cb1-11">group1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(mu1, sigma, n1)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Traditional t-test</span></span>
<span id="cb1-14">t_stat, p_val_ttest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stats.ttest_ind(group1, group0, equal_var<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Regression approach</span></span>
<span id="cb1-17">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate([group0, group1])</span>
<span id="cb1-18">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate([np.zeros(n0), np.ones(n1)])</span>
<span id="cb1-19">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.add_constant(x)</span>
<span id="cb1-20">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.OLS(y, X).fit()</span>
<span id="cb1-21"></span>
<span id="cb1-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compare</span></span>
<span id="cb1-23"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"t-test statistic: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>t_stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-24"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Regression t-stat for β₁: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>tvalues[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-25"></span>
<span id="cb1-26"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">t-test p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p_val_ttest<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb1-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Regression p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pvalues[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>t-test statistic: 3.258749
Regression t-stat for β₁: 3.258749

t-test p-value: 0.002190
Regression p-value: 0.002190</code></pre>
</div>
</div>
<p>As promised, the two-sample equal-variance t-test yields identical results to a linear regression with a dummy variable.</p>
</section>
</section>
<section id="one-way-anova" class="level2">
<h2 class="anchored" data-anchor-id="one-way-anova">One-Way ANOVA</h2>
<p>The two-sample t-test generalizes naturally to comparing means across more than two groups—that’s one-way ANOVA. The classical ANOVA asks: “Are the means of <img src="https://latex.codecogs.com/png.latex?k"> groups all equal, or does at least one differ from the others?”</p>
<p>From a regression perspective, this is just a linear model with a categorical predictor that has <img src="https://latex.codecogs.com/png.latex?k"> levels. We create <img src="https://latex.codecogs.com/png.latex?k-1"> dummy variables (leaving one group as the reference), and the F-test from ANOVA is equivalent to testing whether all the dummy variable coefficients are simultaneously zero.</p>
<p>Let’s see this in action. We’ll simulate data from three groups with different means and compare the classical ANOVA F-test to the F-test from a linear regression.</p>
<div id="67dffc65" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb3-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ols</span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulate three groups with different means</span></span>
<span id="cb3-8">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb3-9">n_per_group <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span></span>
<span id="cb3-10">group_a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, n_per_group)</span>
<span id="cb3-11">group_b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, n_per_group)</span>
<span id="cb3-12">group_c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, n_per_group)</span>
<span id="cb3-13"></span>
<span id="cb3-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create dataframe</span></span>
<span id="cb3-15">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({</span>
<span id="cb3-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value'</span>: np.concatenate([group_a, group_b, group_c]),</span>
<span id="cb3-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'group'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_per_group <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_per_group <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_per_group</span>
<span id="cb3-18">})</span>
<span id="cb3-19"></span>
<span id="cb3-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Classical one-way ANOVA</span></span>
<span id="cb3-21">f_stat_anova, p_val_anova <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stats.f_oneway(group_a, group_b, group_c)</span>
<span id="cb3-22"></span>
<span id="cb3-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Regression approach</span></span>
<span id="cb3-24">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'value ~ C(group)'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df).fit()</span>
<span id="cb3-25">anova_table <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sm.stats.anova_lm(model, typ<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb3-26"></span>
<span id="cb3-27"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Classical ANOVA:"</span>)</span>
<span id="cb3-28"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  F-statistic: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>f_stat_anova<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-29"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p_val_anova<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-30"></span>
<span id="cb3-31"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Regression ANOVA:"</span>)</span>
<span id="cb3-32"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  F-statistic: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>anova_table<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>loc[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C(group)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'F'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb3-33"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>anova_table<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>loc[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'C(group)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PR(&gt;F)'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classical ANOVA:
  F-statistic: 5.145429
  p-value: 0.008824

Regression ANOVA:
  F-statistic: 5.145429
  p-value: 0.008824</code></pre>
</div>
</div>
<p>Nice! The F-statistics and p-values match perfectly. The ANOVA is testing whether the group coefficients in the regression are all zero—which is exactly what the classical ANOVA F-test does.</p>
</section>
<section id="chi-squared-test-of-independence" class="level2">
<h2 class="anchored" data-anchor-id="chi-squared-test-of-independence">Chi-Squared Test of Independence</h2>
<p>The chi-squared test asks whether two categorical variables are independent. The classic example: is group (A vs.&nbsp;B) independent of outcome (success vs.&nbsp;failure)? If you arrange the counts in a 2×2 contingency table, the chi-squared test tells you whether the proportions differ significantly between groups.</p>
<p>From a regression perspective, this is testing whether a binary outcome’s probability depends on a categorical predictor—which is exactly what logistic regression does. For a 2×2 table, we model the log-odds of the outcome as a function of group membership, and testing independence is equivalent to testing whether the logistic regression coefficient equals zero.</p>
<blockquote class="blockquote">
<p>Note: unlike linear regression where we can use t-tests on coefficients to determine if a predictor matters, logistic regression and other GLMs typically use likelihood ratio tests to compare the full model with one that has the predictor of interest dropped.</p>
</blockquote>
<p>Let’s simulate some binary outcome data for two groups and compare the classical chi-squared test to a logistic regression.</p>
<div id="e6e21473" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb5-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb5-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> stats</span>
<span id="cb5-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb5-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> logit</span>
<span id="cb5-6"></span>
<span id="cb5-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulate binary outcomes for two groups</span></span>
<span id="cb5-8">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb5-9">n_a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb5-10">n_b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb5-11"></span>
<span id="cb5-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Group A: 30% success rate</span></span>
<span id="cb5-13">group_a_outcomes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.binomial(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.30</span>, n_a)</span>
<span id="cb5-14"></span>
<span id="cb5-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Group B: 50% success rate</span></span>
<span id="cb5-16">group_b_outcomes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.binomial(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.50</span>, n_b)</span>
<span id="cb5-17"></span>
<span id="cb5-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create contingency table</span></span>
<span id="cb5-19">contingency_table <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.crosstab(</span>
<span id="cb5-20">    index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.Series([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_b, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'group'</span>),</span>
<span id="cb5-21">    columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.Series(np.concatenate([group_a_outcomes, group_b_outcomes]), name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'outcome'</span>)</span>
<span id="cb5-22">)</span>
<span id="cb5-23"></span>
<span id="cb5-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Classical chi-squared test</span></span>
<span id="cb5-25">chi2_stat, p_val_chi2, dof, expected <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stats.chi2_contingency(contingency_table)</span>
<span id="cb5-26"></span>
<span id="cb5-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Logistic regression approach</span></span>
<span id="cb5-28">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({</span>
<span id="cb5-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'outcome'</span>: np.concatenate([group_a_outcomes, group_b_outcomes]),</span>
<span id="cb5-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'group'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_b</span>
<span id="cb5-31">})</span>
<span id="cb5-32"></span>
<span id="cb5-33">logit_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logit(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'outcome ~ C(group)'</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df).fit(disp<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb5-34"></span>
<span id="cb5-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the likelihood ratio test statistic (comparable to chi-squared)</span></span>
<span id="cb5-36">lr_stat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logit_model.llr  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Likelihood ratio test statistic</span></span>
<span id="cb5-37">p_val_lr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logit_model.llr_pvalue</span>
<span id="cb5-38"></span>
<span id="cb5-39"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Classical Chi-Squared Test:"</span>)</span>
<span id="cb5-40"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  χ² statistic: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>chi2_stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-41"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p_val_chi2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-42"></span>
<span id="cb5-43"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Logistic Regression (Likelihood Ratio Test):"</span>)</span>
<span id="cb5-44"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  LR χ² statistic: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lr_stat<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-45"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"  p-value: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>p_val_lr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.6f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Classical Chi-Squared Test:
  χ² statistic: 8.299616
  p-value: 0.003965

Logistic Regression (Likelihood Ratio Test):
  LR χ² statistic: 9.232498
  p-value: 0.002378</code></pre>
</div>
</div>
<p>Whoa, this time we get slightly different test statistics and p-values; what’s happening here?</p>
<p>The classical Pearson chi-squared test and the likelihood ratio test from logistic regression are actually <em>different test statistics</em> testing the same hypothesis. Both are asymptotically chi-squared distributed under the null, but they use different formulas and give different values for finite samples. The p-values are close and lead to the same conclusion—both are testing whether group membership and outcome are independent.</p>
<p>There are many valid ways to answer questions using data, and appropriate approaches will tend to agree with each other.</p>
</section>
<section id="advantages-of-a-model-based-testing-approach" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-a-model-based-testing-approach">Advantages of a Model-Based Testing Approach</h2>
<p>We’ve seen that classical tests like the t-test, ANOVA, and chi-squared are special cases of regression models. But so what? Why bother with this reformulation when the canned tests work just fine?</p>
<p>The model-based perspective offers some real practical advantages that become apparent once you start working with messier, real-world data. Let’s look at two key benefits.</p>
<section id="clear-and-explicit-assumptions" class="level3">
<h3 class="anchored" data-anchor-id="clear-and-explicit-assumptions">Clear and Explicit Assumptions</h3>
<p>In the model-based approach all key assumptions are expressed compactly in the model specification itself. Here’s the model we wrote down for comparing group means:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Y%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20X%20+%20%5Cepsilon%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cepsilon%20%5Coverset%7Biid%7D%7B%5Csim%7D%20N(0,%20%5Csigma%5E2)">.</p>
<p>From this model spec, we can immediately see the assumptions being made:</p>
<ul>
<li>The outcome is assumed to be normally distributed</li>
<li>The variance of the outcome is assumed to be equal in the two groups (homoscedasticity)</li>
<li>The samples in each group are assumed to be independent and identically distributed</li>
</ul>
<p>We don’t have to memorize separate lists of assumptions for each test in the statistical zoo. We can simply read the model specification and see exactly what we’re assuming about the data generating process. And if one of these assumptions seems questionable for our context, we know exactly which part of the model to modify.</p>
</section>
<section id="extension-to-covariates" class="level3">
<h3 class="anchored" data-anchor-id="extension-to-covariates">Extension to Covariates</h3>
<p>This model-based approach makes it straightforward to include covariates in the analysis. Suppose we’re comparing means between two groups, but we also have a continuous covariate (like age, tenure, marketing spend, baseline measurement, etc.) that might affect the outcome.</p>
<p>In the classical testing framework, we’d need to reach for a different test—maybe ANCOVA—and navigate a new set of conditions and assumptions. But in the regression framework, we simply add another term to our model:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Y%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20X_%7B%5Ctext%7Bgroup%7D%7D%20+%20%5Cbeta_2%20X_%7B%5Ctext%7Bcovariate%7D%7D%20+%20%5Cepsilon%20"></p>
<p>Now <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"> represents the group difference <em>adjusted for</em> the covariate. We’re still testing <img src="https://latex.codecogs.com/png.latex?H_0:%20%5Cbeta_1%20=%200">, just as before. The logic is identical; we’ve just extended the model.</p>
<p>The same logic applies for testing interactions between group membership and covariates.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Y%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20X_%7B%5Ctext%7Bgroup%7D%7D%20+%20%5Cbeta_2%20X_%7B%5Ctext%7Bcovariate%7D%7D%20+%20%5Cbeta_3%20X_%7B%5Ctext%7Bgroup%7D%7D%20%5Ccdot%20X_%7B%5Ctext%7Bcovariate%7D%7D%20+%20%5Cepsilon%20"></p>
<p>I don’t even know what canned stat test lets you test interactions like that, but luckily it doesn’t matter. The model-based approach makes these extensions feel like natural elaborations rather than jumps to entirely different procedures. We’re reasoning from first principles about how we think the data were generated, rather than searching through flowcharts for the “right” test.</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>We’ve just scratched the surface of this simple idea that we can take a model-based approach to inference in many cases where the default might be to use a traditional canned statistical procedure. Doing so has many benefits, including making assumptions clear, extensibility to more complex scenarios with other covariates and interactions, and naturally enabling us to use Bayesian inference to do inference on the model parameters. Try it out; next time you reach for an off-the-shelf statistical test, see if you can take a model-based approach instead.</p>
<p>Happy modeling!</p>
</section>

 ]]></description>
  <category>statistics</category>
  <guid>https://randomrealizations.com/posts/model-based-stat-testing/</guid>
  <pubDate>Mon, 19 Jan 2026 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/enso-thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Logistic Regression with PyTorch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/logistic-regression-with-pytorch/</link>
  <description><![CDATA[ 




<!-- Well, dear reader, while I know you've grown to appreciate the meticulous craftsmanship of the typical Random Realizations article, I've been curious lately whether we might both benefit from a more rapid publication cadence with more incremental and perhaps slightly less polished posts.
So today we'll give it a try, and you can let me know how it's working for you. -->
<p>Note from December 2025: Well dear reader, it looks like I wrote this post back in July and forgot to publish it, so here’s my early Christmas present to you. Enjoy!</p>
<p>In this post we’ll bridge the gap between traditional ML and deep learning by showing that logistic regression is a special case of a neural network, and we’ll compare the classic scikit-learn logistic regression to a neural network implementation that we’ll build in PyTorch. Then we’ll add some hidden layers to our PyTorch model to go from logistic regression to the multi-layer perceptron, a simple deep neural network that’s like the major scale of deep learning model architectures.</p>
<!-- ![Some MNIST Digits](digits.png "") -->
<section id="multiclass-logistic-regressiontraditional-ml-vs-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="multiclass-logistic-regressiontraditional-ml-vs-neural-network">Multiclass Logistic Regression—Traditional ML vs Neural Network</h2>
<p>We want to classify <img src="https://latex.codecogs.com/png.latex?N"> instances, each a <img src="https://latex.codecogs.com/png.latex?D"> dimensional input, into one of <img src="https://latex.codecogs.com/png.latex?K"> discrete classes by predicting the probability mass function over the <img src="https://latex.codecogs.com/png.latex?K"> classes. In matrix notation, the classical ML model is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20z%20=%20%20X%20W%5ET%20+%20b%20"> <img src="https://latex.codecogs.com/png.latex?%20%5Chat%7Bp%7D%20=%20%5Ctext%7Bsoftmax%7D(z)%20=%20%5Cfrac%7B%5Cexp%20(z)%7D%20%7B%5Csum_%7Bk=1%7D%5EK%20%5Cexp%20(z_k)%7D"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20D%7D"> is the <img src="https://latex.codecogs.com/png.latex?D"> dimensional input data for each instance</li>
<li><img src="https://latex.codecogs.com/png.latex?W%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BK%20%5Ctimes%20D%7D"> is the coefficient matrix (<img src="https://latex.codecogs.com/png.latex?D"> coefficients for each class)</li>
<li><img src="https://latex.codecogs.com/png.latex?b%20%5Cin%20%5Cmathbb%7BR%7D%5EK"> is the intercept for each class</li>
<li><img src="https://latex.codecogs.com/png.latex?z%20%5Cin%20%5Cmathbb%7BR%7D%5E%7BN%20%5Ctimes%20K%7D"> are the <img src="https://latex.codecogs.com/png.latex?K"> raw logits or linear scores for each instance</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsoftmax%7D(%5Ccdot):%5Cmathbb%7BR%7D%5EK%5Crightarrow(0,1)%5EK"> is applied to each instance to transform the logits in <img src="https://latex.codecogs.com/png.latex?(-%5Cinfty,%20%5Cinfty)"> to probabilities in <img src="https://latex.codecogs.com/png.latex?(0,1)">.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bp%7D%20%5Cin%20(0,1)%5E%7BN%20%5Ctimes%20K%7D"> are the <img src="https://latex.codecogs.com/png.latex?K"> class probabilities predicted for each instance</li>
</ul>
<p>In neural network terms we can express the above formulation as a network with</p>
<ul>
<li>Input layer: <img src="https://latex.codecogs.com/png.latex?X"></li>
<li>Linear layer: <img src="https://latex.codecogs.com/png.latex?z=XW%5ET+b"></li>
<li>Non-linear activation: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7Bsoftmax%7D(z)"></li>
</ul>
<p>In both cases, model training is guided by a negative log likelihood loss function.</p>
<blockquote class="blockquote">
<p>FYI these formulations are also closely related to multi-class gradient boosting, which We talked about back in the <a href="../../posts/gradient-boosting-multi-class-classification-from-scratch/">gradient boosting for multi-class classification from scratch</a> post. You can go back and reread that post for some additional intuition on how multi-class classification works.</p>
</blockquote>
<p>Let’s implement logistic regression as a traditional ML model and as a neural network.</p>
</section>
<section id="mnist-data" class="level2">
<h2 class="anchored" data-anchor-id="mnist-data">MNIST Data</h2>
<p>We’ll train our logistic regression models to classify the handwritten digits in the classic <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>. Adapting this <a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html">scikit-learn example</a>, we’ll load up the data, plot some of the digits, normalize the input images, and then fit a classical logistic regression model.</p>
<div id="50537e0a" class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> fetch_openml</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LogisticRegression</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.utils <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> check_random_state</span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load data from https://www.openml.org/d/554</span></span>
<span id="cb1-11">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fetch_openml(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mnist_784"</span>, version<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, return_X_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Shuffle the data</span></span>
<span id="cb1-14">random_state <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> check_random_state(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-15">permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random_state.permutation(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb1-16">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X[permutation]</span>
<span id="cb1-17">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y[permutation]</span>
<span id="cb1-18">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.reshape((X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># train test split</span></span>
<span id="cb1-21">train_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span></span>
<span id="cb1-22">test_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span></span>
<span id="cb1-23">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(</span>
<span id="cb1-24">    X, y, train_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_samples, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_samples</span>
<span id="cb1-25">)</span>
<span id="cb1-26"></span>
<span id="cb1-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Normalize image data</span></span>
<span id="cb1-28">scaler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb1-29">X_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.fit_transform(X_train)</span>
<span id="cb1-30">X_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler.transform(X_test)</span></code></pre></div>
</div>
<div id="bde8a220" class="cell" data-execution_count="152">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Number of classes: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(np.unique(y))<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shape of X: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>X<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of classes: 10
Shape of X: (70000, 784)</code></pre>
</div>
</div>
<p>We have <img src="https://latex.codecogs.com/png.latex?K=10"> classes corresponding to the digits 0-9. The image data in <code>X</code> is stored as a <img src="https://latex.codecogs.com/png.latex?N%20%5Ctimes%20D"> array with <img src="https://latex.codecogs.com/png.latex?N=70000"> images and each image having <img src="https://latex.codecogs.com/png.latex?D=784"> pixels. In this raw form, the images are flattened out into a single dimension, which is ideal for modeling. To visualize them, we’ll need to reshape each image from <img src="https://latex.codecogs.com/png.latex?1%20%5Ctimes%20784"> to <img src="https://latex.codecogs.com/png.latex?28%20%5Ctimes%2028">.</p>
<div id="9c38bb45" class="cell">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_digits(X, n_rows, n_cols):</span>
<span id="cb4-2">    X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb4-3">    n_images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n_rows <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> n_cols</span>
<span id="cb4-4">    fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_cols, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_rows))</span>
<span id="cb4-5"></span>
<span id="cb4-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Flatten axs to iterate easily</span></span>
<span id="cb4-7">    axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs.flatten()</span>
<span id="cb4-8"></span>
<span id="cb4-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_images):</span>
<span id="cb4-10">        axs[i].imshow(X[i], cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gray"</span>)</span>
<span id="cb4-11">        axs[i].axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"off"</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># turn off axes completely</span></span>
<span id="cb4-12"></span>
<span id="cb4-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove all spacing between plots</span></span>
<span id="cb4-14">    plt.subplots_adjust(wspace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, hspace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb4-15">    plt.tight_layout(pad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb4-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># return fig, axs</span></span>
<span id="cb4-17"></span>
<span id="cb4-18">plot_digits(X, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># post image</span></span>
<span id="cb4-19"></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/logistic-regression-with-pytorch/nn_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img" alt="grid of digit images"></p>
<figcaption>Some instances from the MNIST hand-written digit dataset</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="logistic-regression-with-scikit-learn" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-with-scikit-learn">Logistic Regression with scikit-learn</h2>
<p>We’ll start with the traditional logistic regression model implementation from scikit-learn.</p>
<div id="25a08d16" class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Classical Logistic Regression</span></span>
<span id="cb5-2"></span>
<span id="cb5-3">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogisticRegression(penalty<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, solver<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sag"</span>, tol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>) </span>
<span id="cb5-4">clf.fit(X_train, y_train)</span>
<span id="cb5-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Test Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> clf<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>score(X_test, y_test)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test Accuracy: 89.39%</code></pre>
</div>
</div>
<p>Now to visualize what this model is doing, let’s have a look at its coefficients.</p>
<div id="38d06056" class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">clf.coef_.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="95">
<pre><code>(10, 784)</code></pre>
</div>
</div>
<p>Recall the coefficients <img src="https://latex.codecogs.com/png.latex?W"> are in a <img src="https://latex.codecogs.com/png.latex?K%20%5Ctimes%20D"> array, so each of the <img src="https://latex.codecogs.com/png.latex?K"> rows contains the <img src="https://latex.codecogs.com/png.latex?D"> coefficients for the corresponding class. Let’s reshape each of the rows into a 28 by 28 image and plot them.</p>
<div id="513712bd" class="cell" data-execution_count="96">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_class_weights(weights: np.ndarray, title: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Classification Weights"</span>):</span>
<span id="cb9-2">    num_classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> weights.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb9-3">    n_rows, n_cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, (num_classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb9-4"></span>
<span id="cb9-5">    fig, axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span>
<span id="cb9-6">    scale <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">abs</span>(weights).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>()</span>
<span id="cb9-7"></span>
<span id="cb9-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set up shared color scale</span></span>
<span id="cb9-9">    cmap <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.cm.viridis</span>
<span id="cb9-10">    norm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.Normalize(vmin<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span>scale, vmax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scale)</span>
<span id="cb9-11"></span>
<span id="cb9-12">    axs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs.flatten()</span>
<span id="cb9-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_classes):</span>
<span id="cb9-14">        ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axs[i]</span>
<span id="cb9-15">        im <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ax.imshow(</span>
<span id="cb9-16">            weights[i].reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>),</span>
<span id="cb9-17">            interpolation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nearest"</span>,</span>
<span id="cb9-18">            cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cmap,</span>
<span id="cb9-19">            norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>norm</span>
<span id="cb9-20">        )</span>
<span id="cb9-21">        ax.set_xticks(())</span>
<span id="cb9-22">        ax.set_yticks(())</span>
<span id="cb9-23">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ax.set_xlabel(f"Class {i}")</span></span>
<span id="cb9-24">        ax.set_xlabel(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Class </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>, labelpad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb9-25">        ax.xaxis.set_label_position(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'top'</span>)</span>
<span id="cb9-26"></span>
<span id="cb9-27">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_classes, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(axs)):</span>
<span id="cb9-28">        axs[j].axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'off'</span>)</span>
<span id="cb9-29"></span>
<span id="cb9-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add colorbar BELOW the entire figure</span></span>
<span id="cb9-31">    cbar_ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> fig.add_axes([<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.03</span>])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [left, bottom, width, height]</span></span>
<span id="cb9-32">    fig.colorbar(</span>
<span id="cb9-33">        plt.cm.ScalarMappable(norm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>norm, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cmap),</span>
<span id="cb9-34">        cax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cbar_ax,</span>
<span id="cb9-35">        orientation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"horizontal"</span></span>
<span id="cb9-36">    ).set_label(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"value"</span>)</span>
<span id="cb9-37"></span>
<span id="cb9-38">    fig.suptitle(title)</span>
<span id="cb9-39">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Make space for suptitle</span></span>
<span id="cb9-40">    plt.show()</span></code></pre></div>
</details>
</div>
<div id="d801fd5e" class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">plot_class_weights(clf.coef_, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Scikit-learn Logistic Regression Coefficients"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/logistic-regression-with-pytorch/nn_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img" alt="Traditional logistic regression coefficients for each class"></p>
<figcaption>Traditional logistic regression coefficients for each class</figcaption>
</figure>
</div>
</div>
</div>
<p>Intuitively, for a given digit, we’d expect the coefficients to be positive on pixels where the digit is typically located, and we’d expect the coefficients to be zero or perhaps even negative on pixels where other digits tend to be located. We can see that’s more or less what this model is doing.</p>
</section>
<section id="logistic-regression-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-in-pytorch">Logistic Regression in PyTorch</h2>
<p>I recommend checking out the PyTorch <a href="https://docs.pytorch.org/tutorials/beginner/basics/intro.html">Basic Tutorial</a> to get started with the library’s API. Our basic flow for creating PyTorch models will look like</p>
<ol type="1">
<li>Create <code>Dataset</code> and <code>DataLoader</code> objects.</li>
<li>Build the model.</li>
<li>Train the model.</li>
</ol>
<section id="create-pytorch-dataset-and-dataloader-objects" class="level3">
<h3 class="anchored" data-anchor-id="create-pytorch-dataset-and-dataloader-objects">Create PyTorch <code>Dataset</code> and <code>DataLoader</code> Objects</h3>
<div id="b50a8d9a" class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.utils.data <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TensorDataset, DataLoader</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.optim <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> optim</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Convert to torch tensors</span></span>
<span id="cb11-7">X_train_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(X_train, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb11-8">y_train_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(y_train.astype(np.int64), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">long</span>)</span>
<span id="cb11-9">X_test_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(X_test, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb11-10">y_test_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(y_test.astype(np.int64), dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">long</span>)</span>
<span id="cb11-11"></span>
<span id="cb11-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Wrap in TensorDataset</span></span>
<span id="cb11-13">train_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TensorDataset(X_train_tensor, y_train_tensor)</span>
<span id="cb11-14">test_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TensorDataset(X_test_tensor, y_test_tensor)</span>
<span id="cb11-15"></span>
<span id="cb11-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create DataLoaders</span></span>
<span id="cb11-17">batch_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span></span>
<span id="cb11-18">train_loader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(train_dataset, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batch_size, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb11-19">test_loader <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataLoader(test_dataset, batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>batch_size, shuffle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
</section>
<section id="build-the-model" class="level3">
<h3 class="anchored" data-anchor-id="build-the-model">Build the Model</h3>
<div id="98b1afca" class="cell" data-execution_count="128">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Logistic Regression Model</span></span>
<span id="cb12-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> LogReg(nn.Module):</span>
<span id="cb12-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb12-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb12-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">784</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb12-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.activation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.LogSoftmax(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># returns log probabilities</span></span>
<span id="cb12-7"></span>
<span id="cb12-8">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb12-9">        z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.linear(x)</span>
<span id="cb12-10">        p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.activation(z)</span>
<span id="cb12-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> p</span>
<span id="cb12-12"></span>
<span id="cb12-13">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LogReg()</span></code></pre></div>
</div>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">Train the Model</h3>
<div id="4dc0164b" class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hyperparameters</span></span>
<span id="cb13-2">num_epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb13-3">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span></span>
<span id="cb13-4"></span>
<span id="cb13-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Loss</span></span>
<span id="cb13-6">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.NLLLoss() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># expects log probabilities</span></span>
<span id="cb13-7"></span>
<span id="cb13-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Optimizer</span></span>
<span id="cb13-9">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.SGD(model.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learning_rate)</span>
<span id="cb13-10"></span>
<span id="cb13-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training loop</span></span>
<span id="cb13-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_epochs):</span>
<span id="cb13-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> train_loader:</span>
<span id="cb13-14">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Flatten</span></span>
<span id="cb13-15">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(images)</span>
<span id="cb13-16">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> criterion(outputs, labels)</span>
<span id="cb13-17"></span>
<span id="cb13-18">        optimizer.zero_grad()</span>
<span id="cb13-19">        loss.backward()</span>
<span id="cb13-20">        optimizer.step()</span>
<span id="cb13-21"></span>
<span id="cb13-22">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Epoch [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epoch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_epochs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">], Training Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/5], Training Loss: 0.4151
Epoch [2/5], Training Loss: 0.1858
Epoch [3/5], Training Loss: 0.1102
Epoch [4/5], Training Loss: 0.5600
Epoch [5/5], Training Loss: 0.4414</code></pre>
</div>
</div>
</section>
<section id="evaluate-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluate-the-model">Evaluate the Model</h3>
<div id="5c79af15" class="cell" data-execution_count="130">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluate model</span></span>
<span id="cb15-2">model.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb15-3">correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb15-4">total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb15-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb15-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> test_loader:</span>
<span id="cb15-7">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb15-8">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(images)</span>
<span id="cb15-9">        _, predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(outputs.data, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb15-10">        total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> labels.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb15-11">        correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> (predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> labels).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>().item()</span>
<span id="cb15-12"></span>
<span id="cb15-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Test Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> total<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test Accuracy: 90.71%</code></pre>
</div>
</div>
</section>
<section id="pytorch-model-weights" class="level3">
<h3 class="anchored" data-anchor-id="pytorch-model-weights">PyTorch Model Weights</h3>
<p>Let’s take a look at the weights.</p>
<div id="2b787d80" class="cell" data-execution_count="131">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">weights <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.linear.weight.detach().numpy()</span>
<span id="cb17-2">weights.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="131">
<pre><code>(10, 784)</code></pre>
</div>
</div>
<div id="79baf6df" class="cell" data-execution_count="132">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">plot_class_weights(weights, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PyTorch Logistic Regression Weights"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/logistic-regression-with-pytorch/nn_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="PyTorch logistic regression coefficients for each class"></p>
<figcaption>PyTorch logistic regression coefficients for each class</figcaption>
</figure>
</div>
</div>
</div>
<p>Nice! We can see that the weights from the neural network are qualitatively similar to the coefficients from the logistic regression model. Interestingly, the neural network weight patterns look a bit more noisy than the classical logistic regression coefficients, and yet, the models are performing similarly on the test data (89-90%). Likely there are a lot of logistic regression parameter solutions that yield similar performance, and these two models have found slightly different solutions in parameter space.</p>
</section>
</section>
<section id="multilayer-perceptron-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="multilayer-perceptron-in-pytorch">Multilayer Perceptron in PyTorch</h2>
<p>Well, logistic regression is great and all, but of course, the reason for messing around with PyTorch is so that we can start building more interesting neural network architectures. The next obvious step is to build out a <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a> (MLP). The MLP is a network with</p>
<ul>
<li>An input layer</li>
<li>One or more hidden layers comprised of a linear transformation passed to a non-linear activation function</li>
<li>An output layer, e.g.&nbsp;returning class probabilities</li>
</ul>
<p>When we add “hidden” layers between the input and output layers, the network earns the modifier “deep”, meaning that MLP’s are deep networks. We’ll build a model with two hidden layers that uses <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear unit activation functions</a> (ReLU). The ReLU just replaces negative inputs with zero and passes positive inputs through unchanged—a very simple form of non-linearity.</p>
<div id="a6038b9e" class="cell" data-execution_count="153">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MLP(nn.Module):</span>
<span id="cb20-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb20-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb20-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb20-5">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">784</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>),   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Input layer</span></span>
<span id="cb20-6">            nn.ReLU(),</span>
<span id="cb20-7">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>),   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hidden layer</span></span>
<span id="cb20-8">            nn.ReLU(),</span>
<span id="cb20-9">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb20-10">            nn.LogSoftmax(dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)       <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># output layer - log probabilities</span></span>
<span id="cb20-11">        )</span>
<span id="cb20-12"></span>
<span id="cb20-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb20-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net(x)</span>
<span id="cb20-15"></span>
<span id="cb20-16">mlp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MLP()</span></code></pre></div>
</div>
<div id="1a69b7d4" class="cell" data-execution_count="154">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hyperparameters</span></span>
<span id="cb21-2">num_epochs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span></span>
<span id="cb21-3">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span></span>
<span id="cb21-4"></span>
<span id="cb21-5">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.Adam(mlp.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learning_rate)</span>
<span id="cb21-6">criterion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.NLLLoss() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># expects log probability</span></span>
<span id="cb21-7"></span>
<span id="cb21-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Training loop</span></span>
<span id="cb21-9">mlp.train()</span>
<span id="cb21-10"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(num_epochs):</span>
<span id="cb21-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> train_loader:</span>
<span id="cb21-12">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Flatten</span></span>
<span id="cb21-13">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mlp(images)</span>
<span id="cb21-14">        loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> criterion(outputs, labels)</span>
<span id="cb21-15"></span>
<span id="cb21-16">        optimizer.zero_grad()</span>
<span id="cb21-17">        loss.backward()</span>
<span id="cb21-18">        optimizer.step()</span>
<span id="cb21-19"></span>
<span id="cb21-20">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Epoch [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epoch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>num_epochs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">], Training Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/20], Training Loss: 0.4061
Epoch [2/20], Training Loss: 0.0821
Epoch [3/20], Training Loss: 0.0214
Epoch [4/20], Training Loss: 0.0585
Epoch [5/20], Training Loss: 0.0027
Epoch [6/20], Training Loss: 0.0086
Epoch [7/20], Training Loss: 0.0005
Epoch [8/20], Training Loss: 0.0018
Epoch [9/20], Training Loss: 0.0016
Epoch [10/20], Training Loss: 0.0809
Epoch [11/20], Training Loss: 0.0024
Epoch [12/20], Training Loss: 0.0000
Epoch [13/20], Training Loss: 0.0002
Epoch [14/20], Training Loss: 0.0001
Epoch [15/20], Training Loss: 0.0001
Epoch [16/20], Training Loss: 0.0004
Epoch [17/20], Training Loss: 0.0015
Epoch [18/20], Training Loss: 0.0011
Epoch [19/20], Training Loss: 0.0004
Epoch [20/20], Training Loss: 0.0000</code></pre>
</div>
</div>
<div id="1bc26754" class="cell" data-execution_count="155">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Evaluate model</span></span>
<span id="cb23-2">mlp.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb23-3">correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb23-4">total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb23-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb23-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> images, labels <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> test_loader:</span>
<span id="cb23-7">        images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> images.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">28</span>)</span>
<span id="cb23-8">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mlp(images)</span>
<span id="cb23-9">        _, predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(outputs.data, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb23-10">        total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> labels.size(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb23-11">        correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> (predicted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> labels).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>().item()</span>
<span id="cb23-12"></span>
<span id="cb23-13"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Test Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> total<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test Accuracy: 95.83%</code></pre>
</div>
</div>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Nice! In this post we’ve bridged the gap between classic ML and deep learning by showing that logistic regression is a special case of a neural network, building a logistic regression model in PyTorch, and then adding hidden layers to that model to obtain a multilayer perceptron.</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>pytorch</category>
  <guid>https://randomrealizations.com/posts/logistic-regression-with-pytorch/</guid>
  <pubDate>Fri, 04 Jul 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Bayesian Modeling Primer</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/bayesian-modeling-primer/</link>
  <description><![CDATA[ 




<p>Well, dear reader, I know I haven’t been posting very much lately. That’s because I’ve been busy moving to a new city and working a new DS gig and learning some new things, including Bayesian modeling. In particular I’ve been reading Richard McElreath’s excellent book <a href="https://xcelab.net/rm/">Statistical Rethinking</a>, which I recommend to you as well. As a dedicated reader of this blog, I’m sure you’re perfectly capable of digesting a 600 page statistics textbook on your own, but just for fun, today I present to you my Bayesian statistics crash course.</p>
<p>My primary goal is to illuminate the major steps in the Bayesian workflow, that way you have a mental framework where you can store and contextualize new pieces of information as you learn. My secondary goal is to give you an intuitive understanding of Bayesian modeling from two interconnected perspectives: a mathematical formulation based primarily in probability theory and a probabilistic programming approach based on writing code to generate random data. Each perspective supports the other, and they are both necessary to grasp the full picture. I will attempt to weave these two perspectives throughout the description of the workflow, which is motivated by a toy example we’ll use throughout the post.</p>
<p>Let’s do this! ➡️</p>
<section id="the-rock-paper-scissors-pro" class="level2">
<h2 class="anchored" data-anchor-id="the-rock-paper-scissors-pro">🪨📄✂️ The Rock Paper Scissors Pro</h2>
<p>I spent a summer as an intern at RAND Corporation during my PhD. It was a fascinating place full of fascinating characters. One of the researchers, Fritz R, liked to take each cohort of interns out for drinks at some point in the summer. After picking up our first round himself, Fritz offered to buy a second drink for any of the interns who could beat him in a rock paper scissors (RPS) match, warning us that he was “pretty good at it.”</p>
<p>Let’s fact check his claim. We’d like to know something about his actual RPS win rate, but that is unobservable. We can’t observe it directly, but we could observe some match outcomes and make an inference about what his actual win rate might plausibly be.</p>
<p>Let’s say that after facing off with the 10 interns, Fritz racks up the following match outcomes.</p>
<div id="cell-3" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">observed_outcomes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<p>He won 7 out of 10 matches—not bad. But is his performance the result of skill or simply a lucky round? We’re going to address this question using Bayesian statistical analysis.</p>
</section>
<section id="the-bayesian-workflow-in-3-steps" class="level2">
<h2 class="anchored" data-anchor-id="the-bayesian-workflow-in-3-steps">🛠️ The Bayesian Workflow in 3 Steps</h2>
<p>I consider the Bayesian workflow to have 3 major steps:</p>
<ol type="1">
<li><strong>Modeling</strong> - specify the data generating process as a generative model</li>
<li><strong>Inference</strong> - use the model, the observed data, and some inference algorithm to infer the values of unknown model parameters</li>
<li><strong>Interpretation</strong> - summarize and interpret the inferred model parameters to answer your analysis questions</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/modeling-inference-interpretation.png" title="." class="img-fluid figure-img"></p>
<figcaption>The Bayesian Workflow: modeling, inference, interpretation.</figcaption>
</figure>
</div>
</section>
<section id="step-1.-modeling" class="level2">
<h2 class="anchored" data-anchor-id="step-1.-modeling">⚙️ Step 1. Modeling</h2>
<section id="modeling-the-data-generating-process" class="level3">
<h3 class="anchored" data-anchor-id="modeling-the-data-generating-process">Modeling the Data Generating Process</h3>
<p>In this step, we’re going to build a <em>generative model</em>, i.e.&nbsp;a model that can simulate data similar to our observed data. If you’re coming from ML, the key mental shift is to think about modeling the <em>data generating process (DGP)</em>, rather than curve-fitting the data itself. Practically this means our model is a set of random variables which relate to one another in some way and from which we can draw realizations… random realizations, that is. You can invent a DGP as follows:</p>
<ol type="1">
<li>Identify the key variables in the system.</li>
<li>Define each variable as a draw from some probability distribution, or in terms of the other variables.</li>
<li>Use unknown parameters as needed in the probability distributions or in the functional relationships among the key variables.</li>
</ol>
<p>In our RPS example, there is one key variable—Fritz’s match outcome. We can define the match outcome variable as a random draw from some distribution, e.g.&nbsp;a Bernoulli distribution. The Bernoulli distribution has one parameter—the success probability—which corresponds here to Fritz’s actual true win rate. Given some true win rate, we can simulate match outcomes by drawing realizations from the Bernoulli distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20y_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?y_i%20=%200"> if Fritz loses to intern <img src="https://latex.codecogs.com/png.latex?i"> and <img src="https://latex.codecogs.com/png.latex?y_i%20=%201"> if he wins, and <img src="https://latex.codecogs.com/png.latex?i=1,%5Cdots,N"> where <img src="https://latex.codecogs.com/png.latex?N=10">. In this DGP, the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> corresponds to Fritz’s true win rate.</p>
<p>This is a good start, but we can’t simulate data from this model yet because <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> has no particular value. So, what value should we use?</p>
</section>
<section id="probability-as-relative-plausibility" class="level3">
<h3 class="anchored" data-anchor-id="probability-as-relative-plausibility">Probability as Relative Plausibility</h3>
<p>One of the key ideas in Bayesian modeling is that we can represent the relative plausibility of potential values of any unobserved variable using a probability distribution. Highly plausible values get higher probability, and less plausible values get lower probability.</p>
<p><em>It is this view of probability as a measure of relative plausibility that distinguishes Bayesian statistics from Frequentist statistics, which views probability as the relative frequency of events.</em></p>
<p>We don’t know the true value of Fritz’s RPS win rate, but even before collecting any data, we might have some contextual knowledge about how the world works which can provide some prior information about the relative plausibility of its possible values. For me it’s easiest to think in terms of how surprising a given true value would be. I wouldn’t be surprised at all if his win rate was near 0.5, but I would be shocked if it was 0.9 or 0.1, hence 0.5 has higher relative plausibility than 0.9 or 0.1.</p>
<p>Let’s represent the prior relative plausibility of values of Fritz’s RPS win rate with a probability distribution. Below are a few different probability distributions defined over the possible values <img src="https://latex.codecogs.com/png.latex?0%20%5Cle%20%5Ctheta%20%5Cle%201">.</p>
<div id="cell-7" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> scipy.stats <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> beta, bernoulli</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># set colors for later</span></span>
<span id="cb2-7">prior_color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C0"</span></span>
<span id="cb2-8">post_color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"C1"</span></span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># prior beta parameters</span></span>
<span id="cb2-11">parameters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb2-12">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb2-13">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>),</span>
<span id="cb2-14">    (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb2-15">]</span>
<span id="cb2-16"></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># plot the prior</span></span>
<span id="cb2-18">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb2-19">plt.figure()</span>
<span id="cb2-20"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, (alpha, beta_val) <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(parameters):</span>
<span id="cb2-21">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> beta.pdf(x, alpha, beta_val)</span>
<span id="cb2-22">    label <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Beta($</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">alpha$=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, $</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">beta$=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_val<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span></span>
<span id="cb2-23">    plt.plot(x, y, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>label)</span>
<span id="cb2-24"></span>
<span id="cb2-25">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"win rate (theta)"</span>)</span>
<span id="cb2-26">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Probability Density"</span>)</span>
<span id="cb2-27">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Relative Plausibility of RPS Win Rate'</span>)</span>
<span id="cb2-28">plt.legend()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img" alt="beta distribution priors"></p>
</figure>
</div>
</div>
</div>
<p>Each of these PDFs has a mode at <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5"> and decreases toward 0 and 1. They’re all aligned with the relative plausibilities we discussed earlier.</p>
<p>You can check the relative plausibility between two possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> implied by a given pdf by taking the ratio of the height of the pdf at one value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> versus the height at another value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<p>For example, let’s compare <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5"> to <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.7"> for a Beta(10, 10) prior.</p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">beta.pdf(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> beta.pdf(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>np.float64(4.802710683776413)</code></pre>
</div>
</div>
<p>The Beta(10, 10) distribution implies that a 0.5 win rate is about 5 times more plausible than a 0.7 win rate, which sounds, ahem, plausible.</p>
</section>
<section id="priors" class="level3">
<h3 class="anchored" data-anchor-id="priors">Priors</h3>
<p>We can include this prior information about the relative plausibility of values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> in our model as follows.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta%20%5Csim%20%5Ctext%7BBeta%7D(10,%2010)%0A"> <img src="https://latex.codecogs.com/png.latex?%0Ay_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)%0A"></p>
<p>In Bayesian parlance, we call the probability distribution that represents the relative plausibilities of an unobserved parameter its <em>prior distribution</em>, or simply its <em>prior</em>. Notice that with the addition of the prior for <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, our model is now fully generative.</p>
</section>
<section id="implementing-the-generative-model" class="level3">
<h3 class="anchored" data-anchor-id="implementing-the-generative-model">Implementing the generative model</h3>
<p>Let’s implement the DGP using random variables from <code>scipy</code>.</p>
<div id="cell-11" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Implementing the DGP as a generative model</span></span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> draw_from_prior(alpha_param, beta_param):</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> beta.rvs(alpha_param, beta_param)</span>
<span id="cb5-5"></span>
<span id="cb5-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_one_outcome(theta, N):</span>
<span id="cb5-7">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bernoulli.rvs(theta, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb5-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>: theta, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"y"</span>: y, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>: np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(y)}</span>
<span id="cb5-9"></span>
<span id="cb5-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_outcomes(n_outcomes, alpha_param, beta_param, N):</span>
<span id="cb5-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame([</span>
<span id="cb5-12">        simulate_one_outcome(theta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>draw_from_prior(alpha_param, beta_param), N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb5-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_outcomes)</span>
<span id="cb5-14">    ])</span>
<span id="cb5-15"></span>
<span id="cb5-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># set DGP parameters</span></span>
<span id="cb5-17">alpha_param, beta_param <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb5-18">N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb5-19"></span>
<span id="cb5-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># simulate outcomes from the generative model</span></span>
<span id="cb5-21">outcome_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1_000</span>, alpha_param, beta_param, N)</span>
<span id="cb5-22">outcome_df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">theta</th>
<th data-quarto-table-cell-role="th">y</th>
<th data-quarto-table-cell-role="th">sum_y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.451620</td>
<td>[0, 0, 1, 1, 1, 0, 0, 1, 0, 0]</td>
<td>4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.460305</td>
<td>[0, 1, 1, 0, 1, 1, 1, 0, 1, 1]</td>
<td>7</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.555594</td>
<td>[0, 0, 1, 1, 1, 1, 0, 1, 0, 0]</td>
<td>5</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.518724</td>
<td>[1, 0, 0, 0, 1, 1, 1, 0, 1, 0]</td>
<td>5</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.569247</td>
<td>[1, 0, 1, 1, 0, 1, 1, 1, 1, 1]</td>
<td>8</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Each time you run this simulation, you first draw a new value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from its prior, then that value is used in the Bernoulli distribution to draw an array of binary match win/loss observations. To help us summarize the match observations in each simulated outcome, we also compute the sum of the match values, i.e.&nbsp;the number of wins.</p>
</section>
<section id="prior-predictive-check" class="level3">
<h3 class="anchored" data-anchor-id="prior-predictive-check">Prior Predictive Check</h3>
<p>But how do we know that the prior we chose is reasonable? There are two places we can look: (1) at the parameter itself and (2) at the downstream variables it influences. We already looked at the parameter itself by inspecting its pdf and thinking about the relative plausibilities it implies. To look at its impact on the downstream variables, we can simply run simulations from the model and inspect the outcome data it produces. If we see it’s generating lots of highly implausible outcomes, then we know something isn’t right. This process is called a <em>prior predictive check</em>, because we’re checking the simulated outcomes (a.k.a. predictions) implied by the prior. Let’s run our model simulation 1000 times and have a look at the distribution of the number of wins out of 10 matches that it predicts, i.e.&nbsp;the sum of the <code>y</code> variable from each simulation.</p>
<div id="cell-13" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">outcome_df.hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb6-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum(y)'</span>)</span>
<span id="cb6-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb6-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Prior Predictive Check'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img" alt="prior predictive check of number of match wins"></p>
</figure>
</div>
</div>
</div>
<p>The histogram shows most of the simulations yield between 3 and 7 wins, with very few outcomes less than 3 or greater than 7. That seems pretty reasonable.</p>
<p>Let’s look at what the prior predictive check might look like when things aren’t quite right.</p>
<div id="cell-15" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1_000</span>, alpha_param<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, beta_param<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>).hist(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb7-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum(y)'</span>)</span>
<span id="cb7-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)</span>
<span id="cb7-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Prior Predictive Check'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img" alt="prior predictive check of the win rate, with unexpected distribution shape"></p>
</figure>
</div>
</div>
</div>
<p>In this simulation, many of the outcomes are close to 0 or 10 wins out of 10. From our prior knowledge about RPS, we know it would be possible but very unusual for someone to win either 0/10 or 10/10 matches. This tips us off that something isn’t right with our priors. At this point we would iterate on our priors until we find something reasonable like our Beta(10, 10).</p>
<p>Once we’ve got our generative model and its priors nailed down, we’re ready to move from the modeling step to the inference step!</p>
</section>
</section>
<section id="step-2.-inference" class="level2">
<h2 class="anchored" data-anchor-id="step-2.-inference">🧮 Step 2. Inference</h2>
<section id="the-goal-of-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="the-goal-of-bayesian-inference">The Goal of Bayesian Inference</h3>
<p>In the inference step, we use observed outcome data to infer the plausible values of the unobserved parameters. Whereas simulation passes information forward from parameters to outcomes, inference passes it backwards from observed outcomes to parameters. It’s analogous to model fitting or training in machine learning; it’s the part where we use data to learn about the model parameters. The specific output of inference is the updated relative plausibility of the unknown model parameters. Whereas we represent the prior relative plausibilities with the prior distribution, we represent the posterior relative plausibilities (after incorporating information from the data) with the <em>posterior distribution</em>, or simply, the <em>posterior</em>. Like the prior, our model’s posterior distribution is a probability density defined over the possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, where larger values indicate higher relative plausibility.</p>
</section>
<section id="analytical-formulation-of-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="analytical-formulation-of-bayesian-inference">Analytical Formulation of Bayesian Inference</h3>
<p>Let’s nail down the mathematical formulation of Bayesian inference. We have data <img src="https://latex.codecogs.com/png.latex?y"> and parameter(s) <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. These have a joint probability density <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)">. This joint distribution of data and parameters is defined by our generative model of the system—simulating data from our DGP is equivalent to drawing realizations from the joint distribution <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)">. Using the definition of conditional probability,, we can write the joint distribution as:</p>
<p><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)%20=%20p(y%7C%5Ctheta)p(%5Ctheta)%20"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,%20y)"> is the <em>joint distribution</em> of parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and data <img src="https://latex.codecogs.com/png.latex?y"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)"> is the <em>prior </em> distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(y%7C%5Ctheta)"> is the <em>likelihood</em>—the conditional distribution of observed data <img src="https://latex.codecogs.com/png.latex?y"> given parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</li>
</ul>
<p>When we do inference we are interested in the relative plausibility of unknown parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> given data <img src="https://latex.codecogs.com/png.latex?y">, which we quantify as the conditional distribution of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> given <img src="https://latex.codecogs.com/png.latex?y">. Using Baye’s Rule, we can write the posterior as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(%5Ctheta%20%7C%20y)%20=%20%5Cfrac%7B%20p(%5Ctheta,%20y)%20%7D%20%7B%20p(y)%20%7D%20=%20%5Cfrac%20%7B%20p(y%7C%5Ctheta)p(%5Ctheta)%20%20%7D%20%7B%20p(y)%20%7D%20%20"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%20%7C%20y)"> is the <em>posterior</em> distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></li>
<li><img src="https://latex.codecogs.com/png.latex?p(y)"> is the <em>marginal likelihood</em> of the data <img src="https://latex.codecogs.com/png.latex?y"> (to be explained soon)</li>
</ul>
<p>Technically, the joint distribution and the posterior are functions of <em>both</em> parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and data <img src="https://latex.codecogs.com/png.latex?y">. But in practice when we compute the posterior, we’ll have some actual observed data—say <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D">—so that <img src="https://latex.codecogs.com/png.latex?y"> is actually fixed at <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">. Substituting the fixed value <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D"> in the posterior, we get</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(%5Ctheta%20%7C%20y_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cfrac%7B%20p(%5Ctheta,%20y_%7B%5Ctext%7Bobs%7D%7D)%20%7D%20%7B%20p(y_%7B%5Ctext%7Bobs%7D%7D)%20%7D%20=%20%20%5Cfrac%20%7B%20p(y_%7B%5Ctext%7Bobs%7D%7D%7C%5Ctheta)p(%5Ctheta)%20%20%7D%20%7B%20%20%5Cint%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)p(%5Ctheta)%20d%20%5Ctheta%20%20%7D%20%20"></p>
<p>If we view <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D"> as fixed, then the posterior can be interpreted as just the slice of the joint distribution where <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">. To get a proper conditional probability distribution, we just need to divide the sliced joint density function by the area under <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta,y_%7B%5Ctext%7Bobs%7D%7D)"> along the <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> axis. And guess what? That’s exactly what the marginal likelihood is doing; <img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cint%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)p(%5Ctheta)%20d%20%5Ctheta"> is just the area under the sliced joint density, and it’s there in the denominator to normalize the sliced joint density so that we get a proper conditional distribution for the posterior.</p>
</section>
<section id="computing-the-posterior-using-grid-approximation" class="level3">
<h3 class="anchored" data-anchor-id="computing-the-posterior-using-grid-approximation">Computing the Posterior using Grid Approximation</h3>
<p>Let’s compute the posterior using the formulas we cooked up in the previous section. Earlier when we wrote down our generative model, we already identified all the pieces we need:</p>
<ul>
<li><strong>the prior</strong>—since <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Csim%20%5Ctext%7BBeta%7D(10,%2010)">, <img src="https://latex.codecogs.com/png.latex?p(%5Ctheta)"> is the probability density function of a <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BBeta%7D(10,10)"> random variable.</li>
<li><strong>the likelihood</strong>—since <img src="https://latex.codecogs.com/png.latex?y_i%20%5Csim%20%5Ctext%7BBernoulli%7D(%5Ctheta)">, the likelihood is the probability mass function of a Bernoulli random variable with parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">… well, almost.</li>
</ul>
<p>The one remaining detail to iron out is that our observed data <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bobs%7D%7D=%5By_1,%5Cdots,y_N%5D"> consists of <img src="https://latex.codecogs.com/png.latex?N=10"> observations of the binary match outcomes. Our likelihood needs to reflect the conditional probability of the entire dataset given <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, not just a single observation. We know from probability theory that the joint probability of two independent events is the product of their individual probabilities. Therefore, assuming independence among our observations, the joint likelihood of the full dataset is the product of the likelihood of each observation.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p(y_%7B%5Ctext%7Bobs%7D%7D%7C%5Ctheta)%20=%20p(y_1,%5Cdots,y_N%7C%5Ctheta)%20=%20%20%5Cprod_%7Bi=1%7D%5EN%20p(y_i%7C%5Ctheta)%20"></p>
<p>Let’s implement the prior, the likelihood, the joint distribution, and the posterior in python and plot out the prior and the posterior distribution of the parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta">.</p>
<div id="cell-18" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0. Defining the observed data</span></span>
<span id="cb8-2"></span>
<span id="cb8-3">y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(observed_outcomes)</span>
<span id="cb8-4">sum_y_obs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(y_obs)</span>
<span id="cb8-5"></span>
<span id="cb8-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Functions for Prior and Likelihood</span></span>
<span id="cb8-7"></span>
<span id="cb8-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> prior(theta):</span>
<span id="cb8-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> beta.pdf(theta, alpha_param, beta_param)</span>
<span id="cb8-10"></span>
<span id="cb8-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> likelihood(theta, y):</span>
<span id="cb8-12">    product_of_likelihoods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span></span>
<span id="cb8-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> y_i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> y:</span>
<span id="cb8-14">        product_of_likelihoods <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> bernoulli.pmf(y_i, p<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>theta)</span>
<span id="cb8-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> product_of_likelihoods</span>
<span id="cb8-16"></span>
<span id="cb8-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Function for Joint Density p(y, theta)</span></span>
<span id="cb8-18"></span>
<span id="cb8-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> joint_density(theta, y):</span>
<span id="cb8-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> likelihood(theta, y) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> prior(theta)</span>
<span id="cb8-21"></span>
<span id="cb8-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Computing the Posterior by "Slicing" and Normalizing</span></span>
<span id="cb8-23"></span>
<span id="cb8-24"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> posterior_from_joint_slice(theta_values, y_observed_data):</span>
<span id="cb8-25">    </span>
<span id="cb8-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute grid of p(theta, y_obs) over values of theta and fixed y_obs</span></span>
<span id="cb8-27">    unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([</span>
<span id="cb8-28">        joint_density(theta, y_observed_data) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_values</span>
<span id="cb8-29">    ])</span>
<span id="cb8-30">    </span>
<span id="cb8-31">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># numerical integration  to get marginal likelihood p(y_obs</span></span>
<span id="cb8-32">    delta_theta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> theta_values[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> theta_values[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] </span>
<span id="cb8-33">    marginal_likelihood_approx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> delta_theta)</span>
<span id="cb8-34">    </span>
<span id="cb8-35">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># p(theta | y_obs) = p(theta, y_obs) / p(y_obs)</span></span>
<span id="cb8-36">    normalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> unnormalized_posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> marginal_likelihood_approx</span>
<span id="cb8-37">    </span>
<span id="cb8-38">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> normalized_posterior_values</span></code></pre></div>
</div>
<div id="cell-19" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define a grid of theta values</span></span>
<span id="cb9-2">theta_grid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.999</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>) </span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate prior over the grid of theta values</span></span>
<span id="cb9-5">prior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([prior(theta) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_grid])</span>
<span id="cb9-6"></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate likelihood values over the grid of theta values and fixed y_obs</span></span>
<span id="cb9-8">likelihood_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([likelihood(theta, y_obs) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> theta_grid])</span>
<span id="cb9-9"></span>
<span id="cb9-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate the posterior over the grid of theta values and fixed y_obs</span></span>
<span id="cb9-11">posterior_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> posterior_from_joint_slice(theta_grid, y_obs)</span></code></pre></div>
</div>
<div id="cell-20" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Plotting</span></span>
<span id="cb10-2">plt.plot(theta_grid, prior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Prior p(theta) ~ Beta(</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb10-3">plt.plot(theta_grid, posterior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Posterior p(theta|y_obs)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb10-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian Inference: Prior, Likelihood, and Posterior'</span>)</span>
<span id="cb10-5">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'$\theta$ (Probability of Match Win)'</span>)</span>
<span id="cb10-6">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb10-7">plt.legend()</span>
<span id="cb10-8">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb10-9">plt.xlim(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb10-10">plt.ylim(bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb10-11">plt.tight_layout()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img" alt="plot of prior and posterior computed with  grid approximation"></p>
</figure>
</div>
</div>
</div>
<p>From the figure we can see that while the prior is centered at <img src="https://latex.codecogs.com/png.latex?%5Ctheta=0.5">, the posterior is actually pulled slightly toward larger values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> by the observed data, indicating increased relative plausibility on win rates greater than 0.5.</p>
<p>It’s nice to see that the math works and that we can successfully implement it in code, but grid approximation is a pedagogical endeavor. In practice, when models start to get complicated, we’ll need a more flexible approach for finding the posterior.</p>
</section>
<section id="sampling-from-the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="sampling-from-the-posterior">Sampling from the Posterior</h3>
<p>It turns out that we can do inference using our generative model and our observed data without having to compute the likelihood directly. But while the forward problem of simulating data from the model is quite straightforward, it’s less obvious how to approach the inverse problem of doing inference. There is no silver bullet here. In fact there are tons of different algorithms for doing inference on probabilistic models, but luckily, since virtually all the important inference algorithms use sampling-based approaches, e.g.&nbsp;Hamiltonian Monte Carlo, Metropolis-Hastings, and Variational Inference, no matter which algorithm we use under the hood, we’ll end up with the same kind of output at the end—samples of the unknown parameters which have been drawn from the posterior.</p>
<p>To get some intuition for how we can do inference by sampling from the generative model,, we’re going to implement a very simple inference algorithm called <em>rejection sampling</em>. The key idea is based on the insight we discussed earlier that the posterior is essentially a slice through the joint distribution where the data is fixed to what we actually observed. Since simulating from the generative model is equivalent to drawing samples from the joint distribution, isolating simulation outcomes where the simmulated data is equal to the observed data is equivalent to sampling from the joint distribution along the slice, and hence equivalent to sampling from the posterior.</p>
<p>The algorithm for doing Bayesian inference via rejection sampling is as follows</p>
<ol type="1">
<li>Generate a sample <img src="https://latex.codecogs.com/png.latex?(%5Ctheta%5E*,y%5E*)"> from the generative model.</li>
<li>Keep the sample if <img src="https://latex.codecogs.com/png.latex?y%5E*=y_%7B%5Ctext%7Bobs%7D%7D">, otherwise discard it.</li>
<li>Repeat 1 and 2 until the desired number of retained samples is collected.</li>
<li>The retained samples of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> can be interpreted as samples from the posterior.</li>
</ol>
<p>Let’s do this in python. In our example, the order of the wins and losses over the <img src="https://latex.codecogs.com/png.latex?N=10"> match observations doesn’t matter, so we’ll focus on the number of wins. Since <img src="https://latex.codecogs.com/png.latex?%5Csum_iy_i=7"> in our observed data, we’ll isolate the simulation outcomes where <code>sum_y</code> equals 7.</p>
<div id="cell-22" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb11-2">outcome_df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y != @sum_y_obs'</span>).plot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>, kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scatter"</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"outcomes where $y^* </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">ne y_{</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">text</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{obs}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}$"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb11-3">outcome_df.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y == @sum_y_obs'</span>).plot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>, kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"scatter"</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.4</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"outcomes where $y^* = y_{</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\\</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">text</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{obs}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">}$"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb11-4">ax.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sum_y_obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb11-5">ax.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sum_y_obs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"black"</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb11-6">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Samples from the Generative Model"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img" alt="scatter plot of samples from joint distribution with posterior slice highlighted"></p>
</figure>
</div>
</div>
</div>
<p>Boom! By isolating the outcomes where <img src="https://latex.codecogs.com/png.latex?y=y_%7B%5Ctext%7Bobs%7D%7D">, we effectively have samples from the posterior. Let’s draw a larger number of samples so we get an adequate sample from the posterior.</p>
<div id="cell-24" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># drawing a large number of samples and isolating outcomes where y = y_obs</span></span>
<span id="cb12-2">rejection_sampling_outcome_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_outcomes(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10_000</span>, alpha_param, beta_param, N).query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum_y == @sum_y_obs'</span>)</span>
<span id="cb12-3"></span>
<span id="cb12-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># posterior samples from rejection sampling</span></span>
<span id="cb12-5">posterior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rejection_sampling_outcome_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>]</span></code></pre></div>
</div>
<div id="cell-25" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">posterior_samples.hist(bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color)</span>
<span id="cb13-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>)</span>
<span id="cb13-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Samples from the Posterior"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img" alt="histogram of posterior"></p>
</figure>
</div>
</div>
</div>
<p>Now let’s put all the pieces together.</p>
<div id="cell-27" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb14-2">prior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outcome_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"theta"</span>]</span>
<span id="cb14-3">prior_samples.hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"prior samples"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb14-4">posterior_samples.hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"posterior samples"</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb14-5">ax.plot(theta_grid, prior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Prior p(theta) ~ Beta(</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">,</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>beta_param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prior_color)</span>
<span id="cb14-6">ax.plot(theta_grid, posterior_values, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Posterior p(theta|y_obs)'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb14-7">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bayesian Inference: Prior and Posterior'</span>)</span>
<span id="cb14-8">plt.xlabel(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'$\theta$ (Probability of Match Win)'</span>)</span>
<span id="cb14-9">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Density'</span>)</span>
<span id="cb14-10">plt.legend()</span>
<span id="cb14-11">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb14-12">plt.xlim(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb14-13">plt.ylim(bottom<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb14-14">plt.tight_layout()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="plot of prior and posterior as line plot from grid approximation and histogram from sampling"></p>
</figure>
</div>
</div>
</div>
<p>In this figure we have:</p>
<ol type="1">
<li>the functional form of the prior</li>
<li>the functional form of the posterior from grid approximation</li>
<li>samples from the prior drawn directly from the generative model</li>
<li>and samples of the posterior obtained by applying rejection sampling to the generative model.</li>
</ol>
<p>Great! Our sampling algorithms are generating samples from the prior and the posterior which are consistent with the functional forms we computed earlier!</p>
<p>While we used rejection sampling here, regardless of what sampling algorithm we choose, we’ll end up with the same thing after inference—a set of samples from the posterior distribution for each unknown parameter. Once we have those samples, we’re ready to move to the interpretation and analysis step.</p>
</section>
</section>
<section id="step-3.-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="step-3.-interpretation">🔬 Step 3. Interpretation</h2>
<p>So how do we get insight into our analysis questions from this <code>posterior_samples</code> array? Well we’ve got samples from the posterior distribution of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> which represents our updated beliefs about the relative plausibility of different values of Fritz’s actual underlying RPS win rate. We can use them just like any other dataset to answer questions about his win rate.</p>
<p>Let’s start with getting a point estimate of his true win rate. We can simply take the mean of the samples.</p>
<div id="cell-31" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'point estimate of theta: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_samples)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>point estimate of theta: 0.5655783348185507</code></pre>
</div>
</div>
<p>To get a confidence interval, often called a <em>credible interval</em> in the Bayesian context, we can just pull the quantiles of the posterior distribution. Note there are fancier ways to do this, e.g.&nbsp;computing highest posterior density intervals (HPDIs), but conceptually we’re basically just looking at the quantiles of the sample.</p>
<div id="cell-33" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'89% credible interval of theta:: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>quantile(posterior_samples, [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.055</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.945</span>])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>89% credible interval of theta:: [0.42005907 0.70367915]</code></pre>
</div>
</div>
<p>What’s the probability that Fritz’s win rate is actually really good, say greater than 75%? We can just check the samples directly for the proportion greater than 0.75.</p>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'P[theta &gt; 0.75]: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>P[theta &gt; 0.75]: 0.011933174224343675</code></pre>
</div>
</div>
<p>This means our analysis implies theres only a 1.5% chance that his true win rate is larger than 75%.</p>
<p>Now that we’ve taken a look at interpreting the posterior samples to get some insight into the unknown parameter <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> representing Fritz’s actual RPS win rate, we can take it one step further and make some predictions.</p>
<section id="posterior-predictive-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-distribution">Posterior Predictive Distribution</h3>
<p>We have one more character to meet in this cast of Bayesian players—the <em>posterior predictive</em> distribution.</p>
<p><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)"></p>
<p>It represents the most plausible distribution of <em>new</em> data <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bnew%7D%7D"> given that we observed data $y_{}, i.e.&nbsp;based on the posterior rather than the prior. Mathematically it is obtained by integrating the product of the likelihood for the new data and the posterior distribution over the parameter space.</p>
<p><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)%20=%20%5Cint%20p(y_%7B%5Ctext%7Bnew%7D%7D%7C%5Ctheta)%20p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)%20d%5Ctheta"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7C%5Ctheta)"> is the likelihood of the new data, assuming a specific parameter value <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. It’s the same likelihood function we used before, evaluated on <img src="https://latex.codecogs.com/png.latex?y_%7B%5Ctext%7Bnew%7D%7D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?p(%5Ctheta%7Cy_%7B%5Ctext%7Bobs%7D%7D)"> is just the posterior distribution</li>
<li>the integral <img src="https://latex.codecogs.com/png.latex?%5Cint%20%5Cdots%20d%20%5Ctheta"> effectively “averages” the likelihood of the new data over all possible values of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, weighted by how plausible each value is according to the posterior.</li>
</ul>
<p>In terms of the DGP, we can generate samples from the posterior predictive by</p>
<ol type="1">
<li>Drawing a sample of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> from the posterior distribution.</li>
<li>Using this value of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> to generate a value of <img src="https://latex.codecogs.com/png.latex?y"> from the generative model.</li>
</ol>
<p>Let’s implement this in python.</p>
<div id="cell-37" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> simulate_posterior_predictive(posterior_samples, N):</span>
<span id="cb21-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pd.DataFrame([</span>
<span id="cb21-3">        simulate_one_outcome(theta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>theta, N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb21-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> theta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> posterior_samples</span>
<span id="cb21-5">    ])</span>
<span id="cb21-6"></span>
<span id="cb21-7">posterior_predictive_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> simulate_posterior_predictive(posterior_samples, N)</span></code></pre></div>
</div>
<p>Here again, we can inspect the distribution using any familiar techniques for working with samples of data. Here’s a histogram of the posterior predictive.</p>
<div id="cell-39" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>].hist(density<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>post_color)</span>
<span id="cb22-2">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Posterior Predictive Distribution"</span>)</span>
<span id="cb22-3">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum(y)"</span>)</span>
<span id="cb22-4">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"probability mass"</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/bayesian-modeling-primer/bayes_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img" alt="posterior predictive of number of wins"></p>
</figure>
</div>
</div>
</div>
<p>We can use this for forecasting, e.g.&nbsp;what’s the probability that Fritz wins at least 7 games in his next round?</p>
<div id="cell-41" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Probability of winning &gt;= 7 in next round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sum_y"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability of winning &gt;= 7 in next round: 0.32537788385043753</code></pre>
</div>
</div>
<p>Here’s where things can get interesting. In addition to forecasting the outcome itself, we can also compute the probabilities of events that depend on the outcome. For example, let’s say Fritz has only $50 left in his wallet, and he wants to know the probability that he can cover his bill after the next round. Let’s assume each drink costs $12. We can compute that probability as follows.</p>
<div id="cell-43" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># probability that Fritz's next round bill is less than or equal to $50</span></span>
<span id="cb25-2"></span>
<span id="cb25-3">cost_per_drink <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">12.0</span></span>
<span id="cb25-4">posterior_predictive_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb25-5">    posterior_predictive_df</span>
<span id="cb25-6">    .assign(losses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> x.sum_y)</span>
<span id="cb25-7">    .assign(bill <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: cost_per_drink <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x.losses)</span>
<span id="cb25-8">)</span>
<span id="cb25-9"></span>
<span id="cb25-10"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Probability next round bill &lt;= $50: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean(posterior_predictive_df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bill"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Probability next round bill &lt;= $50: 0.5369928400954654</code></pre>
</div>
</div>
</section>
</section>
<section id="summary-of-the-bayesian-workflow" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-the-bayesian-workflow">Summary of the Bayesian Workflow</h2>
<p>Wow, we covered a lot of ground today. Let’s summarize the key points.</p>
<p>The Bayesian Analysis Workflow has three major steps:</p>
<ol type="1">
<li>Modeling
<ul>
<li>We build a <em>generative model</em> that describes the relationships among key variables and unknown parameters to represent the data generating process.</li>
<li>We encode our prior knowledge about the relative plausibility of different parameter values in the <em>prior distribution</em> of the parameters.</li>
<li>We can use <em>prior predictive checks</em> to simulate outcome data from the generative model to sanity check our modeling assumptions.</li>
</ul></li>
<li>Inference
<ul>
<li>Based on our modeling assumptions, we can use observed data to infer the <em>posterior distribution</em>, which quantifies the relative plausibility of different values of the unknown parameters after observing data.</li>
<li>We can view simulations from the generative model as sampling from the joint distribution of data and parameters, and we can view the posterior as the result of conditioning the joint distribution on the data we actually observed.</li>
<li>We can do inference by using sampling-based inference algorithms like rejection sampling, which uses logic on top of our generative model to isolate samples from the posterior distribution.</li>
</ul></li>
<li>Interpretation
<ul>
<li>After inference we can use data analysis tools to summarize the samples from the posterior distribution to compute point estimates, intervals, and probabilities of interest.</li>
<li>If we simulate data from our generative model using the posterior rather than the prior, we get samples from the <em>posterior predictive </em>distribution, which predicts future outcomes given observed data.</li>
<li>Again, we can analyze these posterior predictive samples to compute point estimates, intervals, or probabilities of future outcomes.</li>
</ul></li>
</ol>
<p>Phew! Hopefully that’s a helpful introduction to the Bayesian workflow and the major ideas behind it. The techniques we looked at here are mostly for pedagogical purposes; if you want to apply Bayesian methods to practical problems, you’ll want to use a probabilistic programming language like PyMC, pyro, or stan. Maybe we’ll get into some of those tools in future posts. See you then!</p>
<section id="resources" class="level3">
<h3 class="anchored" data-anchor-id="resources">Resources</h3>
<ul>
<li><a href="https://xcelab.net/rm/">Statistical Rethinking</a> - This page links to where you can obtain the book and also to a number of repos where folks have ported the R and Stan code examples to python libraries like PyMC and pyro.</li>
</ul>
</section>
<section id="reader-exercises" class="level3">
<h3 class="anchored" data-anchor-id="reader-exercises">Reader Exercises</h3>
<p>You didn’t think you’d get away without homework did you? Here are a couple suggestions for exercises.</p>
<ul>
<li>compute the posterior predictive distribution <img src="https://latex.codecogs.com/png.latex?p(y_%7B%5Ctext%7Bnew%7D%7D%7Cy_%7B%5Ctext%7Bobs%7D%7D)"> for the RPS example using grid approximation.</li>
<li>Suppose that each RPS match was played as best out of three. Rewrite the generative model to generate both sub-match and match outcomes. Do inference with rejection sampling. Use your model to find the probability that Fritz wins his next match by winning two submatches in a row.</li>
</ul>
</section>
</section>

 ]]></description>
  <category>bayesian</category>
  <category>python</category>
  <guid>https://randomrealizations.com/posts/bayesian-modeling-primer/</guid>
  <pubDate>Wed, 04 Jun 2025 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/bayesian-modeling-primer/hist.png" medium="image" type="image/png" height="107" width="144"/>
</item>
<item>
  <title>Analyzing After Tax Retirement Income: Roth vs. Traditional 401(k)</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/traditional-vs-roth-401k/</link>
  <description><![CDATA[ 




<p>Today we’re taking a break from our typical hard hitting algorithm deep dives for a quick foray into the world of personal finance. We’ll take on a question I recently encountered while setting up my retirement account with my new employer—which is more efficient, the traditional 401(k) or the Roth 401(k)? US-based readers will recognize these as the two main types of employer-sponsored retirement accounts. When I searched for traditional vs Roth 401(k), the articles I found gave only very hand-wavy guidance on which is better in a given situation. So, today I’ll share my quantitative analysis of which account type provides superior performance for a given set of personal circumstances. We’ll implement the analysis in python, so you can run the numbers for your own situation and determine which employer-sponsored account type is better for you.</p>
<section id="traditional-401k-vs-roth-401k" class="level2">
<h2 class="anchored" data-anchor-id="traditional-401k-vs-roth-401k">Traditional 401(k) vs Roth 401(k)</h2>
<p>I’ll let JLCollins explain the <a href="https://jlcollinsnh.com/2015/06/02/stocks-part-viii-the-401k-403b-tsp-ira-roth-buckets/">background on 401(k)s</a>; read that post first if you’re not already familiar with the concepts of taxable accounts, IRAs, 401(k)s, and the basic rules of Roth vs traditional. The key distinction is * In a traditional 401(k), money you contribute now is deducted from your taxable income, meaning you’ll pay less in income tax now. During retirement however, withdrawals from the account will count toward your taxable income, so you’ll pay tax then. * In a Roth 401(k), money you contribute now does count toward your taxable income, meaning you’ll pay income tax on any contributions now. During retirement however, withdrawals do not count toward your taxable income and are therefore tax free.</p>
<p>Essentially you can either pay tax now (Roth) or pay tax later (traditional). The hand-wavy advice points out that which account is better for you depends on your income tax rate now versus your income tax rate during retirement. High tax rate now and low tax rate during retirement could favor traditional, while low tax rate now and high tax rate during retirement could favor Roth. Let’s put some numbers on this advice.</p>
<p>I’ll assume that you’re following the <a href="https://www.mrmoneymustache.com/2011/04/10/post-4-what-am-i-supposed-to-do-with-all-this-money/">sage advice of Mr.&nbsp;Money Mustache</a> and (after paying off any high-interest debt) maxing out your 401(k) contribution for the year. In 2024, the IRS has set a maximum combined contribution of $23,000; i.e.&nbsp;the sum of your Roth and traditional contributions cannot exceed this limit. Also, once you contribute to these accounts, you may not begin withdrawals (without penalty) until the age of 59.5.</p>
</section>
<section id="analysis-formulation" class="level2">
<h2 class="anchored" data-anchor-id="analysis-formulation">Analysis Formulation</h2>
<p>Let’s state the question precisely—which account type will yield me the most money during retirement after withdrawal and after all taxes are paid? Let’s think through the Roth vs traditional scenarios, setting aside the same amount of money today and liquidating the entire account at retirement; we’ll compare how much money we have at retirement after liquidating and settling any tax obligations.</p>
<p><strong>Roth</strong>: I contribute <code>contribution = 23_000</code> now, plus I pay income tax on this contribution in the amount of <code>current_income_tax_rate * contribution</code>. Over the years from now to retirement <code>retirement_age - current_age</code>, my contribution grows at some average long term yearly rate <code>investment_growth_rate</code>. At retirement, I liquidate the entire account, paying no income tax on the proceeds.</p>
<p><strong>Traditional</strong>: I contribute <code>contribution = 23_000</code> now. For fair comparison with the Roth, I invest an additional amount <code>current_income_tax_rate * contribution</code> (the extra income tax I would have paid had I chosen the Roth) in a normal taxable investment account as well. Over the time from now to retirement, the 401(k) and the taxable account both grow at the average long term rate <code>investment_growth_rate</code>. However, in the taxable account, I’ll also need to pay income tax every year on any dividends that I earn; the S&amp;P500 has recently paid out 1.5-2% in dividends each year, let’s call it <code>dividend_rate</code>. At retirement, I liquidate both accounts, paying income tax on the proceeds from the 401(k) at the rate of <code>retirement_income_tax_rate</code> and paying capital gains tax on the proceeds from the taxable account at the rate of <code>retirement_capital_gains_tax_rate</code>.</p>
<p>Let’s code up a function that takes in all our parameters and returns the total liquidation value after taxes of the Roth versus traditional 401(k)s as described above.</p>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span></code></pre></div>
</div>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_401k_liquidation_value(</span>
<span id="cb2-2">    current_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">37</span>,</span>
<span id="cb2-3">    current_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.35</span>,</span>
<span id="cb2-4">    contribution <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">23_000</span>,</span>
<span id="cb2-5">    retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">59.5</span>,</span>
<span id="cb2-6">     investment_growth_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.07</span>,</span>
<span id="cb2-7">     dividend_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02</span>,</span>
<span id="cb2-8">    retirement_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.24</span>,</span>
<span id="cb2-9">    retirement_capital_gains_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 0%, 15%, 20%</span></span>
<span id="cb2-10">):</span>
<span id="cb2-11"></span>
<span id="cb2-12">    investment_growth_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> investment_growth_rate) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_age)</span>
<span id="cb2-13">    dividend_income_tax_drag_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> dividend_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> current_income_tax_rate) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> (retirement_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> current_age)</span>
<span id="cb2-14"></span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Roth 401k</span></span>
<span id="cb2-16">    roth_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contribution </span>
<span id="cb2-17">    roth_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor</span>
<span id="cb2-18">    total_roth_401k_liquidation_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roth_401k_value</span>
<span id="cb2-19">    </span>
<span id="cb2-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># traditional  401k</span></span>
<span id="cb2-21">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contribution </span>
<span id="cb2-22">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> current_income_tax_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> contribution </span>
<span id="cb2-23">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor</span>
<span id="cb2-24">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> investment_growth_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> dividend_income_tax_drag_factor</span>
<span id="cb2-25">    traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> retirement_income_tax_rate)</span>
<span id="cb2-26">    taxable_account_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> retirement_capital_gains_tax_rate)</span>
<span id="cb2-27">    total_traditional_401k_liquidation_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> traditional_401k_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> taxable_account_value</span>
<span id="cb2-28"></span>
<span id="cb2-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {</span>
<span id="cb2-30">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'traditional'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(total_traditional_401k_liquidation_value), </span>
<span id="cb2-31">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'roth'</span>: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">round</span>(total_roth_401k_liquidation_value)</span>
<span id="cb2-32">    }</span></code></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">get_401k_liquidation_value()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>{'traditional': 106882, 'roth': 105405}</code></pre>
</div>
</div>
<p>Somehow it’s not surprising that these two options seem to yield very similar after-tax performance—no arbitrage right?</p>
<p>Let’s write a function to perturb some of our parameter values to see under what conditions one option dominates the other.</p>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> plot_liquidation_value_by_parameter_values(param, grid_values, func<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>get_401k_liquidation_value):</span>
<span id="cb5-2">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [func(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>{param: x}) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> grid_values]</span>
<span id="cb5-3">    df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(y, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.Series(grid_values, name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>param))</span>
<span id="cb5-4">    fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb5-5">    df.plot(ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax)</span>
<span id="cb5-6">    plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'liquidation value'</span>)</span>
<span id="cb5-7">    plt.title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'Liquidation Value at Retirement by </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>param<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb5-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> fig, ax</span></code></pre></div>
</div>
<section id="income-tax-rate-at-retirement" class="level3">
<h3 class="anchored" data-anchor-id="income-tax-rate-at-retirement">Income Tax Rate at Retirement</h3>
<p>It seems that income tax rate at retirement is by far the most important determining factor in whether traditional or Roth 401(k) is a better option.</p>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_income_tax_rate'</span>, np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.38</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So given the other parameters I’ve set, Roth outperforms traditional when our income tax rate in retirement exceeds about 25%. <a href="https://www.irs.gov/filing/federal-income-tax-rates-and-brackets">According to the IRS</a> in 2023, an individual tax payer is in the 24% bracket if their income is between about $95k and $180k. So, how much income do you expect to pull in retirement? If we’re really building FIRE wealth, the kind indicated by Mr.&nbsp;Money Mustache and JLCollins, our income in retirement could easily exceed $180k, which would push us into the 32% bracket where Roth is more efficient than traditional.</p>
</section>
<section id="capital-gains-tax-rate-at-retirement" class="level3">
<h3 class="anchored" data-anchor-id="capital-gains-tax-rate-at-retirement">Capital Gains Tax Rate at Retirement</h3>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_capital_gains_tax_rate'</span>, np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.20</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In 2023, <a href="https://www.irs.gov/filing/federal-income-tax-rates-and-brackets">according to the IRS</a> as a single tax payer, if your income is between $44k and $492k, you’ll pay 15% capital gains. Over $492k you’ll jump up to 20% where Roth dominates traditional.</p>
</section>
<section id="retirement-age" class="level3">
<h3 class="anchored" data-anchor-id="retirement-age">Retirement Age</h3>
<div id="cell-16" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">plot_liquidation_value_by_parameter_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retirement_age'</span>, np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">59.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">75</span>, num<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/traditional-vs-roth-401k/401k_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For retirement ages beyond 59.5, traditional’s edge over Roth grows slightly.</p>
</section>
</section>
<section id="bottom-line" class="level2">
<h2 class="anchored" data-anchor-id="bottom-line">Bottom Line</h2>
<p>When I plugged in my actual parameters, I found that because I was only employed for 6 months last year, my current income tax rate pushed me into the regime where Roth performs better than traditional. However for this next year, I expect to be in a higher income tax bracket where traditional will be a better deal than Roth.</p>
<p>That said, the most important factor is your income tax rate at the time of withdrawal during retirement, which is based on your taxable income at that time. But how, I hear you asking, am I supposed to know what to plug in for my post-retirement income? That quantity is unknown. This illuminates the fundamental limitation of this kind of analysis—what to do about uncertain inputs to the calculation? That’s a question that we might take on in a future post, so stay tuned!</p>
</section>

 ]]></description>
  <category>personal finance</category>
  <guid>https://randomrealizations.com/posts/traditional-vs-roth-401k/</guid>
  <pubDate>Sat, 14 Dec 2024 08:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/enso-thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>SHAP from Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/shap-from-scratch/</link>
  <description><![CDATA[ 




<p>Ahh, SHAP. As you know it’s become one of the leading frameworks for explaining ML model predictions. I’d guess it’s popularity is due to its appealing theoretical basis, its universal applicability to any type of ML model, and its easy-to-use python package. SHAP promises to turn your black box ML model into a nice friendly interpretable model. The hilarious irony is that, when I first started using it in my work, SHAP itself was a complete black box to me. In this post, we’ll change all that by diving into the SHAP paper, illuminating the key theoretical ideas behind its development step by step, and implementing it from scratch in python. If you aren’t already familiar with how to compute and interpret SHAP values in practice, I’d recommend that you go check out the <a href="https://shap.readthedocs.io/en/latest/index.html">documentation for the shap python package</a> before diving into this post.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_main.jpg" class="img-fluid figure-img"></p>
<figcaption>Snow, trees, and mountains overlook Lake Tahoe.</figcaption>
</figure>
</div>
<section id="what-is-shap" class="level2">
<h2 class="anchored" data-anchor-id="what-is-shap">What is SHAP?</h2>
<p>SHAP (SHapley Additive exPlanations) is a conceptual framework for creating explanations of ML model predictions. The term also refers to a set of computational methods for generating these explanations and a python library which implements them. The “SHAP” <a href="https://en.wikipedia.org/wiki/Backronym">backronym</a> was introduced in <a href="https://arxiv.org/abs/1705.07874">Lundberg and Lee 2017</a>, which I call the <em>SHAP paper</em>, that expanded on several previously existing ideas which we’ll build up in the following sections. The key concepts are:</p>
<ul>
<li><em>Shapley values</em>, a concept from cooperative game theory which originally had nothing to do with machine learning</li>
<li><em>Shapley regression values</em>, which showed how to use Shapley values to generate explanations of model predictions</li>
<li><em>Shapley sampling values</em>, which offered a computationally tractable way to compute Shapley regression values for any type of model.</li>
</ul>
<p>The SHAP paper tied Shapley regression values and several other existing model explanation methods together by showing they are all members of a class called “additive feature attribution methods.” Under the right conditions, these additive feature attribution methods can generate Shapley values, and when they do we can call them SHAP values.</p>
<p>After establishing this theoretical framework, the authors go on to discuss various computational methods for computing SHAP values; some are model-agnostic, meaning they work with any type of model, and others are model-specific, meaning they work for specific types of models. It turns out that the previously existing Shapley sampling values method is a model-agnostic approach, but while it’s the most intuitive, computationally speaking it’s relatively inefficient. Thus the authors propose a novel model-agnostic approach called Kernel SHAP, which is really just <a href="https://lime-ml.readthedocs.io/en/latest/">LIME</a> parameterized to yield SHAP values.</p>
<p>Model-specific approaches can be potentially much more efficient than model-agnostic ones by taking advantage of model idiosyncrasies. For example, there is an analytical solution for the SHAP values of linear models, so Linear SHAP is extremely efficient. Similarly, Deep SHAP (proposed in the SHAP paper) and Tree SHAP (proposed later in <a href="https://www.sciencedirect.com/science/article/pii/S2666827022000500#b20">Lundberg et al 2020</a>) take advantage of idiosyncrasies of deep learning and tree-based models to compute SHAP values efficiently.</p>
<p>The important thing about these different methods is that they provide computationally tractable ways to compute SHAP values, but ultimately, they are all based on the Shapley sampling values method—the original method to compute what we now call SHAP values. Thus, for the remainder of this post, we’ll focus on this method, building it up from Shapley values to Shapley regression values to Shapley sampling values and ultimately implementing it from scratch in python.</p>
</section>
<section id="shapley-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-values">Shapley Values</h2>
<p>The <a href="https://en.wikipedia.org/wiki/Shapley_value">Shapley value</a> is named in honor of Nobel prize winning economist Loyd Shapley who introduced the idea in the field of coalitional game theory in the 1950’s. Shapley proposed a way to determine how a coalition of players can fairly share the payout they receive from a cooperative game. We’ll introduce the mathematical formalism in the next section, so for now let’s just touch on the intuition for the approach. Essentially, the method distributes the payout among the players according to the expected contribution of each player across all possible combinations of the players. The thought experiment works as follows:</p>
<ol type="1">
<li>Draw a random permutation (ordering) of the players.</li>
<li>Have the first player play alone, generating some payout. Then have the first two players play together, generating some payout. Then the first three, and so on.</li>
<li>As each new player is added, attribute the change in the payout to this new player.</li>
<li>Repeat this experiment for all permutations of the players. A player’s Shapley value is the average change in payout (across all permutations) when that player is added to the game.</li>
</ol>
<p>Next we’ll see how this idea can be applied to model explanations.</p>
</section>
<section id="shapley-regression-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-regression-values">Shapley Regression Values</h2>
<p>The next idea came from <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.446">Lipovetsky and Conklin 2001</a>, who proposed a way to use Shapley values to explain the predictions of a linear regression model. <em>Shapley regression values</em> assign an importance value to each feature that represents the effect on the model prediction of including that feature. The basic idea is to train a second model without the feature of interest, and then to compare the predictions from the model with the feature and the model without the feature. This procedure of training two models and comparing their predictions is repeated for all possible subsets of the other features; the average difference in predictions is the Shapley value for the feature of interest.</p>
<p>The Shapley value for feature <img src="https://latex.codecogs.com/png.latex?i"> on instance <img src="https://latex.codecogs.com/png.latex?x"> is given by equation 4 in the SHAP paper:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cphi_i%20=%20%5Csum_%7BS%20%5Csubseteq%20F%20%5Csetminus%20%5C%7Bi%5C%7D%7D%0A%5Cfrac%7B%7CS%7C!(%7CF%7C%20-%20%7CS%7C%20-%201)!%7D%7B%7CF%7C!%7D%0A%5Bf_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D(x_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D)%20-%20f_S(x_S)%20%5D%0A"></p>
<p>where</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cphi_i"> is the Shapley value for feature of interest <img src="https://latex.codecogs.com/png.latex?i">,</li>
<li>the <img src="https://latex.codecogs.com/png.latex?%5Csubseteq"> symbol indicates the item on its left is a subset of the object on its right,</li>
<li><img src="https://latex.codecogs.com/png.latex?F"> is the set of all features,</li>
<li>the vertical bars indicate the number of elements in a set, e.g.&nbsp;<img src="https://latex.codecogs.com/png.latex?%7CF%7C"> is the total number of features,</li>
<li><img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D"> is the set of all features except the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?S"> is a particular subset of features not including the feature of interest,</li>
<li><img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a “subset model”—a model that uses only the features in <img src="https://latex.codecogs.com/png.latex?S"> for both training and prediction,</li>
<li>and <img src="https://latex.codecogs.com/png.latex?f_%7BS%20%5Ccup%20%5C%7Bi%5C%7D%7D"> is asubset model using features in <img src="https://latex.codecogs.com/png.latex?S"> and the feature of interest.</li>
</ul>
<p>To reiterate, this is the most important equation when it comes to understanding SHAP, as it defines the Shapley value; let’s make sure we understand what’s going on by implementing it in python.</p>
<p>We start with the feature subsets. Notice that the sum is indexed over all subsets of <img src="https://latex.codecogs.com/png.latex?F%20%5Csetminus%20%5C%7Bi%5C%7D">, which is the set of all features except the <img src="https://latex.codecogs.com/png.latex?i">th feature, the one we’re calculating the Shapley value for. Let’s write a function that takes a list of items and returns an iterable that yields all possible subsets of those items.</p>
<div id="cell-7" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations </span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_all_subsets(items):</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span>  get_all_subsets([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]):</span>
<span id="cb1-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(2,)
(0, 1)
(0, 2)
(1, 2)
(0, 1, 2)</code></pre>
</div>
</div>
<p>To get all subsets of features, other than the feature of interest, we could do something like this.</p>
<div id="cell-9" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb3-2">    all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb3-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> get_all_subsets(all_other_features)</span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>):</span>
<span id="cb3-6">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(s)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>()
(0,)
(1,)
(3,)
(0, 1)
(0, 3)
(1, 3)
(0, 1, 3)</code></pre>
</div>
</div>
<p>So for each of the feature subsets, we’ll need to calculate the summand, which is the product of a quotient with a bunch of factorials and the difference in predicted values between two subset models. Let’s start with those subset models. Subset model <img src="https://latex.codecogs.com/png.latex?f_%7BS%7D"> is a model trained only on the features in subset <img src="https://latex.codecogs.com/png.latex?S">. We can write a function that takes an untrained model, a training dataset, a feature subset to use, and a single instance to predict on; the function will then train a model using only features in the subset, and it will issue a prediction for the single instance we gave it.</p>
<div id="cell-11" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> subset_model(model, X_train, y_train, feature_subset, instance):</span>
<span id="cb5-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance.shape) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Instance must be a 1D array'</span></span>
<span id="cb5-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(feature_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb5-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> y.mean() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># a model with no features predicts E[y]</span></span>
<span id="cb5-5">    X_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.take(feature_subset, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-6">    model.fit(X_subset, y_train)</span>
<span id="cb5-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model.predict(instance.take(feature_subset).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
</div>
<p>Next let’s have a look at <img src="https://latex.codecogs.com/png.latex?%7CS%7C!(%7CF%7C-%7CS%7C-1)!/%7CF%7C!">. The keen reader will notice this factor kind of looks like the answers to those combinatorics questions like how many unique ways can you order the letters in the word MISSISSIPPI. The combinatorics connection is that Shapley values are defined in terms of all permutations of the players , where the included players come first, then the player of interest, followed by the excluded players. In ML models, the order of features doesn’t matter, so we can work with unordered subsets of features, scaling the prediction difference terms by the number of permutations that involve the same sets of included and excluded features. With that in mind, we can see that including the factor in each term of the sum gives us a weighted average over all feature combinations, where the numerator gives the number of permutations in which the included features come first, followed by the feature of interest, followed by the excluded features, and the denominator is the total number of feature permutations.</p>
<div id="cell-13" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb6-2"></span>
<span id="cb6-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> permutation_factor(n_features, n_subset):</span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> factorial(n_subset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features)</span></code></pre></div>
</div>
<p>Now we can put these pieces together to compute equation 4—a single Shapley regression value for a single instance and feature of interest.</p>
<div id="cell-15" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_single_shap_value(untrained_model,</span>
<span id="cb7-2">                              X_train,</span>
<span id="cb7-3">                              y_train,</span>
<span id="cb7-4">                              feature_of_interest,</span>
<span id="cb7-5">                              instance):</span>
<span id="cb7-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb7-7">    n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb7-8">    shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb7-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> get_all_other_feature_subsets(n_features, feature_of_interest):</span>
<span id="cb7-10">        n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb7-11">        prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-12">            untrained_model,</span>
<span id="cb7-13">            X_train, y_train,</span>
<span id="cb7-14">            subset,</span>
<span id="cb7-15">            instance</span>
<span id="cb7-16">        )</span>
<span id="cb7-17">        prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subset_model(</span>
<span id="cb7-18">            untrained_model,</span>
<span id="cb7-19">            X_train, y_train,</span>
<span id="cb7-20">            subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature_of_interest,),</span>
<span id="cb7-21">            instance</span>
<span id="cb7-22">        )</span>
<span id="cb7-23">        factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_factor(n_features, n_subset)</span>
<span id="cb7-24">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb7-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_value</span></code></pre></div>
</div>
<p>Let’s use this function to compute a single Shapley regression value for a linear model and a small training dataset with 3 features.</p>
<div id="cell-17" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb8-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression </span>
<span id="cb8-3"></span>
<span id="cb8-4">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb8-5"></span>
<span id="cb8-6">compute_single_shap_value(untrained_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>LinearRegression(),</span>
<span id="cb8-7">                          X_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X, y_train<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y,</span>
<span id="cb8-8">                          feature_of_interest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,</span>
<span id="cb8-9">                          instance<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>-0.07477140629329351</code></pre>
</div>
</div>
<p>That gives us a single Shapley value corresponding to a single feature value in a single instance. To get useful model explanations, we’d need to compute Shapley values for each feature of each instance in some dataset of instances. You might notice there’s a big problem with the formulation above. Namely, we are going to have to train a whole bunch of new subset models—one for each subset of the features. If our model has <img src="https://latex.codecogs.com/png.latex?M"> features, we’ll have to train <img src="https://latex.codecogs.com/png.latex?2%5EM"> models, so this will get impractical in a hurry, especially if we’re trying to train anything other than linear models.</p>
</section>
<section id="shapley-sampling-values" class="level2">
<h2 class="anchored" data-anchor-id="shapley-sampling-values">Shapley Sampling Values</h2>
<p>Next, <a href="https://link.springer.com/article/10.1007/s10115-013-0679-x">Štrumbelj and Kononenko 2014</a> proposed <em>Shapley sampling values</em>, a method which provides a much more efficient way to approximate the subset models used to calculate Shapley regression values. In this approach, the effect of removing some features from the model is approximated by the conditional expectation of the model given the known features.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20f_S(x_S)%20%20:=%20E%5Bf(x)%20%7C%20x_S%5D%20%20"></p>
<p>This means we’re approximating the output of a subset model by averaging over outputs of the full model. That’s great because now we don’t have to train all those new subset models, we can just query our full model over some set of inputs and average over the outputs to compute these conditional expectation subset models.</p>
<p>Now how exactly do we compute that conditional expectation? First we rewrite the above conditional expectation (equation 10 in the SHAP paper)</p>
<p><img src="https://latex.codecogs.com/png.latex?%20E%5Bf(x)%20%7C%20x_S%5D%20%20=%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> is the set of excluded or missing features. Beside this equation in the paper they give the note “expectation over <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D%20%7C%20x_S">, which means we’re taking the expectation over the missing features given the known features. Then we get another step (equation 11)</p>
<p><img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7Cx_S%7D%20%5Bf(x)%5D%20%5Capprox%20E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%20%5Bf(x)%5D"></p>
<p>Now it’s not an equality but an approximation. The authors give the note “assume feature independence”. The intuition here is that if the missing features are correlated with the known features, then their distribution depends on the particular values taken by the known features. But here the authors make the simplifying assumption that known and missing features are independent, which allows us to replace the conditional expectation with an unconditional expectation over the missing features.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>So is this assumption that features in <img src="https://latex.codecogs.com/png.latex?S"> are independent from features in <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BS%7D"> a problem? The short answer is… maybe 🤷‍♀️? It’s potentially problematic enough that people have worked out some ways to relax this assumption, e.g.&nbsp;<a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.PartitionExplainer.html">partition masking</a>, but that makes <em>Owen values</em> instead of Shapley values, so we’ll save it for another post.</p>
</div>
</div>
<p>Anyway, how do we compute this unconditional expectation over the missing features in practice? We’ll need to use a so-called <em>background dataset</em>, which is just some set of observations of our feature variables that represents their distribution. A good candidate is the training data we used to train our model. Štrumbelj and Kononenko 2014 propose a way to estimate this conditional expectation using resampling of the background dataset.</p>
<p>The idea is to notice that the instance of interest <img src="https://latex.codecogs.com/png.latex?x"> is a feature vector comprised of the set of “known” features <img src="https://latex.codecogs.com/png.latex?x_S"> and the set of excluded features <img src="https://latex.codecogs.com/png.latex?x_%7B%5Cbar%7BS%7D%7D"> such that <img src="https://latex.codecogs.com/png.latex?x=%5C%7Bx_S,x_%7B%5Cbar%7BS%7D%7D%20%5C%7D">. Our resampling scheme will be based on constructing “masked” samples <img src="https://latex.codecogs.com/png.latex?x%5E*=%5C%7Bx_S,z_%7B%5Cbar%7BS%7D%7D%20%5C%7D"> where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D"> are values of the missing features drawn from some random observation in the background dataset. We can then compute an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)"> of the conditional expectation <img src="https://latex.codecogs.com/png.latex?E_%7Bx_%7B%5Cbar%7BS%7D%7D%7D%5Bf(x)%5D"> as</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D_S(x)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bk=1%7D%5En%20f(%5C%7Bx_S,%20z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D%20%5C%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?z_%7B%5Cbar%7BS%7D%7D%5E%7B(k)%7D"> is the vector of values of the excluded features from the <img src="https://latex.codecogs.com/png.latex?k">-th row of the background dataset. Algorithmically, we can view this as first drawing a sample of observations from the background dataset, second “masking” features in <img src="https://latex.codecogs.com/png.latex?S"> in the sampled background dataset by replacing the observed values <img src="https://latex.codecogs.com/png.latex?z_S"> on each row with the values in the instance <img src="https://latex.codecogs.com/png.latex?x_S">, third using the full model <img src="https://latex.codecogs.com/png.latex?f"> to predict on each of these masked samples in the background dataset, and finally averaging over these predictions. We can implement a new subset model function that takes a fully trained model, a background dataset,a feature subset, and an instance for explanation and returns an approximation of the subset model prediction.</p>
<div id="cell-21" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb10-2"></span>
<span id="cb10-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> subset_model_approximation(trained_model, </span>
<span id="cb10-4">                               background_dataset,</span>
<span id="cb10-5">                               feature_subset,  </span>
<span id="cb10-6">                               instance):</span>
<span id="cb10-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">""" </span></span>
<span id="cb10-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Approximate subset model prediction  (Equation 11)</span></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    \hat{f}_S(x) = E_{x_{\hat{S</span><span class="re">}}}</span><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">[f_S(x)]</span></span>
<span id="cb10-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    for feature subset S on single instance x</span></span>
<span id="cb10-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb10-12">    masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset.copy()</span>
<span id="cb10-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb10-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb10-15">            masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb10-16">    conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb10-17">        trained_model.predict(masked_background_dataset)</span>
<span id="cb10-18">    )</span>
<span id="cb10-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>If we replace our <code>subset_model</code> function with this new <code>subset_model_approximation</code> function in our <code>compute_single_shap_value</code> function from earlier, then we’ll be computing Shapley sampling values. And according to the SHAP paper: “if we assume feature independence when approximating conditional expectations (using Equation 11 to estimate subset model output) … then SHAP values can be estimated directly using the Shapley sampling values method.” That means we’ll be computing SHAP values!</p>
</section>
<section id="how-to-implement-shap-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="how-to-implement-shap-from-scratch">How to Implement SHAP from Scratch</h2>
<p>Let’s put the pieces together and implement a class for a model explainer that computes SHAP values via the Shapley sampling values method. We’ll talk through a couple of points after the code.</p>
<div id="cell-24" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Any, Callable, Iterable</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> math <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> factorial</span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chain, combinations</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ShapFromScratchExplainer():</span>
<span id="cb11-7">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>,</span>
<span id="cb11-8">                 model: Callable[[np.ndarray], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>], </span>
<span id="cb11-9">                 background_dataset: np.ndarray,</span>
<span id="cb11-10">                 max_samples: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb11-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model</span>
<span id="cb11-12">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> max_samples:</span>
<span id="cb11-13">            max_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(max_samples, background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]) </span>
<span id="cb11-14">            rng <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.default_rng()</span>
<span id="cb11-15">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> rng.choice(background_dataset, </span>
<span id="cb11-16">                                                 size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_samples, </span>
<span id="cb11-17">                                                 replace<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb11-19">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> background_dataset</span>
<span id="cb11-20"></span>
<span id="cb11-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> shap_values(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X: np.ndarray) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> np.ndarray:</span>
<span id="cb11-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"SHAP Values for instances in DataFrame or 2D array"</span></span>
<span id="cb11-23">        shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.empty(X.shape)</span>
<span id="cb11-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb11-25">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-26">                shap_values[i, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._compute_single_shap_value(j, X[i, :])</span>
<span id="cb11-27">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_values</span>
<span id="cb11-28">       </span>
<span id="cb11-29">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _compute_single_shap_value(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-30">                                   feature: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>,</span>
<span id="cb11-31">                                   instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"Compute a single SHAP value (equation 4)"</span></span>
<span id="cb11-33">        n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(instance)</span>
<span id="cb11-34">        shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb11-35">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> subset <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_other_feature_subsets(n_features, feature):</span>
<span id="cb11-36">            n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(subset)</span>
<span id="cb11-37">            prediction_without_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-38">                subset, </span>
<span id="cb11-39">                instance</span>
<span id="cb11-40">            )</span>
<span id="cb11-41">            prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._subset_model_approximation(</span>
<span id="cb11-42">                subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (feature,), </span>
<span id="cb11-43">                instance</span>
<span id="cb11-44">            )</span>
<span id="cb11-45">            factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._permutation_factor(n_features, n_subset)</span>
<span id="cb11-46">            shap_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (prediction_with_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction_without_feature)</span>
<span id="cb11-47">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> shap_value</span>
<span id="cb11-48">    </span>
<span id="cb11-49">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _get_all_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, items: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> Iterable:</span>
<span id="cb11-50">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> chain.from_iterable(combinations(items, r) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(items)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb11-51">    </span>
<span id="cb11-52">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _get_all_other_feature_subsets(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, feature_of_interest):</span>
<span id="cb11-53">        all_other_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [j <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_features) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> feature_of_interest]</span>
<span id="cb11-54">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._get_all_subsets(all_other_features)</span>
<span id="cb11-55"></span>
<span id="cb11-56">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _permutation_factor(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_features, n_subset):</span>
<span id="cb11-57">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (</span>
<span id="cb11-58">            factorial(n_subset) </span>
<span id="cb11-59">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> factorial(n_features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_subset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb11-60">            <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> factorial(n_features) </span>
<span id="cb11-61">        )</span>
<span id="cb11-62">    </span>
<span id="cb11-63">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> _subset_model_approximation(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, </span>
<span id="cb11-64">                                    feature_subset: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">tuple</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, ...], </span>
<span id="cb11-65">                                    instance: np.array) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb11-66">        masked_background_dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.background_dataset.copy()</span>
<span id="cb11-67">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(masked_background_dataset.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]):</span>
<span id="cb11-68">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> feature_subset:</span>
<span id="cb11-69">                masked_background_dataset[:, j] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instance[j]</span>
<span id="cb11-70">        conditional_expectation_of_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(</span>
<span id="cb11-71">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.model(masked_background_dataset)</span>
<span id="cb11-72">        )</span>
<span id="cb11-73">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> conditional_expectation_of_model          </span></code></pre></div>
</div>
<p>The <code>SHAPExplainerFromScratch</code> API is similar to that of the <a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html"><code>KernelExplainer</code></a> from the python library, taking two required arguments during instantiation:</p>
<ul>
<li><code>model</code>: “User supplied function that takes a matrix of samples (# samples x # features) and computes the output of the model for those samples.” That means if our model is a scikit-learn model, we’ll need to pass in its predict method, not the model object itself.</li>
<li><code>background_dataset</code>: “The background dataset to use for integrating out features.” We know about this idea from the Shapley sampling values section above; a good choice for this data could be the training dataset we used to fit the model. By default, we’ll use all the rows of this background dataset, but we’ll also implement the ability to sample down to the desired number of rows with an argument called <code>max_samples</code>.</li>
</ul>
<p>Like the <code>KernelExplainer</code>, this class has a method called <code>shap_values</code> which estimates the SHAP values for a set of instances. It takes an argument <code>X</code> which is “a matrix of samples (# samples x # features) on which to explain the model’s output.” This <code>shap_values</code> method just loops through each feature value of each instance of the input samples <code>X</code> and calls an internal method named <code>_compute_single_shap_value</code> to compute each SHAP value. The <code>_compute_single_shap_value</code> method is the real workhorse of the class. It implements equation 4 from the SHAP paper as described in the Shapley regression values section above by calling a few other internal helper methods corresponding to functions that we’ve already written.</p>
</section>
<section id="testing-the-implementation" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-implementation">Testing the Implementation</h2>
<p>Let’s check our work by comparing SHAP values computed by our implementation with those from the SHAP python library. We’ll use our old friend the diabetes dataset, training a linear model, a random forest, and a gradient boosting machine.</p>
<div id="cell-27" class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_diabetes</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb12-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GradientBoostingRegressor, RandomForestRegressor</span>
<span id="cb12-5"></span>
<span id="cb12-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_diabetes(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, return_X_y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb12-7">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb12-8">                                                    random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb12-9"></span>
<span id="cb12-10">lin_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-11">rfr_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomForestRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb12-12">gbt_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingRegressor().fit(X_train, y_train)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>Here’s a little function to compare the SHAP values generated by our implementation and those from the library <code>KernelExplainer</code>.</p>
<div id="cell-29" class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> shap</span>
<span id="cb13-2"></span>
<span id="cb13-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compare_methods(model, X_background, X_instances):</span>
<span id="cb13-4">        </span>
<span id="cb13-5">    library_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> shap.KernelExplainer(model.predict, X_background)</span>
<span id="cb13-6">    library_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> library_explainer.shap_values(X_instances)</span>
<span id="cb13-7"></span>
<span id="cb13-8">    from_scratch_explainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ShapFromScratchExplainer(model.predict, X_background)</span>
<span id="cb13-9">    from_scratch_shap_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> from_scratch_explainer.shap_values(X_instances)</span>
<span id="cb13-10"></span>
<span id="cb13-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.allclose(library_shap_values, from_scratch_shap_values)</span></code></pre></div>
</div>
<div id="cell-30" class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">compare_methods(lin_model, </span>
<span id="cb14-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb14-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"995f763174824efebecb5c2522c3f6f5","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">compare_methods(rfr_model, </span>
<span id="cb16-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb16-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"f9468451774b4c758a7696ff10fabc74","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="77">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">compare_methods(gbt_model, </span>
<span id="cb18-2">                X_background<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_train[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, :], </span>
<span id="cb18-3">                X_instances<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X_test[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, :])</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8bdea22fab86440db68c4a669beaf7df","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>True</code></pre>
</div>
</div>
<p>Beautiful! Our Implementation is consistent with the SHAP library explainer!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well I hope this one was helpful to you. The research phase actually took me a lot longer than I expected; it just took me a while to figure out what SHAP really is and how those different ideas and papers fit together. I thought the implementation itself was pretty fun and relatively easy. What do you think?</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://arxiv.org/abs/1705.07874">The SHAP Paper (Lundberg and Lee, 2017)</a></li>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning by Christoph Molnar</a></li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>from scratch</category>
  <guid>https://randomrealizations.com/posts/shap-from-scratch/</guid>
  <pubDate>Sun, 04 Aug 2024 07:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/shap-from-scratch/shap_from_scratch_thumb.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
