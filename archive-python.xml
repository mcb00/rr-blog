<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html#category=python</link>
<atom:link href="https://randomrealizations.com/archive-python.xml" rel="self" type="application/rss+xml"/>
<description>A blog about data science, statistics, machine learning, and the scientific method</description>
<image>
<url>https://randomrealizations.com/opengraph.png</url>
<title>Random Realizations</title>
<link>https://randomrealizations.com/archive.html#category=python</link>
<height>76</height>
<width>144</width>
</image>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Tue, 26 Dec 2023 05:00:00 GMT</lastBuildDate>
<item>
  <title>The Ultimate Guide to XGBoost Parameter Tuning</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index.html</link>
  <description><![CDATA[ 



<p>Ahh, the dark art of hyperparameter tuning. It’s a key step in the machine learning workflow, and it’s an activity that can easily be overlooked or be overkill. Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils. Today I’ll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework. I’ll give you some intuition for how to think about the key parameters in XGBoost, and I’ll show you an efficient strategy for parameter tuning GBTs. I’ll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like. You can download a notebook with this tuning workflow from my <a href="https://github.com/mcb00/ds-templates">data science templates repository</a>. Finally we’ll wrap up with the kind of cautionary tale data scientists tell their colleagues around the campfire about when all this fancy hyperparameter tuning can backfire catastrophically—ignore at your own peril.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_main.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Lunar halo on a frosty night in Johnson City, TN</figcaption>
</figure>
</div>
<section id="xgboost-parameters" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-parameters">XGBoost Parameters</h2>
<p>Gradient boosting algorithms like XGBoost have two main types of hyperparameters: <em>tree parameters</em> which control the decision tree trained at each boosting round and <em>boosting parameters</em> which control the boosting procedure itself. Below I’ll highlight my favorite parameters, but you can see the full list in the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">documentation</a>.</p>
<section id="tree-parameters" class="level3">
<h3 class="anchored" data-anchor-id="tree-parameters">Tree Parameters</h3>
<p>In theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, <a href="../../posts/consider-the-decision-tree/">decision trees</a> are typically the best choice. In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.</p>
<section id="tree-construction-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="tree-construction-algorithm">Tree construction algorithm</h4>
<p>The tree construction algorithm boils down to split finding, and different algorithms have different ways of generating candidate splits to consider. In XGBoost we have the parameter:</p>
<ul>
<li><code>tree_method</code> - select tree construction algorithm: <code>exact</code>, <code>hist</code>, <code>approx</code>, or the horrifying default—<code>auto</code>—which outsources your choice of tree construction algo to XGBoost and which you should never ever use. I’ve been burned by this hidden <code>tree_method=auto</code> default multiple times before learning my lesson. Why is the online model worse than the offline model? Why is this model suddenly taking so much longer to train? Avoid these debugging nightmares and set <code>tree_method</code> explicitly; the exact method tends to be slow and ironically less accurate, so I use either approx or hist.</li>
</ul>
</section>
<section id="tree-complexity-parameters" class="level4">
<h4 class="anchored" data-anchor-id="tree-complexity-parameters">Tree complexity parameters</h4>
<p>Tree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be. I use these two parameters:</p>
<ul>
<li><code>max_depth</code> - maximum number of split levels allowed. Reasonable values are usually from 3-12.</li>
<li><code>min_child_weight</code> - minimum allowable sum of hessian values over data in a node. When using the default squared error objective, this is the minimum number of samples allowed in a leaf node (see <a href="https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm">this explanation</a> of why that’s true). For a squared error objective, values in [1, 200] usually work well.</li>
</ul>
<p>These two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing min child weight makes trees less expressive and therefore is a powerful way to counter overfitting. Note that <code>gamma</code> (a.k.a. <code>min_split_loss</code>) also limits node splitting, but I usually don’t use it because <code>min_child_weight</code> seems to work well enough on its own.</p>
</section>
<section id="sampling-parameters" class="level4">
<h4 class="anchored" data-anchor-id="sampling-parameters">Sampling parameters</h4>
<p>XGBoost can randomly sample rows and columns to be used for training each tree; you might think of this as <em>bagging</em>. We have a few parameters:</p>
<ul>
<li><code>subsample</code> - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.</li>
<li><code>colsample_bytree</code>, <code>colsample_bylevel</code>, <code>colsample_bynode</code> - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split. Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.</li>
</ul>
</section>
<section id="regularization-parameters" class="level4">
<h4 class="anchored" data-anchor-id="regularization-parameters">Regularization parameters</h4>
<p>In XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero. I usually use:</p>
<ul>
<li><code>reg_lambda</code> - L2 regularization of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. Valid values are in [0,<img src="https://latex.codecogs.com/png.latex?%5Cinfty">), but good values typically fall in [0,10].</li>
</ul>
<p>There is also an L1 regularization parameter called <code>reg_alpha</code>; feel free to use it instead. It seems that using one or the other is usually sufficient.</p>
</section>
</section>
<section id="boosting-parameters-and-early-stopping" class="level3">
<h3 class="anchored" data-anchor-id="boosting-parameters-and-early-stopping">Boosting Parameters and Early Stopping</h3>
<p>Trained gradient boosting models take the form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20F(%5Cmathbf%7Bx%7D)%20=%20b%20+%20%5Ceta%20%5Csum_%7Bk=1%7D%5E%7BK%7D%20f_k(%5Cmathbf%7Bx%7D)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?b"> is the constant base predicted value, <img src="https://latex.codecogs.com/png.latex?f_k(%5Ccdot)"> is the base learner for round <img src="https://latex.codecogs.com/png.latex?k">, parameter <img src="https://latex.codecogs.com/png.latex?K"> is the number of boosting rounds, and parameter <img src="https://latex.codecogs.com/png.latex?%5Ceta"> is the learning rate. In XGBoost these parameters correspond with:</p>
<ul>
<li><code>num_boost_round</code> (<img src="https://latex.codecogs.com/png.latex?K">) - the number of boosting iterations</li>
<li><code>learning_rate</code> (<img src="https://latex.codecogs.com/png.latex?%5Ceta">) - the scaling or “shrinkage” factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. Fun fact: the <img src="https://latex.codecogs.com/png.latex?%5Ceta"> character is called “eta”, and <code>learning_rate</code> is aliased to <code>eta</code> in xgboost, so you can use parameter <code>eta</code> instead of <code>learning_rate</code> if you like.</li>
</ul>
<p>These two parameters are very closely linked; the optimal value of one depends on the value of the other. To illustrate their relationship, we can train two different XGBoost models on the same training dataset, where one model has a lower learning rate than the other.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_regression </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split </span>
<span id="cb1-5"></span>
<span id="cb1-6">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_regression(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5000</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-7">X_train, X_valid, y_train, y_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9">eta1, eta2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span></span>
<span id="cb1-10">reg1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta1, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-11">reg1.fit(X_train, y_train, </span>
<span id="cb1-12">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-13">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-14">reg2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>eta2, early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>)</span>
<span id="cb1-15">reg2.fit(X_train, y_train, </span>
<span id="cb1-16">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_train, y_train), (X_valid, y_valid)], </span>
<span id="cb1-17">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-18"></span>
<span id="cb1-19">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots()</span>
<span id="cb1-20"></span>
<span id="cb1-21">best_round1, best_round2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.best_iteration, reg2.best_iteration</span>
<span id="cb1-22">obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-23">obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb1-24">best_obj1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj1[best_round1]</span>
<span id="cb1-25">best_obj2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> obj2[best_round1]</span>
<span id="cb1-26"></span>
<span id="cb1-27">plt.plot(reg1.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-b'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-28">plt.plot(reg2.evals_result()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'validation_1'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-r'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb1-29">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-30">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1), </span>
<span id="cb1-31">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round1, best_obj1<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-32">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-33">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-34">ax.annotate(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'learning_rate=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>eta2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">best_iteration=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>best_round2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, </span>
<span id="cb1-35">            xy<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2), </span>
<span id="cb1-36">            xytext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(best_round2, best_obj2<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>),</span>
<span id="cb1-37">            horizontalalignment<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'right'</span>,</span>
<span id="cb1-38">            arrowprops<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>))</span>
<span id="cb1-39">plt.legend()</span>
<span id="cb1-40">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RMSE'</span>) </span>
<span id="cb1-41">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting round'</span>) </span>
<span id="cb1-42">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Validation Scores by Boosting Round'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img" alt="XGBoost objective curves for two models"></p>
<figcaption class="figure-caption">Hold-out validation score (RMSE) by boosting round for two XGBoost models differing only by learning rate.</figcaption>
</figure>
</div>
</div>
</div>
<p>The above figure shows root mean squared error measured on a held-out validation dataset for two different XGBoost models: one with a higher learning rate and one with a lower learning rate. The figure demonstrates two key properties of the boosting parameters:</p>
<ol type="1">
<li>While training a model with a given learning rate, the evaluation score (computed on a hold-out set) tends to improve with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.</li>
<li>All else constant, a smaller learning rate leads to a model with more boosting rounds and better evaluation score.</li>
</ol>
<p>We can leverage the first property to make our tuning more efficient by using XGBoost’s <code>early_stopping_rounds: int</code> argument, which terminates training after observing the specified number of boosting rounds <a href="https://github.com/dmlc/xgboost/pull/6942">without sufficient improvement</a> to the evaluation metric. The models above were trained using <code>early_stopping_rounds=50</code>, which terminates training after 50 boosting rounds without improvement in RMSE on the validation data. For each model, the arrow indicates the boosting round with the best score.</p>
<p>The figure also exemplifies the second property, where the model with lower learning rate attains a better validation score but requires more boosting rounds to trigger early stopping. Note that smaller and smaller learning rates will provide diminishing improvements to the validation score.</p>
</section>
</section>
<section id="an-efficient-parameter-search-strategy-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="an-efficient-parameter-search-strategy-for-xgboost">An Efficient Parameter Search Strategy for XGBoost</h2>
<p>Efficiency is the key to effective parameter tuning, because wasting less time means searching more parameter values and finding better models in a given amount of time. But as we just saw, there is a tradeoff between accuracy and training time via the learning rate. Given infinite time and compute resources, we would just choose an arbitrarily tiny learning rate and search through tree parameter values while using early stopping to choose the number of boosting rounds. The problem is that tiny learning rates require tons of boosting rounds, which will make our ideal search prohibitively slow when confronted with the reality of finite time and resources. So what can we do?</p>
<p>My approach is based on the claim that <em>good tree parameters at one learning rate are also good tree parameters at other learning rates</em>. The intuition is that given two models—one with good tree parameters and one with bad tree parameters—the model with good tree parameters will score better, regardless of the learning rate. Thus, tree parameters are “independent” of boosting parameters—See <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> for justification of this claim.</p>
<p>Independence between tree parameters and boosting parameters suggests a two-stage procedure where we first find optimal tree parameters, then we maximize performance by pushing boosting parameters to the extreme. The procedure is:</p>
<ol type="1">
<li><strong>Tune tree parameters.</strong> Fix the learning rate at a relatively high value (like 0.3ish) and enable early stopping so that each model trains within a few seconds. Use your favorite hyperparameter tuning technique to find the optimal tree parameters.</li>
<li><strong>Tune boosting parameters.</strong> Using these optimal tree parameter values, fix the learning rate as low as you want and train your model, using early stopping to identify the optimal number of boosting rounds.</li>
</ol>
<p>Why is this a good idea? Because by starting with a high learning rate and early stopping enabled, you can burn through hundreds of model training trials and find some really good tree parameters in a few minutes. Then, with the confidence that your tree parameters are actually quite good, you can set a really low learning rate and boost a few thousand rounds to get a model with the best of both tree parameter and boosting parameter worlds.</p>
<blockquote class="blockquote">
<p>You can check out <a href="https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing">this notebook</a> where I justify this approach by running two parameter searches—one with high learning rate and one with low learning rate—showing that they recover the same optimal tree parameters.</p>
</blockquote>
</section>
<section id="tuning-xgboost-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="tuning-xgboost-parameters-with-optuna">Tuning XGBoost Parameters with Optuna</h2>
<p><a href="https://optuna.readthedocs.io/en/stable/">Optuna</a> is a model-agnostic python library for hyperparameter tuning. I like it because it has a flexible API that abstracts away the details of the search algorithm being used. That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more. Another massive benefit is that optuna provides a specific <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.XGBoostPruningCallback.html">XGBoost integration</a> which terminates training early on lousy parameter combinations.</p>
<p>You can install optuna with anaconda, e.g.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode .zsh code-with-copy"><code class="sourceCode zsh"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> conda install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge optuna</span></code></pre></div>
</section>
<section id="example-tuning-the-bluebook-for-bulldozers-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="example-tuning-the-bluebook-for-bulldozers-regression-model">Example: Tuning the Bluebook for Bulldozers Regression Model</h2>
<p>To illustrate the procedure, we’ll tune the parameters for the regression model we built back in the <a href="../../posts/xgboost-for-regression-in-python/">XGBoost for regression</a> post. First we’ll load up the bulldozer data and prepare the features and target just like we did before.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time </span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb3-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_squared_error</span>
<span id="cb3-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb</span>
<span id="cb3-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> optuna </span>
<span id="cb3-8"></span>
<span id="cb3-9">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../xgboost-for-regression-in-python/Train.csv'</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb3-10"></span>
<span id="cb3-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> encode_string_features(df):</span>
<span id="cb3-12">    out_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb3-13">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> feature, feature_type <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> df.dtypes.items():</span>
<span id="cb3-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> feature_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'object'</span>:</span>
<span id="cb3-15">            out_df[feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> out_df[feature].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb3-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> out_df</span>
<span id="cb3-17"></span>
<span id="cb3-18">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encode_string_features(df)</span>
<span id="cb3-19"></span>
<span id="cb3-20">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb3-21">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pd.Timestamp(year<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1970</span>, month<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, day<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-22">    ).dt.days</span>
<span id="cb3-23"></span>
<span id="cb3-24">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>])</span>
<span id="cb3-25"></span>
<span id="cb3-26"></span>
<span id="cb3-27">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb3-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,</span>
<span id="cb3-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>,</span>
<span id="cb3-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ModelID'</span>,</span>
<span id="cb3-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datasource'</span>,</span>
<span id="cb3-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auctioneerID'</span>,</span>
<span id="cb3-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>,</span>
<span id="cb3-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineHoursCurrentMeter'</span>,</span>
<span id="cb3-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'UsageBand'</span>,</span>
<span id="cb3-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDesc'</span>,</span>
<span id="cb3-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>,</span>
<span id="cb3-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiSecondaryDesc'</span>,</span>
<span id="cb3-39">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelSeries'</span>,</span>
<span id="cb3-40">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>,</span>
<span id="cb3-41">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>,</span>
<span id="cb3-42">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiProductClassDesc'</span>,</span>
<span id="cb3-43">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'state'</span>,</span>
<span id="cb3-44">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroup'</span>,</span>
<span id="cb3-45">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>,</span>
<span id="cb3-46">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Drive_System'</span>,</span>
<span id="cb3-47">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span>,</span>
<span id="cb3-48">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Forks'</span>,</span>
<span id="cb3-49">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pad_Type'</span>,</span>
<span id="cb3-50">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ride_Control'</span>,</span>
<span id="cb3-51">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick'</span>,</span>
<span id="cb3-52">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Transmission'</span>,</span>
<span id="cb3-53">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Turbocharged'</span>,</span>
<span id="cb3-54">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Extension'</span>,</span>
<span id="cb3-55">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Width'</span>,</span>
<span id="cb3-56">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure_Type'</span>,</span>
<span id="cb3-57">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Engine_Horsepower'</span>,</span>
<span id="cb3-58">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics'</span>,</span>
<span id="cb3-59">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pushblock'</span>,</span>
<span id="cb3-60">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ripper'</span>,</span>
<span id="cb3-61">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Scarifier'</span>,</span>
<span id="cb3-62">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tip_Control'</span>,</span>
<span id="cb3-63">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tire_Size'</span>,</span>
<span id="cb3-64">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler'</span>,</span>
<span id="cb3-65">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler_System'</span>,</span>
<span id="cb3-66">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>,</span>
<span id="cb3-67">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics_Flow'</span>,</span>
<span id="cb3-68">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Track_Type'</span>,</span>
<span id="cb3-69">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Undercarriage_Pad_Width'</span>,</span>
<span id="cb3-70">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick_Length'</span>,</span>
<span id="cb3-71">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Thumb'</span>,</span>
<span id="cb3-72">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pattern_Changer'</span>,</span>
<span id="cb3-73">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Type'</span>,</span>
<span id="cb3-74">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Backhoe_Mounting'</span>,</span>
<span id="cb3-75">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Type'</span>,</span>
<span id="cb3-76">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Travel_Controls'</span>,</span>
<span id="cb3-77">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Differential_Type'</span>,</span>
<span id="cb3-78">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Steering_Controls'</span>,</span>
<span id="cb3-79">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span></span>
<span id="cb3-80"> ]</span>
<span id="cb3-81"></span>
<span id="cb3-82">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span></span></code></pre></div>
</details>
</div>
<p>But this time, since we’re going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes. We make four different <code>xgboost.DMatrix</code> datasets for this process: training, validation, training+validation, and test. Training and validation are for the parameter search, and training+validation and test are for the final model.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-2">n_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb4-3"></span>
<span id="cb4-4">sorted_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>)</span>
<span id="cb4-5">train_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test)] </span>
<span id="cb4-6">valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>(n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> n_test):<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test] </span>
<span id="cb4-7">test_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sorted_df[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_test:]</span>
<span id="cb4-8"></span>
<span id="cb4-9">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], </span>
<span id="cb4-10">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-11">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], </span>
<span id="cb4-12">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-13">dtest <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>test_df[target], </span>
<span id="cb4-14">                    enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb4-15">dtrainvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[features], </span>
<span id="cb4-16">                          label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pd.concat([train_df, valid_df])[target], </span>
<span id="cb4-17">                          enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
</section>
<section id="preliminaries-base-parameters-and-scoring-function" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries-base-parameters-and-scoring-function">Preliminaries: base parameters and scoring function</h2>
<p>We’ll go ahead and set a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping. We’ll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">metric <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span></span>
<span id="cb5-2">base_params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb5-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg:squarederror'</span>,</span>
<span id="cb5-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'eval_metric'</span>: metric,</span>
<span id="cb5-5">}</span></code></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> score_model(model: xgb.core.Booster, dmat: xgb.core.DMatrix) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>:</span>
<span id="cb6-2">    y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dmat.get_label() </span>
<span id="cb6-3">    y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(dmat) </span>
<span id="cb6-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> mean_squared_error(y_true, y_pred, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
</section>
<section id="stage-1-tune-tree-parameters-with-optuna" class="level2">
<h2 class="anchored" data-anchor-id="stage-1-tune-tree-parameters-with-optuna">Stage 1: Tune Tree Parameters with Optuna</h2>
<p>Next we need to choose a fixed learning rate and tune the tree parameters. We want a learning rate that allows us to train within a few seconds, so we need to time model training. Start with a high learning rate (like 0.8) and work down until you find a rate that takes a few seconds. Below I end up landing at 0.3, which takes about 4 seconds to train on my little laptop.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span></span>
<span id="cb7-2"></span>
<span id="cb7-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate</span>
<span id="cb7-6">}</span>
<span id="cb7-7">params.update(base_params)</span>
<span id="cb7-8">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb7-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain,</span>
<span id="cb7-10">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb7-11">                  num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb7-12">                  early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb7-13">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb7-14"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>time<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> seconds'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>4.5 seconds</code></pre>
</div>
</div>
<p>Then we implement our optuna objective, a function taking an optuna study <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html"><code>Trial</code></a> object and returning the score we want to optimize. We use the <code>suggest_categorical</code>, <code>suggest_float</code>, and <code>suggest_int</code> methods of the <code>Trial</code> object to define the search space for each parameter. Note the use of the pruning callback function which we pass into the <code>callback</code> argument of the XGBoost <code>train</code> function; this is a must, since it allows optuna to terminate training on lousy models after a few boosting rounds. After training a model with the selected parameter values, we stash the optimal number of boosting rounds from early stopping into an optuna user attribute using the <a href="https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/003_attributes.html"><code>trial.user_attrs()</code></a> method. Finally we return the score computed by our <code>model_score</code> function.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> objective(trial):</span>
<span id="cb9-2">    params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb9-3">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: trial.suggest_categorical(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist'</span>]),</span>
<span id="cb9-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>),</span>
<span id="cb9-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: trial.suggest_int(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">250</span>),</span>
<span id="cb9-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>),</span>
<span id="cb9-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>: trial.suggest_float(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg_lambda'</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.001</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>, log<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>),</span>
<span id="cb9-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: learning_rate,</span>
<span id="cb9-10">    }</span>
<span id="cb9-11">    num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb9-12">    params.update(base_params)</span>
<span id="cb9-13">    pruning_callback <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.integration.XGBoostPruningCallback(trial, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'valid-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>metric<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb9-14">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb9-15">                      evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb9-16">                      early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb9-17">                      verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,</span>
<span id="cb9-18">                      callbacks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[pruning_callback])</span>
<span id="cb9-19">    trial.set_user_attr(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best_iteration'</span>, model.best_iteration)</span>
<span id="cb9-20">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> model.best_score</span></code></pre></div>
</div>
<p>To create a new optuna study and search through 50 parameter combinations, you could just run these two lines.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>)</span>
<span id="cb10-2">study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span></code></pre></div>
<p>But, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials—who knows how long 50 trials will take. I also want the results to be reproducible. So, to set the random seed and run the optimization for around 300 seconds (long enough to go make a nice cup of tea, stretch, and come back), I do something like this:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">sampler <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.samplers.TPESampler(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb11-2">study <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optuna.create_study(direction<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'minimize'</span>, sampler<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sampler)</span>
<span id="cb11-3">tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb11-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">while</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> tic <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>:</span>
<span id="cb11-5">    study.optimize(objective, n_trials<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 1 =============================='</span>)</span>
<span id="cb12-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>value<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb12-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>learning_rate<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>study<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_trial<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>user_attrs[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"best_iteration"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb12-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'best tree params --------------------------'</span>)</span>
<span id="cb12-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> study.best_trial.params.items():</span>
<span id="cb12-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 1 ==============================
best score = 0.23107522766919256
boosting params ---------------------------
fixed learning rate: 0.3
best boosting round: 23
best tree params --------------------------
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469</code></pre>
</div>
</div>
<p>If we decide we want to tune the tree parameters a little more, we can just call <code>study.optimize(...)</code> again, adding as many trials as we want to the study. Once we’re happy with the tree parameters, we can proceed to stage 2.</p>
</section>
<section id="stage-2-intensify-the-boosting-parameters" class="level2">
<h2 class="anchored" data-anchor-id="stage-2-intensify-the-boosting-parameters">Stage 2: Intensify the Boosting Parameters</h2>
<p>Now we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate; here I use 0.01, but you could go lower. The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you’ll need to max out the evaluation metric on the validation data.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">low_learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span></span>
<span id="cb14-2"></span>
<span id="cb14-3">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb14-4">params.update(base_params)</span>
<span id="cb14-5">params.update(study.best_trial.params)</span>
<span id="cb14-6">params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> low_learning_rate</span>
<span id="cb14-7">model_stage2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, </span>
<span id="cb14-8">                         num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>,</span>
<span id="cb14-9">                         evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb14-10">                         early_stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb14-11">                         verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stage 2 =============================='</span>)</span>
<span id="cb15-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_stage2, dvalid)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting params ---------------------------'</span>)</span>
<span id="cb15-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'fixed learning rate: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb15-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'best boosting round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stage 2 ==============================
best score = 0.22172991931438446
boosting params ---------------------------
fixed learning rate: 0.01
best boosting round: 1446</code></pre>
</div>
</div>
</section>
<section id="train-and-evaluate-the-final-model" class="level2">
<h2 class="anchored" data-anchor-id="train-and-evaluate-the-final-model">Train and Evaluate the Final Model</h2>
<p>Now we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2. Then we evaluate on the held out test data.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">model_final <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrainvalid, </span>
<span id="cb17-2">                        num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model_stage2.best_iteration,</span>
<span id="cb17-3">                        verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Final Model =========================='</span>)</span>
<span id="cb18-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'test score = </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score_model(model_final, dtest)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb18-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'parameters ---------------------------'</span>)</span>
<span id="cb18-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k, v <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> params.items():</span>
<span id="cb18-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(k, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, v)</span>
<span id="cb18-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'num_boost_round: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>model_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>best_iteration<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Final Model ==========================
test score = 0.21621863543987274
parameters ---------------------------
objective : reg:squarederror
eval_metric : rmse
tree_method : approx
max_depth : 10
min_child_weight : 6
subsample : 0.9729188669457949
colsample_bynode : 0.8491983767203796
reg_lambda : 0.008587261143813469
learning_rate : 0.01
num_boost_round: 1446</code></pre>
</div>
</div>
<p>Back in the <a href="../../posts/xgboost-for-regression-in-python/">regression post</a> we got an RMSE of about 0.231 just using default parameter values, which put us in 5th place on the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard">leaderboard for the Kagle dozers competition</a>. Now with about 10 minutes of hyperparameter tuning, our RMSE is down to 0.216 which puts us in 1st place by a huge margin. 🙌</p>
</section>
<section id="what-could-possibly-go-wrong" class="level2">
<h2 class="anchored" data-anchor-id="what-could-possibly-go-wrong">What could possibly go wrong?</h2>
<p>Hyperparameter tuning can easily be overlooked in the move-fast-and-break-everything hustle of building an ML product, but it can also easily become overkill or even downright harmful, depending on the application. There are three key questions to ask:</p>
<ol type="1">
<li>How much value is created by an incremental gain in model prediction accuracy?</li>
<li>What is the cost of increasing model prediction accuracy?</li>
<li>Is my model answering the right question?</li>
</ol>
<p>Sometimes a small gain in model prediction performance translates into millions of dollars of impact. The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org’s KPIs, and get mad respect, bonuses, and promoted. But the reality is that often additional model accuracy doesn’t really change business KPIs by very much. Try to figure out the actual value of improved model accuracy and proceed accordingly.</p>
<p>Remember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself. It can also lead us to larger and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.</p>
<p>Worst of all and quite counterintuitively, it’s possible that improving a model’s prediction accuracy can compromise overall business KPIs. I’ve seen this with my own eyes at work; offline testing shows that hyperparameter tuning significantly improves a model’s prediction accuracy, but when the model goes into production, an AB test shows that the business KPIs are actually worse. What happened? In this case, the model’s prediction was being used indirectly to infer the relationship between one of the features and the prediction target to inform automatic business decisions. Answering questions about how changing an input will affect an output requires causal reasoning, and <a href="https://arxiv.org/abs/1608.00060">traditional ML models are not the right tool for the job</a>. I’ll have a lot more to say about that soon; let this story foreshadow an epic new epoch on Random Realizations….</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna. If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/index.html</guid>
  <pubDate>Tue, 26 Dec 2023 05:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-parameter-tuning-with-optuna/optuna_thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>XGBoost for Binary and Multi-Class Classification in Python</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-for-classification-in-python/index.html</link>
  <description><![CDATA[ 



<p>Today we continue the <a href="../../gradient-boosting-series.html">saga on gradient boosting</a> with a down-to-Earth tutorial on the essentials of solving classification problems with XGBoost. We’ll run through two examples: one for binary classification and another for multi-class classification. In both cases I’ll show you how to train XGBoost models using either the scikit-learn interface or the native xgboost training API. Once trained, we’ll evaluate the models with validation data then inspect them with feature importance and partial dependence plots. You can use the XGBoost classification notebook in my <a href="https://github.com/mcb00/ds-templates">ds-templates repository</a> to follow along with your own dataset.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-classification-main.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Afternoon in the Mara</figcaption>
</figure>
</div>
<section id="preparing-data-for-xgboost-classifier" class="level2">
<h2 class="anchored" data-anchor-id="preparing-data-for-xgboost-classifier">Preparing Data for XGBoost Classifier</h2>
<p>Our dataset must satisfy two requirements to be used in an XGBoost classifier. First all feature data must be numeric—no strings and no datetimes; if you have non-numeric features, you need to <a href="../../posts/xgboost-for-regression-in-python/#prepare-raw-data-for-xgboost">transform your feature data</a>. Second, the target must be integer encoded using <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D"> for binary targets and <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1,%5Cdots,K%5C%7D"> for multiclass targets. Note that if your data is encoded to positive integers (no 0 class) XGBoost will throw potentially cryptic errors. You can use the scikit-learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"><code>LabelEncoder</code></a> (which we’ll do below) to generate a valid target encoding.</p>
</section>
<section id="xgboost-training-apis" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-training-apis">XGBoost Training APIs</h2>
<p>The <code>xgboost</code> python library offers two API’s for training classification models: the native <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training"><code>train</code></a> function and a wrapper class called <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn"><code>XGBClassifier</code></a>, which offers an API consistent with the scikit-learn universe. I’ll show you how to use both approaches in the examples below, but if you’re planning to use other utilities from scikit-learn, you might find the <code>XGBClassifier</code> approach to be more convenient, since the trained model object will generally play nice with sklearn functionality.</p>
</section>
<section id="binary-classification-example" class="level2">
<h2 class="anchored" data-anchor-id="binary-classification-example">Binary Classification Example</h2>
<section id="breast-cancer-wisconsin-dataset" class="level3">
<h3 class="anchored" data-anchor-id="breast-cancer-wisconsin-dataset">Breast Cancer Wisconsin Dataset</h3>
<p>We’ll demonstrate binary classification in XGBoost using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer">breast cancer wisconsin data</a>, one of scikit-learn’s built-in toy datasets. This is a tiny dataset with 569 observations of 30 features and a binary target representing whether samples are malignant or benign..</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np </span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt </span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datasets</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb </span>
<span id="cb1-6"></span>
<span id="cb1-7">dbunch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.load_breast_cancer(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb1-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.frame</span>
<span id="cb1-9">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.feature_names </span>
<span id="cb1-10">target_names <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.target_names </span>
<span id="cb1-11">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span> </span>
<span id="cb1-12">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 569 entries, 0 to 568
Data columns (total 31 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   mean radius              569 non-null    float64
 1   mean texture             569 non-null    float64
 2   mean perimeter           569 non-null    float64
 3   mean area                569 non-null    float64
 4   mean smoothness          569 non-null    float64
 5   mean compactness         569 non-null    float64
 6   mean concavity           569 non-null    float64
 7   mean concave points      569 non-null    float64
 8   mean symmetry            569 non-null    float64
 9   mean fractal dimension   569 non-null    float64
 10  radius error             569 non-null    float64
 11  texture error            569 non-null    float64
 12  perimeter error          569 non-null    float64
 13  area error               569 non-null    float64
 14  smoothness error         569 non-null    float64
 15  compactness error        569 non-null    float64
 16  concavity error          569 non-null    float64
 17  concave points error     569 non-null    float64
 18  symmetry error           569 non-null    float64
 19  fractal dimension error  569 non-null    float64
 20  worst radius             569 non-null    float64
 21  worst texture            569 non-null    float64
 22  worst perimeter          569 non-null    float64
 23  worst area               569 non-null    float64
 24  worst smoothness         569 non-null    float64
 25  worst compactness        569 non-null    float64
 26  worst concavity          569 non-null    float64
 27  worst concave points     569 non-null    float64
 28  worst symmetry           569 non-null    float64
 29  worst fractal dimension  569 non-null    float64
 30  target                   569 non-null    int64  
dtypes: float64(30), int64(1)
memory usage: 137.9 KB</code></pre>
</div>
</div>
<p>In this dataset, the features are all numeric, so no need to do preprocessing before passing to XGBoost. Below we’ll have a look at the target to ensure it’s encoded in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1%5C%7D"> and to check the class balance.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(df[target].unique())</span>
<span id="cb3-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(target_names)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1]
['malignant' 'benign']</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df.target.value_counts().sort_index().plot.bar()</span>
<span id="cb5-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'target'</span>) </span>
<span id="cb5-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img" alt="bar plot showing the count of observations in each class"></p>
<figcaption class="figure-caption">class counts for the breast cancer dataset</figcaption>
</figure>
</div>
</div>
</div>
<p>Next We randomly split data into train and validation sets.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb6-2"></span>
<span id="cb6-3">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span> </span>
<span id="cb6-4"></span>
<span id="cb6-5">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_valid, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb6-6">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>((519, 31), (50, 31))</code></pre>
</div>
</div>
</section>
<section id="training-with-the-train-function" class="level3">
<h3 class="anchored" data-anchor-id="training-with-the-train-function">Training with the <code>train</code> function</h3>
<p>We need to set a couple of <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">model parameters</a>, most notably <code>objective</code>, which should be set to <code>binary:logistic</code> for binary classification. I also prefer to explicitly set <code>tree_method</code> to something other than its default of <code>auto</code>; usually I’ll start with <code>exact</code> on small datasets or <code>approx</code> on larger ones. Note also that The <code>train</code> function expects to receive data as <code>DMatrix</code> objects, not pandas dataframes, so we need to create dense matrix objects as well.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb8-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'exact'</span>,</span>
<span id="cb8-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary:logistic'</span>,</span>
<span id="cb8-4">}</span>
<span id="cb8-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb8-6"></span>
<span id="cb8-7">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features])</span>
<span id="cb8-8">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features])</span>
<span id="cb8-9">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb8-10">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb8-11">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-logloss:0.46232   valid-logloss:0.49033
[10]    train-logloss:0.04394   valid-logloss:0.13434
[20]    train-logloss:0.01515   valid-logloss:0.12193
[30]    train-logloss:0.00995   valid-logloss:0.11988
[40]    train-logloss:0.00766   valid-logloss:0.12416
[49]    train-logloss:0.00657   valid-logloss:0.12799</code></pre>
</div>
</div>
</section>
<section id="training-with-xgbclassifier" class="level3">
<h3 class="anchored" data-anchor-id="training-with-xgbclassifier">Training with <code>XGBClassifier</code></h3>
<p>The <code>XGBClassifier</code> takes dataframes or numpy arrays as input, so this time we don’t need to create those dense matrix objects.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb10-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'exact'</span>,</span>
<span id="cb10-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary:logistic'</span>,</span>
<span id="cb10-4">}</span>
<span id="cb10-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb10-6"></span>
<span id="cb10-7">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb10-8">clf.fit(train_df[features], train_df[target], </span>
<span id="cb10-9">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb10-10">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-logloss:0.46232    validation_1-logloss:0.49033
[10]    validation_0-logloss:0.04394    validation_1-logloss:0.13434
[20]    validation_0-logloss:0.01515    validation_1-logloss:0.12193
[30]    validation_0-logloss:0.00995    validation_1-logloss:0.11988
[40]    validation_0-logloss:0.00766    validation_1-logloss:0.12416
[49]    validation_0-logloss:0.00657    validation_1-logloss:0.12799</code></pre>
</div>
</div>
</section>
<section id="evaluating-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model">Evaluating the Model</h3>
<p>We’ll use the <code>sklearn.metrics</code> module to evaluate model performance on the held-out validation set. Have a look at the <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics">scikit-learn metrics for classification</a> for examples of other metrics to use.</p>
<p>One thing to watch out for when computing metrics is the difference between the actual labels (usually called <code>y_true</code>), the model’s predicted labels (usually called <code>y_pred</code>), and the models predicted probabilities (usually called <code>y_score</code>). If you’re using the <code>XGBClassifier</code> wrapper, you can get predicted labels with the <code>predict</code> method and predicted probabilities with the <code>predict_proba</code> method. Also note that whereas <code>predict</code> returns a vector of size (num data), <code>predict_proba</code> returns a vector of size (num data, num classes); thus for binary classification, we’ll take just the second column of the array which gives the probability of class 1.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_df[target]</span>
<span id="cb12-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict(valid_df[features])</span>
<span id="cb12-3">y_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict_proba(valid_df[features])[:,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
</div>
<p>Probably the simplest classification metric is accuracy, the proportion of labels we predicted correctly.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> metrics </span>
<span id="cb13-2"></span>
<span id="cb13-3">metrics.accuracy_score(y_true, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>0.96</code></pre>
</div>
</div>
<p>We can generate a classification report with several different metrics at once.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(metrics.classification_report(y_true, y_pred, target_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>target_names))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

   malignant       0.93      0.93      0.93        15
      benign       0.97      0.97      0.97        35

    accuracy                           0.96        50
   macro avg       0.95      0.95      0.95        50
weighted avg       0.96      0.96      0.96        50
</code></pre>
</div>
</div>
<p>And we can compute the AUC, a popular classification metric based on the ROC curve, which depends on the predicted probability rather than the predicted labels.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">metrics.roc_auc_score(y_true, y_score)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>0.9885714285714287</code></pre>
</div>
</div>
</section>
<section id="feature-importance" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance">Feature Importance</h3>
<p>Because of the <a href="../../posts/xgboost-for-regression-in-python/#feature-importance-for-xgboost">limitations of the built-in XGBoost feature importance metrics</a> I recommend that you use either <a href="https://scikit-learn.org/stable/modules/permutation_importance.html">permutation feature importance</a> or perhaps <a href="https://shap.readthedocs.io/en/latest/index.html">SHAP feature importance</a>.</p>
<p>Here we’ll compute the permutation feature importance, which tells us by how much the model’s performance changes when we scramble a particular feature’s values at prediction time. This reflects how much the model relies on each feature when making predictions.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> permutation_importance</span>
<span id="cb19-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_scorer</span>
<span id="cb19-3"></span>
<span id="cb19-4">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(metrics.log_loss, greater_is_better<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, needs_proba<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb19-5">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(clf, valid_df[features], valid_df[target], </span>
<span id="cb19-6">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb20-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb20-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb20-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'change in log likelihood'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption class="figure-caption">top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="partial-dependence" class="level2">
<h2 class="anchored" data-anchor-id="partial-dependence">Partial Dependence</h2>
<p>A <a href="https://scikit-learn.org/stable/modules/partial_dependence.html">partial dependence plot (PDP)</a> is a representation of the dependence between the model output and one or more feature variables. In binary classification, the model output is the probability of the so-called positive class, i.e.&nbsp;the class with encoded label 1, which corresponds to probability of “benign” in this example.. We can loosely interpret the partial dependence as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say “loosely” because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PartialDependenceDisplay</span>
<span id="cb21-2"></span>
<span id="cb21-3">PartialDependenceDisplay.from_estimator(clf, </span>
<span id="cb21-4">                                        valid_df[features], </span>
<span id="cb21-5">                                        [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'worst area'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'area error'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean area'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img" alt="line plots showing partial dependence of probability of benign"></p>
<figcaption class="figure-caption">PDP of target probability of benign vs three features</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="multi-class-classification-example" class="level2">
<h2 class="anchored" data-anchor-id="multi-class-classification-example">Multi-Class Classification Example</h2>
<section id="forest-cover-type-dataset" class="level3">
<h3 class="anchored" data-anchor-id="forest-cover-type-dataset">Forest Cover Type Dataset</h3>
<p>We’ll illustrate multi-class classification using the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype">scikit-learn forest cover type dataset</a>, which has around 580k observations of 54 features and a target with 7 classes.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">dbunch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.fetch_covtype(as_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb22-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.frame</span>
<span id="cb22-3">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dbunch.feature_names </span>
<span id="cb22-4">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 581012 entries, 0 to 581011
Data columns (total 55 columns):
 #   Column                              Non-Null Count   Dtype  
---  ------                              --------------   -----  
 0   Elevation                           581012 non-null  float64
 1   Aspect                              581012 non-null  float64
 2   Slope                               581012 non-null  float64
 3   Horizontal_Distance_To_Hydrology    581012 non-null  float64
 4   Vertical_Distance_To_Hydrology      581012 non-null  float64
 5   Horizontal_Distance_To_Roadways     581012 non-null  float64
 6   Hillshade_9am                       581012 non-null  float64
 7   Hillshade_Noon                      581012 non-null  float64
 8   Hillshade_3pm                       581012 non-null  float64
 9   Horizontal_Distance_To_Fire_Points  581012 non-null  float64
 10  Wilderness_Area_0                   581012 non-null  float64
 11  Wilderness_Area_1                   581012 non-null  float64
 12  Wilderness_Area_2                   581012 non-null  float64
 13  Wilderness_Area_3                   581012 non-null  float64
 14  Soil_Type_0                         581012 non-null  float64
 15  Soil_Type_1                         581012 non-null  float64
 16  Soil_Type_2                         581012 non-null  float64
 17  Soil_Type_3                         581012 non-null  float64
 18  Soil_Type_4                         581012 non-null  float64
 19  Soil_Type_5                         581012 non-null  float64
 20  Soil_Type_6                         581012 non-null  float64
 21  Soil_Type_7                         581012 non-null  float64
 22  Soil_Type_8                         581012 non-null  float64
 23  Soil_Type_9                         581012 non-null  float64
 24  Soil_Type_10                        581012 non-null  float64
 25  Soil_Type_11                        581012 non-null  float64
 26  Soil_Type_12                        581012 non-null  float64
 27  Soil_Type_13                        581012 non-null  float64
 28  Soil_Type_14                        581012 non-null  float64
 29  Soil_Type_15                        581012 non-null  float64
 30  Soil_Type_16                        581012 non-null  float64
 31  Soil_Type_17                        581012 non-null  float64
 32  Soil_Type_18                        581012 non-null  float64
 33  Soil_Type_19                        581012 non-null  float64
 34  Soil_Type_20                        581012 non-null  float64
 35  Soil_Type_21                        581012 non-null  float64
 36  Soil_Type_22                        581012 non-null  float64
 37  Soil_Type_23                        581012 non-null  float64
 38  Soil_Type_24                        581012 non-null  float64
 39  Soil_Type_25                        581012 non-null  float64
 40  Soil_Type_26                        581012 non-null  float64
 41  Soil_Type_27                        581012 non-null  float64
 42  Soil_Type_28                        581012 non-null  float64
 43  Soil_Type_29                        581012 non-null  float64
 44  Soil_Type_30                        581012 non-null  float64
 45  Soil_Type_31                        581012 non-null  float64
 46  Soil_Type_32                        581012 non-null  float64
 47  Soil_Type_33                        581012 non-null  float64
 48  Soil_Type_34                        581012 non-null  float64
 49  Soil_Type_35                        581012 non-null  float64
 50  Soil_Type_36                        581012 non-null  float64
 51  Soil_Type_37                        581012 non-null  float64
 52  Soil_Type_38                        581012 non-null  float64
 53  Soil_Type_39                        581012 non-null  float64
 54  Cover_Type                          581012 non-null  int32  
dtypes: float64(54), int32(1)
memory usage: 241.6 MB</code></pre>
</div>
</div>
<p>Here again the features are all numeric, so we don’t need to further preprocess them. Let’s have a look at the target.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cover_Type'</span>].value_counts().sort_index().plot.bar()</span>
<span id="cb24-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cover type'</span>) </span>
<span id="cb24-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'count'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img" alt="bar plot showing the count of observations in each class"></p>
<figcaption class="figure-caption">class counts for the forest cover type dataset</figcaption>
</figure>
</div>
</div>
</div>
<p>For multi-class classification, our target variable must take values in <img src="https://latex.codecogs.com/png.latex?%5C%7B0,1,%5Cdots,K%5C%7D">. However, from the histogram of the cover type above, we see that it takes values in <img src="https://latex.codecogs.com/png.latex?%5C%7B1,2,%5Cdots,7%5C%7D">. To fix this we can use the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">scikit-learn label encoder</a> to create a valid target column.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LabelEncoder </span>
<span id="cb25-2"></span>
<span id="cb25-3">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'encoded'</span></span>
<span id="cb25-4">enc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LabelEncoder()</span>
<span id="cb25-5">df[target] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> enc.fit_transform(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cover_Type'</span>])</span>
<span id="cb25-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(np.sort(df[target].unique()))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0 1 2 3 4 5 6]</code></pre>
</div>
</div>
<p>Then we can create training and validation sets.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span></span>
<span id="cb27-2"></span>
<span id="cb27-3">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(df, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_valid, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb27-4">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>((561012, 56), (20000, 56))</code></pre>
</div>
</div>
</section>
<section id="training-with-the-train-function-1" class="level3">
<h3 class="anchored" data-anchor-id="training-with-the-train-function-1">Training with the <code>train</code> function</h3>
<p>If you’re training with the <code>train</code> function, multi-class classification can be done with two objectives: <code>multi:softmax</code> and <code>multi:softprob</code>. Both use the same loss function—negative multinomial log likelihood—but the softmax option produces a trained <code>Booster</code> object whose predict method returns a 1d array of predicted labels, whereas the softprob option produces a trained <code>Booster</code> object whose predict method returns a 2d array of predicted probabilities. In either case, you also need to explicitly tell XGBoost how many classes the target has with the <code>num_class</code> parameter.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb29-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb29-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'multi:softprob'</span>,</span>
<span id="cb29-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'num_class'</span>: df[target].nunique()</span>
<span id="cb29-5">}</span>
<span id="cb29-6">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb29-7"></span>
<span id="cb29-8">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features])</span>
<span id="cb29-9">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features])</span>
<span id="cb29-10">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb29-11">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb29-12">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-mlogloss:1.42032  valid-mlogloss:1.42366
[2] train-mlogloss:1.00541  valid-mlogloss:1.00963
[4] train-mlogloss:0.80557  valid-mlogloss:0.81109
[6] train-mlogloss:0.69432  valid-mlogloss:0.70085
[8] train-mlogloss:0.62653  valid-mlogloss:0.63350
[9] train-mlogloss:0.60111  valid-mlogloss:0.60794</code></pre>
</div>
</div>
</section>
<section id="training-with-xgbclassifier-1" class="level3">
<h3 class="anchored" data-anchor-id="training-with-xgbclassifier-1">Training with <code>XGBClassifier</code></h3>
<p>In multi-class classification, I think the scikit-learn <code>XGBClassifier</code> wrapper is quite a bit more convenient than the native <code>train</code> function. You can set the <code>objective</code> parameter to <code>multi:softprob</code>, and <code>XGBClassifier.fit</code> will produce a model having both <code>predict</code> and <code>predict_proba</code> methods. Also there is no need to explicitly set the number of classes in the target and no need to create the <code>DMatrix</code> objects.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb31-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb31-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'multi:softprob'</span>,</span>
<span id="cb31-4">}</span>
<span id="cb31-5">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb31-6"></span>
<span id="cb31-7">clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb31-8">clf.fit(train_df[features], train_df[target], </span>
<span id="cb31-9">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb31-10">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-mlogloss:1.42032   validation_1-mlogloss:1.42366
[2] validation_0-mlogloss:1.00541   validation_1-mlogloss:1.00963
[4] validation_0-mlogloss:0.80557   validation_1-mlogloss:0.81109
[6] validation_0-mlogloss:0.69432   validation_1-mlogloss:0.70085
[8] validation_0-mlogloss:0.62653   validation_1-mlogloss:0.63350
[9] validation_0-mlogloss:0.60111   validation_1-mlogloss:0.60794</code></pre>
</div>
</div>
</section>
<section id="evaluating-the-model-1" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model-1">Evaluating the Model</h3>
<p>This time, we’ll keep the entire 2d array of predicted probabilities in <code>y_score</code>.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> valid_df[target]</span>
<span id="cb33-2">y_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict(valid_df[features])</span>
<span id="cb33-3">y_score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> clf.predict_proba(valid_df[features])</span>
<span id="cb33-4">y_true.shape, y_pred.shape, y_score.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>((20000,), (20000,), (20000, 7))</code></pre>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">metrics.accuracy_score(y_true, y_pred)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>0.77425</code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(metrics.classification_report(y_true, y_pred))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              precision    recall  f1-score   support

           0       0.77      0.74      0.75      7365
           1       0.78      0.84      0.81      9725
           2       0.75      0.85      0.80      1207
           3       0.82      0.78      0.80        85
           4       0.93      0.26      0.40       317
           5       0.76      0.31      0.44       627
           6       0.88      0.68      0.77       674

    accuracy                           0.77     20000
   macro avg       0.81      0.64      0.68     20000
weighted avg       0.78      0.77      0.77     20000
</code></pre>
</div>
</div>
<p>Some binary classification metrics, like AUC, can be extended to the multi-class setting by computing the metric for each class, then averaging in some way to get an overall score. The details are controlled by the <code>average</code> and <code>multi_class</code> parameters, which are described in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">documentation</a>.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">metrics.roc_auc_score(y_true, y_score, average<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'weighted'</span>, multi_class<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ovr'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>0.9129422094408693</code></pre>
</div>
</div>
</section>
<section id="feature-importance-1" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-1">Feature Importance</h3>
<p>We can compute permutation feature importance with exactly the same code that we used for the binary classifier.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(metrics.log_loss, greater_is_better<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, needs_proba<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb41-2">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(clf, valid_df[features], valid_df[target], </span>
<span id="cb41-3">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb42-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb42-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb42-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'change in multivariate log likelihood'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption class="figure-caption">top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="partial-dependence-1" class="level3">
<h3 class="anchored" data-anchor-id="partial-dependence-1">Partial Dependence</h3>
<p>Recall that partial dependence reflects how the expected model output changes with a particular feature. In the multi-class setting, the model has multiple outputs—one probability for each class—so we need to choose which class probability to show in the plots. We choose the target class with the <code>target</code> parameter; be sure to pass in the encoded value, e.g.&nbsp;we need to use the label encoder to transform a raw class label back into the encoded value. Here we’ll examine partial dependence for the probability of cover type 3.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">PartialDependenceDisplay.from_estimator(clf, </span>
<span id="cb43-2">                                        X<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], </span>
<span id="cb43-3">                                        features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Elevation'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Horizontal_Distance_To_Roadways'</span>], </span>
<span id="cb43-4">                                        target<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>enc.transform([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-for-classification-in-python_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img" alt="line plots showing partial dependence of probability of cover type 3 vs two features"></p>
<figcaption class="figure-caption">PDP of target probability of cover type == 3 vs elevation and distance to roadway</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, for me, those are really the minimal nuts and bolts one needs to get XGBoost models working on classification problems. If you dig this tutorial, or if you have additional insights into using XGBoost to solve classification problems, let me know about it down in the comments!</p>
</section>
<section id="go-deeper" class="level2">
<h2 class="anchored" data-anchor-id="go-deeper">Go Deeper</h2>
<p>If you’re feeling like Alice, and you want to go tumbling down the rabbit hole, might I recommend checking out some of the following:</p>
<ul>
<li><a href="../../posts/xgboost-explained/">XGBoost Explained</a> - for a deep dive into the math</li>
<li><a href="../../posts/xgboost-from-scratch/">XGBoost from Scratch</a> - to see how to implement all those equations in code</li>
<li><a href="../../posts/gradient-boosting-multi-class-classification-from-scratch/">Multi-Class Gradient Boosting from Scratch</a> - to fully grok the multi-class gradient boosting algorithm</li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-for-classification-in-python/index.html</guid>
  <pubDate>Tue, 28 Nov 2023 05:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-for-classification-in-python/xgboost-classification-thumbnail.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Gradient Boosting Multi-Class Classification from Scratch</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/index.html</link>
  <description><![CDATA[ 



<p>Tell me dear reader, who among us, while gazing in wonder at the improbably verdant aloe vera clinging to the windswept rock at Cape Point near the southern tip of Africa, hasn’t wondered: how the heck do gradient boosting trees implement multi-class classification? Today, we’ll unravel this mystery by reviewing the theory and implementing the algorithm for ourselves in python. Specifically, we’ll review the multi-class gradient boosting model originally described in <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Friedman’s classic Greedy Function Approximation paper</a>, and we’ll implement components of the algorithm as we go along. Once we have all the pieces, we’ll write a python class for multi-class gradient boosting with a similar API to the scikit-learn <code>GradientBoostingClassifier</code>.</p>
<p>If you need a refresher on gradient boosting before diving in here, then start with my original <a href="../../posts/gradient-boosting-machine-from-scratch/">gradient boosting from scratch post</a>, which is the first installment in my ongoing <a href="../../gradient-boosting-series/">series on gradient boosting</a>.</p>
<section id="the-multi-class-gradient-boosting-classification-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="the-multi-class-gradient-boosting-classification-algorithm">The multi-class gradient boosting classification algorithm</h2>
<p>Friedman describes the algorithm for training a multi-class classification gradient boosting model in Algorithm 6 of the <a href="(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)">classic Greedy Function Approximation paper</a>. If you want a step-by-step walkthrough of the ideas in the paper, have a look at my post on <a href="../../posts/gradient-boosting-machine-with-any-loss-function/">the generalized gradient boosting algorithm</a>. In high-level terms, the algorithm for multi-class gradient boosting is:</p>
<ol type="1">
<li><p>Set the initial model predictions.</p></li>
<li><p>Repeat the following for each boosting round.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; Repeat the following for each class.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Compute the pseudo residuals.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Train a regression tree to predict the pseudo residuals.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Adjust the tree’s predicted values to optimize the objective function.</p></li>
<li><p>&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Add the new tree to the current composite model.</p></li>
</ol>
<p>Let’s take a look at the details for each of these steps.</p>
</section>
<section id="target-variable-encoding" class="level2">
<h2 class="anchored" data-anchor-id="target-variable-encoding">Target variable encoding</h2>
<p>Following the convention in scikit-learn, when training a multi-class classifier, the target variable in the training dataset should be integer encoded so that the <img src="https://latex.codecogs.com/png.latex?K"> distinct classes are mapped to the integers <img src="https://latex.codecogs.com/png.latex?0,1,%5Cdots,K-1">. In the code for model training, however, it’s going to be more convenient to work with a one hot encoded representation of the target. Therefore we’ll start by writing an internal method to transform the target variable from integer encoding to one hot encoding. Remember that eventually we’ll write a class for our multi-class gradient boosting model, so I’ll write this function like a class method with a leading argument called <code>self</code>.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OneHotEncoder</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _one_hot_encode_labels(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y):</span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.values</span>
<span id="cb1-7">    ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OneHotEncoder()</span>
<span id="cb1-8">    y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ohe.fit_transform(y.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).toarray()</span>
<span id="cb1-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe</span></code></pre></div>
<p>This code takes the integer-encoded target variable, makes sure it’s a numpy array, then uses cikit-learn’s one hot encoder to encode it as a 2D array with observations along the first axis and classes along the second axis. I tend to think of the one hot encoded output as a matrix with <img src="https://latex.codecogs.com/png.latex?n"> rows (the number of observations in the training data) and <img src="https://latex.codecogs.com/png.latex?K"> columns (the number of classes), although it’s technically not a matrix but rather a 2D array.</p>
</section>
<section id="model-predictions-in-raw-space-and-probability-space" class="level2">
<h2 class="anchored" data-anchor-id="model-predictions-in-raw-space-and-probability-space">Model predictions in raw space and probability space</h2>
<p>In amulti-class classification problem with <img src="https://latex.codecogs.com/png.latex?K"> classes, the model prediction for a particular observation returns a list of <img src="https://latex.codecogs.com/png.latex?K"> probabilities, one for each class. Essentially the model prediction is a conditional probability mass function for the discrete target variable, conditioned on the feature values.</p>
<p>So, we need a way to ensure that the model output is a valid probability mass function, i.e.&nbsp;each probability is in (0, 1) and the <img src="https://latex.codecogs.com/png.latex?K"> class probabilities sum to 1. Analogous to logistic regression, we can accomplish this by using the model to first make a raw prediction which can be any real number, then using something like the inverse logit function to transform the raw model prediction into a number between 0 and 1 that can be interpreted as a probability. Again analogous to logistic regression, in the multi-class setting we use <img src="https://latex.codecogs.com/png.latex?K"> different models, one for each class, to generate the raw predictions, then we transform the raw model predictions into probabilities using the softmax function,, which takes a length-<img src="https://latex.codecogs.com/png.latex?K"> vector of real numbers as input and returns a probability mass function over <img src="https://latex.codecogs.com/png.latex?K"> discrete classes.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5C%7BF_1(%5Cmathbf%7Bx%7D),%5Cdots,F_K(%5Cmathbf%7Bx%7D)%5C%7D=%5C%7BF_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> be the list of <img src="https://latex.codecogs.com/png.latex?K"> raw model outputs, and let <img src="https://latex.codecogs.com/png.latex?%5C%7Bp_1(%5Cmathbf%7Bx%7D),%5Cdots,p_K(%5Cmathbf%7Bx%7D)%5C%7D=%5C%7Bp_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> be the corresponding probability mass function over the <img src="https://latex.codecogs.com/png.latex?K"> classes, then the softmax function is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20p_k(%5Cmathbf%7Bx%7D)%20=%20%5Ctext%7Bsoftmax%7D_k(%5C%7BF_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK)%0A%20%20%20%20=%20%5Cfrac%7Be%5E%7BF_k(%5Cmathbf%7Bx%7D)%7D%7D%7B%5Csum_%7Bl=1%7D%5EK%20e%5E%7BF_l(%5Cmathbf%7Bx%7D)%7D%7D"></p>
<pre><code>Let's implement an internal softmax method that transforms the raw predictions into probabilities.</code></pre>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _softmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, raw_predictions):</span>
<span id="cb3-2">    numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(raw_predictions) </span>
<span id="cb3-3">    denominator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(raw_predictions), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> denominator</span></code></pre></div>
</section>
<section id="initial-model-predictions" class="level2">
<h2 class="anchored" data-anchor-id="initial-model-predictions">Initial model predictions</h2>
<p>We’re now ready to implement model training, starting with line 1 of the algorithm which sets the initial model predictions. In our code, we’ll keep the raw model predictions <img src="https://latex.codecogs.com/png.latex?%5C%7BF_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> for the <img src="https://latex.codecogs.com/png.latex?n"> observations in the training dataset in a size <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20K"> array called <code>raw_predictions</code>, and we’ll keep the corresponding probabilities <img src="https://latex.codecogs.com/png.latex?%5C%7Bp_k(%5Cmathbf%7Bx%7D)%5C%7D_1%5EK"> in another <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20K"> array called <code>probabilities</code>. Perhaps the simplest reasonable initialization is to set the probabilities to <img src="https://latex.codecogs.com/png.latex?1/K">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?p_k(%5Cmathbf%7Bx%7D)=1/K">, which implies <img src="https://latex.codecogs.com/png.latex?F_k(%5Cmathbf%7Bx%7D)=0">.</p>
<p>We’ll go ahead and create that one hot encoded representation of the target, then use it to set the right size for the model prediction arrays.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._one_hot_encode_labels(y)</span>
<span id="cb4-2">raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_ohe.shape)</span>
<span id="cb4-3">probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span></code></pre></div>
</section>
<section id="boosting" class="level2">
<h2 class="anchored" data-anchor-id="boosting">Boosting</h2>
<p>Line 2 of the algorithm kicks off a loop to iteratively perform boosting rounds. Within each round, line 3 specifies that we iterate through each of the <img src="https://latex.codecogs.com/png.latex?K"> classes, adding a new booster model for each class at each boosting round. We’ll keep all the boosters in a list called <code>boosters</code>, where each element is itself a list which we’ll call <code>class_trees</code> that contains the <img src="https://latex.codecogs.com/png.latex?K"> trees we trained in a given boosting round. For each round and each class, we compute the pseudo residuals (negative gradients), train a decision tree to predict them, update the tree’s predicted values to optimize the overall objective function, then update the current raw and probability predictions before storing the new tree in that round’s list of class trees.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> m <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators):</span>
<span id="cb5-3">    class_trees <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb5-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb5-5">        negative_gradients <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._negative_gradients(y_ohe[:, k], probabilities[:, k])</span>
<span id="cb5-6">        hessians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._hessians(probabilities[:, k])</span>
<span id="cb5-7">        tree <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_depth)</span>
<span id="cb5-8">        tree.fit(X, negative_gradients)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb5-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._update_terminal_nodes(tree, X, negative_gradients, hessians)</span>
<span id="cb5-10">        raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> tree.predict(X)</span>
<span id="cb5-11">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb5-12">        class_trees.append(tree)</span>
<span id="cb5-13">    <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters.append(class_trees)</span></code></pre></div>
<p>Next we’ll dive into the details of the pseudo residual computation and the adjustment to the tree booster predicted values.</p>
</section>
<section id="pseudo-residuals" class="level2">
<h2 class="anchored" data-anchor-id="pseudo-residuals">Pseudo Residuals</h2>
<p>For each observation in the training dataset, the pseudo residual is the negative gradient of the objective function with respect to the corresponding model prediction. The objective function for multi-class classification is the Multinomial Negative Log Likelihood. For a single observation, the objective is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J(%5C%7B%20y_k,%20p_k(%5Cmathbf%7Bx%7D)%20%5C%7D_1%5EK)%20=%20-%5Csum_%7Bk=1%7D%5EK%20y_k%20%5Clog%20p_k(%5Cmathbf%7Bx%7D)%20"></p>
<p>We can rewrite the objective in terms of our raw model output <img src="https://latex.codecogs.com/png.latex?F"> like this.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J(%5C%7B%20y_k,%20F_k(%5Cmathbf%7Bx%7D)%20%5C%7D_1%5EK)%20=%20-%5Csum_%7Bk=1%7D%5EK%20y_k%20%5Clog%20%5Cfrac%7Be%5E%7BF_k(%5Cmathbf%7Bx%7D)%7D%7D%7B%5Csum_%7Bl=1%7D%5EK%20e%5E%7BF_l(%5Cmathbf%7Bx%7D)%7D%7D"></p>
<p>The negative gradient of the objective with respect to raw model prediction <img src="https://latex.codecogs.com/png.latex?F_k(%5Cmathbf%7Bx%7D_i)"> for training example <img src="https://latex.codecogs.com/png.latex?i"> is given by</p>
<p><img src="https://latex.codecogs.com/png.latex?%20r_%7Bik%7D%20=%20-J'(F_k(%5Cmathbf%7Bx%7D_i))%20=%20-%5Cleft%5B%20%5Cfrac%7B%5Cpartial%20J(%5C%7B%20y_%7Bil%7D,%20F_l(%5Cmathbf%7Bx_i%7D)%5C%7D_%7Bl=1%7D%5EK)%7D%7B%5Cpartial%20F_k(%5Cmathbf%7Bx%7D_i)%20%7D%20%5Cright%5D%0A=y_%7Bik%7D%20-%20p_%7Bk%7D(%5Cmathbf%7Bx%7D_i)"></p>
<p>You can take a look at the <a href="https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1">derivation</a> if you’re curious how to work it out yourself. Note that this formula has a nice intuition. When <img src="https://latex.codecogs.com/png.latex?y_%7Bik%7D=1">, if predicted probability <img src="https://latex.codecogs.com/png.latex?p_k(%5Cmathbf%7Bx%7D_i)"> is terrible and close to 0, then the pseudo residual will be positive, and the next boosting round will try to increase the predicted probability. Otherwise if the predicted probability is already good and close to 1, the pseudo residual will be close to 0 and the next boosting round won’t change the predicted probability very much.</p>
<p>We can easily implement an internal method to compute the negative gradients over the training dataset as follows.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _negative_gradients(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y_ohe, probabilities):</span>
<span id="cb6-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities</span></code></pre></div>
</section>
<section id="adjusting-the-trees-predicted-values" class="level2">
<h2 class="anchored" data-anchor-id="adjusting-the-trees-predicted-values">Adjusting the trees’ predicted values</h2>
<p>After training a regression tree to predict the pseudo residuals, we need to adjust the predicted values in its terminal nodes to optimize the overall objective function. In the Greedy Function Approximation paper, Friedman actually specifies finding the optimal value using a numerical optimization routine like line search. We could express that like</p>
<p><img src="https://latex.codecogs.com/png.latex?%20v%20=%20%5Ctext%7Bargmin%7D_v%20%5Csum_%7Bi%20%5Cin%20t%7D%20J(y_%7Bi%7D,%20F(%5Cmathbf%7Bx%7D_i)%20+%20v)%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?t"> is the set of samples falling into this terminal node.</p>
<p>In the scikit-learn implementation of gradient boosting classification, the authors instead use the approach from <a href="https://www.researchgate.net/publication/228776646_Additive_Logistic_Regression_A_Statistical_View_of_Boosting">FHT00</a> which uses a single Newton descent step to approximate the optimal predicted value for each terminal node. See code and comments for the function <code>_update_terminal_regions</code> in the scikit-learn gradient boosting module. The updated value is computed like</p>
<p><img src="https://latex.codecogs.com/png.latex?%20v%20=%20-%5Cfrac%7B%5Csum_%7Bi%20%5Cin%20t%7D%20J'(F(%5Cmathbf%7Bx%7D_i))%7D%7B%5Csum_%7Bi%20%5Cin%20t%7D%20J''(F(%5Cmathbf%7Bx%7D_i))%7D%20"></p>
<p>We already found the first derivative of the objective, so we just need to calculate the second derivative.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20J''(F_k(%5Cmathbf%7Bx%7D_i))%20=%0A%5Cleft%5B%20%5Cfrac%7B%5Cpartial%20J(%5C%7B%20y_%7Bil%7D,%20F_l(%5Cmathbf%7Bx_i%7D)%5C%7D_%7Bl=1%7D%5EK)%7D%7B%5Cpartial%20%5E2%20F_k(%5Cmathbf%7Bx%7D_i)%20%7D%20%5Cright%5D%0A=%20p_k(%5Cmathbf%7Bx%7D_i)%20(1%20-%20p_k(%5Cmathbf%7Bx%7D_i))%0A"></p>
<p>Here’s the internal method to compute the second derivative .</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _hessians(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, probabilities): </span>
<span id="cb7-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities)</span></code></pre></div>
<p>Then we can implement the internal method for updating the tree predicted values. I give more details about how to manually set scikit-learn’s decision tree predicted values in <a href="../../posts/gradient-boosting-machine-with-any-loss-function/">the post on gradient boosting with any loss function</a>.</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _update_terminal_nodes(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, tree, X, negative_gradients, hessians):</span>
<span id="cb8-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Update the terminal node predicted values'''</span></span>
<span id="cb8-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminal node id's</span></span>
<span id="cb8-4">    leaf_nodes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.nonzero(tree.tree_.children_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb8-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute leaf for each sample in ``X``.</span></span>
<span id="cb8-6">    leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tree.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(X)</span>
<span id="cb8-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> leaf <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> leaf_nodes:</span>
<span id="cb8-8">        samples_in_this_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> leaf)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb8-9">        negative_gradients_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> negative_gradients.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb8-10">        hessians_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hessians.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb8-11">        val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(negative_gradients_in_leaf) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(hessians_in_leaf)</span>
<span id="cb8-12">        tree.tree_.value[leaf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val</span></code></pre></div>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>At inference time, the user supplies an <code>X</code> with multiple observations of the feature variables, and our model needs to issue a prediction for each observation. We’ll start by implementing the <code>predict_proba</code> method, which takes <code>X</code> as input and returns a length-<img src="https://latex.codecogs.com/png.latex?K"> probability mass function for each observation in <code>X</code>. To do this, we’ll initialize the raw predictions with zeros, just as we did in training, and then for each class, we’ll loop through all the boosters, collecting their predictions on <code>X</code>, scaling by the learning rate, and summing them up. Finally, we use the softmax to transform raw predictions into the probabilities.</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict_proba(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb9-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate probability predictions for the given input data.'''</span></span>
<span id="cb9-3">    raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes))</span>
<span id="cb9-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb9-5">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> booster <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters:</span>
<span id="cb9-6">            raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> booster[k].predict(X)</span>
<span id="cb9-7">    probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb9-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities</span></code></pre></div>
<p>Then to get the predicted labels, we can use the <code>predict_proba</code> method to generate probabilities, simply returning the integer-encoded class label of the largest probability for each observation in <code>X</code>.</p>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb10-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate predicted labels (as integer-encoded array)'''</span></span>
<span id="cb10-3">    probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.predict_proba(X)</span>
<span id="cb10-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.argmax(probabilities, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</section>
<section id="the-complete-multi-class-gradient-boosting-classification-model-implementation" class="level2">
<h2 class="anchored" data-anchor-id="the-complete-multi-class-gradient-boosting-classification-model-implementation">The complete multi-class gradient boosting classification model implementation</h2>
<p>Now we’re ready to implement a multi-class classification gradient boosting model class with public <code>fit</code>, <code>predict_proba</code>, and <code>predict</code> methods. We combine the components above into a <code>fit</code> method for model training, and we add the two prediction methods to complete the model’s functionality.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd </span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.tree <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DecisionTreeRegressor </span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OneHotEncoder</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">class</span> GradientBoostingClassifierFromScratch():</span>
<span id="cb11-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Gradient Boosting Classifier from Scratch.</span></span>
<span id="cb11-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    </span></span>
<span id="cb11-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Parameters</span></span>
<span id="cb11-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    ----------</span></span>
<span id="cb11-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    n_estimators : int</span></span>
<span id="cb11-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        number of boosting rounds</span></span>
<span id="cb11-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb11-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    learning_rate : float</span></span>
<span id="cb11-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        learning rate hyperparameter</span></span>
<span id="cb11-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb11-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    max_depth : int</span></span>
<span id="cb11-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        maximum tree depth</span></span>
<span id="cb11-19"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    '''</span></span>
<span id="cb11-20">    </span>
<span id="cb11-21">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, n_estimators, learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb11-22">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> </span>
<span id="cb11-23">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>learning_rate</span>
<span id="cb11-24">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb11-25">    </span>
<span id="cb11-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> fit(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X, y):</span>
<span id="cb11-27">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Fit the GBM</span></span>
<span id="cb11-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        </span></span>
<span id="cb11-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Parameters</span></span>
<span id="cb11-30"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        ----------</span></span>
<span id="cb11-31"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        X : ndarray of size (number observations, number features)</span></span>
<span id="cb11-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            design matrix</span></span>
<span id="cb11-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            </span></span>
<span id="cb11-34"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        y : ndarray of size (number observations,)</span></span>
<span id="cb11-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">            integer-encoded target labels in {0,1,...,k-1}</span></span>
<span id="cb11-36"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        '''</span></span>
<span id="cb11-37">        </span>
<span id="cb11-38">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(y).nunique()</span>
<span id="cb11-39">        y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._one_hot_encode_labels(y)</span>
<span id="cb11-40"></span>
<span id="cb11-41">        raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_ohe.shape)</span>
<span id="cb11-42">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb11-43">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-44">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> m <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_estimators):</span>
<span id="cb11-45">            class_trees <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb11-46">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb11-47">                negative_gradients <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._negative_gradients(y_ohe[:, k], probabilities[:, k])</span>
<span id="cb11-48">                hessians <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._hessians(probabilities[:, k])</span>
<span id="cb11-49">                tree <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DecisionTreeRegressor(max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.max_depth)</span>
<span id="cb11-50">                tree.fit(X, negative_gradients)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb11-51">                <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._update_terminal_nodes(tree, X, negative_gradients, hessians)</span>
<span id="cb11-52">                raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> tree.predict(X)</span>
<span id="cb11-53">                probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb11-54">                class_trees.append(tree)</span>
<span id="cb11-55">            <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters.append(class_trees)</span>
<span id="cb11-56">    </span>
<span id="cb11-57">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _one_hot_encode_labels(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y):</span>
<span id="cb11-58">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">isinstance</span>(y, pd.Series): y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y.values</span>
<span id="cb11-59">        ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OneHotEncoder()</span>
<span id="cb11-60">        y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ohe.fit_transform(y.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)).toarray()</span>
<span id="cb11-61">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe</span>
<span id="cb11-62">        </span>
<span id="cb11-63">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _negative_gradients(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, y_ohe, probabilities):</span>
<span id="cb11-64">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> y_ohe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities</span>
<span id="cb11-65">    </span>
<span id="cb11-66">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _hessians(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, probabilities): </span>
<span id="cb11-67">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> probabilities)</span>
<span id="cb11-68"></span>
<span id="cb11-69">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _softmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, raw_predictions):</span>
<span id="cb11-70">        numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.exp(raw_predictions) </span>
<span id="cb11-71">        denominator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(raw_predictions), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb11-72">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> numerator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> denominator</span>
<span id="cb11-73">        </span>
<span id="cb11-74">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> _update_terminal_nodes(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, tree, X, negative_gradients, hessians):</span>
<span id="cb11-75">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Update the terminal node predicted values'''</span></span>
<span id="cb11-76">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># terminal node id's</span></span>
<span id="cb11-77">        leaf_nodes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.nonzero(tree.tree_.children_left <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb11-78">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># compute leaf for each sample in ``X``.</span></span>
<span id="cb11-79">        leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tree.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(X)</span>
<span id="cb11-80">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> leaf <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> leaf_nodes:</span>
<span id="cb11-81">            samples_in_this_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.where(leaf_node_for_each_sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> leaf)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb11-82">            negative_gradients_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> negative_gradients.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-83">            hessians_in_leaf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> hessians.take(samples_in_this_leaf, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb11-84">            val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(negative_gradients_in_leaf) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(hessians_in_leaf)</span>
<span id="cb11-85">            tree.tree_.value[leaf, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> val</span>
<span id="cb11-86">          </span>
<span id="cb11-87">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict_proba(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb11-88">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate probability predictions for the given input data.'''</span></span>
<span id="cb11-89">        raw_predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>  np.zeros(shape<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(X.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes))</span>
<span id="cb11-90">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.n_classes):</span>
<span id="cb11-91">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> booster <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.boosters:</span>
<span id="cb11-92">                raw_predictions[:, k] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.learning_rate <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> booster[k].predict(X)</span>
<span id="cb11-93">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>._softmax(raw_predictions)</span>
<span id="cb11-94">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> probabilities</span>
<span id="cb11-95">        </span>
<span id="cb11-96">    <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> predict(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, X):</span>
<span id="cb11-97">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">'''Generate predicted labels (as 1-d array)'''</span></span>
<span id="cb11-98">        probabilities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.predict_proba(X)</span>
<span id="cb11-99">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> np.argmax(probabilities, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
</section>
<section id="testing-our-implementation" class="level2">
<h2 class="anchored" data-anchor-id="testing-our-implementation">Testing our implementation</h2>
<p>Let’s test our implementation alongside the scikit-learn <code>GradientBoostingClassifier</code> to ensure it works as expected.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_classification</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split</span>
<span id="cb12-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> accuracy_score</span>
<span id="cb12-4"></span>
<span id="cb12-5">X, y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_classification(n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span>, </span>
<span id="cb12-6">                           n_classes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, </span>
<span id="cb12-7">                           n_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>,</span>
<span id="cb12-8">                           n_informative<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb12-9">                           random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb12-10"></span>
<span id="cb12-11">X_train, X_test, y_train, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.ensemble <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> GradientBoostingClassifier</span>
<span id="cb13-2"></span>
<span id="cb13-3">gbc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingClassifier(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, </span>
<span id="cb13-4">                                 learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb13-5">                                 max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb13-6">gbc.fit(X_train, y_train)</span>
<span id="cb13-7">accuracy_score(y_test, gbc.predict(X_test))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>0.7756</code></pre>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">gbcfs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> GradientBoostingClassifierFromScratch(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, </span>
<span id="cb15-2">                                              learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, </span>
<span id="cb15-3">                                              max_depth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb15-4">gbcfs.fit(X_train, y_train)</span>
<span id="cb15-5">accuracy_score(y_test, gbcfs.predict(X_test))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>0.7768</code></pre>
</div>
</div>
<p>Beautiful. Our implementation is performing comparably to the sklearn gradient boosting classifier!</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, there you have it, another epic scratch build for the books. I think the most interesting thing about the multi-class gradient boosting algorithm is that it generates multi-dimensional predictions based on a single objective function by training multiple decision trees in each boosting round. That’s a very interesting extension of the classic gradient boosting machine! If you have questions about the implementation, or if you found this post helpful, please leave a comment below to tell me about it.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="(https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)">Friedman’s Greedy Function Approximation paper</a></li>
<li><a href="https://www.researchgate.net/publication/228776646_Additive_Logistic_Regression_A_Statistical_View_of_Boosting">Friedman, Hastie, and Tibshirani 2000: paper on additive logistic regression</a></li>
</ul>
</section>

 ]]></description>
  <category>python</category>
  <category>gradient boosting</category>
  <category>from scratch</category>
  <guid>https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/index.html</guid>
  <pubDate>Sun, 15 Oct 2023 04:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/gradient-boosting-multi-class-classification-from-scratch/multi-class-classification-from-scratch.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>XGBoost for Regression in Python</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/xgboost-for-regression-in-python/index.html</link>
  <description><![CDATA[ 



<p>In this post I’m going to show you my process for solving regression problems with XGBoost in python, using either the native <code>xgboost</code> API or the scikit-learn interface. This is a powerful methodology that can produce world class results in a short time with minimal thought or effort. While we’ll be working on an old Kagle competition for predicting the sale prices of bulldozers and other heavy machinery, you can use this flow to solve whatever tabular data regression problem you’re working on.</p>
<p>This post serves as the explanation and documentation for the XGBoost regression jupyter notebook from my <a href="https://github.com/mcb00/ds-templates">ds-templates repo</a> on GitHub, so go ahead and download the notebook and follow along with your own data.</p>
<p>If you’re not already comfortable with the ideas behind gradient boosting and XGBoost, you’ll find it helpful to read some of my previous posts to get up to speed. I’d start with this <a href="../../posts/gradient-boosting-machine-from-scratch/">introduction to gradient boosting</a>, and then read this <a href="../../posts/xgboost-explained/">explanation of how XGBoost works</a>.</p>
<p>Let’s get into it! 🚀</p>
<section id="install-and-import-the-xgboost-library" class="level2">
<h2 class="anchored" data-anchor-id="install-and-import-the-xgboost-library">Install and import the <code>xgboost</code> library</h2>
<p>If you don’t already have it, go ahead and <a href="https://anaconda.org/conda-forge/xgboost">use conda to install the xgboost library</a>, e.g.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode .zsh code-with-copy"><code class="sourceCode zsh"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">$</span> conda install <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-c</span> conda-forge xgboost</span></code></pre></div>
<p>Then import it along with the usual suspects.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> xgboost <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> xgb</span></code></pre></div>
</div>
</section>
<section id="read-dataset-into-python" class="level2">
<h2 class="anchored" data-anchor-id="read-dataset-into-python">Read dataset into python</h2>
<p>In this example we’ll work on the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/overview">Kagle Bluebook for Bulldozers</a> competition, which asks us to build a regression model to predict the sale price of heavy equipment. Amazingly, you can solve your own regression problem by swapping this data out with your organization’s data before proceeding with the tutorial.</p>
<p>Go ahead and download the <code>Train.zip</code> file from Kagle and extract it into <code>Train.csv</code>. Then read the data into a pandas dataframe.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Train.csv'</span>, parse_dates<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
</div>
<p>Notice I cheated a little bit, checking the columns ahead of time and telling pandas to treat the <code>saledate</code> column as a date. In general it will make life easier to read in any date-like columns as dates.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">df.info()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 401125 entries, 0 to 401124
Data columns (total 53 columns):
 #   Column                    Non-Null Count   Dtype         
---  ------                    --------------   -----         
 0   SalesID                   401125 non-null  int64         
 1   SalePrice                 401125 non-null  int64         
 2   MachineID                 401125 non-null  int64         
 3   ModelID                   401125 non-null  int64         
 4   datasource                401125 non-null  int64         
 5   auctioneerID              380989 non-null  float64       
 6   YearMade                  401125 non-null  int64         
 7   MachineHoursCurrentMeter  142765 non-null  float64       
 8   UsageBand                 69639 non-null   object        
 9   saledate                  401125 non-null  datetime64[ns]
 10  fiModelDesc               401125 non-null  object        
 11  fiBaseModel               401125 non-null  object        
 12  fiSecondaryDesc           263934 non-null  object        
 13  fiModelSeries             56908 non-null   object        
 14  fiModelDescriptor         71919 non-null   object        
 15  ProductSize               190350 non-null  object        
 16  fiProductClassDesc        401125 non-null  object        
 17  state                     401125 non-null  object        
 18  ProductGroup              401125 non-null  object        
 19  ProductGroupDesc          401125 non-null  object        
 20  Drive_System              104361 non-null  object        
 21  Enclosure                 400800 non-null  object        
 22  Forks                     192077 non-null  object        
 23  Pad_Type                  79134 non-null   object        
 24  Ride_Control              148606 non-null  object        
 25  Stick                     79134 non-null   object        
 26  Transmission              183230 non-null  object        
 27  Turbocharged              79134 non-null   object        
 28  Blade_Extension           25219 non-null   object        
 29  Blade_Width               25219 non-null   object        
 30  Enclosure_Type            25219 non-null   object        
 31  Engine_Horsepower         25219 non-null   object        
 32  Hydraulics                320570 non-null  object        
 33  Pushblock                 25219 non-null   object        
 34  Ripper                    104137 non-null  object        
 35  Scarifier                 25230 non-null   object        
 36  Tip_Control               25219 non-null   object        
 37  Tire_Size                 94718 non-null   object        
 38  Coupler                   213952 non-null  object        
 39  Coupler_System            43458 non-null   object        
 40  Grouser_Tracks            43362 non-null   object        
 41  Hydraulics_Flow           43362 non-null   object        
 42  Track_Type                99153 non-null   object        
 43  Undercarriage_Pad_Width   99872 non-null   object        
 44  Stick_Length              99218 non-null   object        
 45  Thumb                     99288 non-null   object        
 46  Pattern_Changer           99218 non-null   object        
 47  Grouser_Type              99153 non-null   object        
 48  Backhoe_Mounting          78672 non-null   object        
 49  Blade_Type                79833 non-null   object        
 50  Travel_Controls           79834 non-null   object        
 51  Differential_Type         69411 non-null   object        
 52  Steering_Controls         69369 non-null   object        
dtypes: datetime64[ns](1), float64(2), int64(6), object(44)
memory usage: 162.2+ MB</code></pre>
</div>
</div>
</section>
<section id="prepare-raw-data-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="prepare-raw-data-for-xgboost">Prepare raw data for XGBoost</h2>
<p>When faced with a new tabular dataset for modeling, we have two format considerations: data types and missingness. From the call to <code>df.info()</code> above, we can see we have both mixed types and missing values.</p>
<p>When it comes to missing values, some models like the gradient booster or random forest in scikit-learn require purely non-missing inputs. One of the great strengths of XGBoost is that it relaxes this requirement, allowing us to pass in missing feature values, so we don’t have to worry about them.</p>
<p>Regarding data types, all ML models for tabular data require inputs to be numeric, either integers or floats, so we’re going to have to deal with those <code>object</code> columns.</p>
<section id="encode-string-features" class="level3">
<h3 class="anchored" data-anchor-id="encode-string-features">Encode string features</h3>
<p>The simplest way to encode string variables is to map each unique string value to an integer; this is called <em>integer encoding</em>.</p>
<p>We can easily accomplish this by using the <a href="https://pandas.pydata.org/docs/user_guide/categorical.html">categorical data type in pandas</a>. The category type is a bit like the factor type in R; pandas stores the underlying data as integers, and it keeps a mapping from the integers back to the original string values. XGBoost is able to access the numeric data underlying the categorical features for model training and prediction. This is a nice way to encode string features because it’s easy to implement and it preserves the original category levels in the data frame. If you prefer to generate your own integer mappings, you can also do it with the scikit-learn <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html">OrdinalEncoder</a>.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> encode_string_features(df):</span>
<span id="cb6-2">    out_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb6-3">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> feature, feature_type <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> df.dtypes.items():</span>
<span id="cb6-4">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> feature_type <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'object'</span>:</span>
<span id="cb6-5">            out_df[feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> out_df[feature].astype(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'category'</span>)</span>
<span id="cb6-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> out_df</span>
<span id="cb6-7"></span>
<span id="cb6-8">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encode_string_features(df)</span></code></pre></div>
</div>
</section>
<section id="encode-date-and-timestamp-features" class="level3">
<h3 class="anchored" data-anchor-id="encode-date-and-timestamp-features">Encode date and timestamp features</h3>
<p>While dates feel sort of numeric, they are not quite numbers, so we need to transform them into numeric columns that XGBoost can understand. Unfortunately, encoding timestamps isn’t as straightforward as encoding strings, so we actually might need to engage in a little bit of feature engineering. A single date has many different attributes, e.g.&nbsp;days since epoch, year, quarter, month, day, day of year, day of week, is holiday, etc. Often a simple time index is the most useful information in a date column, so here we’ll just start by adding a feature that gives the number of days since some epoch date.</p>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb7-2">    df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> pd.Timestamp(year<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1970</span>, month<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, day<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-3">    ).dt.days</span></code></pre></div>
</div>
</section>
<section id="transform-the-target-if-necessary" class="level3">
<h3 class="anchored" data-anchor-id="transform-the-target-if-necessary">Transform the target if necessary</h3>
<p>In the interest of speed and efficiency, we didn’t bother doing any EDA with the feature data. Part of my justification for this is that trees are incredibly robust to outliers, colinearity, missingness, and other assorted nonsense in the feature data. However, they are not necessarily robust to nonsense in the target variable, so it’s worth having a look at it before proceeding any further.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">df.SalePrice.hist()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-7-output-1.png" class="img-fluid" alt="histogram of sale price showing right-skewed data"></p>
</div>
</div>
<p>Often when predicting prices it makes sense to use log price, especially when they span multiple orders of magnitude or have a strong right skew. These data look pretty friendly, lacking outliers and exhibiting only a mild positive skew; we could probably get away without doing any transformation. But checking the evaluation metric used to score the Kagle competition, we see they’re using root mean squared log error. That’s equivalent to using RMSE on log-transformed target data, so let’s go ahead and work with log prices.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.log1p(df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalePrice'</span>])</span>
<span id="cb9-2">df.logSalePrice.hist()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-8-output-1.png" class="img-fluid" alt="histogram of log sale price showing a more symetric distribution"></p>
</div>
</div>
</section>
</section>
<section id="train-and-evaluate-the-xgboost-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="train-and-evaluate-the-xgboost-regression-model">Train and Evaluate the XGBoost regression model</h2>
<p>Having prepared our dataset, we are now ready to train an XGBoost model. Let’s walk through the flow step-by-step.</p>
<section id="split-the-data-into-training-and-validation-sets" class="level3">
<h3 class="anchored" data-anchor-id="split-the-data-into-training-and-validation-sets">Split the data into training and validation sets</h3>
<p>First we split the dataset into a training set and a validation set. Of course since we’re going to evaluate against the validation set a number of times as we iterate, it’s best practice to keep a separate test set reserved to check our final model to ensure it generalizes well. Assuming that final test set is hidden away, we can use the rest of the data for training and validation.</p>
<p>There are two main ways we might want to select the validation set. If there isn’t a temporal ordering of the observations, we might be able to randomly sample. In practice, it’s common that observations have a temporal ordering, and that models are trained on observations up to a certain time and used to predict on observations occuring after that time. Since this data is temporal, we don’t want to split randomly; instead we’ll split on observation date, reserving the latest observations for the validation set.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Temporal Validation Set</span></span>
<span id="cb10-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> train_test_split_temporal(df, datetime_column, n_test):</span>
<span id="cb10-3">    idx_sort <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argsort(df[datetime_column])</span>
<span id="cb10-4">    idx_train, idx_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> idx_sort[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_valid], idx_sort[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>n_valid:]</span>
<span id="cb10-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> df.iloc[idx_train, :], df.iloc[idx_test, :]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb11-2"></span>
<span id="cb11-3">train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split_temporal(df, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>, n_valid)</span>
<span id="cb11-4">train_df.shape, valid_df.shape</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>((389125, 55), (12000, 55))</code></pre>
</div>
</div>
</section>
<section id="specify-target-and-feature-columns" class="level3">
<h3 class="anchored" data-anchor-id="specify-target-and-feature-columns">Specify target and feature columns</h3>
<p>Next we’ll put together a list of our features and define the target column. I like to have an actual list defined in the code so it’s easy to explicitly see everything we’re puting into the model and easier to add or remove features as we iterate. Just run something like <code>list(df.columns)</code> in a cel to get a copy-pasteable list of columns, then edit it down to the full list of features, i.e.&nbsp;remove the target, date columns, and other non-feature columns..</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#  list(df.columns)</span></span></code></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb14-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SalesID'</span>,</span>
<span id="cb14-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineID'</span>,</span>
<span id="cb14-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ModelID'</span>,</span>
<span id="cb14-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'datasource'</span>,</span>
<span id="cb14-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'auctioneerID'</span>,</span>
<span id="cb14-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>,</span>
<span id="cb14-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MachineHoursCurrentMeter'</span>,</span>
<span id="cb14-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'UsageBand'</span>,</span>
<span id="cb14-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDesc'</span>,</span>
<span id="cb14-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiBaseModel'</span>,</span>
<span id="cb14-12">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiSecondaryDesc'</span>,</span>
<span id="cb14-13">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelSeries'</span>,</span>
<span id="cb14-14">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiModelDescriptor'</span>,</span>
<span id="cb14-15">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductSize'</span>,</span>
<span id="cb14-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'fiProductClassDesc'</span>,</span>
<span id="cb14-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'state'</span>,</span>
<span id="cb14-18">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroup'</span>,</span>
<span id="cb14-19">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ProductGroupDesc'</span>,</span>
<span id="cb14-20">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Drive_System'</span>,</span>
<span id="cb14-21">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span>,</span>
<span id="cb14-22">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Forks'</span>,</span>
<span id="cb14-23">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pad_Type'</span>,</span>
<span id="cb14-24">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ride_Control'</span>,</span>
<span id="cb14-25">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick'</span>,</span>
<span id="cb14-26">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Transmission'</span>,</span>
<span id="cb14-27">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Turbocharged'</span>,</span>
<span id="cb14-28">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Extension'</span>,</span>
<span id="cb14-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Width'</span>,</span>
<span id="cb14-30">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure_Type'</span>,</span>
<span id="cb14-31">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Engine_Horsepower'</span>,</span>
<span id="cb14-32">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics'</span>,</span>
<span id="cb14-33">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pushblock'</span>,</span>
<span id="cb14-34">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ripper'</span>,</span>
<span id="cb14-35">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Scarifier'</span>,</span>
<span id="cb14-36">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tip_Control'</span>,</span>
<span id="cb14-37">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tire_Size'</span>,</span>
<span id="cb14-38">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler'</span>,</span>
<span id="cb14-39">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Coupler_System'</span>,</span>
<span id="cb14-40">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Tracks'</span>,</span>
<span id="cb14-41">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hydraulics_Flow'</span>,</span>
<span id="cb14-42">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Track_Type'</span>,</span>
<span id="cb14-43">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Undercarriage_Pad_Width'</span>,</span>
<span id="cb14-44">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stick_Length'</span>,</span>
<span id="cb14-45">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Thumb'</span>,</span>
<span id="cb14-46">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Pattern_Changer'</span>,</span>
<span id="cb14-47">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Grouser_Type'</span>,</span>
<span id="cb14-48">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Backhoe_Mounting'</span>,</span>
<span id="cb14-49">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blade_Type'</span>,</span>
<span id="cb14-50">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Travel_Controls'</span>,</span>
<span id="cb14-51">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Differential_Type'</span>,</span>
<span id="cb14-52">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Steering_Controls'</span>,</span>
<span id="cb14-53">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate_days_since_epoch'</span></span>
<span id="cb14-54"> ]</span>
<span id="cb14-55"></span>
<span id="cb14-56">target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'logSalePrice'</span></span></code></pre></div>
</div>
</section>
<section id="create-dmatrix-data-objects" class="level3">
<h3 class="anchored" data-anchor-id="create-dmatrix-data-objects">Create <code>DMatrix</code> data objects</h3>
<p>XGBoost uses a data type called dense matrix for efficient training and prediction, so next we need to create <code>DMatrix</code> objects for our training and validation datasets. Remember how we decided to encode our string columns by casting them as pandas categorical types? For this to work, we need to set the <code>enable_categoricals</code> argument to <code>True</code>.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">dtrain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_df[target], </span>
<span id="cb15-2">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb15-3">dvalid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.DMatrix(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[features], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>valid_df[target], </span>
<span id="cb15-4">                     enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
</section>
<section id="set-the-xgboost-parameters" class="level3">
<h3 class="anchored" data-anchor-id="set-the-xgboost-parameters">Set the XGBoost parameters</h3>
<p>XGBoost has <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">numerous hyperparameters</a>. Fortunately, just a handful of them tend to be the most influential; furthermore, the default values are not bad in most situations. I like to start out with a dictionary containing the default values for the parameters I’m most likely to adjust later, with one exception. I dislike the default value of <code>auto</code> for the <code>tree_method</code> parameter, which tells XGBoost to choose a tree method on it’s own. I’ve been burned by this ambiguity in the past, so now I prefer to set it to <code>approx</code>. For training there is one required boosting parameter called <code>num_boost_round</code> which I set to 50 as a starting point.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># default values for important parameters</span></span>
<span id="cb16-2">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb16-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'tree_method'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'approx'</span>,</span>
<span id="cb16-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>,</span>
<span id="cb16-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'max_depth'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,</span>
<span id="cb16-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'min_child_weight'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb16-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb16-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'colsample_bynode'</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb16-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'reg:squarederror'</span>,</span>
<span id="cb16-10">}</span>
<span id="cb16-11">num_boost_round <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span></code></pre></div>
</div>
</section>
<section id="train-the-xgboost-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-xgboost-model">Train the XGBoost model</h3>
<p>The <code>xgb.train()</code> function takes our training dataset and parameters, and it returns a trained XGBoost model, which is an object of class <code>xgb.core.Booster</code>. Check out the <a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training">documentation on the learning API</a> to see all the training options. During training, I like to have XGBoost print out the evaluation metric on the train and validation set after every few boosting rounds and again at the end of training; that can be done by setting <code>evals</code> and <code>verbose_eval</code>.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb17-2">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb17-3">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-rmse:6.74240  valid-rmse:6.80825
[10]    train-rmse:0.31071  valid-rmse:0.34532
[20]    train-rmse:0.21950  valid-rmse:0.24364
[30]    train-rmse:0.20878  valid-rmse:0.23669
[40]    train-rmse:0.20164  valid-rmse:0.23254
[49]    train-rmse:0.19705  valid-rmse:0.23125</code></pre>
</div>
</div>
</section>
<section id="train-the-xgboost-model-using-the-sklearn-interface" class="level3">
<h3 class="anchored" data-anchor-id="train-the-xgboost-model-using-the-sklearn-interface">Train the XGBoost model using the sklearn interface</h3>
<p>If you prefer scikit-learn-like syntax, you can use the <a href="https://xgboost.readthedocs.io/en/latest/python/sklearn_estimator.html">sklearn estimator interface</a> to create and train XGBoost models. The <code>XGBRegressor</code> class, which is available in the <code>xgboost</code> library that we already imported, constructs an <code>XGBRegressor</code> object with <code>fit</code> and <code>predict</code> methods like you’re used to using in scikit-learn. The <code>fit</code> and <code>predict</code> methods take pandas dataframes, so you don’t need to create <code>DMatrix</code> data objects yourself; however, since these methods still have to transform input data into <code>DMatrix</code> objects internally, training and prediction seem to be slower via the sklearn interface.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scikit-learn interface</span></span>
<span id="cb19-2">reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb19-3">reg.fit(train_df[features], train_df[target], </span>
<span id="cb19-4">        eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb19-5">        verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] validation_0-rmse:6.74240   validation_1-rmse:6.80825
[10]    validation_0-rmse:0.31071   validation_1-rmse:0.34532
[20]    validation_0-rmse:0.21950   validation_1-rmse:0.24364
[30]    validation_0-rmse:0.20878   validation_1-rmse:0.23669
[40]    validation_0-rmse:0.20164   validation_1-rmse:0.23254
[49]    validation_0-rmse:0.19705   validation_1-rmse:0.23125</code></pre>
</div>
</div>
<p>Since not all features of XGBoost are available through the scikit-learn estimator interface, you might want to get the native <code>xgb.core.Booster</code> object back out of the sklearn wrapper.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">booster <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> reg.get_booster()</span></code></pre></div>
</div>
</section>
<section id="evaluate-the-xgboost-model-and-check-for-overfitting" class="level3">
<h3 class="anchored" data-anchor-id="evaluate-the-xgboost-model-and-check-for-overfitting">Evaluate the XGBoost model and check for overfitting</h3>
<p>We get the model evaluation metrics on the training and validation sets printed to stdout when we use the <code>evals</code> argument to the training API. Typically I just look at those printed metrics, but sometimes it’s helpful to retain them in a variable for further inspection via, e.g.&nbsp;plotting. To do that we need to train again, passing an empty dictionary to the <code>evals_result</code> argument. In the objective curves, I’m looking for signs of overfitting, which could include validation scores staying the same or getting worse over later iterations or huge gaps between training and validation scores.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">evals_result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {}</span>
<span id="cb22-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.train(params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params, dtrain<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dtrain, num_boost_round<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round,</span>
<span id="cb22-3">                  evals<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(dtrain, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>), (dvalid, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>)],</span>
<span id="cb22-4">                  verbose_eval<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb22-5">                  evals_result<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>evals_result)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0] train-rmse:6.74240  valid-rmse:6.80825
[10]    train-rmse:0.31071  valid-rmse:0.34532
[20]    train-rmse:0.21950  valid-rmse:0.24364
[30]    train-rmse:0.20878  valid-rmse:0.23669
[40]    train-rmse:0.20164  valid-rmse:0.23254
[49]    train-rmse:0.19705  valid-rmse:0.23125</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">pd.DataFrame({</span>
<span id="cb24-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>: evals_result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'train'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>],</span>
<span id="cb24-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>: evals_result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'valid'</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmse'</span>]</span>
<span id="cb24-4">}).plot()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'boosting round'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span> plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'objective'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img" alt="line plot showing objective function versus training iteration for training and validation sets"></p>
<figcaption class="figure-caption">These objective curves look pretty good–no obvious signs of trouble.</figcaption>
</figure>
</div>
</div>
</div>
<p>While we could just look at the validation RMSE in the printed output from model training, let’s go ahead and compute it by hand, just to be sure.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mean_squared_error</span>
<span id="cb25-2"></span>
<span id="cb25-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># squared=False returns RMSE</span></span>
<span id="cb25-4">mean_squared_error(y_true<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>dvalid.get_label(), </span>
<span id="cb25-5">                   y_pred<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model.predict(dvalid), </span>
<span id="cb25-6">                   squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>0.23124987</code></pre>
</div>
</div>
<p>So, how good is that RMSLE of 0.231? Well, checking the <a href="https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard">Kagle leaderboard</a> for this competition, we would have come in around 5th out of 474. That’s not bad for 10 minutes of work doing the bare minimum necessary to transform the raw data into a format consumable by XGBoost and then training a model using default hyperparameter values. To improve our model from here we would want to explore some feature engineering and some hyperparameter tuning, which we’ll save for another post.</p>
<blockquote class="blockquote">
<p>Wait, why was that so easy? Since XGBoost made it’s big Kagle debut in the <a href="https://www.kaggle.com/c/higgs-boson">2014 Higgs Boson competition</a>, presumably no one in this 2013 competition was using it yet. A second potential reason is that we’re using a different validation set from that used for the final leaderboard (which is long closed), but our score is likely still a decent approximation for how we would have done in the competition.</p>
</blockquote>
</section>
</section>
<section id="xgboost-model-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-model-interpretation">XGBoost Model Interpretation</h2>
<p>Next let’s have a look at how to apply a couple of the most common model interpretation techniques, feature importance and partial dependence, to XGBoost.</p>
<blockquote class="blockquote">
<p>Remember we have two trained models floating around: one called <code>model</code> of class <code>xgb.core.Booster</code> which is compatible with xgboost library utilities and another called <code>reg</code> of class <code>XGBRegressor</code> which is compatible with scikit-learn utilities. We need to be sure to use the model that’s compatible with whatever utility we’re using.</p>
</blockquote>
<blockquote class="blockquote">
<p>While these interpretation tools are still very common, there’s a newer, more comprehensive, and self-consistent model interpretation framework called <a href="https://shap.readthedocs.io/en/latest/">SHAP</a> that’s worth checking out.</p>
</blockquote>
<section id="feature-importance-for-xgboost" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-for-xgboost">Feature Importance for XGBoost</h3>
<p>While XGBoost automatically computes feature importance by three different metrics during training, you should only use them with great care and skepticism. The three metrics are</p>
<ul>
<li><strong>weight</strong>: the number of splits that use the feature</li>
<li><strong>gain</strong>: the average gain in the objective function from splits which use the feature</li>
<li><strong>cover</strong>: the average number of training samples affected by splits that use the feature</li>
</ul>
<p>The first problem with these metrics is that they are computed using only the training dataset, which means they don’t reflect how useful a feature is when predicting on out-of-sample data. If your model is overfit on some nonsense feature, it will still have a high importance. Secondly, I think they are difficult to interpret; all three are specific to decision trees and reflect domain-irrelevant idiosyncrasies like whether a feature is used nearer the root or the leaves of a tree. Anyway let’s see what these metrics have to say about our features.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">fig, (ax1, ax2, ax3) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb27-2">xgb.plot_importance(model, importance_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'weight'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importance_type=weight'</span>, </span>
<span id="cb27-3">                    max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, show_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax1, )</span>
<span id="cb27-4">xgb.plot_importance(model, importance_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cover'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importance_type=cover'</span>, </span>
<span id="cb27-5">                    max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, show_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax2)</span>
<span id="cb27-6">xgb.plot_importance(model, importance_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gain'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importance_type=gain'</span>, </span>
<span id="cb27-7">                    max_num_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, show_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax3)</span>
<span id="cb27-8">plt.tight_layout()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img" alt="three horizontal bar plots showing feature importance for weight, cover, and gain metrics"></p>
<figcaption class="figure-caption">top 10 features according to each built-in XGBoost feature importance metric</figcaption>
</figure>
</div>
</div>
</div>
<p>Wow, notice that the top 10 features by weight and by cover are completely different. This should forever cause you to feel skeptical whenever you see a feature importance plot.</p>
<p>Luckily, there is a better way. IMHO, <a href="https://scikit-learn.org/stable/modules/permutation_importance.html">permutation feature importance</a> is better aligned with our intuition about what feature importance should mean. It tells us by how much the model performance decreases when the values of a particular feature are randomly shuffled during prediction. This effectively breaks the relationship between the feature and the target, thus revealing how much the model relies on that feature for prediction. It also has the benefit that it can be computed using either training data or out-of-sample data.</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> permutation_importance</span>
<span id="cb28-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> make_scorer</span>
<span id="cb28-3"></span>
<span id="cb28-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># make a scorer for RMSE</span></span>
<span id="cb28-5">scorer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_scorer(mean_squared_error, squared<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb28-6">permu_imp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> permutation_importance(reg, valid_df[features], valid_df[target], </span>
<span id="cb28-7">                                   n_repeats<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, scoring<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>scorer)</span></code></pre></div>
</div>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">importances_permutation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.Series(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> permu_imp[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'importances_mean'</span>], index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>features)</span>
<span id="cb29-2">importances_permutation.sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>:].plot.barh()</span>
<span id="cb29-3">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Permutation Importance on Out-of-Sample Set'</span>)</span>
<span id="cb29-4">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'drop in RMSE'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img" alt="horizontal bar plot showing permutation feature importance"></p>
<figcaption class="figure-caption">top 10 features by permutation importance on validation set</figcaption>
</figure>
</div>
</div>
</div>
<p>Now we can see which features the model relies on most for out-of-sample predictions. These are good candidate features to dig into with some EDA and conversations with any domain expert collaborators.</p>
</section>
</section>
<section id="partial-dependence-plots-for-xgboost" class="level2">
<h2 class="anchored" data-anchor-id="partial-dependence-plots-for-xgboost">Partial Dependence Plots for XGBoost</h2>
<p>A <a href="https://scikit-learn.org/stable/modules/partial_dependence.html">partial dependence plot (PDP)</a> is a representation of the dependence between the target variable and one or more feature variables. We can loosely interpret it as showing how the expected value of the target changes across values of a particular feature, marginalizing over other features. I say “loosely” because it comes with caveats, a particularly serious one being that correlation among features tends to invalidate the above interpretation. Anyway, we can treat PDPs as useful heuristics for getting a sense of how a model thinks the target changes with feature values.</p>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.inspection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PartialDependenceDisplay</span>
<span id="cb30-2"></span>
<span id="cb30-3">PartialDependenceDisplay.from_estimator(reg, </span>
<span id="cb30-4">                                        valid_df[features].query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade &gt;= 1960'</span>), </span>
<span id="cb30-5">                                        [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'YearMade'</span>])<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-25-output-1.png" class="img-fluid figure-img" alt="line plot showing partial dependence of logSalePrice on YearMade"></p>
<figcaption class="figure-caption">PDP of target logSalePrice on feature YearMade</figcaption>
</figure>
</div>
</div>
</div>
<p>It looks like the log sale price tends to increase in a non-linear way with year made.</p>
<div class="cell" data-execution_count="71">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The PDPs for categorical features expect numeric data, not pandas categorical types,</span></span>
<span id="cb31-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># so the sklearn API for partial dependence won't work directly with the dataframe we've been using.</span></span>
<span id="cb31-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The workaround is to create a new dataframe where categorical columns are encoded numerically,</span></span>
<span id="cb31-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># retrain the XGBoost model using the sklearn interface, create the PDPs,</span></span>
<span id="cb31-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># then add the category levels as the tick labels for the PDP.</span></span>
<span id="cb31-6"></span>
<span id="cb31-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> cat_pdp():</span>
<span id="cb31-8">    cat_feature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Enclosure'</span></span>
<span id="cb31-9">    modified_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.copy()</span>
<span id="cb31-10">    cat_codes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> modified_df[cat_feature].cat.codes</span>
<span id="cb31-11">    cat_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(modified_df[cat_feature].cat.categories)</span>
<span id="cb31-12">    cat_labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'NaN'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> cat_labels <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> cat_codes.unique() <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> cat_labels</span>
<span id="cb31-13">    modified_df[cat_feature] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cat_codes</span>
<span id="cb31-14"></span>
<span id="cb31-15">    n_valid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12000</span></span>
<span id="cb31-16">    train_df, valid_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split_temporal(modified_df, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'saledate'</span>, n_valid)</span>
<span id="cb31-17">    train_df.shape, valid_df.shape</span>
<span id="cb31-18"></span>
<span id="cb31-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># scikit-learn interface</span></span>
<span id="cb31-20">    reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> xgb.XGBRegressor(n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>num_boost_round, enable_categorical<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>params)</span>
<span id="cb31-21">    reg.fit(train_df[features], train_df[target], </span>
<span id="cb31-22">            eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(train_df[features], train_df[target]), (valid_df[features], valid_df[target])], </span>
<span id="cb31-23">            verbose<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb31-24">    PartialDependenceDisplay.from_estimator(reg, valid_df[features], [cat_feature], categorical_features<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[cat_feature])</span>
<span id="cb31-25">    plt.xticks(ticks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cat_codes.unique(), labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cat_labels)</span>
<span id="cb31-26">cat_pdp()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/xgboost-for-regression-in-python/index_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img" alt="bar plot showing partial dependence of logSalePrice on Enclosure"></p>
<figcaption class="figure-caption">PDP of target logSalePrice on categorical feature Enclosure</figcaption>
</figure>
</div>
</div>
</div>
<p>You can imagine how useful these model interpretation tools can be, both for understanding data and for improving your models.</p>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>There you have it, a simple flow for solving regression problems with XGBoost in python. Remember you can use the XGBoost regression notebook from my <a href="https://github.com/mcb00/ds-templates">ds-templates repo</a> to make it easy to follow this flow on your own problems. If you found this helpful, or if you have additional ideas about solving regression problems with XGBoost, let me know down in the comments.</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>gradient boosting</category>
  <category>xgboost</category>
  <guid>https://randomrealizations.com/posts/xgboost-for-regression-in-python/index.html</guid>
  <pubDate>Mon, 18 Sep 2023 04:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/xgboost-for-regression-in-python/kigali-branches.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Blogging with Quarto and Jupyter: The Complete Guide</title>
  <dc:creator>Matt Bowers</dc:creator>
  <link>https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/index.html</link>
  <description><![CDATA[ 



<!-- 
![](thumbnail.png "quarto, jupyter, and python logos in hexagons")
-->
<p>Ahh, blogging. I think we can all agree it’s probably one of the greatest forms of written communication to have ever existed.</p>
<p>Whats that you say? You’d like to set up your own blog? And you say you want to use a dead simple, data science friendly tech stack? And you wouldn’t be caught dead handing over your painstakingly crafted content to Medium? No worries, friend, I know exactly what you need.</p>
<p>Enter <a href="https://quarto.org">Quarto</a>.</p>
<p>In this post we’ll set up a blog using a lightweight tech stack consisting of a terminal running quarto, git, and jupyter, and we’ll use Github Pages to host our website for free. Optionally, for a few dollars a year, we can even host our website at our own custom domain.</p>
<p>A quick note on how to use this post. <a href="https://quarto.org/docs/websites/website-blog.html">Quarto’s documentation on blogging</a> provides a nice high-level overview of the blogging workflow, and I refer to it and many other bits of Quarto documentation here. At the time of writing, the handful of other blog posts about setting up quarto blogs are aimed at the RStudio user. This post exists to provide a jupyter and python-centric path for you to follow through the entire setup of your new quarto blog, and to impart my opinionated recommendations about best practices.</p>
<p>Let’s get into it!</p>
<section id="what-is-quarto" class="level2">
<h2 class="anchored" data-anchor-id="what-is-quarto">What is Quarto?</h2>
<p>Quarto is a way to render plain text source files containing markdown and code in python, R, and other languages into published formats like websites, books, slides, journal articles, etc. There is clearly a lot that we can do with it, but Today, we’ll use it to make a nice looking blog out of some jupyter notebook files.</p>
<p>Quarto follows the familiar convention of using a project directory to house all material for a given project. The directory will include source files like jupyter notebooks or Rmarkdown files, as well as configuration files that control how output files are rendered. We can then use the quarto command line utility to perform actions like previewing and rendering within the project directory.</p>
</section>
<section id="instantiate-your-blog" class="level2">
<h2 class="anchored" data-anchor-id="instantiate-your-blog">Instantiate your blog</h2>
<section id="create-a-new-quarto-project" class="level3">
<h3 class="anchored" data-anchor-id="create-a-new-quarto-project">Create a new Quarto project</h3>
<p>After <a href="https://quarto.org/docs/get-started/">installing quarto</a> fire up a new terminal and check that the install was successful by running</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">quarto</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--version</span></span></code></pre></div>
<p>Now think of a name for your blog’s project directory; this will also be the name of its git repository. The name will have no effect on your website’s name or URL, so don’t think too hard. The <a href="https://quarto.org/docs/websites/website-blog.html">quarto documentation</a> calls it <code>myblog</code>, so we’ll one-up them and call ours <code>pirate-ninja-blog</code>. Run the following command to create it in the current directory.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb2-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">quarto</span> create-project pirate-ninja-blog <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--type</span> website:blog</span></code></pre></div>
<p>That command creates a directory called <code>pirate-ninja-blog</code> containing everything you need to render your new blog. You can preview your website by running</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb3-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">quarto</span> preview pirate-ninja-blog</span></code></pre></div>
<p>Your local website will open in a new browser window. As you edit various aspects of your blog, the preview will update with your changes. This preview feature is so simple and so great.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/pirate-ninja-blog-screenshot.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Previewing your blog with quarto preview command</figcaption>
</figure>
</div>
</section>
<section id="set-up-a-git-repo" class="level3">
<h3 class="anchored" data-anchor-id="set-up-a-git-repo">Set up a git repo</h3>
<p>Change into your project directory and we’ll start setting up your git repo.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb4-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> pirate-ninja-blog</span></code></pre></div>
<p>initialize a new git repo.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb5-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> init <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-b</span> main</span></code></pre></div>
<p>The <code>_site/</code> directory is where quarto puts the rendered output files, so you’ll want to ignore it in git. I also like to just ignore any hidden files too, so add the following to your <code>.gitignore</code> file.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>.gitignore</strong></pre>
</div>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/.quarto/</span></span>
<span id="cb6-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">/_site/</span></span>
<span id="cb6-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">.</span><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">*</span></span></code></pre></div>
</div>
<p>For now we’ll just stage the <code>.gitignore</code> file for the initial commit. Eventually you’ll want to commit the other files in your project too, either now or later as you edit them.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb7-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> add .gitignore </span>
<span id="cb7-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> commit <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Initial commit."</span></span></code></pre></div>
<p>Then follow GitHub’s instructions to <a href="https://docs.github.com/en/migrations/importing-source-code/using-the-command-line-to-import-source-code/adding-locally-hosted-code-to-github#adding-a-local-repository-to-github-using-git">add the local repo to GitHub using git</a>. Basically just create a new blank repo on GitHub’s website, copy the remote repository url, then add the remote repo url to your local git repo.</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb8-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> remote add origin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span>REMOTE_URL<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span></span></code></pre></div>
<p>Then you’ll be able to push any commits you make to your remote repository on GitHub by saying <code>git push</code>.</p>
</section>
</section>
<section id="understand-the-components-of-a-quarto-blog" class="level2">
<h2 class="anchored" data-anchor-id="understand-the-components-of-a-quarto-blog">Understand the components of a Quarto blog</h2>
<section id="contents-of-the-quarto-project-directory" class="level3">
<h3 class="anchored" data-anchor-id="contents-of-the-quarto-project-directory">Contents of the quarto project directory</h3>
<p>Let’s have a quick look at what quarto put inside of the project directory.</p>
<pre><code>_quarto.yml
about.qmd
index.qmd
profile.jpg
posts
styles.css
_site</code></pre>
<ul>
<li>Quarto uses yaml files to specify configurations. The <code>_quarto.yml</code> file specifies project-wide configurations.</li>
<li>Quarto’s markdown file type uses extension <code>qmd``. Each qmd file will correspond to a page in our website.</code>index.qmd<code>is the homepage and</code>about.qmd` is the About page.</li>
<li><code>profile.jpg</code> is an image that is included on the about page.</li>
<li><code>styles.css</code> defines css styles for the website.</li>
<li><code>posts</code> is a directory where we can put qmd and other documents which will be rendered into blog posts.</li>
<li><code>posts/_metadata.yml</code> contains configurations that apply to all documents in the <code>posts</code> directory.</li>
<li><code>_site</code> is a directory that contains the rendered website. Whereas all the other files and directories constitute the source code for our blog, <code>_site</code> is the rendered output, i.e.&nbsp;the website itself.</li>
</ul>
<p>Let’s take a closer look at these components and start to make the blog yours.</p>
</section>
<section id="project-wide-configurations" class="level3">
<h3 class="anchored" data-anchor-id="project-wide-configurations">Project-wide Configurations</h3>
<p>The <code>_quarto.yml</code> file controls project-wide configurations, website options, and HTML document options. Options in this file are specified in yaml in a key/value structure with three top level keys: <code>project</code>, <code>website</code>, and <code>format</code>. <a href="https://quarto.org/docs/reference/projects/websites.html">The quarto website options documentation</a> has the full list of options that you can set here. It will be very helpful to take a look at some example <code>_quarto.yml</code> files in the wild, such as the one from <a href="https://github.com/quarto-dev/quarto-web/blob/main/_quarto.yml">quarto.org</a> or even the one from <a href="https://github.com/mcb00/rr-blog/blob/main/_quarto.yml">this blog</a>.</p>
<p>Under the <code>website</code> key, go ahead and set the title and description for your blog.</p>
<pre><code>website:
  title: "Pirate Ninja Blog"
  description: "A blog about pirates, ninjas, and other things"</code></pre>
<p>You can also customize your <a href="https://quarto.org/docs/reference/projects/websites.html#navbar">navbar</a> which is visible at the top of all pages on your site. Also go ahead and set your github and twitter urls for the icons in the navbar.</p>
<p>Under the <code>format</code> key, you can also try changing the <a href="https://quarto.org/docs/output-formats/html-themes.html">HTML theme</a> to one of the other 25 built-in themes.</p>
</section>
<section id="the-about-page" class="level3">
<h3 class="anchored" data-anchor-id="the-about-page">The About Page</h3>
<p>The <code>about.qmd</code> file defines an About page for the blog. Go ahead and fill in your details in the <code>about.qmd</code> file; you can also replace the <code>profile.jpg</code> file with your own image. Have a look at <a href="https://quarto.org/docs/websites/website-about.html">the quarto documentation on About pages</a> to explore more functionality. Notably, you can change the <code>template</code> option to change the page layout.</p>
</section>
<section id="the-homepage" class="level3">
<h3 class="anchored" data-anchor-id="the-homepage">The Homepage</h3>
<p>The <code>index.qmd</code> file defines the landing page for your website. It is a <a href="https://quarto.org/docs/websites/website-listings.html">listing page</a> which shows links to all the pages in the <code>posts</code> directory. For now we don’t need to change anything here.</p>
</section>
<section id="the-posts-directory" class="level3">
<h3 class="anchored" data-anchor-id="the-posts-directory">The <code>posts/</code> directory</h3>
<p>The <code>posts</code> directory contains all your blog posts. There aren’t really requirements for subdirectory structure inside the <code>posts</code> directory, but it’s a best practice to create a new subdirectory for each new blog post. This just helps keep auxillary files like images or conda environment files organized. Out of the box, the <code>posts</code> directory looks like this.</p>
<pre><code>posts
├── _metadata.yml
├── post-with-code
│&nbsp;&nbsp; ├── image.jpg
│&nbsp;&nbsp; └── index.qmd
└── welcome
    ├── index.qmd
    └── thumbnail.jpg</code></pre>
<p>There are two reasons we want to be deliberate about how we organize and name things in the <code>posts</code> directory. First, the vast majority of our blog’s content will live here, so we don’t want it to be a big confusing mess. Second, the directory sstructure and file naming will be reflected in the URLs to our blog posts; if you prefer tidy-looking URLs, and I know you do, then you want to use tidy directory and file names in the <code>posts</code> directory.</p>
<p>You can check how the URLs look by navigating to one of the pre-populated posts in the site preview in your browser. For instance, the welcome post’s URL would be</p>
<pre><code>https://example.com/posts/welcome/</code></pre>
<p>When quarto renders the qmd file at <code>posts/welcome/index.qmd</code> it creates an output document in the website at <code>posts/welcome/index.html</code>. In fact the full URL to the post is,</p>
<pre><code>https://example.com/posts/welcome/index.html</code></pre>
<p>but the browser knows if you give it a URL with a path ending in a <code>/</code>, then it should look for the <code>index.html</code> file inside that directory.</p>
<p>So I think the best practice here is to name your new post subdirectory with the title of the post in all lower case with dashes for spaces, e.g.&nbsp;<code>post-with-code</code>. Then to force all output pages to be called <code>index.html</code>, you can set the <code>output-file</code> key in the <code>posts/_metadata.yml</code> file like this.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>posts/_metadata.yml</strong></pre>
</div>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">output-file:</span> index.html</span></code></pre></div>
</div>
<p>Note that alternative naming conventions are possible; notably you might want to prefix each post name with the date in yyyy-mm-dd format, so the post subdirectories sort temporally and look nice in a list. That’s the convention used in Quarto’s own blog at <a href="https://github.com/quarto-dev/quarto-web/tree/main">quarto.org</a>, As long as you keep everything for a given post inside its subdirectory, you should be good to go with nice-looking URLs.</p>
</section>
</section>
<section id="authoring-posts-with-jupyter" class="level2">
<h2 class="anchored" data-anchor-id="authoring-posts-with-jupyter">Authoring posts with jupyter</h2>
<section id="creating-a-new-post" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-new-post">Creating a new post</h3>
<p>It turns out that quarto will render not only <code>.qmd</code> files, but also <code>.ipynb</code> files in the <code>posts</code> directory. So let’s create a new blog post from a notebook.</p>
<p>I think it’s a best practice to write draft posts in their own git branches, that way if you need to deploy some kind of hotfix to main while you’re drafting a post, you won’t have to deploy a half-written post livin on the main branch. To start a new post, create a new development branch, change into the posts directory, create a new subdirectory with your preferred naming convention, change into that new directory, and fire up jupyter.</p>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb15-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> checkout <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-b</span> new-post</span>
<span id="cb15-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> posts</span>
<span id="cb15-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mkdir</span> new-post</span>
<span id="cb15-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">cd</span> new-post</span>
<span id="cb15-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">jupyter</span> notebook</span></code></pre></div>
<p>Now create a new notebook from the jupyter UI. In order for quarto to recognize the document, the first cell of the notebook must be a raw text cell (press <code>r</code> in command mode to change a cell to raw text), and it must contain the document’s yaml front matter. You can use the following as a frontmatter template.</p>
<pre><code>---
title: New Post
date: 2023-07-12
description: A nice new post
categories: [nonsense, code]
---</code></pre>
<p>Now to preview your post, open a new terminal, change into your blog’s project directory and run the <code>quarto preview</code> command. You’ll see a link to the new post in the listing on the homepage. I usually like to have the preview open in a browser while I’m editing the jupyter notebook, just to make sure things look the way I want in the rendered output. From here you can keep editing the notebook, and the preview will update in the browser dynamically.</p>
</section>
<section id="markdown-and-code-cells" class="level3">
<h3 class="anchored" data-anchor-id="markdown-and-code-cells">Markdown and code cells</h3>
<p>From here you can put text in markdown cells and you can write code in code cells. Let’s add a markdown cell with some markdown formatting.</p>
<pre><code>## A nice heading

Here is some lovely text and an equation.

$$ a^2 + b^2 = c^2 $$

Here's a list.

- a link to an [external website](https://quarto.org).
- a link to [another post in this blog](/posts/welcome/index.qmd).</code></pre>
<p>This markdown will be rendered into the HTML page for the post. The last line in the above cell demonstrates the best practice for using relative urls to link to other resources within your website. Instead of providing the full url in the parentheses, just give the path to the qmd or ipynb file that you want to link to. Note that paths need to start with the <code>/</code> at the root of the quarto project, since without it, quarto will try to resolve paths relative to the location of the current document instead of the root of the project.</p>
<p>Then create a code cell with some code. Try something like this.</p>
<div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hello, Quarto!'</span>)</span></code></pre></div>
<p>By default, both code and cell output will be rendered into the HTML output. So far our jupyter notebook looks like this.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/authoring-in-jupyter.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">View of a new post being written in jupyter notebook</figcaption>
</figure>
</div>
<p>Back in the browser window running your blog preview, you can see the rendered page of the new post.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/new-post-preview.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">View of the preview of the rendered post</figcaption>
</figure>
</div>
</section>
<section id="figures" class="level3">
<h3 class="anchored" data-anchor-id="figures">Figures</h3>
<p>Let’s add a figure to our post. Add a new code cell with the following code.</p>
<div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># | fig-cap: This is my lovely line plot</span></span>
<span id="cb19-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># | fig-alt: A line plot extending up and to the right</span></span>
<span id="cb19-3"></span>
<span id="cb19-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb19-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb19-6"></span>
<span id="cb19-7">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb19-8">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb19-9">plt.plot(x, y)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<p>Notice a couple of important details. First I placed a semicolon at the end of the last line. That supresses the <code>[&lt;matplotlib.lines.Line2D at 0x1111d00a0&gt;]</code> text output, which would otherwise show up in your blog post too.</p>
<p>Second, I added a couple of special comments at the top of the cell. Quarto allows you to specify numerous <a href="https://quarto.org/docs/computations/execution-options.html">code execution options</a>, designated by the <code># |</code> prefix, to control the behavior and appearance of the code and output at a cell level. I set two keys here, <code>fig-cap</code> and <code>fig-alt</code> which respectively set the figure caption text and the image alt tag text. The <code>fig-alt</code> key is particularly important to set on all your figures because it provides the non-visual description for screenreader users reading your post. The alt tag should be a simple description of what the plot is and possibly what it shows or means. Be a friend of the blind and visually impaired community and set <code>fig-alt</code> on all of your figures.</p>
</section>
<section id="version-control" class="level3">
<h3 class="anchored" data-anchor-id="version-control">Version control</h3>
<p>As you edit your new post, go ahead and commit your changes on your development branch. Once you’ve finished your new post, you can merge it into main like this.</p>
<div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb20-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> checkout main</span>
<span id="cb20-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">git</span> merge new-post</span></code></pre></div>
<p>Then you can push to GitHub by running <code>git push</code>. You should also be sure to run a final <code>quarto preview</code> to check that everything looks good before publishing to the web.</p>
</section>
</section>
<section id="publishing-your-blog-to-the-web" class="level2">
<h2 class="anchored" data-anchor-id="publishing-your-blog-to-the-web">Publishing your blog to the web</h2>
<section id="hosting-with-github-pages" class="level3">
<h3 class="anchored" data-anchor-id="hosting-with-github-pages">Hosting with GitHub Pages</h3>
<p>It’s likely that the easiest (read best) option for you is to host your blog on <a href="https://pages.github.com/">GitHub Pages</a>. This is because GitHub pages is free, and since you already have your blog’s source code checked into a remote repository at GitHub, it’s very easy to set up. <a href="https://quarto.org/docs/publishing/github-pages.html">Quarto’s documentation on publishing to GitHub Pages</a> outlines three ways to publish your website, but I recommend their option 2, using the <code>quarto publish</code> command. Once you set up your <code>gh-pages</code> branch as described in the documentation, you simply run <code>quarto publish</code> at the command line and your updates are deployed to your website.</p>
</section>
<section id="setting-up-your-domain-name" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-your-domain-name">Setting up your domain name</h3>
<p>By default, if you choose to host with GitHub Pages, your website will be published to a url in the form <code>https://username.github.io/reponame/</code>. You can certainly do this; for example Jake VanderPlas’s awesome blog Pythonic Perambulations lives at <a href="http://jakevdp.github.io">http://jakevdp.github.io</a>.</p>
<p>But, like me, you might want to get your own custom domain by buying, or really renting, one from a registrar. I use <a href="https://www.namecheap.com">Namecheap</a>. If you decide to go for a custom domain, refer to <a href="https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/about-custom-domains-and-github-pages">GitHub’s documentation on custom domains</a>. You’ll also need to point your domain registrar to the IP address where GitHub Pages is hosting your website. For an example of how to do this at Namecheap, see <a href="https://www.namecheap.com">Namecheap’s documentation about GitHub Pages</a></p>
<p>Whether you decide to use the standard <code>github.io</code> domain or your own custom domain, be sure to set the <code>site-url</code> key in your <code>_quarto.yml</code> file to ensure other quarto functionality works correctly. For example</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>_quarto.yml</strong></pre>
</div>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb21-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">website:</span></span>
<span id="cb21-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">site-url:</span> https://example.com/</span></code></pre></div>
</div>
<p>Edit: I found that after upgrading to quarto 1.3, using <code>quarto publish</code> to publish from the <code>gh-pages</code> branch obliterates the <code>CNAME</code> file that is created when you set a custom domain in your repository settings &gt; Pages &gt; Custom Domain. That breaks the mapping from your custom domain to your published website. See this <a href="https://github.com/quarto-dev/quarto-cli/discussions/3249">disscussion thread</a> for details. The fix is to manually create a <code>CNAME</code> file in the root of your project, and include it in the rendered website using the <code>resources</code> option under the <code>project</code> key in <code>_quarto.yml</code>. The <code>CNAME</code> file should just contain your custom domain, excluding any <code>https://</code>.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>CNAME</strong></pre>
</div>
<div class="sourceCode" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb22-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">example.com</span></span></code></pre></div>
</div>
<p>With the <code>CNAME</code> file in the root of your quarto project, you can then include it in the rendered output.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>_quarto.yml</strong></pre>
</div>
<div class="sourceCode" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb23-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">project:</span></span>
<span id="cb23-2">  <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">resources:</span></span>
<span id="cb23-3">    <span class="ex" style="color: null;
background-color: null;
font-style: inherit;">-</span> CNAME</span></code></pre></div>
</div>
</section>
</section>
<section id="keep-in-touch-with-your-readers" class="level2">
<h2 class="anchored" data-anchor-id="keep-in-touch-with-your-readers">Keep in touch with your readers</h2>
<section id="rss-feed" class="level3">
<h3 class="anchored" data-anchor-id="rss-feed">RSS Feed</h3>
<p>The RSS feed is handy for syndicating your posts to feed readers, other websites, and to your email subscribers. As described in <a href="https://quarto.org/docs/websites/website-blog.html#rss-feed">quarto’s documentation on RSS feeds</a>, you can automatically generate an RSS feed for your blog by first setting the value of <code>site-url</code> under the <code>website</code> key in <code>_quarto.yml</code>, and then setting <code>feed: true</code> under the <code>listing</code> key in the frontmatter of <code>index.qmd</code>. This will generate an RSS feed in the root of your website called <code>index.xml</code>. Once you have an RSS feed, go ahead and submit it to <a href="https://python-bloggers.com">Python-Bloggers</a> to have your work syndicated to a wider audience and to strengthen our little community of independent data science blogs.</p>
</section>
<section id="email-subscriptions" class="level3">
<h3 class="anchored" data-anchor-id="email-subscriptions">Email Subscriptions</h3>
<p>The idea here is to have a form field on your website where readers can input their email address to be added to your mailing list. <a href="https://quarto.org/docs/websites/website-blog.html#subscriptions">Quarto’s documentation on subscriptions</a> describes how to set up a subscribe box on your blog using MailChimp, so we won’t repeat it here. Once you have some subscribers, you can send them updates whenever you write a new post. You could do this manually or, in my case, set up an <a href="https://mailchimp.com/help/share-your-blog-posts-with-mailchimp/">automation through MailChimp</a> which uses your RSS feed to send out email updates to the list about new posts.</p>
</section>
<section id="comments" class="level3">
<h3 class="anchored" data-anchor-id="comments">Comments</h3>
<p>Quarto has build-in support for three different comment systems: hypothesis, utterances, and giscus. The good news is that these are all free to use, easy to set up, and AFAIK do not engage in any sketchy tracking activities. The bad news is that none of them are ideal because they all require the user to create an account and login to leave a comment. We want to encourage readers to comment, so we don’t want them to have to create accounts or deal with passwords or pick all the squares with bicycles or any such nonsense, just to leave a little comment. To that end, I’ve actually been working on self-hosted login-free comments for this blog using <a href="https://isso-comments.de">isso</a>, but it’s a bit more involved than these built-in solutions, so we’ll have to discuss it at length in a future post.</p>
<p>If you prefer an easy, out-of-the-box solution, I can recommend utterances, which uses GitHub issues to store comments for each post. I used utterances for comments on the first jekyll-based incarnation of this blog; you can still see the utterances comments on posts before this one. Go check out the <a href="https://quarto.org/docs/reference/projects/websites.html#comments">Quarto documentation on comments</a> to see how to set up utterances in your project.</p>
</section>
<section id="analytics" class="level3">
<h3 class="anchored" data-anchor-id="analytics">Analytics</h3>
<p>As a data enthusiast, you’ll likely enjoy collecting some data about page views and visitors to your site. You might be tempted to use Google Analytics to do this; indeed quarto makes it very easy to just add a line to your <code>_quarto.yml</code> file to set it up. Unfortunately, in this case, going with the easy and free solution means supporting <a href="https://en.wikipedia.org/wiki/Privacy_concerns_regarding_Google">Google’s dubious corporate surveillance activities</a>. Be a conscientious internet citizen and avoid using Google Analytics on your blog. Fortunately, there are numerous privacy-friendly alternatives to Google Analytics. For this blog I’m self-hosting <a href="https://umami.is">umami analytics</a>, which might warrant its own post in the future.</p>
</section>
</section>
<section id="more-humbly-suggested-best-practices" class="level2">
<h2 class="anchored" data-anchor-id="more-humbly-suggested-best-practices">More humbly suggested best practices</h2>
<section id="using-conda-environments-for-reproducibility" class="level3">
<h3 class="anchored" data-anchor-id="using-conda-environments-for-reproducibility">Using conda environments for reproducibility</h3>
<p>As you know, it’s a good practice to use an environment manager to keep track of packages, their versions, and other dependencies for software in a data science project. The same applies to blog posts; especially if you’re using unusual or bleeding-edge packages in a post. This will help us out a lot when we have to go back and re-run a notebook a couple years later to regenerate the output. Here we’ll use <a href="https://docs.conda.io/projects/conda/en/latest/index.html">conda</a> as our environment manager.</p>
<p>To be clear, I don’t bother doing this if I’m just using fairly stable functionality in standard packages like pandas, numpy, and matplotlib, but we’ll do it here for illustration. From a terminal sitting inside our post subdirectory at <code>posts/new-post</code>, create a new conda environment with the packages you’re using in the post.</p>
<div class="sourceCode" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb24-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-p</span> ./venv jupyter numpy matplotlib</span></code></pre></div>
<p>Note the <code>-p</code> flag which tells conda to save the environment to <code>./venv</code> in the current working directory. This will save all the installed packages here in the post directory instead of in your system-wide location for conda environments. Note also that you’ll want to avoid checking anything in the <code>venv</code> directory into source control, so add <code>venv</code> to the <code>.gitignore</code> file at the root of the quarto project to ignore all <code>venv</code> directories throughout your quarto project.</p>
<p>Now whenever you work on this post, you’ll navigate to the post subdirectory with a terminal and activate the conda environment.</p>
<div class="sourceCode" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb25-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> activate ./venv</span></code></pre></div>
<p>Then you can fire up your jupyter notebook from the command line, and it will use the active conda environment.</p>
<p>Since we don’t want to check the <code>venv</code> directory with all its installed libraries into source control, we need to create an <code>environment.yml</code> file from which the environment can later be reproduced. With the local conda environment active, run the following.</p>
<div class="sourceCode" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb26-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> env export <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">--from-history</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> environment.yml</span></code></pre></div>
<p>The <code>--from-history</code> flag tells conda to skip adding a bunch of system specific stuff that will gunk up your environment yaml file and make it harder to use for cross-platform reproducibility. This <code>environment.yml</code> file is the only environment management artifact that you need to check into git.</p>
<p>Later if you need to recreate the environment from the <code>environment.yml</code> file, you can use the following command.</p>
<div class="sourceCode" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb27-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">conda</span> env create <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-f</span> environment.yml <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-p</span> ./venv<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">`</span></span></code></pre></div>
</section>
<section id="image-file-best-practices" class="level3">
<h3 class="anchored" data-anchor-id="image-file-best-practices">Image file best practices</h3>
<p>Let’s talk about image file sizes. The key idea is that we want images to have just enough resolution to look good; any more than that and we’re just draging around larger-than-necessary files and wasting bandwidth and slowing down page load times.</p>
<p>You can read all about <a href="https://www.foregroundweb.com/image-size/">choosing optimal image sizes</a>, but the TLDR is that images should be just large enough (in pixels) to fill the containers they occupy on the page. In our quarto blog, the two most common kinds of images are inline images we put in the body of posts and image thumbnails that show up as the associated image for a post, e.g.&nbsp;in the listing on our homepage. The inline image container seems to be about 800 pixels wide in my browser and the thumbnails are smaller, so adding some margin of error, I decided to go for 1000x750 for inline images and 500x375 for the thumbnails.</p>
<p>I use a command line tool called <a href="https://imagemagick.org">Image Magick</a> to resize image files. Go ahead and <a href="https://formulae.brew.sh/formula/imagemagick">install image magick with homebrew</a>, and let’s add some images to our new post.</p>
<p>For this example I’ll use a nice shot of the <a href="https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Why_London_Underground_is_nicknamed_The_Tube.jpg/1920px-Why_London_Underground_is_nicknamed_The_Tube.jpg">London Underground</a> from Wikipedia. Save your image as <code>image.jpg</code>. Then use image magick to create two new resized images for inline and thumbnail use.</p>
<div class="sourceCode" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb28-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">convert</span> image.jpg <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-resize</span> 1000x1000 main.jpg </span>
<span id="cb28-2"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">convert</span> image.jpg <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-resize</span> 500x500 thumbnail.jpg </span></code></pre></div>
<p>These commands do not change the aspect ratio of the image; they just reduce the size so that the image fits within the size specified.</p>
<p>Now move both of your new images into the post subdirectory at <code>posts/new-post/</code>. To specify the thumbnail image, set the <code>image</code> key in the post’s front matter. Be sure to also add an alt tag description of the image using the <code>image-alt</code> key to keep it accessible for screen reader users. Our post’s frontmatter now looks like this.</p>
<pre><code>---
title: New Post
date: 2023-07-12
description: A nice new post
categories: [nonsense, code]
image: thumbnail.jpg
image-alt: "A London Underground train emerging from a tunnel"
---</code></pre>
<p>To include an image within the body of a post, use markdown in the post to include the image. I added a markdown cell just under the front matter containing the following.</p>
<pre><code>![A London Underground train emerging from a tunnel](main.jpg "")</code></pre>
<p>In your preview browser window, you can see we have the thumbnail for our new post on the homepage listing.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/homepage-with-new-post-thumbnail.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">A screenshot of the homepage showing the new post’s thumbnail image</figcaption>
</figure>
</div>
<p>And we also have the inline image appearing in the body of the post.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/new-post-with-inline-image.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">A screenshot of the new post showing the image included in the body of the post</figcaption>
</figure>
</div>
<p>You can take a look at <a href="https://github.com/mcb00/rr-blog">the source code for this blog</a> to see some examples of including images in posts.</p>
</section>
</section>
<section id="seo" class="level2">
<h2 class="anchored" data-anchor-id="seo">SEO</h2>
<p>SEO is a huge topic, but here we’ll just focus on a few fundamental technical aspects that we want to be sure to get right. This boils down to registering with the top search engines by market share and ensuring that we’re providing them with the information they need to properly index our pages.</p>
<p>I checked the <a href="https://www.statista.com/statistics/216573/worldwide-market-share-of-search-engines/#main-content">top search engines by global market share</a> and as of 2023 it looks like Google has about 85%, Bing has about 8%, and the others have 2% or less each. So let’s focus on setting our site up to work well with Google search and Bing to get over 90% coverage.</p>
<section id="google-search-console-and-bing-webmaster-tools" class="level3">
<h3 class="anchored" data-anchor-id="google-search-console-and-bing-webmaster-tools">Google Search Console and Bing Webmaster Tools</h3>
<p><a href="https://search.google.com/search-console/about">Google Search Console</a> is a tool for web admins to help analyze search traffic and identify any technical issues that might prevent pages from appearing or ranking well in search. Go ahead and set up an account and register your blog in search console. You can refer to <a href="https://developers.google.com/search/docs/monitor-debug/search-console-start">Google’s documentation on search console</a> to guide you through setup and configuration.</p>
<p>Once you get set up on GSC, you can also create an account for <a href="https://www.bing.com/webmasters/about">Bing Webmaster Tools</a>. Do this after setting up GSC because there is an option to import your information from your GSC account.</p>
<p>Once you’re set up with GSC and BWT, you’ll get email alerts anytime they crawl your site and detect any indexing problems. When that happens, track down the issues and fix them so your pages can appear in organic searches.</p>
</section>
<section id="sitemap" class="level3">
<h3 class="anchored" data-anchor-id="sitemap">Sitemap</h3>
<p>A sitemap is an xml document that lists all the pages on your website. It’s a map for the search engine bots that crawl the web looking for new pages to index. Quarto will automatically generate a sitemap called <code>sitemap.xml</code> in the root of your website, as long as you’ve filled out the <code>site-url</code> key in <code>_quarto.yml</code>. You can submit your website for indexing by providing your sitemap in Google Search Console and Bing Webmaster Tools.</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Boy howdy, that was a lot, but at this point you should have a fully functioning blog, built with a minimalist, data-science-friendly tech stack consisting of quarto, jupyter, and GitHub. If you do create a blog using quarto, drop a link to it in the comments, and we can all check it out and celebrate your creation!</p>
</section>

 ]]></description>
  <category>python</category>
  <category>tutorial</category>
  <category>blogging</category>
  <guid>https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/index.html</guid>
  <pubDate>Wed, 06 Sep 2023 04:00:00 GMT</pubDate>
  <media:content url="https://randomrealizations.com/posts/blogging-with-quarto-and-jupyter/thumbnail.png" medium="image" type="image/png" height="108" width="144"/>
</item>
</channel>
</rss>
