{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: XGBoost Hyperparameter Tuning with Optuna\n",
    "categories:\n",
    "- python\n",
    "- tutorial\n",
    "- gradient boosting\n",
    "- xgboost\n",
    "date: '2023-12-18'\n",
    "description: My strategy for efficiently tuning XGBoost parameters with optuna\n",
    "draft: false\n",
    "image: optuna_thumbnail.jpg \n",
    "image-alt: lunar halo\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh, the dark art of hyperparameter tuning.\n",
    "It's a key step in the machine learning workflow,\n",
    "and it's an activity that can easily be overlooked or be overkill.\n",
    "Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils.\n",
    "Today I'll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework.\n",
    "I'll give you some intuition for how to think about the key parameters in XGBoost,\n",
    "and I'll show you an efficient strategy for parameter tuning GBTs.\n",
    "I'll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lunar halo on a frosty night in Johnson City, TN](optuna_main.jpg \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you do hyperparameter tuning?\n",
    "\n",
    "Hyperparameter tuning can easily be overlooked in the move-fast-and-break-everything husstle of building an ML product, but it can also easily become overkill, depending on the application.\n",
    "There are two key questions to ask:\n",
    "\n",
    "1. How much value is created by an incremental gain in model prediction accuracy?\n",
    "1. What is the cost of increasing model prediction accuracy?\n",
    "\n",
    "The point is that sometimes a small gain in model prediction performance translates into millions of dollars of impact.\n",
    "The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org's KPIs, and get mad respect, bonuses, and promoted.\n",
    "But the reality is that ofen additional model accuracy doesn't really change business KPIs by very much.\n",
    "Try to figure out the actual value of improved model accuracy and proceed accordingly.\n",
    "\n",
    "Remember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself.\n",
    "It can also lead us to deeper and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.\n",
    "Blindly chasing prediction accuracy can even backfire and make a system worse,e.g. by [degrading causal reasoning in decision-making systems](https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html#common-issues-with-propensity-score).\n",
    "\n",
    "Moral of the story: think before you tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Parameters\n",
    "\n",
    "Gradient boosting algorithms like XGBoost have two main types of hyperparameters: *tree parameters* which control the decision tree trained at each boosting round and *boosting parameters* which control the boosting procedure itself.\n",
    "Below I'll highlight my favorite parameters, but you can see the full list in the [documentation](https://xgboost.readthedocs.io/en/stable/parameter.html).\n",
    "\n",
    "### Tree Parameters\n",
    "In theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, [decision trees](/posts/consider-the-decision-tree/) \n",
    "are typically the best choice.\n",
    "In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.\n",
    "\n",
    "#### Tree construction algorithm\n",
    "The tree construction algorithm boils down to split finding, and\n",
    "different algorithms have different ways of generating candidate splits to consider.\n",
    "In XGBoost we have the parameter:\n",
    "\n",
    "* `tree_method` - select tree construction algorithm: `exact`, `hist`, or default `approx`. \n",
    "The exact method tends to be slow, so I usually consider approx and hist in parameter searches.\n",
    "\n",
    "#### Tree complexity parameters\n",
    "Tree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be.\n",
    "I use these two parameters:\n",
    "\n",
    "* `max_depth` - maximum number of split levels allowed. Reasonable values are usually from 3-12.\n",
    "* `min_child_weight` - minimum allowable sum of hessian values over data in a node. When using mean squared error as the objective, this is the minimum number of samples allowed in a leaf node. In that case, values in [1, 200] usually work well.\n",
    "\n",
    "These two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing  min child weight makes trees less expressive and therefore is a powerful way to counter overfitting.\n",
    "Note that `gamma` (a.k.a. `min_split_loss`) also limits node splitting, but I usually don't use it because `min_child_weight` seems to work well enough on its own.\n",
    "\n",
    "#### Sampling parameters\n",
    "XGBoost can randomly sample rows and columns to be used for training each tree;\n",
    "you might think of this as *bagging*.\n",
    "We have a few parameters:\n",
    "\n",
    "* `subsample` - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.\n",
    "* `colsample_bytree`, `colsample_bylevel`, `colsample_bynode` - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split.  Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.\n",
    "\n",
    "#### Regularization parameters\n",
    "In XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero.\n",
    "I usually use:\n",
    "\n",
    "* `reg_lambda` - L2 regularization  of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. \n",
    "Valid values are in [0,$\\infty$), but good values typically fall in [1,10].\n",
    "\n",
    "There is also an L1 regularization parameter called `reg_alpha`; feel free to use it instead.\n",
    "It seems that using one or the other is usually sufficient.\n",
    "\n",
    "### Boosting Parameters\n",
    "Trained gradient boosting models take the form:\n",
    "\n",
    "$$ F(\\mathbf{x}) = b + \\eta \\sum_{k=1}^{K} f_k(\\mathbf{x}) $$ \n",
    "\n",
    "where $b$ is the constant base predicted value, $f_k(\\cdot)$ is the base learner for round $k$, parameter $K$ is the number of boosting rounds, and parameter $\\eta$ is the learning rate.\n",
    "In XGBoost these parameters are controlled by:\n",
    "\n",
    "* `num_boost_round` - the number of boosting iterations. \n",
    "* `learning_rate` - the scaling or \"shrinkage\" factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. \n",
    "\n",
    "These two parameters are very closely linked; the optimal value of one depends on the value of the other,\n",
    "where smaller learning rates require more boosting rounds to reach optimality.\n",
    "While training a model with a given learning rate, accuracy tends to increase with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.\n",
    "We can leverage this fact to make our tuning more efficient by using XGBoost's `early_stopping_rounds: int` argument, which terminates training after observing the specified number of boosting rounds without any improvement to the evaluation metric on the validation set.\n",
    "\n",
    "## An Efficient Parameter Search Strategy for XGBoost\n",
    "Efficiency is the key to effective parameter tuning, because wasting less time means searching more  parameter values and finding better models in a given amount of time.\n",
    "Parameter search involves training models over and over, and what determines training time?\n",
    "Well given a training dataset and a tree construction algorithm, by far the most important parameter is the number of boosting rounds.\n",
    "So we want to avoid any unnecessary boosting rounds during parameter search.\n",
    "\n",
    "Fortunately, the tree parameters tend to be independent of the boosting parameters, meaning that if we find a good combination of tree parameters, they will usually work well across various boosting parameter values.\n",
    "This insight leads to a strategy where I first simultaneously tune tree parameters and the learning rate while holding the boosting rounds fixed at a moderately small value for fast training.\n",
    "Then I can optionally train a model with optimal tree parameters and aggressive boosting parameters.\n",
    "Specifically, my strategy goes like this:\n",
    "\n",
    "1. With early stopping enabled, fix the number of boosting rounds at a reasonable value, and perform a parameter search over all other relevant parameters. Note the best iteration of the best model found during the search.\n",
    "1. Optionally, with early stopping enabled,  train a new model using the optimal tree parameter values from stage 1, fix the learning rate at a very small value ($\\le 0.01$), and boost until early stopping is invoked.\n",
    "\n",
    "If using the one-stage procedure, train the final model using the optimal parameter values, and set the number of boosting rounds to the best iteration of the best model from the search.\n",
    "If using the second aggressive boosting step, train the final model using the optimal tree parameters from stage 1, the small learning rate you chose for stage 2, and the best boosting round from stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning XGBoost Parameters with Optuna\n",
    "\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/)\n",
    "is a model-agnostic python library for hyperparameter tuning.\n",
    "I like it because it has a flexible API that abstracts away the details of the search algorithm being used.\n",
    "That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more.\n",
    "Another massive benefit is that optuna provides a specific [XGBoost integration](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.XGBoostPruningCallback.html) \n",
    "which terminates training early on lousy parameter combinations.\n",
    "\n",
    "You can install optuna with anaconda, e.g.\n",
    "\n",
    "```.zsh\n",
    "$ conda install -c conda-forge optuna\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Tuning the Bluebook for Bulldozers Regression Model\n",
    "\n",
    "To illustrate the procedure, we'll tune the parameters for the regression model we built back in the [XGBoost for regression](/posts/xgboost-for-regression-in-python/) post.\n",
    "First we'll load up the bulldozer data and prepare the features and target just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/n6r4g5mj3m3dhls3j66tztmh0000gn/T/ipykernel_69825/3284650136.py:11: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../xgboost-for-regression-in-python/Train.csv', parse_dates=['saledate']);\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| output: false\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import optuna \n",
    "\n",
    "df = pd.read_csv('../xgboost-for-regression-in-python/Train.csv', parse_dates=['saledate']);\n",
    "\n",
    "def encode_string_features(df):\n",
    "    out_df = df.copy()\n",
    "    for feature, feature_type in df.dtypes.items():\n",
    "        if feature_type == 'object':\n",
    "            out_df[feature] = out_df[feature].astype('category')\n",
    "    return out_df\n",
    "\n",
    "df = encode_string_features(df)\n",
    "\n",
    "df['saledate_days_since_epoch'] = (\n",
    "    df['saledate'] - pd.Timestamp(year=1970, month=1, day=1)\n",
    "    ).dt.days\n",
    "\n",
    "df['logSalePrice'] = np.log1p(df['SalePrice'])\n",
    "\n",
    "\n",
    "features = [\n",
    "    'SalesID',\n",
    "    'MachineID',\n",
    "    'ModelID',\n",
    "    'datasource',\n",
    "    'auctioneerID',\n",
    "    'YearMade',\n",
    "    'MachineHoursCurrentMeter',\n",
    "    'UsageBand',\n",
    "    'fiModelDesc',\n",
    "    'fiBaseModel',\n",
    "    'fiSecondaryDesc',\n",
    "    'fiModelSeries',\n",
    "    'fiModelDescriptor',\n",
    "    'ProductSize',\n",
    "    'fiProductClassDesc',\n",
    "    'state',\n",
    "    'ProductGroup',\n",
    "    'ProductGroupDesc',\n",
    "    'Drive_System',\n",
    "    'Enclosure',\n",
    "    'Forks',\n",
    "    'Pad_Type',\n",
    "    'Ride_Control',\n",
    "    'Stick',\n",
    "    'Transmission',\n",
    "    'Turbocharged',\n",
    "    'Blade_Extension',\n",
    "    'Blade_Width',\n",
    "    'Enclosure_Type',\n",
    "    'Engine_Horsepower',\n",
    "    'Hydraulics',\n",
    "    'Pushblock',\n",
    "    'Ripper',\n",
    "    'Scarifier',\n",
    "    'Tip_Control',\n",
    "    'Tire_Size',\n",
    "    'Coupler',\n",
    "    'Coupler_System',\n",
    "    'Grouser_Tracks',\n",
    "    'Hydraulics_Flow',\n",
    "    'Track_Type',\n",
    "    'Undercarriage_Pad_Width',\n",
    "    'Stick_Length',\n",
    "    'Thumb',\n",
    "    'Pattern_Changer',\n",
    "    'Grouser_Type',\n",
    "    'Backhoe_Mounting',\n",
    "    'Blade_Type',\n",
    "    'Travel_Controls',\n",
    "    'Differential_Type',\n",
    "    'Steering_Controls',\n",
    "    'saledate_days_since_epoch'\n",
    " ]\n",
    "\n",
    "target = 'logSalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this time, since we're going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes.\n",
    "We make four different `xgboost.DMatrix` datasets for this process: training, validation, training+validation, and test. \n",
    "Training and validation are for the parameter search, and training+validation and test are for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = 12000\n",
    "n_test = 12000\n",
    "\n",
    "sorted_df = df.sort_values(by='saledate')\n",
    "train_df = sorted_df[:-(n_valid + n_test)] \n",
    "valid_df = sorted_df[-(n_valid + n_test):-n_test] \n",
    "test_df = sorted_df[-n_test:]\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], \n",
    "                     enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], \n",
    "                     enable_categorical=True)\n",
    "dtest = xgb.DMatrix(data=test_df[features], label=test_df[target], \n",
    "                    enable_categorical=True)\n",
    "dtrainvalid = xgb.DMatrix(data=pd.concat([train_df, valid_df])[features], \n",
    "                          label=pd.concat([train_df, valid_df])[target], \n",
    "                          enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: base parameters and scoring function\n",
    "\n",
    "There are a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping.\n",
    "We'll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'rmse'\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': metric,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, dmat):\n",
    "    y_true = dmat.get_label() \n",
    "    y_pred = model.predict(dmat) \n",
    "    return mean_squared_error(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Tune Tree Parameters with Optuna\n",
    "\n",
    "Next we implement our optuna objective, a function taking an optuna study `Trial` object and returning the score we want to optimize.\n",
    "We use the `suggest_categorical`, `suggest_float`, and `suggest_int` methods of the `Trial` object to define the search space for each parameter.\n",
    "Note the use of the pruning callback function which we pass into the `callback` argument of the XGBoost `train` function; this is a must, since it allows optuna to prune lousy models after a few boosting rounds.\n",
    "Then we train XGBoost using 500 boosting rounds, which takes only a few seconds on my little laptop.\n",
    "Finally we return the score as computed by our `model_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'tree_method': trial.suggest_categorical('tree_method', ['approx', 'hist']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 250),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 01.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.5, log=True),\n",
    "    }\n",
    "    num_boost_round = 500\n",
    "    params.update(base_params)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, f'valid-{metric}')\n",
    "    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "                      evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                      early_stopping_rounds=50,\n",
    "                      verbose_eval=0,\n",
    "                      callbacks=[pruning_callback])\n",
    "\n",
    "    return model.best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new optuna study and search through 50 parameter combinations, you could just run these two lines.\n",
    "\n",
    "```python\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "```\n",
    "\n",
    "But, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials&mdash;who knows how long 50 trials will take.\n",
    "So, to run the optimization for around 600 seconds (long enough to go make a nice cup of tea, stretch,  and come back), I do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-18 12:26:24,038] A new study created in memory with name: no-name-ad01e2fe-3ccd-490e-b492-54447aa24eb7\n",
      "[I 2023-12-18 12:26:38,732] Trial 0 finished with value: 0.22716975344231757 and parameters: {'tree_method': 'hist', 'max_depth': 7, 'min_child_weight': 47, 'subsample': 0.644239586943823, 'colsample_bynode': 0.6324287447379262, 'reg_lambda': 0.008813955729768966, 'learning_rate': 0.12041680713201537}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:26:42,022] Trial 1 finished with value: 0.25134318137258616 and parameters: {'tree_method': 'hist', 'max_depth': 7, 'min_child_weight': 22, 'subsample': 0.3972059827740082, 'colsample_bynode': 0.44287882577128346, 'reg_lambda': 0.002971964781260193, 'learning_rate': 0.4677686278959682}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:26:49,585] Trial 2 finished with value: 0.23955581193610404 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 159, 'subsample': 0.5397802867383634, 'colsample_bynode': 0.32612741318879956, 'reg_lambda': 0.7376608679950835, 'learning_rate': 0.28612595871780716}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:27:02,476] Trial 3 finished with value: 0.25317110696354855 and parameters: {'tree_method': 'approx', 'max_depth': 10, 'min_child_weight': 150, 'subsample': 0.12674048080369518, 'colsample_bynode': 0.5020030741859819, 'reg_lambda': 0.6507847408918562, 'learning_rate': 0.27106934190511833}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:27:11,176] Trial 4 finished with value: 0.2344890258673693 and parameters: {'tree_method': 'hist', 'max_depth': 6, 'min_child_weight': 124, 'subsample': 0.8384650353318115, 'colsample_bynode': 0.9785481516951317, 'reg_lambda': 0.006989660133416335, 'learning_rate': 0.2691079295087314}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:27:11,292] Trial 5 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:11,340] Trial 6 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:11,499] Trial 7 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:11,622] Trial 8 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:11,885] Trial 9 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:27:12,156] Trial 10 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:12,211] Trial 11 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:12,295] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:12,362] Trial 13 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:12,691] Trial 14 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:27:12,722] Trial 15 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:18,090] Trial 16 finished with value: 0.23288484523013378 and parameters: {'tree_method': 'hist', 'max_depth': 8, 'min_child_weight': 59, 'subsample': 0.9971527738989727, 'colsample_bynode': 0.6303681358590325, 'reg_lambda': 0.0013205491061391572, 'learning_rate': 0.3396323337822498}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:27:23,436] Trial 17 finished with value: 0.23423225976339854 and parameters: {'tree_method': 'hist', 'max_depth': 9, 'min_child_weight': 55, 'subsample': 0.9557099472068988, 'colsample_bynode': 0.6283670990192435, 'reg_lambda': 0.0011599224814947305, 'learning_rate': 0.34956253784885155}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:27:23,708] Trial 18 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:23,776] Trial 19 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:23,899] Trial 20 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:30,581] Trial 21 finished with value: 0.23423016620639964 and parameters: {'tree_method': 'hist', 'max_depth': 9, 'min_child_weight': 52, 'subsample': 0.9700205817814173, 'colsample_bynode': 0.617142077958053, 'reg_lambda': 0.001954390146872763, 'learning_rate': 0.37120345347791667}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:27:35,018] Trial 22 pruned. Trial was pruned at iteration 115.\n",
      "[I 2023-12-18 12:27:41,836] Trial 23 finished with value: 0.23390462686857855 and parameters: {'tree_method': 'hist', 'max_depth': 8, 'min_child_weight': 77, 'subsample': 0.9919432543294208, 'colsample_bynode': 0.49972867797901577, 'reg_lambda': 0.009463263496616772, 'learning_rate': 0.3393132674075993}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:27:41,890] Trial 24 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:41,952] Trial 25 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:43,797] Trial 26 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:27:43,998] Trial 27 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:44,052] Trial 28 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:44,095] Trial 29 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:45,201] Trial 30 pruned. Trial was pruned at iteration 35.\n",
      "[I 2023-12-18 12:27:48,998] Trial 31 pruned. Trial was pruned at iteration 94.\n",
      "[I 2023-12-18 12:27:51,458] Trial 32 pruned. Trial was pruned at iteration 58.\n",
      "[I 2023-12-18 12:27:51,577] Trial 33 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:51,648] Trial 34 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:51,698] Trial 35 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:56,846] Trial 36 pruned. Trial was pruned at iteration 73.\n",
      "[I 2023-12-18 12:27:59,142] Trial 37 pruned. Trial was pruned at iteration 37.\n",
      "[I 2023-12-18 12:27:59,281] Trial 38 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:59,494] Trial 39 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:27:59,630] Trial 40 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:02,765] Trial 41 pruned. Trial was pruned at iteration 76.\n",
      "[I 2023-12-18 12:28:02,861] Trial 42 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:07,119] Trial 43 finished with value: 0.236773698502372 and parameters: {'tree_method': 'hist', 'max_depth': 11, 'min_child_weight': 57, 'subsample': 0.8878580657533407, 'colsample_bynode': 0.6334541545702823, 'reg_lambda': 0.006647085490262035, 'learning_rate': 0.36092073181986006}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:28:07,224] Trial 44 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:07,291] Trial 45 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:09,628] Trial 46 pruned. Trial was pruned at iteration 57.\n",
      "[I 2023-12-18 12:28:09,695] Trial 47 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:09,764] Trial 48 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:11,431] Trial 49 pruned. Trial was pruned at iteration 20.\n",
      "[I 2023-12-18 12:28:11,641] Trial 50 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:11,677] Trial 51 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:12,003] Trial 52 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:28:12,041] Trial 53 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:12,079] Trial 54 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:12,135] Trial 55 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:12,205] Trial 56 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:12,383] Trial 57 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:28:12,442] Trial 58 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:14,479] Trial 59 pruned. Trial was pruned at iteration 38.\n",
      "[I 2023-12-18 12:28:14,678] Trial 60 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:18,284] Trial 61 pruned. Trial was pruned at iteration 65.\n",
      "[I 2023-12-18 12:28:18,486] Trial 62 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:19,951] Trial 63 pruned. Trial was pruned at iteration 22.\n",
      "[I 2023-12-18 12:28:20,300] Trial 64 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:28:22,235] Trial 65 pruned. Trial was pruned at iteration 56.\n",
      "[I 2023-12-18 12:28:22,335] Trial 66 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:22,405] Trial 67 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:24,925] Trial 68 pruned. Trial was pruned at iteration 60.\n",
      "[I 2023-12-18 12:28:25,042] Trial 69 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:25,106] Trial 70 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:25,154] Trial 71 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:25,225] Trial 72 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:25,274] Trial 73 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:25,445] Trial 74 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:25,638] Trial 75 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:26,042] Trial 76 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:28:26,626] Trial 77 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:28:26,712] Trial 78 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:26,785] Trial 79 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:26,963] Trial 80 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:27,444] Trial 81 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:28:27,513] Trial 82 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:27,992] Trial 83 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:28:28,478] Trial 84 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:28:28,548] Trial 85 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:28,683] Trial 86 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:29,015] Trial 87 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:28:29,091] Trial 88 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:33,841] Trial 89 pruned. Trial was pruned at iteration 97.\n",
      "[I 2023-12-18 12:28:36,785] Trial 90 pruned. Trial was pruned at iteration 87.\n",
      "[I 2023-12-18 12:28:36,908] Trial 91 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:36,976] Trial 92 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:37,048] Trial 93 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:37,137] Trial 94 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:37,336] Trial 95 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:28:37,933] Trial 96 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:28:37,976] Trial 97 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:38,091] Trial 98 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:40,780] Trial 99 pruned. Trial was pruned at iteration 58.\n",
      "[I 2023-12-18 12:28:40,968] Trial 100 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:43,584] Trial 101 pruned. Trial was pruned at iteration 65.\n",
      "[I 2023-12-18 12:28:46,946] Trial 102 pruned. Trial was pruned at iteration 84.\n",
      "[I 2023-12-18 12:28:47,083] Trial 103 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:47,188] Trial 104 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:47,523] Trial 105 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:28:47,588] Trial 106 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:47,655] Trial 107 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:47,727] Trial 108 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:47,823] Trial 109 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:50,173] Trial 110 pruned. Trial was pruned at iteration 58.\n",
      "[I 2023-12-18 12:28:54,895] Trial 111 pruned. Trial was pruned at iteration 95.\n",
      "[I 2023-12-18 12:28:58,921] Trial 112 pruned. Trial was pruned at iteration 81.\n",
      "[I 2023-12-18 12:28:59,117] Trial 113 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:28:59,268] Trial 114 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:02,341] Trial 115 pruned. Trial was pruned at iteration 72.\n",
      "[I 2023-12-18 12:29:02,419] Trial 116 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:03,726] Trial 117 pruned. Trial was pruned at iteration 20.\n",
      "[I 2023-12-18 12:29:10,015] Trial 118 pruned. Trial was pruned at iteration 93.\n",
      "[I 2023-12-18 12:29:10,610] Trial 119 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:29:10,712] Trial 120 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:14,928] Trial 121 pruned. Trial was pruned at iteration 85.\n",
      "[I 2023-12-18 12:29:17,919] Trial 122 pruned. Trial was pruned at iteration 57.\n",
      "[I 2023-12-18 12:29:23,073] Trial 123 pruned. Trial was pruned at iteration 87.\n",
      "[I 2023-12-18 12:29:27,926] Trial 124 pruned. Trial was pruned at iteration 96.\n",
      "[I 2023-12-18 12:29:27,990] Trial 125 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:30,690] Trial 126 pruned. Trial was pruned at iteration 62.\n",
      "[I 2023-12-18 12:29:30,836] Trial 127 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:31,644] Trial 128 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:29:31,833] Trial 129 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:32,063] Trial 130 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:35,187] Trial 131 pruned. Trial was pruned at iteration 63.\n",
      "[I 2023-12-18 12:29:41,343] Trial 132 finished with value: 0.23348809037003834 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 60, 'subsample': 0.9502764526123828, 'colsample_bynode': 0.9824296160005543, 'reg_lambda': 0.002176750613760986, 'learning_rate': 0.34483813340570657}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:29:41,466] Trial 133 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:46,456] Trial 134 pruned. Trial was pruned at iteration 112.\n",
      "[I 2023-12-18 12:29:46,517] Trial 135 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:46,606] Trial 136 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:46,680] Trial 137 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:46,850] Trial 138 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:46,913] Trial 139 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:47,906] Trial 140 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:29:51,646] Trial 141 pruned. Trial was pruned at iteration 76.\n",
      "[I 2023-12-18 12:29:56,230] Trial 142 finished with value: 0.23332554995383448 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 61, 'subsample': 0.9553721721313656, 'colsample_bynode': 0.9552282809262806, 'reg_lambda': 0.002469659849466033, 'learning_rate': 0.36245786759714094}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:29:56,338] Trial 143 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:56,453] Trial 144 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:57,215] Trial 145 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:29:57,614] Trial 146 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:29:59,296] Trial 147 pruned. Trial was pruned at iteration 39.\n",
      "[I 2023-12-18 12:29:59,338] Trial 148 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:29:59,478] Trial 149 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:03,481] Trial 150 pruned. Trial was pruned at iteration 78.\n",
      "[I 2023-12-18 12:30:05,878] Trial 151 pruned. Trial was pruned at iteration 43.\n",
      "[I 2023-12-18 12:30:09,669] Trial 152 pruned. Trial was pruned at iteration 73.\n",
      "[I 2023-12-18 12:30:09,718] Trial 153 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:11,498] Trial 154 pruned. Trial was pruned at iteration 34.\n",
      "[I 2023-12-18 12:30:11,601] Trial 155 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:11,802] Trial 156 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:16,702] Trial 157 pruned. Trial was pruned at iteration 42.\n",
      "[I 2023-12-18 12:30:16,988] Trial 158 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:17,385] Trial 159 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:30:17,735] Trial 160 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:30:19,373] Trial 161 pruned. Trial was pruned at iteration 28.\n",
      "[I 2023-12-18 12:30:21,330] Trial 162 pruned. Trial was pruned at iteration 34.\n",
      "[I 2023-12-18 12:30:23,622] Trial 163 pruned. Trial was pruned at iteration 41.\n",
      "[I 2023-12-18 12:30:26,934] Trial 164 pruned. Trial was pruned at iteration 69.\n",
      "[I 2023-12-18 12:30:30,819] Trial 165 pruned. Trial was pruned at iteration 98.\n",
      "[I 2023-12-18 12:30:31,561] Trial 166 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:30:31,653] Trial 167 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:31,727] Trial 168 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:31,832] Trial 169 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:32,049] Trial 170 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:36,167] Trial 171 pruned. Trial was pruned at iteration 78.\n",
      "[I 2023-12-18 12:30:37,895] Trial 172 pruned. Trial was pruned at iteration 31.\n",
      "[I 2023-12-18 12:30:37,966] Trial 173 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:38,838] Trial 174 pruned. Trial was pruned at iteration 16.\n",
      "[I 2023-12-18 12:30:40,726] Trial 175 pruned. Trial was pruned at iteration 36.\n",
      "[I 2023-12-18 12:30:41,816] Trial 176 pruned. Trial was pruned at iteration 17.\n",
      "[I 2023-12-18 12:30:41,900] Trial 177 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:42,054] Trial 178 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:42,147] Trial 179 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:46,545] Trial 180 finished with value: 0.23529337122801272 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 59, 'subsample': 0.8974618962096889, 'colsample_bynode': 0.9967038986254961, 'reg_lambda': 0.005339111496470953, 'learning_rate': 0.39615710466759463}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:30:47,926] Trial 181 pruned. Trial was pruned at iteration 23.\n",
      "[I 2023-12-18 12:30:51,224] Trial 182 pruned. Trial was pruned at iteration 64.\n",
      "[I 2023-12-18 12:30:53,110] Trial 183 pruned. Trial was pruned at iteration 31.\n",
      "[I 2023-12-18 12:30:54,183] Trial 184 pruned. Trial was pruned at iteration 17.\n",
      "[I 2023-12-18 12:30:54,240] Trial 185 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:55,958] Trial 186 pruned. Trial was pruned at iteration 31.\n",
      "[I 2023-12-18 12:30:56,430] Trial 187 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:30:56,710] Trial 188 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:56,977] Trial 189 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:58,372] Trial 190 pruned. Trial was pruned at iteration 30.\n",
      "[I 2023-12-18 12:30:58,484] Trial 191 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:30:58,676] Trial 192 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:30:59,875] Trial 193 pruned. Trial was pruned at iteration 26.\n",
      "[I 2023-12-18 12:30:59,967] Trial 194 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:00,551] Trial 195 pruned. Trial was pruned at iteration 15.\n",
      "[I 2023-12-18 12:31:00,874] Trial 196 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:31:00,953] Trial 197 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:05,269] Trial 198 finished with value: 0.2343940307920827 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 58, 'subsample': 0.9097895674592593, 'colsample_bynode': 0.9241807358904308, 'reg_lambda': 0.0025696750172718758, 'learning_rate': 0.36712952127659226}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:31:07,254] Trial 199 pruned. Trial was pruned at iteration 36.\n",
      "[I 2023-12-18 12:31:11,686] Trial 200 pruned. Trial was pruned at iteration 87.\n",
      "[I 2023-12-18 12:31:13,213] Trial 201 pruned. Trial was pruned at iteration 24.\n",
      "[I 2023-12-18 12:31:13,541] Trial 202 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:31:14,486] Trial 203 pruned. Trial was pruned at iteration 15.\n",
      "[I 2023-12-18 12:31:14,677] Trial 204 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:14,778] Trial 205 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:15,027] Trial 206 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:19,849] Trial 207 pruned. Trial was pruned at iteration 100.\n",
      "[I 2023-12-18 12:31:20,918] Trial 208 pruned. Trial was pruned at iteration 18.\n",
      "[I 2023-12-18 12:31:22,142] Trial 209 pruned. Trial was pruned at iteration 21.\n",
      "[I 2023-12-18 12:31:23,167] Trial 210 pruned. Trial was pruned at iteration 17.\n",
      "[I 2023-12-18 12:31:23,364] Trial 211 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:23,446] Trial 212 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:25,151] Trial 213 pruned. Trial was pruned at iteration 31.\n",
      "[I 2023-12-18 12:31:26,172] Trial 214 pruned. Trial was pruned at iteration 21.\n",
      "[I 2023-12-18 12:31:31,456] Trial 215 pruned. Trial was pruned at iteration 97.\n",
      "[I 2023-12-18 12:31:31,947] Trial 216 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:31:33,008] Trial 217 pruned. Trial was pruned at iteration 16.\n",
      "[I 2023-12-18 12:31:34,202] Trial 218 pruned. Trial was pruned at iteration 18.\n",
      "[I 2023-12-18 12:31:34,353] Trial 219 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:34,643] Trial 220 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:39,109] Trial 221 finished with value: 0.233104007067142 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 53, 'subsample': 0.9388678832479473, 'colsample_bynode': 0.6996061630235367, 'reg_lambda': 0.001885517075291961, 'learning_rate': 0.354269346020961}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:31:39,950] Trial 222 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:31:41,664] Trial 223 pruned. Trial was pruned at iteration 33.\n",
      "[I 2023-12-18 12:31:41,808] Trial 224 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:41,936] Trial 225 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:43,221] Trial 226 pruned. Trial was pruned at iteration 23.\n",
      "[I 2023-12-18 12:31:45,530] Trial 227 pruned. Trial was pruned at iteration 21.\n",
      "[I 2023-12-18 12:31:47,660] Trial 228 pruned. Trial was pruned at iteration 40.\n",
      "[I 2023-12-18 12:31:47,785] Trial 229 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:48,163] Trial 230 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:31:48,240] Trial 231 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:48,523] Trial 232 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-18 12:31:50,737] Trial 233 pruned. Trial was pruned at iteration 40.\n",
      "[I 2023-12-18 12:31:50,834] Trial 234 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:50,967] Trial 235 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:53,031] Trial 236 pruned. Trial was pruned at iteration 40.\n",
      "[I 2023-12-18 12:31:53,107] Trial 237 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:53,197] Trial 238 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:54,463] Trial 239 pruned. Trial was pruned at iteration 20.\n",
      "[I 2023-12-18 12:31:54,590] Trial 240 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:31:54,855] Trial 241 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:00,079] Trial 242 pruned. Trial was pruned at iteration 78.\n",
      "[I 2023-12-18 12:32:05,524] Trial 243 pruned. Trial was pruned at iteration 78.\n",
      "[I 2023-12-18 12:32:05,680] Trial 244 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:06,056] Trial 245 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:32:06,310] Trial 246 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:08,107] Trial 247 pruned. Trial was pruned at iteration 20.\n",
      "[I 2023-12-18 12:32:08,299] Trial 248 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:08,502] Trial 249 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:10,311] Trial 250 pruned. Trial was pruned at iteration 31.\n",
      "[I 2023-12-18 12:32:10,610] Trial 251 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:32:11,588] Trial 252 pruned. Trial was pruned at iteration 19.\n",
      "[I 2023-12-18 12:32:11,697] Trial 253 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:13,737] Trial 254 pruned. Trial was pruned at iteration 37.\n",
      "[I 2023-12-18 12:32:15,376] Trial 255 pruned. Trial was pruned at iteration 26.\n",
      "[I 2023-12-18 12:32:15,592] Trial 256 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:15,881] Trial 257 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:21,612] Trial 258 finished with value: 0.23191489668074397 and parameters: {'tree_method': 'hist', 'max_depth': 12, 'min_child_weight': 47, 'subsample': 0.9870930918970289, 'colsample_bynode': 0.5475181669854663, 'reg_lambda': 0.010495785532101563, 'learning_rate': 0.3557977314311295}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:32:26,299] Trial 259 finished with value: 0.2347719539013114 and parameters: {'tree_method': 'hist', 'max_depth': 12, 'min_child_weight': 37, 'subsample': 0.9866422279264327, 'colsample_bynode': 0.4883681179837229, 'reg_lambda': 0.014019118578073005, 'learning_rate': 0.35531826318683224}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:32:28,200] Trial 260 pruned. Trial was pruned at iteration 23.\n",
      "[I 2023-12-18 12:32:28,752] Trial 261 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:32:31,485] Trial 262 pruned. Trial was pruned at iteration 37.\n",
      "[I 2023-12-18 12:32:31,692] Trial 263 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:31,908] Trial 264 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:33,112] Trial 265 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:32:34,131] Trial 266 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:32:36,441] Trial 267 pruned. Trial was pruned at iteration 26.\n",
      "[I 2023-12-18 12:32:36,645] Trial 268 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:43,266] Trial 269 finished with value: 0.23312724441107543 and parameters: {'tree_method': 'hist', 'max_depth': 12, 'min_child_weight': 82, 'subsample': 0.9425170730642002, 'colsample_bynode': 0.5415243553072694, 'reg_lambda': 0.004620792774382424, 'learning_rate': 0.3546491184263859}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:32:43,438] Trial 270 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:43,621] Trial 271 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:45,694] Trial 272 pruned. Trial was pruned at iteration 27.\n",
      "[I 2023-12-18 12:32:50,510] Trial 273 pruned. Trial was pruned at iteration 73.\n",
      "[I 2023-12-18 12:32:50,576] Trial 274 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:50,663] Trial 275 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:50,741] Trial 276 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:50,812] Trial 277 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:50,869] Trial 278 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:50,923] Trial 279 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:51,178] Trial 280 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:32:52,147] Trial 281 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:32:52,330] Trial 282 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:32:52,410] Trial 283 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:52,477] Trial 284 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:52,642] Trial 285 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:52,800] Trial 286 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:54,847] Trial 287 pruned. Trial was pruned at iteration 23.\n",
      "[I 2023-12-18 12:32:55,753] Trial 288 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:32:55,891] Trial 289 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:56,297] Trial 290 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:32:56,391] Trial 291 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:56,513] Trial 292 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:56,959] Trial 293 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:32:57,108] Trial 294 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:57,619] Trial 295 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:32:57,700] Trial 296 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:57,886] Trial 297 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:32:58,755] Trial 298 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:32:59,295] Trial 299 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:33:00,014] Trial 300 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:33:00,536] Trial 301 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:33:00,676] Trial 302 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:00,819] Trial 303 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:01,016] Trial 304 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-18 12:33:04,749] Trial 305 pruned. Trial was pruned at iteration 49.\n",
      "[I 2023-12-18 12:33:04,855] Trial 306 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:04,921] Trial 307 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:09,319] Trial 308 pruned. Trial was pruned at iteration 73.\n",
      "[I 2023-12-18 12:33:09,374] Trial 309 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:09,517] Trial 310 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:09,564] Trial 311 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:10,355] Trial 312 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:33:10,917] Trial 313 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:33:11,038] Trial 314 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:11,138] Trial 315 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:11,966] Trial 316 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:33:12,051] Trial 317 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:12,189] Trial 318 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:12,810] Trial 319 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:33:13,087] Trial 320 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:13,633] Trial 321 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:33:16,733] Trial 322 pruned. Trial was pruned at iteration 41.\n",
      "[I 2023-12-18 12:33:16,871] Trial 323 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:16,971] Trial 324 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:17,055] Trial 325 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:17,230] Trial 326 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:17,785] Trial 327 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:33:18,079] Trial 328 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:33:18,202] Trial 329 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:18,425] Trial 330 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:19,419] Trial 331 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:33:19,553] Trial 332 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:19,914] Trial 333 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:33:20,759] Trial 334 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:33:20,858] Trial 335 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:20,942] Trial 336 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:21,102] Trial 337 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:21,194] Trial 338 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:26,010] Trial 339 pruned. Trial was pruned at iteration 73.\n",
      "[I 2023-12-18 12:33:26,081] Trial 340 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:26,565] Trial 341 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:33:26,861] Trial 342 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:28,020] Trial 343 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:33:28,621] Trial 344 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:33:28,748] Trial 345 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:28,825] Trial 346 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:28,974] Trial 347 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:29,080] Trial 348 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:29,149] Trial 349 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:30,212] Trial 350 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:33:30,546] Trial 351 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:33:30,688] Trial 352 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:39,032] Trial 353 finished with value: 0.23343779502854026 and parameters: {'tree_method': 'approx', 'max_depth': 11, 'min_child_weight': 52, 'subsample': 0.9989539655781324, 'colsample_bynode': 0.9742603236964631, 'reg_lambda': 0.6773986703998108, 'learning_rate': 0.3564647601205808}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:33:39,116] Trial 354 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:39,198] Trial 355 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:39,294] Trial 356 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:39,639] Trial 357 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-18 12:33:40,935] Trial 358 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:33:45,831] Trial 359 pruned. Trial was pruned at iteration 34.\n",
      "[I 2023-12-18 12:33:46,065] Trial 360 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:46,152] Trial 361 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:46,448] Trial 362 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:48,845] Trial 363 pruned. Trial was pruned at iteration 29.\n",
      "[I 2023-12-18 12:33:49,050] Trial 364 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:49,448] Trial 365 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:33:49,530] Trial 366 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:49,608] Trial 367 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:52,239] Trial 368 pruned. Trial was pruned at iteration 29.\n",
      "[I 2023-12-18 12:33:52,498] Trial 369 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:53,519] Trial 370 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:33:53,589] Trial 371 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:54,290] Trial 372 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:33:56,501] Trial 373 pruned. Trial was pruned at iteration 33.\n",
      "[I 2023-12-18 12:33:56,577] Trial 374 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:57,498] Trial 375 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:33:58,274] Trial 376 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:33:58,587] Trial 377 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:58,868] Trial 378 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:59,473] Trial 379 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:33:59,613] Trial 380 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:33:59,725] Trial 381 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:00,256] Trial 382 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:34:01,992] Trial 383 pruned. Trial was pruned at iteration 19.\n",
      "[I 2023-12-18 12:34:02,300] Trial 384 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:02,897] Trial 385 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:34:02,973] Trial 386 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:05,390] Trial 387 pruned. Trial was pruned at iteration 18.\n",
      "[I 2023-12-18 12:34:05,650] Trial 388 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:05,726] Trial 389 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:06,535] Trial 390 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:34:06,671] Trial 391 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:06,857] Trial 392 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:06,965] Trial 393 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:08,847] Trial 394 pruned. Trial was pruned at iteration 17.\n",
      "[I 2023-12-18 12:34:10,318] Trial 395 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:34:10,945] Trial 396 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:34:11,038] Trial 397 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:11,188] Trial 398 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:11,368] Trial 399 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:12,098] Trial 400 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:34:13,030] Trial 401 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:34:14,956] Trial 402 pruned. Trial was pruned at iteration 15.\n",
      "[I 2023-12-18 12:34:15,246] Trial 403 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:16,233] Trial 404 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:34:16,380] Trial 405 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:16,585] Trial 406 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:16,966] Trial 407 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:34:17,170] Trial 408 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:18,101] Trial 409 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:34:18,249] Trial 410 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:18,477] Trial 411 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:18,705] Trial 412 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:21,217] Trial 413 pruned. Trial was pruned at iteration 34.\n",
      "[I 2023-12-18 12:34:21,348] Trial 414 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:21,429] Trial 415 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:22,420] Trial 416 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:34:23,309] Trial 417 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:34:23,876] Trial 418 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:34:24,309] Trial 419 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:25,492] Trial 420 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:34:25,629] Trial 421 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:26,225] Trial 422 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:34:26,315] Trial 423 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:26,473] Trial 424 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:26,829] Trial 425 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:26,928] Trial 426 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:27,174] Trial 427 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:28,268] Trial 428 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:34:28,862] Trial 429 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:34:29,014] Trial 430 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:29,784] Trial 431 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:34:29,854] Trial 432 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:30,853] Trial 433 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:34:32,025] Trial 434 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:34:32,283] Trial 435 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:33,290] Trial 436 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:34:33,907] Trial 437 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:34:33,987] Trial 438 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:34,062] Trial 439 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:34,257] Trial 440 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:34,540] Trial 441 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:34:34,829] Trial 442 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:34:35,052] Trial 443 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:35,947] Trial 444 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:34:36,102] Trial 445 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:41,644] Trial 446 finished with value: 0.23434319453785954 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 47, 'subsample': 0.8601568728387651, 'colsample_bynode': 0.6321239197280608, 'reg_lambda': 0.002514642420010908, 'learning_rate': 0.369753984591031}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:34:46,990] Trial 447 finished with value: 0.23396074954875357 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 39, 'subsample': 0.8447410663935376, 'colsample_bynode': 0.6304024187804362, 'reg_lambda': 0.001694163688834619, 'learning_rate': 0.3682688477641369}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:34:51,659] Trial 448 finished with value: 0.23385707767753605 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 38, 'subsample': 0.8261774085835734, 'colsample_bynode': 0.6295839741224639, 'reg_lambda': 0.0013046844421193235, 'learning_rate': 0.377480915420473}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:34:52,968] Trial 449 pruned. Trial was pruned at iteration 19.\n",
      "[I 2023-12-18 12:34:53,129] Trial 450 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:34:54,466] Trial 451 pruned. Trial was pruned at iteration 22.\n",
      "[I 2023-12-18 12:34:55,376] Trial 452 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:34:56,781] Trial 453 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:35:00,861] Trial 454 finished with value: 0.2345876243527982 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 22, 'subsample': 0.8251740994270491, 'colsample_bynode': 0.6127893874992416, 'reg_lambda': 0.0012033948069377007, 'learning_rate': 0.36153717628473964}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:35:01,021] Trial 455 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:01,176] Trial 456 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:02,119] Trial 457 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:35:03,066] Trial 458 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:35:03,226] Trial 459 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:07,130] Trial 460 finished with value: 0.2341164188078632 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 14, 'subsample': 0.8566009579465991, 'colsample_bynode': 0.6304618224126808, 'reg_lambda': 0.001645344603634449, 'learning_rate': 0.38260969769694714}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:35:08,471] Trial 461 pruned. Trial was pruned at iteration 20.\n",
      "[I 2023-12-18 12:35:09,458] Trial 462 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:35:10,901] Trial 463 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:35:11,239] Trial 464 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:35:12,129] Trial 465 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:35:13,081] Trial 466 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:35:13,453] Trial 467 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:35:14,346] Trial 468 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:35:14,828] Trial 469 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:35:14,991] Trial 470 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:16,455] Trial 471 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:35:18,384] Trial 472 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:35:19,422] Trial 473 pruned. Trial was pruned at iteration 15.\n",
      "[I 2023-12-18 12:35:19,577] Trial 474 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:19,667] Trial 475 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:20,428] Trial 476 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:35:20,588] Trial 477 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:20,988] Trial 478 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:35:28,466] Trial 479 finished with value: 0.23469050505403277 and parameters: {'tree_method': 'approx', 'max_depth': 12, 'min_child_weight': 26, 'subsample': 0.8759831405165938, 'colsample_bynode': 0.6656834561951989, 'reg_lambda': 0.002418047272706795, 'learning_rate': 0.3586611721307893}. Best is trial 0 with value: 0.22716975344231757.\n",
      "[I 2023-12-18 12:35:28,754] Trial 480 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:28,855] Trial 481 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:29,178] Trial 482 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:32,405] Trial 483 pruned. Trial was pruned at iteration 19.\n",
      "[I 2023-12-18 12:35:32,936] Trial 484 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:35:33,271] Trial 485 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:33,506] Trial 486 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:33,801] Trial 487 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:35:33,891] Trial 488 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:34,191] Trial 489 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:34,356] Trial 490 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:34,694] Trial 491 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:36,576] Trial 492 pruned. Trial was pruned at iteration 16.\n",
      "[I 2023-12-18 12:35:37,441] Trial 493 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:35:37,524] Trial 494 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:37,606] Trial 495 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:37,779] Trial 496 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:38,480] Trial 497 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:35:39,144] Trial 498 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:35:39,363] Trial 499 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:39,482] Trial 500 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:39,574] Trial 501 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:41,584] Trial 502 pruned. Trial was pruned at iteration 28.\n",
      "[I 2023-12-18 12:35:41,848] Trial 503 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:42,129] Trial 504 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:42,580] Trial 505 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:35:44,025] Trial 506 pruned. Trial was pruned at iteration 24.\n",
      "[I 2023-12-18 12:35:44,546] Trial 507 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:35:44,916] Trial 508 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:45,528] Trial 509 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:35:45,781] Trial 510 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:46,078] Trial 511 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:46,416] Trial 512 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:35:46,567] Trial 513 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:47,505] Trial 514 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:35:47,831] Trial 515 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:35:47,918] Trial 516 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:49,316] Trial 517 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:35:49,552] Trial 518 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:49,692] Trial 519 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:50,562] Trial 520 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:35:50,876] Trial 521 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:51,018] Trial 522 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:51,255] Trial 523 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:54,302] Trial 524 pruned. Trial was pruned at iteration 47.\n",
      "[I 2023-12-18 12:35:54,441] Trial 525 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:54,576] Trial 526 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:54,768] Trial 527 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:35:55,718] Trial 528 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:35:55,875] Trial 529 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:56,961] Trial 530 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:35:57,821] Trial 531 pruned. Trial was pruned at iteration 13.\n",
      "[I 2023-12-18 12:35:58,205] Trial 532 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:35:58,368] Trial 533 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:59,075] Trial 534 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:35:59,540] Trial 535 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:59,839] Trial 536 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:35:59,940] Trial 537 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:00,625] Trial 538 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:36:01,073] Trial 539 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:36:01,330] Trial 540 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:36:01,857] Trial 541 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-18 12:36:04,318] Trial 542 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:36:05,032] Trial 543 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:36:05,165] Trial 544 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:05,256] Trial 545 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:05,532] Trial 546 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:05,697] Trial 547 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:06,694] Trial 548 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:36:07,785] Trial 549 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:36:07,929] Trial 550 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:08,017] Trial 551 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:09,677] Trial 552 pruned. Trial was pruned at iteration 19.\n",
      "[I 2023-12-18 12:36:09,817] Trial 553 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:10,401] Trial 554 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-18 12:36:11,084] Trial 555 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:36:12,257] Trial 556 pruned. Trial was pruned at iteration 14.\n",
      "[I 2023-12-18 12:36:12,374] Trial 557 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:12,493] Trial 558 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:12,774] Trial 559 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:36:13,022] Trial 560 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:13,350] Trial 561 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:14,324] Trial 562 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:36:14,458] Trial 563 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:15,082] Trial 564 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:36:15,838] Trial 565 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:36:16,347] Trial 566 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-18 12:36:16,614] Trial 567 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:16,847] Trial 568 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:17,547] Trial 569 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:36:18,433] Trial 570 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-18 12:36:18,580] Trial 571 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:18,655] Trial 572 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:18,976] Trial 573 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:19,246] Trial 574 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:19,392] Trial 575 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:19,788] Trial 576 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-18 12:36:20,132] Trial 577 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:20,683] Trial 578 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-18 12:36:20,921] Trial 579 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:21,417] Trial 580 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:36:22,159] Trial 581 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-18 12:36:22,267] Trial 582 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:22,474] Trial 583 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-18 12:36:23,414] Trial 584 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-18 12:36:26,167] Trial 585 pruned. Trial was pruned at iteration 14.\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "study = optuna.create_study(direction='minimize')\n",
    "tic = time.time()\n",
    "while time.time() - tic < 600:\n",
    "    study.optimize(objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 ==============================\n",
      "best score = 0.22716975344231757\n",
      "best params --------------------------\n",
      "tree_method : hist\n",
      "max_depth : 7\n",
      "min_child_weight : 47\n",
      "subsample : 0.644239586943823\n",
      "colsample_bynode : 0.6324287447379262\n",
      "reg_lambda : 0.008813955729768966\n",
      "learning_rate : 0.12041680713201537\n"
     ]
    }
   ],
   "source": [
    "print('Stage 1 ==============================')\n",
    "print(f'best score = {study.best_trial.value}')\n",
    "print('best params --------------------------')\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are happy with this result, we can go ahead and train a final model on the training+validation set using these parameter values and our fixed number of boosting rounds.\n",
    "If we decide we want to tune the tree parameters a little more, we can just call `study.optimize(...)` again, adding as many trials as we want.\n",
    "Once we're happy with the tree parameters, if we want more accuracy and  are willing to accept longer training time and a bigger model, we can proceed to stage 2.\n",
    "\n",
    "## Stage 2: Tune Boosting Parameters via Early Stopping\n",
    "\n",
    "Now we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate&mdash;here I use 0.01, but you could go lower&mdash;and a large number of boosting rounds.\n",
    "The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you'll need to max out the evaluation metric on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "params = {}\n",
    "params.update(base_params)\n",
    "params.update(study.best_trial.params)\n",
    "params['learning_rate'] = 0.01\n",
    "model_stage2 = xgb.train(params=params, dtrain=dtrain, \n",
    "                         num_boost_round=10000,\n",
    "                         evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                         early_stopping_rounds=50,\n",
    "                         verbose_eval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 ==============================\n",
      "best score = 0.22271610796451569\n",
      "best iteration = 6115\n"
     ]
    }
   ],
   "source": [
    "print('Stage 2 ==============================')\n",
    "print(f'best score = {score_model(model_stage2, dvalid)}')\n",
    "best_iteration = model_stage2.best_iteration\n",
    "print(f'best iteration = {best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Final Model and Evaluate on Test Data\n",
    "\n",
    "Now we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2.\n",
    "Then we evaluate on the held out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "params['learning_rate'] = 0.01\n",
    "model_final = xgb.train(params=params, dtrain=dtrainvalid, \n",
    "                        num_boost_round=best_iteration,\n",
    "                        evals=[(dtrain, 'train')],\n",
    "                        verbose_eval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model ==========================\n",
      "test score = 0.21926622092723846\n",
      "parameters ---------------------------\n",
      "objective : reg:squarederror\n",
      "eval_metric : rmse\n",
      "tree_method : hist\n",
      "max_depth : 7\n",
      "min_child_weight : 47\n",
      "subsample : 0.644239586943823\n",
      "colsample_bynode : 0.6324287447379262\n",
      "reg_lambda : 0.008813955729768966\n",
      "learning_rate : 0.01\n",
      "num_boost_round: 6115\n"
     ]
    }
   ],
   "source": [
    "print('Final Model ==========================')\n",
    "print(f'test score = {score_model(model_final, dtest)}')\n",
    "print('parameters ---------------------------')\n",
    "for k, v in params.items():\n",
    "    print(k, ':', v)\n",
    "print(f'num_boost_round: {best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in the [regression post](/posts/xgboost-for-regression-in-python/) \n",
    "we got an RMSE of about 0.231 just using default parameter values, which put us in about 5th place on the [leaderboard for the Kagle dozers competition](https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard).\n",
    "Now with less than 15 minutes of hyperparameter tuning, our RMSE is down to 0.219 which puts us in 1st place by a huge margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "There it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna.\n",
    "If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
