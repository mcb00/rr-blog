<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Matt Bowers">
<meta name="dcterms.date" content="2022-03-13">
<meta name="description" content="In-depth intuition and mathematical derivation of the XGBoost algorithm">

<title>Random Realizations - How to Understand XGBoost</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Random Realizations</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../gradient-boosting-series.html">
 <span class="menu-text">Gradient Boosting</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mcb00"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mcbwrs"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<span style="font-weight: 600;">Subscribe</span>

<!-- Begin Mailchimp Signup Form -->

<link href="../../../subscribe.css" rel="stylesheet" type="text/css">
<style type="text/css">
    #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif;  width:170px;}
    /* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.
       We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */
    #mc-embedded-subscribe-form{margin-left:-5px;}
</style>

<div id="mc_embed_signup">
<form action="https://dev.us20.list-manage.com/subscribe/post?u=5212e33f7cd396dd4a742431c&amp;id=0a2f69f3f3&amp;f_id=002e29e8f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_self">
    <div id="mc_embed_signup_scroll">

    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required="">
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f718424fc5df77c22533bdaa6_a3c37fb57b" tabindex="-1" value=""></div>
        <div class="optionalParent">
            <div class="clear foot" style="margin-top: 10px;">
                <input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button">
                <p class="brandingLogo"></p>
            </div>
        </div>
    </div>
</form>
</div>

<!--End mc_embed_signup-->
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#xgboost-is-a-gradient-boosting-machine" id="toc-xgboost-is-a-gradient-boosting-machine" class="nav-link active" data-scroll-target="#xgboost-is-a-gradient-boosting-machine">XGBoost is a Gradient Boosting Machine</a></li>
  <li><a href="#descent-algorithm-innovations" id="toc-descent-algorithm-innovations" class="nav-link" data-scroll-target="#descent-algorithm-innovations">Descent Algorithm Innovations</a>
  <ul class="collapse">
  <li><a href="#regularized-objective-function" id="toc-regularized-objective-function" class="nav-link" data-scroll-target="#regularized-objective-function">Regularized Objective Function</a></li>
  <li><a href="#an-aside-on-newtons-method" id="toc-an-aside-on-newtons-method" class="nav-link" data-scroll-target="#an-aside-on-newtons-method">An Aside on Newton’s Method</a></li>
  <li><a href="#tree-boosting-with-newtons-method" id="toc-tree-boosting-with-newtons-method" class="nav-link" data-scroll-target="#tree-boosting-with-newtons-method">Tree Boosting with Newton’s Method</a></li>
  </ul></li>
  <li><a href="#tree-booster-innovations" id="toc-tree-booster-innovations" class="nav-link" data-scroll-target="#tree-booster-innovations">Tree Booster Innovations</a>
  <ul class="collapse">
  <li><a href="#missing-values-and-sparsity-aware-split-finding" id="toc-missing-values-and-sparsity-aware-split-finding" class="nav-link" data-scroll-target="#missing-values-and-sparsity-aware-split-finding">Missing Values and Sparsity-Aware Split Finding</a></li>
  <li><a href="#preventing-further-splitting" id="toc-preventing-further-splitting" class="nav-link" data-scroll-target="#preventing-further-splitting">Preventing Further Splitting</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">Sampling</a></li>
  </ul></li>
  <li><a href="#scalability" id="toc-scalability" class="nav-link" data-scroll-target="#scalability">Scalability</a>
  <ul class="collapse">
  <li><a href="#why-xgboost-is-so-successful" id="toc-why-xgboost-is-so-successful" class="nav-link" data-scroll-target="#why-xgboost-is-so-successful">Why XGBoost is so Successful</a></li>
  </ul></li>
  <li><a href="#wrapping-up" id="toc-wrapping-up" class="nav-link" data-scroll-target="#wrapping-up">Wrapping Up</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  <li><a href="#exercise" id="toc-exercise" class="nav-link" data-scroll-target="#exercise">Exercise</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">How to Understand XGBoost</h1>
  <div class="quarto-categories">
    <div class="quarto-category">gradient boosting</div>
  </div>
  </div>

<div>
  <div class="description">
    In-depth intuition and mathematical derivation of the XGBoost algorithm
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Matt Bowers </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 13, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><img src="20220313_thumbnail.jpg" title="Tree branches on a chilly day in Johnson City, TN" class="img-fluid"></p>
<p>Ahh, XGBoost, what an absolutely stellar implementation of gradient boosting. Once Tianqi Chen and Carlos Guestrin of the University of Washington published the <a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">XGBoost paper</a> and shared the <a href="https://github.com/dmlc/xgboost">open source code</a> in the mid 2010’s, the algorithm quickly gained adoption in the ML community, appearing in over half of winning Kagle submissions in 2015. Nowadays it’s certainly among the most popular gradient boosting libraries, along with LightGBM and CatBoost, although the highly scientific indicator of <a href="https://www.githubcompare.com/dmlc/xgboost+microsoft/lightgbm+catboost/catboost">GitHub stars per year</a> indicates that it is in fact the most beloved gradient boosting package of all. Since it was the first of the modern popular boosting frameworks, and since <a href="https://blog.dataiku.com/the-many-flavors-of-gradient-boosting-algorithms">benchmarking</a> indicates that no other boosting algorithm outperforms it, we can comfortably focus our attention on understanding XGBoost.</p>
<p>The XGBoost authors identify two key aspects of a machine learning system: (1) a flexible statistical model and (2) a scalable learning system to fit that model using data. XGBoost improves on both of these aspects, providing a more flexible and feature-rich statistical model and building a truly scalable system to fit it. In this post we’re going to focus on the statistical modeling innovations, outlining the key differences from the classic gradient boosting machine and divinginto the mathematical derivation of the XGBoost learning algorithm. If you’re not already familiar with <a href="../../../gradient-boosting-machine-from-scratch">gradient boosting</a>, go back and read the earlier posts in the series before jumping in here.</p>
<p>Buckle up, dear reader. Today we understand how XGBoost works, no hand waving required.</p>
<section id="xgboost-is-a-gradient-boosting-machine" class="level2">
<h2 class="anchored" data-anchor-id="xgboost-is-a-gradient-boosting-machine">XGBoost is a Gradient Boosting Machine</h2>
<p>At a high level, XGBoost is an iteratively constructed composite model, just like the classic gradient boosting machine we discussed back in the <a href="../../../gradient-boosting-machine-from-scratch">GBM post</a> . The final model takes the form</p>
<p><span class="math display">\[\hat{y}_i = F(\mathbf{x}_i) = b + \eta \sum_{k=1}^K f_k(\mathbf{x}_i) \]</span></p>
<p>where <span class="math inline">\(b\)</span> is the base prediction, <span class="math inline">\(\eta\)</span> is the learning rate hyperparameter that helps control overfitting by reducing the contributions of each booster, and each of the <span class="math inline">\(K\)</span> boosters <span class="math inline">\(f_k\)</span> is a decision tree. To help us connect the dots between theory and code, whenever we encounter new hyperparameters, I’ll point out their names from the <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">XGBoost Parameter Documentation</a>. So, <span class="math inline">\(b\)</span> can be set by <code>base_score</code>, and <span class="math inline">\(\eta\)</span> is set by either <code>eta</code> or <code>learning_rate</code>.</p>
<p>XGBoost introduces two key statistical learning improvements over the classic gradient boosting model. First, it reimagines the gradient descent algorithm used for training, and second it uses a custom-built decision tree with extra functionality as its booster. We’ll dive into each of these key innovations in the following sections.</p>
</section>
<section id="descent-algorithm-innovations" class="level2">
<h2 class="anchored" data-anchor-id="descent-algorithm-innovations">Descent Algorithm Innovations</h2>
<section id="regularized-objective-function" class="level3">
<h3 class="anchored" data-anchor-id="regularized-objective-function">Regularized Objective Function</h3>
<p>In the post on <a href="../../../gradient-boosting-machine-with-any-loss-function">GBM with any loss function</a>, we looked at loss functions of the form <span class="math inline">\(\sum_i l(y_i,\hat{y}_i)\)</span> which compute some distance between targets <span class="math inline">\(y_i\)</span> and predictions <span class="math inline">\(\hat{y}_i\)</span> and sum them up over the training dataset. XGBoost introduces regularization into the objective function so that the objective takes the form</p>
<p><span class="math display">\[ L = \sum_i l(y_i,\hat{y}_i) + \sum_k \Omega(f_k) \]</span></p>
<p>where <span class="math inline">\(l\)</span> is some twice-differentiable loss function. <span class="math inline">\(\Omega\)</span> is a regularization that penalizes the complexity of each tree booster, taking the form</p>
<p><span class="math display">\[ \Omega(f) = \gamma T + \frac{1}{2} \lambda ||w||^2 \]</span></p>
<p>where <span class="math inline">\(T\)</span> is the number of leaf nodes and <span class="math inline">\(||w||^2\)</span> is the squared sum of the leaf prediction values. This introduces two new hyperparameters: <span class="math inline">\(\gamma\)</span> which penalizes the number of leaf nodes and <span class="math inline">\(\lambda\)</span> which is the L-2 regularization parameter for leaf predicted values. These are set by <code>gamma</code> and <code>reg_lambbda</code> in the XGBoost parametrization. Together, these provide powerful new controls to reduce overfitting due to overly complex tree boosters. Note that <span class="math inline">\(\gamma=\lambda=0\)</span> reduces the objective back to an unregularized loss function as used in the classic GBM.</p>
</section>
<section id="an-aside-on-newtons-method" class="level3">
<h3 class="anchored" data-anchor-id="an-aside-on-newtons-method">An Aside on Newton’s Method</h3>
<p>As we’ll see soon, XGBoost uses <a href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton’s Method</a> to minimize its objective function, so let’s start with a quick refresher.</p>
<p>Newton’s method is an iterative procedure for minimizing a function <span class="math inline">\(s(x)\)</span>. At each step we have some input <span class="math inline">\(x_t\)</span>, and our goal is to find a nudge value <span class="math inline">\(u\)</span> such that</p>
<p><span class="math display">\[ s(x_t + u) \le s(x_t)\]</span></p>
<p>To find a good nudge value <span class="math inline">\(u\)</span>, we generate a local quadratic approximation of the function in the neighborhood of the input <span class="math inline">\(x_t\)</span>, and then we find the input value that would bring us to the minimum of the quadratic approximation.</p>
<p><img src="20220313_newton.png" title="Schematic of Newton's method" class="img-fluid"></p>
<p>The figure shows a single Newton step where we start at <span class="math inline">\(x_t\)</span>, find the local quadratic approximation, and then jump a distance <span class="math inline">\(u\)</span> along the <span class="math inline">\(x\)</span>-axis to land at the minimum of the quadratic. If we iterate in this way, we are likely to land close to the minimum of <span class="math inline">\(s(x)\)</span>.</p>
<p>So how do we compute the quadratic approximation? We use the second order Taylor series expansion of <span class="math inline">\(s(x)\)</span> near the point <span class="math inline">\(x_t\)</span>.</p>
<p><span class="math display">\[ s(x_t + u) \approx  s(x_t) + s'(x_t)u + \frac{1}{2} s''(x_t) u^2 \]</span></p>
<p>To find the nudge value <span class="math inline">\(u\)</span> that minimizes the quadratic approximation, we can take the derivative with respect to <span class="math inline">\(u\)</span>, set it to zero, and solve for <span class="math inline">\(u\)</span>.</p>
<p><span class="math display">\[ 0 = \frac{d}{du}  \left ( s(x_t) + s'(x_t)u + \frac{1}{2} s''(x_t) u^2 \right ) = s'(x_t) + s''(x_t) u \]</span></p>
<p><span class="math display">\[\rightarrow u^* = -\frac{s'(x_t)}{s''(x_t)} \]</span></p>
<p>And as long as <span class="math inline">\(s''(x_t)&gt;0\)</span> (i.e., the parabola is pointing up), <span class="math inline">\(s(x_t + u^*) \le s(x_t)\)</span>.</p>
</section>
<section id="tree-boosting-with-newtons-method" class="level3">
<h3 class="anchored" data-anchor-id="tree-boosting-with-newtons-method">Tree Boosting with Newton’s Method</h3>
<p>This lands us at the heart of XGBoost, which uses Newton’s method, rather than gradient descent, to guide each round of boosting. This explanation will correspond very closely to section 2.2 of the XGBoost paper, but here I’ll explicitly spell out some of the intermediate steps which are omitted from their derivation, and you’ll get some additional commentary from me along the way.</p>
<section id="newton-descent-in-tree-space" class="level4">
<h4 class="anchored" data-anchor-id="newton-descent-in-tree-space">Newton Descent in Tree Space</h4>
<p>Suppose we’ve done <span class="math inline">\(t-1\)</span> boosting rounds, and we want to add the <span class="math inline">\(t\)</span>-th booster to our composite model. Our current model’s prediction for instance <span class="math inline">\(i\)</span> is <span class="math inline">\(\hat{y}_i^{(t-1)}\)</span>. If we add a new tree booster <span class="math inline">\(f_t\)</span> to our model, the objective function would give</p>
<p><span class="math display">\[ L^{(t)} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)} + f_t(\mathbf{x}_i)) + \Omega(f_t) \]</span></p>
<p>We need to choose <span class="math inline">\(f_t\)</span> so that it decreases the loss, i.e.&nbsp;we want</p>
<p><span class="math display">\[ l(y_i, \hat{y}_i^{(t-1)} + f_t(\mathbf{x}_i)) \le l(y_i, \hat{y}_i^{(t-1)})\]</span></p>
<p>Does that sound familiar? In the previous section we used Newton’s method to find a value of <span class="math inline">\(u\)</span> that would make <span class="math inline">\(s(x_t + u) \le s(x_t)\)</span>. Let’s try the same thing with our loss function. To be explicit, the parallels are: <span class="math inline">\(s(\cdot) \rightarrow l(y_i, \cdot)\)</span>, <span class="math inline">\(x_t \rightarrow \hat{y}_i^{(t-1)}\)</span>, and <span class="math inline">\(u \rightarrow f_t(\mathbf{x}_i)\)</span>.</p>
<p>Let’s start by finding the second order Taylor series approximation for the loss around the point <span class="math inline">\(\hat{y}_i^{(t-1)}\)</span>.</p>
<p><span class="math display">\[ l(y_i, \hat{y}_i^{(t-1)} + f_t(\mathbf{x}_i)) \approx l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(\mathbf{x}_i) + \frac{1}{2} h_i f_t(\mathbf{x}_i)^2 \]</span></p>
<p>where</p>
<p><span class="math display">\[ g_i = \frac{\partial}{\partial \hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)})\]</span></p>
<p>and</p>
<p><span class="math display">\[ h_i = \frac{\partial}{\partial^2 \hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)})\]</span></p>
<p>are the first and second order partial derivatives of the loss with respect to the current predictions. The XGBoost paper calls these the gradients and hessians, respectively. Remember that when we specify an actual loss function to use, we would also specify the functional form of the gradients and hessians, so that they are directly computable.</p>
<p>Now we can go back and substitute our quadratic approximation in for the loss function to get an approximation of the objective function in the neighborhood of <span class="math inline">\(\hat{y}_i^{(t-1)}\)</span>..</p>
<p><span class="math display">\[ L^{(t)} \approx \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(\mathbf{x}_i) + \frac{1}{2} h_i f_t(\mathbf{x}_i)^2] + \Omega(f_t) \]</span></p>
<p>Since <span class="math inline">\(l(y_i,\hat{y}_i^{(t-1)})\)</span> is constant regardless of our choice of <span class="math inline">\(f_t\)</span>, we can drop it and instead work with the modified objective, which gives us Equation (3) from the paper.</p>
<p><span class="math display">\[ \tilde{L}^{(t)} = \sum_{i=1}^n [ g_i f_t(\mathbf{x}_i) + \frac{1}{2} h_i f_t(\mathbf{x}_i)^2] + \Omega(f_t) \]</span></p>
<p>Now the authors are about to do something great. They’re about to show how to directly compute the optimal prediction values for the leaf nodes of <span class="math inline">\(f_t\)</span>. We’ll circle back in a moment about how we find a good structure for <span class="math inline">\(f_t\)</span>, i.e.&nbsp;good node splits, but we’re going to find the optimal predicted values for any tree structure having <span class="math inline">\(T\)</span> terminal nodes. Let <span class="math inline">\(I_j\)</span> denote the set of instances <span class="math inline">\(i\)</span> that are in the <span class="math inline">\(j\)</span>-th leaf node of <span class="math inline">\(f_t\)</span>. Then we can rewrite the objective.</p>
<p><span class="math display">\[ \tilde{L}^{(t)} = \sum_{j=1}^T \left [ \sum_{i \in I_j} g_i f_t(\mathbf{x}_i) + \frac{1}{2}  \sum_{i \in I_j} h_i f_t(\mathbf{x}_i)^2 \right ] + \Omega(f_t)\]</span></p>
<p>We notice that for all instances in <span class="math inline">\(I_j\)</span>, the tree yields the same predicted value <span class="math inline">\(f_t(\mathbf{x}_i)=w_j\)</span>. Substituting in <span class="math inline">\(w_j\)</span> for the predicted values and expanding <span class="math inline">\(\Omega(f_t)\)</span> we get</p>
<p><span class="math display">\[ \tilde{L}^{(t)} = \sum_{j=1}^T \left [ \sum_{i \in I_j} g_i w_j + \frac{1}{2}  \sum_{i \in I_j} h_i w_j^2 \right ] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2\]</span></p>
<p>Rearranging terms we obtain Equation (4).</p>
<p><span class="math display">\[ \tilde{L}^{(t)} = \sum_{j=1}^T \left [ w_j \sum_{i \in I_j} g_i + \frac{1}{2}  w_j^2 \left ( \sum_{i \in I_j} h_i + \lambda \right )  \right ] + \gamma T \]</span></p>
<p>For each leaf node <span class="math inline">\(j\)</span>, our modified objective function is quadratic in <span class="math inline">\(w_j\)</span>. To find the optimal predicted values we take the derivative, set to zero, and solve for <span class="math inline">\(w_j\)</span>.</p>
<p><span class="math display">\[ 0 = \frac{d}{dw_j} \left [ w_j \sum_{i \in I_j} g_i + \frac{1}{2}  w_j^2 \left ( \sum_{i \in I_j} h_i + \lambda \right )  \right ] = \left ( \sum_{i \in I_j} h_i + \lambda \right ) w_j + \sum_{i \in I_j} g_i \]</span></p>
<p>This yields Equation (5).</p>
<p><span class="math display">\[ w_j^* = - \frac{\sum_{i \in I_j} g_i } {\sum_{i \in I_j} h_i + \lambda } \]</span></p>
</section>
<section id="split-finding" class="level4">
<h4 class="anchored" data-anchor-id="split-finding">Split Finding</h4>
<p>Now that we know how to find the optimal predicted value for any leaf node, we need to identify a criterion for finding a good tree structure, which boils down to finding the best split for a given node. Back in the [decision tree from scratch](/decision-tree-from-scratch post, we derived a split evaluation metric based on the reduction in the objective function associated with a particular split.<br>
To do that, first we need a way to compute the objective function given a particular tree structure. Substituting the optimal predicted values <span class="math inline">\(w_j^*\)</span> into the objective function, we get Equation (6).</p>
<p><span class="math display">\[ \tilde{L}^{(t)} = -\frac{1}{2} \sum_{j=1}^T \frac{ (\sum_{i \in I_j} g_i )^2 } {\sum_{i \in I_j} h_i + \lambda} + \gamma T \]</span></p>
<p>We can then evaluate potential splits by comparing the objective before making a split to the objective after making a split, where the split with the maximum reduction in objective (a.k.a. gain) is best.</p>
<p>More formally, let <span class="math inline">\(I\)</span> be the set of <span class="math inline">\(n\)</span> data instances in the current node, and let <span class="math inline">\(I_L\)</span> and <span class="math inline">\(I_R\)</span> be the instances that fall into the left and right child nodes of a proposed split. Let <span class="math inline">\(L\)</span> be the total loss for all instances in the node, while <span class="math inline">\(L_L\)</span> and <span class="math inline">\(L_R\)</span> are the losses for the left and right child nodes. The total loss contributed by instances in node <span class="math inline">\(I\)</span> prior to any split is</p>
<p><span class="math display">\[L_{\text{before split}} = -\frac{1}{2} \frac{ (\sum_{i \in I} g_i )^2 } {\sum_{i \in I} h_i + \lambda} + \gamma \]</span></p>
<p>And the loss after splitting <span class="math inline">\(I\)</span> into <span class="math inline">\(I_L\)</span> and <span class="math inline">\(I_R\)</span> is</p>
<p><span class="math display">\[L_{\text{after split}} = L_L + L_R = -\frac{1}{2}  \frac{ (\sum_{i \in I_L} g_i )^2 } {\sum_{i \in I_L} h_i + \lambda} -\frac{1}{2}  \frac{ (\sum_{i \in I_R} g_i )^2 } {\sum_{i \in I_R} h_i + \lambda} + 2 \gamma \]</span></p>
<p>The gain from this split is then</p>
<p><span class="math display">\[ \Delta L = L_{\text{before split}} -  L_{\text{after split}} = L - (L_L + L_R)\]</span> <span class="math display">\[\Delta L = \frac{1}{2} \left [ \frac{ (\sum_{i \in I_L} g_i )^2 } {\sum_{i \in I_L} h_i + \lambda} + \frac{ (\sum_{i \in I_R} g_i )^2 } {\sum_{i \in I_R} h_i + \lambda} - \frac{ (\sum_{i \in I} g_i )^2 } {\sum_{i \in I} h_i + \lambda} \right ] - \gamma \]</span></p>
<p>which is Equation (7) from the paper. In practice it makes sense to accept a split only if the gain is positive, thus the <span class="math inline">\(\gamma\)</span> parameter sets the minimum gain required to make a further split. This is why <span class="math inline">\(\gamma\)</span> can be set with the parameter <code>gamma</code> or the more descriptive<code>min_loss_split</code>.</p>
</section>
</section>
</section>
<section id="tree-booster-innovations" class="level2">
<h2 class="anchored" data-anchor-id="tree-booster-innovations">Tree Booster Innovations</h2>
<section id="missing-values-and-sparsity-aware-split-finding" class="level3">
<h3 class="anchored" data-anchor-id="missing-values-and-sparsity-aware-split-finding">Missing Values and Sparsity-Aware Split Finding</h3>
<p>The XGBoost paper also introduces a modified algorithm for tree split finding which explicitly handles missing feature values. Recall that in order to find the best threshold value for a given feature, we can simply try all possible threshold values, recording the score for each. If some feature values are missing, the XGBoost split finding algorithm simply scores each threshold twice: once with missing value instances in the left node and once with them in the right node. The best split will then specify both the threshold value and to which node instances with missing values should be assigned. The paper calls this the sparsity aware split finding routine, which is defined as Algorithm 2.</p>
</section>
<section id="preventing-further-splitting" class="level3">
<h3 class="anchored" data-anchor-id="preventing-further-splitting">Preventing Further Splitting</h3>
<p>In addition to <code>min_loss_split</code> discussed above, XGBoost offers another parameter for limiting further tree splitting called <code>min_child_weight</code>. This name is a little confusing to me because the word “weight” has various meanings. In the context of this parameter, “weight” refers to the sum of the hessians <span class="math inline">\(\sum h_i\)</span> over instances in the node. For squared error loss <span class="math inline">\(h_i=1\)</span>, so this is equivalent to the number of samples. Thus this parameter generalizes the notion of the minimum number of samples allowed in a terminal node.</p>
</section>
<section id="sampling" class="level3">
<h3 class="anchored" data-anchor-id="sampling">Sampling</h3>
<p>XGBoost takes a cue from Random Forest and introduces both column and row subsampling. These sampling methods can prevent overfitting and reduce training time by limiting the amount of data to be processed during boosting.</p>
<p>Like random forest, XGBoost implements column subsampling, which limits tree split finding to randomly selected subsets of features. XGBoost provides column sampling for each tree, for each depth level within a tree, and for each split point within a tree, controlled by <code>colsample_bytree</code>, <code>colsample_bbylevel</code>, and <code>colsample_bbynode</code> respectively.</p>
<p>One interesting distinction is that XGBoost implements row sampling without replacement using <code>subbsample</code>, whereas random forest uses bootstrapping. The choice to bootstrap rows in RF probably spurred from a desire to use as much data as possible while training on the smaller datasets of the 1990’s when RF was developed. With larger datasets and the ability to generate a large number of trees, XGBoost simply takes a subsample of rows for each tree.</p>
</section>
</section>
<section id="scalability" class="level2">
<h2 class="anchored" data-anchor-id="scalability">Scalability</h2>
<p>Even though we’re focused on statistical learning, I figured I’d comment on why XGBoost is highly scalable. Basically it boils down to efficient, parallelizable, and distributable methods for growing trees. You’ll notice there is a <code>tree_method</code> parameter which allows you to choose between the greedy exact algorithm (like the one we discussed in the decision tree from scratch post) and the approximate algorithm, which offers various scalability-related functionality, notably including the ability to consider only a small number of candidate split points instead of trying all possible splits. The algorithm also uses clever tricks like pre-sorting data for split finding and caching frequently needed values.</p>
<section id="why-xgboost-is-so-successful" class="level3">
<h3 class="anchored" data-anchor-id="why-xgboost-is-so-successful">Why XGBoost is so Successful</h3>
<p>As I mentioned in the intro, XGBoost is simply a very good implementation of the gradient boosting tree model. Therefore it inherits all the benefits of <a href="../../../consider-the-decision-tree">decision trees and tree ensembles</a>, while making even further improvements over the classic gradient boosting machine. These improvements boil down to</p>
<ol type="1">
<li>more ways to control overfitting</li>
<li>elegant handling of custom objectives</li>
<li>scalability</li>
</ol>
<p>First, XGBoost introduces two new tree regularization hyperparameters <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\lambda\)</span> which are baked directly into its objective function. Combining these with the additional column and row sampling functionality provides a variety of ways to reduce overfitting.</p>
<p>Second, the XGBoost formulation provides a much more elegant way to train models on custom objective functions. Recall that for <a href="../../../gradient-boosting-machine-with-any-loss-function">custom objectives</a>, the classic GBM finds tree structure by fitting a squared error decision tree to the gradients of the loss function and then sets each leaf’s predicted value by running a numerical optimization routine to find the optimal predicted value.</p>
<p>The XGBoost formulation improves on this two-stage approach by unifying the generation of tree structure and predicted values. Both the split scoring metric and the predicted values are directly computable from the instance gradient and hessian values, which are connected directly back to the overall training objective. This also removes the need for additional numerical optimizations, which contributes to speed, stability, and scalability.</p>
<p>Finally, speaking of scalability, XGBoost emerged at a time when industrial dataset size was exploding. Many use cases require scalable ML systems, and all use cases benefit from faster training and higher model development velocity.</p>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>Well, there you go, those are the salient ideas behind XGBoost, the gold standard in gradient boosting model implementations. Hopefully now we all understand the mathematical basis for the algorithm and appreciate the key improvements it makes over the classic GBM. If you want to go even deeper, you can join us for the next post where we’ll roll up our sleeves and implement XGBoost entirely from scratch.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p><a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">The XGBoost paper</a></p>
</section>
<section id="exercise" class="level2">
<h2 class="anchored" data-anchor-id="exercise">Exercise</h2>
<p>Proove that the XGBoost Newton Descent generalizes the classic GBM gradient descent. Hint: show that XGBoost with a squared error objective and no regularization reduces to the classic GBM.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>