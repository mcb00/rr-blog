<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Matt Bowers">
<meta name="dcterms.date" content="2021-06-22">
<meta name="description" content="Get up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale.">

<title>Random Realizations – Hello PySpark!</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" defer="" src="https://umami.randomrealizations.com/script.js" data-domains="randomrealizations.com" data-website-id="6ea1ee3b-0737-4922-bf92-ef22afd88db1"></script>


<link rel="stylesheet" href="../../styles.css">
<link rel="canonical" href="https://randomrealizations.com/posts/hello-pyspark/" />
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Random Realizations</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html" rel="" target="">
 <span class="menu-text">Archive</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../gradient-boosting-series.html" rel="" target="">
 <span class="dropdown-text">Gradient Boosting Series</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://python-bloggers.com/" rel="" target="">
 <span class="dropdown-text">PythonBloggers</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/mcb00" rel="" title="Matt on Github" class="quarto-navigation-tool px-1" aria-label="Matt on Github"><i class="bi bi-github"></i></a>
    <a href="https://twitter.com/mcbwrs" rel="" title="Matt on Twitter" class="quarto-navigation-tool px-1" aria-label="Matt on Twitter"><i class="bi bi-twitter"></i></a>
    <a href="https://www.linkedin.com/in/matt-bowers" rel="" title="Matt on Linkedin" class="quarto-navigation-tool px-1" aria-label="Matt on Linkedin"><i class="bi bi-linkedin"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<h5 style="font-weight:600; font-size:1rem">Subscribe</h5>

<form action="https://dev.us20.list-manage.com/subscribe/post?u=5212e33f7cd396dd4a742431c&amp;id=0a2f69f3f3&amp;f_id=002e29e8f0" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_self">

    <div class="form-group">
      <!-- <label for="mce-EMAIL" class="form-label mt-4">Subscribe via email</label> -->
      <input type="email" value="" name="EMAIL" class="form-control" id="mce-EMAIL" placeholder="enter your email" required="">
    </div>
    
    <div style="margin-top: 10px;">
    <button type="submit" class="btn btn-secondary btn-sm">Submit</button>
    
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_f718424fc5df77c22533bdaa6_a3c37fb57b" tabindex="-1" value=""></div>
    


<hr>
</div></form>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop" id="toc-how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop" class="nav-link active" data-scroll-target="#how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop">How to Run PySpark in a Jupyter Notebook on Your Laptop</a>
  <ul class="collapse">
  <li><a href="#install-spark" id="toc-install-spark" class="nav-link" data-scroll-target="#install-spark">Install Spark</a></li>
  <li><a href="#install-pyspark" id="toc-install-pyspark" class="nav-link" data-scroll-target="#install-pyspark">Install PySpark</a></li>
  <li><a href="#the-spark-session-object" id="toc-the-spark-session-object" class="nav-link" data-scroll-target="#the-spark-session-object">The Spark Session Object</a></li>
  <li><a href="#create-a-pyspark-session-in-a-jupyter-notebook" id="toc-create-a-pyspark-session-in-a-jupyter-notebook" class="nav-link" data-scroll-target="#create-a-pyspark-session-in-a-jupyter-notebook">Create a PySpark Session in a Jupyter Notebook</a></li>
  </ul></li>
  <li><a href="#pyspark-concepts" id="toc-pyspark-concepts" class="nav-link" data-scroll-target="#pyspark-concepts">PySpark Concepts</a></li>
  <li><a href="#pyspark-dataframe-essentials" id="toc-pyspark-dataframe-essentials" class="nav-link" data-scroll-target="#pyspark-dataframe-essentials">PySpark Dataframe Essentials</a>
  <ul class="collapse">
  <li><a href="#creating-a-pyspark-dataframe-with-createdataframe" id="toc-creating-a-pyspark-dataframe-with-createdataframe" class="nav-link" data-scroll-target="#creating-a-pyspark-dataframe-with-createdataframe">Creating a PySpark dataframe with <code>createDataFrame()</code></a></li>
  <li><a href="#peeking-at-a-dataframes-contents" id="toc-peeking-at-a-dataframes-contents" class="nav-link" data-scroll-target="#peeking-at-a-dataframes-contents">Peeking at a dataframe’s contents</a></li>
  <li><a href="#select-columns-by-name" id="toc-select-columns-by-name" class="nav-link" data-scroll-target="#select-columns-by-name">Select columns by name</a></li>
  <li><a href="#filter-rows-based-on-column-values" id="toc-filter-rows-based-on-column-values" class="nav-link" data-scroll-target="#filter-rows-based-on-column-values">Filter rows based on column values</a></li>
  <li><a href="#add-new-columns-to-a-dataframe" id="toc-add-new-columns-to-a-dataframe" class="nav-link" data-scroll-target="#add-new-columns-to-a-dataframe">Add new columns to a dataframe</a></li>
  <li><a href="#group-by-and-aggregate" id="toc-group-by-and-aggregate" class="nav-link" data-scroll-target="#group-by-and-aggregate">Group by and aggregate</a></li>
  <li><a href="#run-hive-sql-on-dataframes" id="toc-run-hive-sql-on-dataframes" class="nav-link" data-scroll-target="#run-hive-sql-on-dataframes">Run Hive SQL on dataframes</a></li>
  </ul></li>
  <li><a href="#visualization-with-pyspark" id="toc-visualization-with-pyspark" class="nav-link" data-scroll-target="#visualization-with-pyspark">Visualization with PySpark</a></li>
  <li><a href="#wrapping-up" id="toc-wrapping-up" class="nav-link" data-scroll-target="#wrapping-up">Wrapping Up</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Hello PySpark!</h1>
  <div class="quarto-categories">
    <div class="quarto-category">python</div>
    <div class="quarto-category">PySpark</div>
    <div class="quarto-category">tutorial</div>
  </div>
  </div>

<div>
  <div class="description">
    Get up and running fast with a local pyspark installation, and learn the essentials of working with dataframes at scale.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Matt Bowers </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 22, 2021</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="guiones_wave.jpeg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">A big day at Playa Guiones</figcaption>
</figure>
</div>
<p>Well, you guessed it: it’s time for us to learn PySpark!</p>
<p>I know, I know, I can hear you screaming into your pillow. Indeed we just spent all that time converting from R and learning python and why the hell do we need yet another API for working with dataframes?</p>
<p>That’s a totally fair question.</p>
<p>So what happens when we’re working on something in the real world, where datasets get large in a hurry, and we suddenly have a dataframe that no longer fits into memory? We need a way for our computations and datasets to scale across multiple nodes in a distributed system without having to get too fussy about all the distributed compute details.</p>
<p>Enter PySpark.</p>
<p>I think it’s fair to think of PySpark as a python package for working with arbitrarily large dataframes, i.e., it’s like pandas but scalable. It’s built on top of <a href="https://spark.apache.org/">Apache Spark</a>, a unified analytics engine for large-scale data processing. <a href="https://spark.apache.org/docs/latest/api/python/">PySpark</a> is essentially a way to access the functionality of spark via python code. While there are other high-level interfaces to Spark (such as Java, Scala, and R), for data scientists who are already working extensively with python, PySpark will be the natural interface of choice. PySpark also has great integration with <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">SQL</a>, and it has a companion machine learning library called <a href="https://spark.apache.org/mllib/">MLlib</a> that’s more or less a scalable scikit-learn (maybe we can cover it in a future post).</p>
<p>So, here’s the plan. First we’re going to get set up to run PySpark locally in a jupyter notebook on our laptop. This is my preferred environment for interactively playing with PySpark and learning the ropes. Then we’re going to get up and running in PySpark as quickly as possible by reviewing the most essential functionality for working with dataframes and comparing it to how we would do things in pandas. Once we’re comfortable running PySpark on the laptop, it’s going to be much easier to jump onto a distributed cluster and run PySpark at scale.</p>
<p>Let’s do this.</p>
<section id="how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop" class="level2">
<h2 class="anchored" data-anchor-id="how-to-run-pyspark-in-a-jupyter-notebook-on-your-laptop">How to Run PySpark in a Jupyter Notebook on Your Laptop</h2>
<p>Ok, I’m going to walk us through how to get things installed on a Mac or Linux machine where we’re using homebrew and conda to manage virtual environments. If you have a different setup, your favorite search engine will help you get PySpark set up locally.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It’s possible for Homebrew and Anaconda to interfere with one another. The simple rule of thumb is that whenever you want to use the <code>brew</code> command, first deactivate your conda environment by running <code>conda deactivate</code>. See this <a href="https://stackoverflow.com/questions/42859781/best-practices-with-anaconda-and-brew">Stack Overflow question</a> for more details.</p>
</div>
</div>
<section id="install-spark" class="level3">
<h3 class="anchored" data-anchor-id="install-spark">Install Spark</h3>
<p>Install Spark with homebrew.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> install apache-spark</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next we need to set up a <code>SPARK_HOME</code> environment variable in the shell. Check where Spark is installed.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> info apache-spark</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You should see something like</p>
<pre><code>==&gt; apache-spark: stable 3.3.2 (bottled), HEAD
Engine for large-scale data processing
https://spark.apache.org/
/opt/homebrew/Cellar/apache-spark/3.3.2 (1,453 files, 320.9MB) *
...</code></pre>
<p>Set the <code>SPARK_HOME</code> environment variable to your spark installation path with <code>/libexec</code> appended to the end. To do this I added the following line to my <code>.zshrc</code> file.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">export</span> <span class="va">SPARK_HOME</span><span class="op">=</span>/opt/homebrew/Cellar/apache-spark/3.3.2/libexec</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Restart your shell, and test the installation by starting the Spark shell.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">spark-shell</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.2
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 19.0.2)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; </code></pre>
<p>If you get the <code>scala&gt;</code> prompt, then you’ve successfully installed Spark on your laptop!</p>
</section>
<section id="install-pyspark" class="level3">
<h3 class="anchored" data-anchor-id="install-pyspark">Install PySpark</h3>
<p>Use conda to install the PySpark python package. As usual, it’s advisable to do this in a new virtual environment.</p>
<pre><code>$ conda install pyspark</code></pre>
<p>You should be able to launch an interactive PySpark REPL by saying pyspark.</p>
<pre><code>$ pyspark
...
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/

Using Python version 3.8.3 (default, Jul  2 2020 11:26:31)
Spark context Web UI available at http://192.168.100.47:4041
Spark context available as 'sc' (master = local[*], app id = local-1624127229929).
SparkSession available as 'spark'.
&gt;&gt;&gt; </code></pre>
<p>This time we get a familiar python <code>&gt;&gt;&gt;</code> prompt. This is an interactive shell where we can easily experiment with PySpark. Feel free to run the example code in this post here in the PySpark shell, or, if you prefer a notebook, read on and we’ll get set up to run PySpark in a jupyter notebook.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When I tried following this setup on a new Mac, I hit an error about being unable to find the Java Runtime. This <a href="https://stackoverflow.com/questions/75021908/cannot-start-pyspark-unable-to-locate-a-java-runtime">stack overflow question</a> lead me to the fix.</p>
</div>
</div>
</section>
<section id="the-spark-session-object" class="level3">
<h3 class="anchored" data-anchor-id="the-spark-session-object">The Spark Session Object</h3>
<p>You may have noticed that when we launched that PySpark interactive shell, it told us that something called <code>SparkSession</code> was available as <code>'spark'</code>. So basically, what’s happening here is that when we launch the pyspark shell, it instantiates an object called <code>spark</code> which is an instance of class <code>pyspark.sql.session.SparkSession</code>. The spark session object is going to be our entry point for all kinds of PySpark functionality, i.e., we’re going to be saying things like <code>spark.this()</code> and <code>spark.that()</code> to make stuff happen.</p>
<p>The PySpark interactive shell is kind enough to instantiate one of these spark session objects for us automatically. However, when we’re using another interface to PySpark (like say a jupyter notebook running a python kernal), we’ll have to make a spark session object for ourselves.</p>
</section>
<section id="create-a-pyspark-session-in-a-jupyter-notebook" class="level3">
<h3 class="anchored" data-anchor-id="create-a-pyspark-session-in-a-jupyter-notebook">Create a PySpark Session in a Jupyter Notebook</h3>
<p>There are a few ways to run PySpark in jupyter which you can read about <a href="https://www.datacamp.com/community/tutorials/apache-spark-python">here</a>.</p>
<p>For derping around with PySpark on your laptop, I think the best way is to instantiate a spark session from a jupyter notebook running on a regular python kernel. The method we’ll use involves running a standard jupyter notebook session with a python kernal and using the findspark package to initialize the spark session. So, first install the findspark package.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install <span class="at">-c</span> conda-forge findspark</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Launch jupyter as usual.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode zsh code-with-copy"><code class="sourceCode zsh"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Go ahead and fire up a new notebook using a regular python 3 kernal. Once you land inside the notebook, there are a couple things we need to do to get a spark session instantiated. You can think of this as boilerplate code that we need to run in the first cell of a notebook where we’re going to use PySpark.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyspark</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> findspark</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql <span class="im">import</span> SparkSession</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>findspark.init()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>spark <span class="op">=</span> SparkSession.builder.appName(<span class="st">'My Spark App'</span>).getOrCreate()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>First we’re running findspark’s <code>init()</code> method to find our Spark installation. If you run into errors here, make sure you got the <code>SPARK_HOME</code> environment variable correctly set in the install instructions above. Then we instantiate a spark session as <code>spark</code>. Once you run this, you’re ready to rock and roll with PySpark in your jupyter notebook.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Spark provides a handy web UI that you can use for monitoring and debugging. Once you instantiate the spark session You can open the UI in your web browser at <a href="http://localhost:4040/jobs/">http://localhost:4040/jobs/</a>.</p>
</div>
</div>
</section>
</section>
<section id="pyspark-concepts" class="level2">
<h2 class="anchored" data-anchor-id="pyspark-concepts">PySpark Concepts</h2>
<p>PySpark provides two main abstractions for data: the RDD and the dataframe. <strong>RDD</strong>’s are just a distributed list of objects; we won’t go into details about them in this post. For us, the key object in PySpark is the <strong>dataframe</strong>.</p>
<p>While PySpark dataframes expose much of the functionality you would expect from a library for tabular data manipulation, they behave a little differently from pandas dataframes, both syntactically and under-the-hood. There are a couple of key concepts that will help explain these idiosyncracies.</p>
<p><strong>Immutability</strong> - Pyspark RDD’s and dataframes are immutable. This means that if you change an object, e.g.&nbsp;by adding a column to a dataframe, PySpark returns a reference to a new dataframe; it does not modify the existing dataframe. This is kind of nice, because we don’t have to worry about that whole <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy">view versus copy</a> nonsense that happens in pandas.</p>
<p><strong>Lazy Evaluation</strong> - Lazy evaluation means that when we start manipulating a dataframe, PySpark won’t actually perform any of the computations until we explicitly ask for the result. This is nice because it potentially allows PySpark to do fancy optimizations before executing a sequence of operations. It’s also confusing at first, because PySpark will seem to blaze through complex operations and then take forever to print a few rows of the dataframe.</p>
</section>
<section id="pyspark-dataframe-essentials" class="level2">
<h2 class="anchored" data-anchor-id="pyspark-dataframe-essentials">PySpark Dataframe Essentials</h2>
<section id="creating-a-pyspark-dataframe-with-createdataframe" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-pyspark-dataframe-with-createdataframe">Creating a PySpark dataframe with <code>createDataFrame()</code></h3>
<p>The first thing we’ll need is a way to make dataframes. <a href="https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html"><code>createDataFrame()</code></a> allows us to create PySpark dataframes from python objects like nested lists or pandas dataframes. Notice that <code>createDataFrame()</code> is a method of the spark session class, so we’ll call it from our spark session <code>spark</code>by saying <code>spark.createDataFrame()</code>.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create pyspark dataframe from nested  lists</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>my_df <span class="op">=</span> spark.createDataFrame(</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>[</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        [<span class="dv">2022</span>, <span class="st">"tiger"</span>],</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        [<span class="dv">2023</span>, <span class="st">"rabbit"</span>],</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        [<span class="dv">2024</span>, <span class="st">"dragon"</span>]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    schema<span class="op">=</span>[<span class="st">'year'</span>, <span class="st">'animal'</span>]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s read the seaborn tips dataset into a pandas dataframe and then use it to create a PySpark dataframe.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load tips dataset into a pandas dataframe</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>pandas_df <span class="op">=</span> pd.read_csv(<span class="st">'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv'</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># create pyspark dataframe from a pandas dataframe</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>pyspark_df <span class="op">=</span> spark.createDataFrame(pandas_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In real life when we’re running PySpark on a large-scale distributed system, we would not generally want to use python lists or pandas dataframes to load data into PySpark. Ideally we would want to read data directly from where it is stored on HDFS, e.g.&nbsp;by reading <a href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html">parquet files</a>, or by querying directly from a hive database using <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">spark sql</a>.</p>
</div>
</div>
</section>
<section id="peeking-at-a-dataframes-contents" class="level3">
<h3 class="anchored" data-anchor-id="peeking-at-a-dataframes-contents">Peeking at a dataframe’s contents</h3>
<p>The default print method for the PySpark dataframe will just give you the schema.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pyspark_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: bigint]</code></pre>
</div>
</div>
<p>If we want to peek at some of the data, we’ll need to use the <code>show()</code> method, which is analogous to the pandas <code>head()</code>. Remember that <code>show()</code> will cause PySpark to execute any operations that it’s been lazily waiting to evaluate, so sometimes it can take a while to run.</p>
<div class="cell" data-scrolled="true" data-execution_count="5">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># show the first few rows of the dataframe</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>pyspark_df.show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------+----+------+------+---+------+----+
|total_bill| tip|   sex|smoker|day|  time|size|
+----------+----+------+------+---+------+----+
|     16.99|1.01|Female|    No|Sun|Dinner|   2|
|     10.34|1.66|  Male|    No|Sun|Dinner|   3|
|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|
|     23.68|3.31|  Male|    No|Sun|Dinner|   2|
|     24.59|3.61|Female|    No|Sun|Dinner|   4|
+----------+----+------+------+---+------+----+
only showing top 5 rows
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
[Stage 0:&gt;                                                          (0 + 1) / 1]

                                                                                </code></pre>
</div>
</div>
<p>We thus encounter our first rude awakening. PySpark’s default representation of dataframes in the notebook isn’t as pretty as that of pandas. But no one ever said it would be pretty, they just said it would be scalable.</p>
<p>You can also use the <code>printSchema()</code> method for a nice vertical representation of the schema.</p>
<div class="cell" data-scrolled="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># show the dataframe schema</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>pyspark_df.printSchema()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>root
 |-- total_bill: double (nullable = true)
 |-- tip: double (nullable = true)
 |-- sex: string (nullable = true)
 |-- smoker: string (nullable = true)
 |-- day: string (nullable = true)
 |-- time: string (nullable = true)
 |-- size: long (nullable = true)
</code></pre>
</div>
</div>
</section>
<section id="select-columns-by-name" class="level3">
<h3 class="anchored" data-anchor-id="select-columns-by-name">Select columns by name</h3>
<p>You can select specific columns from a dataframe using the <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.select.html"><code>select()</code></a> method. You can pass either a list of names, or pass names as arguments.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># select some of the columns</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>pyspark_df.select(<span class="st">'total_bill'</span>, <span class="st">'tip'</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># select columns in a list</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>pyspark_df.select([<span class="st">'day'</span>, <span class="st">'time'</span>, <span class="st">'total_bill'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="filter-rows-based-on-column-values" class="level3">
<h3 class="anchored" data-anchor-id="filter-rows-based-on-column-values">Filter rows based on column values</h3>
<p>Analogous to the <code>WHERE</code> clause in SQL, and the <code>query()</code> method in pandas, PySpark provides a <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.filter.html"><code>filter()</code></a> method which returns only the rows that meet the specified conditions. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using <code>and</code> and <code>or</code>, and you can even do a SQL-like <code>in</code> to check if the column value matches any items in a list.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">## compare a column to a value</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>pyspark_df.<span class="bu">filter</span>(<span class="st">'total_bill &gt; 20'</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># compare two columns with arithmetic</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>pyspark_df.<span class="bu">filter</span>(<span class="st">'tip &gt; 0.15 * total_bill'</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># check equality with a string value</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>pyspark_df.<span class="bu">filter</span>(<span class="st">'sex == "Male"'</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># check equality with any of several possible values</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>pyspark_df.<span class="bu">filter</span>(<span class="st">'day in ("Sat", "Sun")'</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co"># use "and" </span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>pyspark_df.<span class="bu">filter</span>(<span class="st">'day == "Fri" and time == "Lunch"'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you’re into boolean indexing with the brackets, PySpark does support that too, but I encourage you to use <code>filter()</code> instead. Check out my rant about <a href="../../8020-pandas-tutorial#select-rows-based-on-their-values-with-query">why you shouldn’t use boolean indexing</a> for the details. The TLDR is that <code>filter()</code> requires less typing, makes your code more readable and portable, and it allows you to chain method calls together using dot chains.</p>
<p>Here’s the boolean indexing equivalent of the last example from above.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using boolean indexing</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>pyspark_df[(pyspark_df.day <span class="op">==</span> <span class="st">'Fri'</span>) <span class="op">&amp;</span> (pyspark_df.time <span class="op">==</span> <span class="st">'Lunch'</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I know, it looks horrendous, but not as horrendous as the error message you’ll get if you forget the parentheses.</p>
</section>
<section id="add-new-columns-to-a-dataframe" class="level3">
<h3 class="anchored" data-anchor-id="add-new-columns-to-a-dataframe">Add new columns to a dataframe</h3>
<p>You can add new columns which are functions of the existing columns with the <a href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html"><code>withColumn()</code></a> method.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyspark.sql.functions <span class="im">as</span> f</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># add a new column using col() to reference other columns</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>pyspark_df.withColumn(<span class="st">'tip_percent'</span>, f.col(<span class="st">'tip'</span>) <span class="op">/</span> f.col(<span class="st">'total_bill'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that we’ve imported the <a href="[pyspark.sql.functions](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)"><code>pyspark.sql.functions</code></a> module. This module contains lots of useful functions that we’ll be using all over the place, so it’s probably a good idea to go ahead and import it whenever you’re using PySpark. BTW, it seems like folks usually import this module as <code>f</code> or <code>F</code>. In this example we’re using the <code>col()</code> function, which allows us to refer to columns in our dataframe using string representations of the column names.</p>
<p>You could also achieve the same result using the dot to reference the other columns, but this requires us to type the dataframe name over and over again, which makes it harder to reuse this code on different dataframes or in <a href="https://blog.mattbowers.dev/8020-pandas-tutorial#Chain-transformations-together-with-the-dot-chain">dot chains</a>.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add a new column using the dot to reference other columns (less recommended)</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>pyspark_df.withColumn(<span class="st">'tip_percent'</span>, pyspark_df.tip <span class="op">/</span> pyspark_df.total_bill)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you want to apply numerical transformations like exponents or logs, use the built-in functions in the <code>pyspark.sql.functions</code> module.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># log </span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>pyspark_df.withColumn(<span class="st">'log_bill'</span>, f.log(f.col(<span class="st">'total_bill'</span>)))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># exponent</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>pyspark_df.withColumn(<span class="st">'bill_squared'</span>, f.<span class="bu">pow</span>(f.col(<span class="st">'total_bill'</span>), <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can implement conditional assignment like SQL’s <code>CASE WHEN</code> construct using the <code>when()</code> function and the <code>otherwise()</code> method.</p>
<div class="cell" data-scrolled="true" data-execution_count="13">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># conditional assignment (like CASE WHEN)</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>pyspark_df.withColumn(<span class="st">'is_male'</span>, f.when(f.col(<span class="st">'sex'</span>) <span class="op">==</span> <span class="st">'Male'</span>, <span class="va">True</span>).otherwise(<span class="va">False</span>))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># using multiple when conditions and values</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>pyspark_df.withColumn(<span class="st">'bill_size'</span>, </span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    f.when(f.col(<span class="st">'total_bill'</span>) <span class="op">&lt;</span> <span class="dv">10</span>, <span class="st">'small'</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    .when(f.col(<span class="st">'total_bill'</span>) <span class="op">&lt;</span> <span class="dv">20</span>, <span class="st">'medium'</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    .otherwise(<span class="st">'large'</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Remember that since PySpark dataframes are immutable, calling <code>withColumns()</code> on a dataframe returns a new dataframe. If you want to persist the result, you’ll need to make an assignment.</p>
<pre><code>pyspark_df = pyspark_df.withColumns(...)</code></pre>
</section>
<section id="group-by-and-aggregate" class="level3">
<h3 class="anchored" data-anchor-id="group-by-and-aggregate">Group by and aggregate</h3>
<p>PySpark provides a <code>groupBy()</code> method similar to the pandas <code>groupby()</code>. Just like in pandas, we can call methods like <code>count()</code> and <code>mean()</code> on our grouped dataframe, and we also have a more flexible <code>agg()</code> method that allows us to specify column-aggregation mappings.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># group by and count</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>pyspark_df.groupBy(<span class="st">'time'</span>).count().show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+-----+
|  time|count|
+------+-----+
|Dinner|  176|
| Lunch|   68|
+------+-----+
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co"># group by and specify column-aggregation mappings with agg()</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>pyspark_df.groupBy(<span class="st">'time'</span>).agg({<span class="st">'total_bill'</span>: <span class="st">'mean'</span>, <span class="st">'tip'</span>: <span class="st">'max'</span>}).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+--------+------------------+
|  time|max(tip)|   avg(total_bill)|
+------+--------+------------------+
|Dinner|    10.0| 20.79715909090909|
| Lunch|     6.7|17.168676470588235|
+------+--------+------------------+
</code></pre>
</div>
</div>
<p>If you want to get fancier with your aggregations, it might just be easier to express them using hive syntax. Read on to find out how.</p>
</section>
<section id="run-hive-sql-on-dataframes" class="level3">
<h3 class="anchored" data-anchor-id="run-hive-sql-on-dataframes">Run Hive SQL on dataframes</h3>
<p>One of the mind-blowing features of PySpark is that it allows you to write hive SQL queries on your dataframes. To take a PySpark dataframe into the SQL world, use the <code>createOrReplaceTempView()</code> method. This method takes one string argument which will be the dataframes name in the SQL world. Then you can use <code>spark.sql()</code> to run a query. The result is returned as a PySpark dataframe.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># put pyspark dataframe in SQL world and query it</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>pyspark_df.createOrReplaceTempView(<span class="st">'tips'</span>)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>spark.sql(<span class="st">'select * from tips'</span>).show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----------+----+------+------+---+------+----+
|total_bill| tip|   sex|smoker|day|  time|size|
+----------+----+------+------+---+------+----+
|     16.99|1.01|Female|    No|Sun|Dinner|   2|
|     10.34|1.66|  Male|    No|Sun|Dinner|   3|
|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|
|     23.68|3.31|  Male|    No|Sun|Dinner|   2|
|     24.59|3.61|Female|    No|Sun|Dinner|   4|
+----------+----+------+------+---+------+----+
only showing top 5 rows
</code></pre>
</div>
</div>
<p>This is awesome for a couple of reasons. First, it allows us to easily express any transformations in hive syntax. If you’re like me and you’ve already been using hive, this will dramatically reduce the PySpark learning curve, because when in doubt, you can always bump a dataframe into the SQL world and simply use hive to do what you need. Second, if you have a hive deployment, PySpark’s SQL world also has access to all of your hive tables. This means you can write queries involving both hive tables and your PySpark dataframes. It also means you can run hive commands, like inserting into a table, directly from PySpark.</p>
<p>Let’s do some aggregations that might be a little trickier to do using the PySpark built-in functions.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co"># run hive query and save result to dataframe</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>tip_stats_by_time <span class="op">=</span> spark.sql(<span class="st">"""</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="st">    select</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="st">        time</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="st">        , count(*) as n </span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="st">        , avg(tip) as avg_tip</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="st">        , percentile_approx(tip, 0.5) as med_tip</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="st">        , avg(case when tip &gt; 3 then 1 else 0 end) as pct_tip_gt_3</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="st">    from </span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="st">        tips</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="st">    group by 1</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span>)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>tip_stats_by_time.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+---+------------------+-------+-------------------+
|  time|  n|           avg_tip|med_tip|       pct_tip_gt_3|
+------+---+------------------+-------+-------------------+
|Dinner|176| 3.102670454545455|    3.0|0.44886363636363635|
| Lunch| 68|2.7280882352941176|    2.2|0.27941176470588236|
+------+---+------------------+-------+-------------------+
</code></pre>
</div>
</div>
</section>
</section>
<section id="visualization-with-pyspark" class="level2">
<h2 class="anchored" data-anchor-id="visualization-with-pyspark">Visualization with PySpark</h2>
<p>There aren’t any tools for visualization included in PySpark. But that’s no problem, because we can just use the <code>toPandas()</code> method on a PySpark dataframe to pull data back into pandas. Once we have a pandas dataframe, we can happily build visualizations as usual. Of course, if your PySpark dataframe is huge, you wouldn’t want to use <code>toPandas()</code> directly, because PySpark will attempt to read the entire contents of its huge dataframe into memory. Instead, it’s best to use PySpark to generate aggregations of your data for plotting or to pull only a sample of your full data into pandas.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># read aggregated pyspark dataframe into pandas for plotting</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>plot_pdf <span class="op">=</span> tip_stats_by_time.toPandas()</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>plot_pdf.plot.bar(x<span class="op">=</span><span class="st">'time'</span>, y<span class="op">=</span>[<span class="st">'avg_tip'</span>, <span class="st">'med_tip'</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="hello-pyspark_files/figure-html/cell-21-output-1.png" class="img-fluid" alt="Figure showing a bar plot of average and median tips by time"></p>
</div>
</div>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping Up</h2>
<p>So that’s a wrap on our crash course in working with PySpark. You now have a good idea of what pyspark is and how to get started manipulating dataframes with it. Stay tuned for a future post on PySpark’s companion ML library MLlib. In the meantime, may no dataframe be too large for you ever again.</p>
</section>

</main> <!-- /main -->
<hr>

<!-- <div style="max-width: 80%;"> -->
<h3>Comments</h3>
<script data-isso="https://isso.randomrealizations.com/" src="https://isso.randomrealizations.com/js/embed.min.js">
</script>
<section id="isso-thread"></section>
<!-- </div> -->

<hr>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="mcb00/blog" issue-term="title" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>