{
 "cells": [
  {
   "cell_type": "raw",
   "id": "12583848",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: 'Decoding Statistical Testing with a Model-Based Perspective'\n",
    "categories:\n",
    "- statistics\n",
    "date: '2026-01-19'\n",
    "description: How to view classical statistical hypothesis testing in a unified model-based framework.\n",
    "image: /enso-thumbnail.jpg\n",
    "image-alt: The enso circle\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4d371",
   "metadata": {},
   "source": [
    "Ahh, statistical hypothesis testing—it's a cornerstone of classical statistical inference. \n",
    "It can also seem a bit daunting when presented as a list of [over 100 different methods](https://en.wikipedia.org/wiki/Category:Statistical_tests) \n",
    "from which the user is expected to choose the right procedure based on their application domain, data, and question.\n",
    "But it doesn't have to be that way—there is a unifying perspective that can help us cut right through all that complexity.\n",
    "\n",
    "*It turns out that many traditional frequentist tests can be viewed as special cases of regression models.*\n",
    "\n",
    "This is a powerful and liberating idea, because it can help us transition from choosing from the zoo of canned methods to building our own bespoke analyses, tailored to our exact use case.\n",
    "Adopting this perspective will allow us to\n",
    "\n",
    "- transcend rote memorization of which test should be applied where and instead reason from first principles\n",
    "- make our assumptions clear and explicit \n",
    "- make extensions like incorporating covariates and interactions more natural\n",
    "\n",
    "So the plan for this post is to take a look at three of the most common statistical tests—the two-sample t-test, the ANOVA, and the chi-squared test for independence. For each test, we'll simulate some data and implement it using the traditional test approach as well as a regression-based approach. I'm going to do a detailed analytical breakdown for the two-sample t-test to show the equivalence between the two approaches, leaving the analytical breakdowns of the other tests to you, dear reader, as exercises.\n",
    "\n",
    "Let's roll!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464629b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Two-Sample t-test\n",
    "\n",
    "We'll look at the t-test from two perspectives—the classical setup and a linear regression reformulation. In each case we'll break the approach down into these items: data generating process, estimator, expectation and variance of the estimator, test statistic, and sampling distribution of the test statistic. You can use this kind of breakdown to understand pretty much any classical statistical test. In this case, the point is to clearly show that the classical t-test and the linear regression formulation yield identical tests. \n",
    "\n",
    "### The Classical t-test Approach\n",
    "\n",
    "**The data generating process**\n",
    "\n",
    "You have two populations or processes $Y_0$ and $Y_1$, and you want to know whether their true means $\\mu_0$ and $\\mu_1$ are equal. We assume that both processes are Gaussian with equal but unknown variance $\\sigma^2$:\n",
    "\n",
    "$$ Y_0 \\sim N(\\mu_0, \\sigma^2), \\quad Y_1 \\sim N(\\mu_1, \\sigma^2) $$\n",
    "\n",
    "**The estimator**\n",
    "\n",
    "You draw $n_0$ samples from group 0 and $n_1$ samples from group 1 for a total of $n=n_0+n_1$ samples, and compute the sample means $\\bar{Y}_0$ and $\\bar{Y}_1$. Your estimator for the difference in means is simply:\n",
    "\n",
    "$$\\hat{\\delta} = \\bar{Y}_1 - \\bar{Y}_0$$\n",
    "\n",
    "**Expectation of the estimator**\n",
    "\n",
    "Since $E[\\bar{Y}_0] = \\mu_0$ and $E[\\bar{Y}_1] = \\mu_1$, we have:\n",
    "\n",
    "$$E[\\hat{\\delta}] = E[\\bar{Y}_1 - \\bar{Y}_0] = \\mu_1 - \\mu_0$$\n",
    "\n",
    "So $\\hat{\\delta}$ is an unbiased estimator of the true difference in means.\n",
    "\n",
    "**Standard error of the estimator**\n",
    "\n",
    "The sample means are independent, so:\n",
    "\n",
    "$$Var[\\hat{\\delta}] = Var[\\bar{Y}_1] + Var[\\bar{Y}_0] = \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0} = \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_0}\\right)$$\n",
    "\n",
    "Since we don't know $\\sigma^2$, we estimate it with the pooled sample variance:\n",
    "\n",
    "$$\\hat{\\sigma}_{\\text{pooled}}^2 = \\frac{(n_0-1)s_0^2 + (n_1-1)s_1^2}{n_0 + n_1 - 2}$$\n",
    "\n",
    "where $s_0^2$ and $s_1^2$ are the sample variances for each group. This gives us the estimated standard error:\n",
    "\n",
    "$$SE(\\hat{\\delta}) = \\sqrt{\\hat{\\sigma}_{\\text{pooled}}^2\\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)}$$\n",
    "\n",
    "**The test statistic**\n",
    "\n",
    "We form the test statistic by dividing our estimator by its standard error:\n",
    "\n",
    "$$t = \\frac{\\hat{\\delta}}{SE(\\hat{\\delta})} = \\frac{\\bar{Y}_1 - \\bar{Y}_0}{\\sqrt{\\hat{\\sigma}_{\\text{pooled}}^2 (1/n_0 + 1/n_1)}}$$\n",
    "\n",
    "**Sampling distribution**\n",
    "\n",
    "Under the null hypothesis $H_0: \\mu_1 = \\mu_0$, this test statistic follows a Student's t-distribution with $n_0 + n_1 - 2$ degrees of freedom.\n",
    "\n",
    "Having horrifying flashbacks to your intro to stats class yet? No worries. Let's look at it from a new perspective.\n",
    "\n",
    "\n",
    "### The Regression Approach\n",
    "\n",
    "**The data generating process**\n",
    "\n",
    "We can express the exact same data generating process as a linear regression model. Stack all observations into a single length-$n$ vector $Y$ and create a dummy variable $X \\in \\{0,1\\}$ indexing which group each observation came from:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X + \\epsilon $$\n",
    "\n",
    "where $\\epsilon \\overset{iid}{\\sim} N(0, \\sigma^2)$.\n",
    "\n",
    "Taking conditional expectations:\n",
    "\n",
    "$$ E[Y|X=0] = \\beta_0 = \\mu_0 $$\n",
    "$$ E[Y|X=1] = \\beta_0 + \\beta_1 = \\mu_1 $$\n",
    "\n",
    "So we can see that $\\beta_1 = \\mu_1 - \\mu_0$, meaning the regression coefficient $\\beta_1$ directly represents the difference in population means.\n",
    "\n",
    "**The estimator**\n",
    "\n",
    "The ordinary least squares estimator for $\\beta_1$ is:\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}$$\n",
    "\n",
    "For our dummy variable where $\\bar{X} = n_1/(n_0 + n_1)$, after some algebra that you can crank through on your own this simplifies to:\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\bar{Y}_1 - \\bar{Y}_0$$\n",
    "\n",
    "Well look at that—the regression coefficient estimate is exactly the difference in sample means!\n",
    "\n",
    "**Expectation of the estimator**\n",
    "\n",
    "By the properties of OLS under our model assumptions:\n",
    "\n",
    "$$E[\\hat{\\beta}_1] = \\beta_1 = \\mu_1 - \\mu_0$$\n",
    "\n",
    "So $\\hat{\\beta}_1$ is also an unbiased estimator of the difference in means.\n",
    "\n",
    "**Standard error of the estimator**\n",
    "\n",
    "The standard error formula for an OLS coefficient is:\n",
    "\n",
    "$$SE(\\hat{\\beta}_1) = \\sqrt{\\hat{\\sigma}^2 \\cdot \\frac{1}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}}$$\n",
    "\n",
    "where $\\hat{\\sigma}^2$ is the residual variance from the regression:\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n_0 + n_1 - 2}\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2$$\n",
    "\n",
    "For our dummy variable, it turns out that:\n",
    "- The residual variance $\\hat{\\sigma}^2$ equals the pooled variance $\\hat{\\sigma}_{\\text{pooled}}^2$\n",
    "- The sum $\\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\frac{n_0 n_1}{n_0 + n_1}$\n",
    "\n",
    "Substituting these:\n",
    "\n",
    "$$SE(\\hat{\\beta}_1) = \\sqrt{\\hat{\\sigma}_{\\text{pooled}}^2 \\cdot \\frac{n_0 + n_1}{n_0 n_1}} = \\sqrt{\\hat{\\sigma}_{\\text{pooled}}^2\\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)}$$\n",
    "\n",
    "This is exactly the same standard error we got from the classical approach.\n",
    "\n",
    "**The test statistic**\n",
    "\n",
    "We form the test statistic by dividing our coefficient estimate by its standard error:\n",
    "\n",
    "$$t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = \\frac{\\bar{Y}_1 - \\bar{Y}_0}{\\sqrt{\\hat{\\sigma}_{\\text{pooled}}^2 (1/n_0 + 1/n_1)}}$$\n",
    "\n",
    "**Sampling distribution**\n",
    "\n",
    "Under the null hypothesis $H_0: \\beta_1 = 0$, this test statistic follows a Student's t-distribution with $n_0 + n_1 - 2$ degrees of freedom (the residual degrees of freedom from the regression).\n",
    "\n",
    "### The Punchline\n",
    "\n",
    "See what just happened? The two approaches give us:\n",
    "\n",
    "* The same point estimate: $\\hat{\\delta} = \\hat{\\beta}_1 = \\bar{Y}_1 - \\bar{Y}_0$\n",
    "* The same standard error: $\\sqrt{\\hat{\\sigma}_{\\text{pooled}}^2(1/n_0 + 1/n_1)}$\n",
    "* The same test statistic: $t = \\frac{\\bar{Y}_1 - \\bar{Y}_0}{\\sqrt{\\hat{\\sigma}_{\\text{pooled}}^2 (1/n_0 + 1/n_1)}}$\n",
    "* The same sampling distribution: $t_{n_0+n_1-2}$\n",
    "* Therefore, the same p-value\n",
    "\n",
    "In other words these approaches are mathematically equivalent. \n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's simulate some data and implement both testing approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4230263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-test statistic: 3.258749\n",
      "Regression t-stat for β₁: 3.258749\n",
      "\n",
      "t-test p-value: 0.002190\n",
      "Regression p-value: 0.002190\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(42)\n",
    "n0, n1 = 20, 25\n",
    "mu0, mu1 = 10, 12\n",
    "sigma = 2\n",
    "group0 = np.random.normal(mu0, sigma, n0)\n",
    "group1 = np.random.normal(mu1, sigma, n1)\n",
    "\n",
    "# Traditional t-test\n",
    "t_stat, p_val_ttest = stats.ttest_ind(group1, group0, equal_var=True)\n",
    "\n",
    "# Regression approach\n",
    "y = np.concatenate([group0, group1])\n",
    "x = np.concatenate([np.zeros(n0), np.ones(n1)])\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Compare\n",
    "print(f\"t-test statistic: {t_stat:.6f}\")\n",
    "print(f\"Regression t-stat for β₁: {model.tvalues[1]:.6f}\")\n",
    "\n",
    "print(f\"\\nt-test p-value: {p_val_ttest:.6f}\")\n",
    "print(f\"Regression p-value: {model.pvalues[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3118c",
   "metadata": {},
   "source": [
    "As promised, the two-sample equal-variance t-test yields identical results to a linear regression with a dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89e441",
   "metadata": {},
   "source": [
    "## One-Way ANOVA\n",
    "\n",
    "The two-sample t-test generalizes naturally to comparing means across more than two groups—that's one-way ANOVA. The classical ANOVA asks: \"Are the means of $k$ groups all equal, or does at least one differ from the others?\" \n",
    "\n",
    "From a regression perspective, this is just a linear model with a categorical predictor that has $k$ levels. We create $k-1$ dummy variables (leaving one group as the reference), and the F-test from ANOVA is equivalent to testing whether all the dummy variable coefficients are simultaneously zero.\n",
    "\n",
    "Let's see this in action. We'll simulate data from three groups with different means and compare the classical ANOVA F-test to the F-test from a linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67dffc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical ANOVA:\n",
      "  F-statistic: 5.145429\n",
      "  p-value: 0.008824\n",
      "\n",
      "Regression ANOVA:\n",
      "  F-statistic: 5.145429\n",
      "  p-value: 0.008824\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulate three groups with different means\n",
    "np.random.seed(42)\n",
    "n_per_group = 20\n",
    "group_a = np.random.normal(10, 2, n_per_group)\n",
    "group_b = np.random.normal(12, 2, n_per_group)\n",
    "group_c = np.random.normal(11, 2, n_per_group)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame({\n",
    "    'value': np.concatenate([group_a, group_b, group_c]),\n",
    "    'group': ['A']*n_per_group + ['B']*n_per_group + ['C']*n_per_group\n",
    "})\n",
    "\n",
    "# Classical one-way ANOVA\n",
    "f_stat_anova, p_val_anova = stats.f_oneway(group_a, group_b, group_c)\n",
    "\n",
    "# Regression approach\n",
    "model = ols('value ~ C(group)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(\"Classical ANOVA:\")\n",
    "print(f\"  F-statistic: {f_stat_anova:.6f}\")\n",
    "print(f\"  p-value: {p_val_anova:.6f}\")\n",
    "\n",
    "print(\"\\nRegression ANOVA:\")\n",
    "print(f\"  F-statistic: {anova_table.loc['C(group)', 'F']:.6f}\")\n",
    "print(f\"  p-value: {anova_table.loc['C(group)', 'PR(>F)']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b3a62a",
   "metadata": {},
   "source": [
    "Nice! The F-statistics and p-values match perfectly. The ANOVA is testing whether the group coefficients in the regression are all zero—which is exactly what the classical ANOVA F-test does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b365f4d",
   "metadata": {},
   "source": [
    "## Chi-Squared Test of Independence\n",
    "\n",
    "The chi-squared test asks whether two categorical variables are independent. The classic example: is group (A vs. B) independent of outcome (success vs. failure)? If you arrange the counts in a 2×2 contingency table, the chi-squared test tells you whether the proportions differ significantly between groups.\n",
    "\n",
    "From a regression perspective, this is testing whether a binary outcome's probability depends on a categorical predictor—which is exactly what logistic regression does. For a 2×2 table, we model the log-odds of the outcome as a function of group membership, and testing independence is equivalent to testing whether the logistic regression coefficient equals zero.\n",
    "\n",
    "> Note: unlike linear regression where we can use t-tests on coefficients to determine if a predictor matters, logistic regression and other GLMs typically use likelihood ratio tests to compare the full model with one that has the predictor of interest dropped. \n",
    "\n",
    "Let's simulate some binary outcome data for two groups and compare the classical chi-squared test to a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e21473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Chi-Squared Test:\n",
      "  χ² statistic: 8.299616\n",
      "  p-value: 0.003965\n",
      "\n",
      "Logistic Regression (Likelihood Ratio Test):\n",
      "  LR χ² statistic: 9.232498\n",
      "  p-value: 0.002378\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "# Simulate binary outcomes for two groups\n",
    "np.random.seed(42)\n",
    "n_a = 100\n",
    "n_b = 100\n",
    "\n",
    "# Group A: 30% success rate\n",
    "group_a_outcomes = np.random.binomial(1, 0.30, n_a)\n",
    "\n",
    "# Group B: 50% success rate\n",
    "group_b_outcomes = np.random.binomial(1, 0.50, n_b)\n",
    "\n",
    "# Create contingency table\n",
    "contingency_table = pd.crosstab(\n",
    "    index=pd.Series(['A']*n_a + ['B']*n_b, name='group'),\n",
    "    columns=pd.Series(np.concatenate([group_a_outcomes, group_b_outcomes]), name='outcome')\n",
    ")\n",
    "\n",
    "# Classical chi-squared test\n",
    "chi2_stat, p_val_chi2, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "# Logistic regression approach\n",
    "df = pd.DataFrame({\n",
    "    'outcome': np.concatenate([group_a_outcomes, group_b_outcomes]),\n",
    "    'group': ['A']*n_a + ['B']*n_b\n",
    "})\n",
    "\n",
    "logit_model = logit('outcome ~ C(group)', data=df).fit(disp=0)\n",
    "\n",
    "# Get the likelihood ratio test statistic (comparable to chi-squared)\n",
    "lr_stat = logit_model.llr  # Likelihood ratio test statistic\n",
    "p_val_lr = logit_model.llr_pvalue\n",
    "\n",
    "print(\"Classical Chi-Squared Test:\")\n",
    "print(f\"  χ² statistic: {chi2_stat:.6f}\")\n",
    "print(f\"  p-value: {p_val_chi2:.6f}\")\n",
    "\n",
    "print(\"\\nLogistic Regression (Likelihood Ratio Test):\")\n",
    "print(f\"  LR χ² statistic: {lr_stat:.6f}\")\n",
    "print(f\"  p-value: {p_val_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63215955",
   "metadata": {},
   "source": [
    " Whoa, this time we get slightly different test statistics and p-values; what's happening here? \n",
    "   \n",
    "The classical Pearson chi-squared test and the likelihood ratio test from logistic regression are \n",
    "actually *different test statistics* testing the same hypothesis. Both are asymptotically \n",
    "chi-squared distributed under the null, but they use different formulas and give different \n",
    "values for finite samples. The p-values are close and lead to the same conclusion—both are \n",
    "testing whether group membership and outcome are independent.\n",
    "\n",
    "There are many valid ways to answer questions using data, and appropriate approaches will tend to agree with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822b46c",
   "metadata": {},
   "source": [
    "## Advantages of a Model-Based Testing Approach\n",
    "\n",
    "We've seen that classical tests like the t-test, ANOVA, and chi-squared are special cases of regression models. But so what? Why bother with this reformulation when the canned tests work just fine?\n",
    "\n",
    "The model-based perspective offers some real practical advantages that become apparent once you start working with messier, real-world data. Let's look at two key benefits.\n",
    "\n",
    "### Clear and Explicit Assumptions\n",
    "\n",
    "In the model-based approach all key assumptions are expressed compactly in the model specification itself. Here's the model we wrote down for comparing group means:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X + \\epsilon $$\n",
    "\n",
    "where $\\epsilon \\overset{iid}{\\sim} N(0, \\sigma^2)$.\n",
    "\n",
    "From this model spec, we can immediately see the assumptions being made:\n",
    "\n",
    "* The outcome is assumed to be normally distributed\n",
    "* The variance of the outcome is assumed to be equal in the two groups (homoscedasticity)\n",
    "* The samples in each group are assumed to be independent and identically distributed\n",
    "\n",
    "We don't have to memorize separate lists of assumptions for each test in the statistical zoo. We can simply read the model specification and see exactly what we're assuming about the data generating process. And if one of these assumptions seems questionable for our context, we know exactly which part of the model to modify.\n",
    "\n",
    "### Extension to Covariates\n",
    "\n",
    "This model-based approach makes it straightforward to include covariates in the analysis.  Suppose we're comparing means between two groups, but we also have a continuous covariate (like age, tenure, marketing spend, baseline measurement, etc.) that might affect the outcome.\n",
    "\n",
    "In the classical testing framework, we'd need to reach for a different test—maybe ANCOVA—and navigate a new set of conditions and assumptions. But in the regression framework, we simply add another term to our model:\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X_{\\text{group}} + \\beta_2 X_{\\text{covariate}} + \\epsilon $$\n",
    "\n",
    "Now $\\beta_1$ represents the group difference *adjusted for* the covariate. We're still testing $H_0: \\beta_1 = 0$, just as before. The logic is identical; we've just extended the model.\n",
    "\n",
    "The same logic applies for testing interactions between group membership and covariates.\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 X_{\\text{group}} + \\beta_2 X_{\\text{covariate}} + \\beta_3 X_{\\text{group}} \\cdot X_{\\text{covariate}} + \\epsilon $$\n",
    "\n",
    "I don't even know what canned stat test lets you test interactions like that, but luckily it doesn't matter.\n",
    "The model-based approach makes these extensions feel like natural elaborations rather than jumps to entirely different procedures. We're reasoning from first principles about how we think the data were generated, rather than searching through flowcharts for the \"right\" test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e788d475",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "We've just scratched the surface of this simple idea that we can take a model-based approach to inference in many cases where the default might be to use a traditional canned statistical procedure. \n",
    "Doing so has many benefits, including making assumptions clear, extensibility to more complex scenarios with other covariates and interactions, and naturally enabling us to use Bayesian inference to do inference on the model parameters. \n",
    "Try it out; next time you reach for an off-the-shelf statistical test, see if you can take a model-based approach instead. \n",
    "\n",
    "Happy modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marketing_env5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
