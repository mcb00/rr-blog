{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Gradient Boosting Multi-Class Classification from Scratch\n",
    "categories:\n",
    "- python\n",
    "- gradient boosting\n",
    "- from scratch\n",
    "date: '2023-10-18'\n",
    "description: How to implement multi-class classification for gradient boosting in python\n",
    "draft: false\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tell me dear reader, who among us, while gazing in awe at the vast emptiness of the star-filled night sky, hasn't wondered:\n",
    "how do gradient boosting trees implement multi-class classification?\n",
    "Today, we'll unravel this mystery by reviewing the original multi-class classification gradient boosting algorithm from the classic Friedman paper and implementing one for ourselves in python.\n",
    "\n",
    "If you need a refresher on gradient boosting, then start with \n",
    "[gradient boosting regression model from scratch](/posts/gradient-boosting-machine-from-scratch/).,\n",
    "which is the first post in this epic [series on gradient boosting](/gradient-boosting-series/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming to terms with multi-class classification\n",
    "\n",
    "Let's establish our terminology and notation  for multi-class classification.\n",
    "First off, *binary* classification means that our target has two discrete levels, e.g. true or false.\n",
    "In *multi-class* classification problems our target has 3 or more possible levels, e.g. setosa, virginica, and versicolor.\n",
    "There are two ways that we'll be representing these discrete data: with integer encoding and with one hot encoding.\n",
    "Integer encoding maps each text label to a specific integer.\n",
    "One hot encoding maps each text label to a vector with a 1 in a specific position and 0's everywhere else.\n",
    "Here's how the three labels setosa, virginica, and versicolor from the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    "might get encoded.\n",
    "\n",
    "\n",
    "| text label | integer encoding | one hot encoding\n",
    "| --- | --- | --- |\n",
    "| setosa | 0 | (1, 0, 0) |\n",
    "| virginica | 1 | (0, 1, 0) |\n",
    "| versicolor | 2 | (0, 0, 1) |\n",
    "\n",
    "While describing things in the mathematical notation, we'll be working with the one hot encoded representation of the labels.\n",
    "In general when the target has $K$ discrete levels,\n",
    "the one hot encoded representation of a single training example is the vector $\\{y_1,y_2,\\dots,y_K\\}$, where $y_k \\in \\{0,1\\}$.\n",
    "For compactness, we'll write this set like $\\{y_k\\}_1^K$.\n",
    "\n",
    "Our model is going to take a feature vector $\\mathbf{x}$ of a specified length as input, and it will return a length-$K$ vector of probabilities $\\{p_k(\\mathbf{x})\\}_1^K$, where $p_k(\\mathbf{x})$ gives the probability that $y_k=1$, and where the probabilities sum to 1 $\\sum_{k=1}^K p_k(\\mathbf{x})=1$.\n",
    "\n",
    "To ensure that model outputs are valid probabilities, i.e. $0 \\le p_k(\\mathbf{x}) \\le 1$ and $\\sum_{k=1}^K p_k(\\mathbf{x})=1$, we'll use the notion of a link function which you might have seen in generalized linear modeling.\n",
    "Recall that a link function takes us from probability space to link space,\n",
    "and an inverse link function takes us from link space to probability space.\n",
    "Here we use the symmetric multiple logistic function as the link and the softmax as the inverse link, i.e.\n",
    "\n",
    "$$ p_k(\\mathbf{x}) = \\text{softmax}_k(\\{F_k(\\mathbf{x})\\}_1^K) \n",
    "    = \\frac{e^{F_k(\\mathbf{x})}}{\\sum_{l=1}^K e^{F_l(\\mathbf{x})}}$$\n",
    "\n",
    "where $\\{F_k(\\mathbf{x})\\}_1^K$ is the length-$K$ vector of raw model outputs.\n",
    "\n",
    "To make all this a bit more concrete, let's consider how this might look for a few rows of the iris data where we have $K=3$ discrete labels and for a model that uses a length-2 feature vector $\\mathbf{x}$ with (petal length, petal width).\n",
    "\n",
    "| text label | $y_1$ | $y_2$ | $y_3$ | $\\mathbf{x}$ | $F_1(\\mathbf{x})$ | $F_2(\\mathbf{x})$ | $F_3(\\mathbf{x})$ | $p_1(\\mathbf{x})$ | $p_2(\\mathbf{x})$ | $p_3(\\mathbf{x})$ |\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "| setosa | 1 | 0 | 0 | (1.4, 0.2) | 1.9 | 1.1 | -0.5 | 0.65 | 0.29 | 0.06 |\n",
    "| versicolor | 0 | 0 | 1 | (4.7, 1.4) | -0.1 | 0.7 | 2.0 | 0.09 | 0.20 | 0.71 |\n",
    "| virginica | 0 | 1 | 0 | (5.8, 2.2) | 0.2 | 0.7 | -1.0 | 0.34 | 0.56 | 0.10 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33899276, 0.55890458, 0.10210266]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def _softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "\n",
    "_softmax(np.array([1.9, 1.1, -0.5]).reshape(1,-1))\n",
    "_softmax(np.array([-0.1, 0.7, 2.0]).reshape(1,-1))\n",
    "_softmax(np.array([0.2, 0.7, -1.0]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "By now you know the general pattern for gradient boosting algorithms; if not, go review the [generic gradient boost algorithm](/posts/gradient-boosting-machine-with-any-loss-function/).\n",
    "\n",
    "1. Set the initial model predictions.\n",
    "\n",
    "1. Repeat the following for each boosting round.\n",
    "\n",
    "1. &nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Compute the pseudo residuals (negative gradients).\n",
    "\n",
    "1. &nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Train a regression tree to predict the pseudo residuals.\n",
    "\n",
    "1. &nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Adjust the tree's predicted values to optimize the overall objective.\n",
    "\n",
    "1. &nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Update the composite model by adding this new tree to it.\n",
    "\n",
    "The only wrinkle in multi-class classification is that at each boosting round, instead of training a single tree to predict pseudo residuals, we'll need to train $K$ trees, one for each level of the target variable.\n",
    "Let's take a look at the details for each of these steps.\n",
    "\n",
    "### Initial predictions\n",
    "\n",
    "The simplest way to do this is to set the initial probabilities to $1/K$, i.e. $p_k(\\mathbf{x})=1/K$ for $k=1,2,\\dots,K$, which implies $F_k(\\mathbf{x})=0$.\n",
    "\n",
    "### Pseudo Residuals\n",
    "\n",
    "For each example in the training dataset, the pseudo residual is the negative gradient of the objective function with respect to the model prediction.\n",
    "The objective function for multi-class classification is the Multinomial Negative Log Likelihood.\n",
    "For a single observation, the objective is\n",
    "\n",
    "$$ J(\\{ y_k, p_k(\\mathbf{x}) \\}_1^K) = -\\sum_{k=1}^K y_k \\log p_k(\\mathbf{x}) $$\n",
    "\n",
    "We can rewrite the objective in terms of our link space raw model output $F$ like this.\n",
    "\n",
    "$$ J(\\{ y_k, F_k(\\mathbf{x}) \\}_1^K) = -\\sum_{k=1}^K y_k \\log \\frac{e^{F_k(\\mathbf{x})}}{\\sum_{l=1}^K e^{F_l(\\mathbf{x})}}$$\n",
    "\n",
    "The negative gradient of the objective with respect to raw model prediction $F_k(\\mathbf{x}_i)$ for training example $i$ is given by\n",
    "\n",
    "$$ r_{ik} = -J'(F_k(\\mathbf{x}_i)) = -\\left[ \\frac{\\partial J(\\{ y_{il}, F_l(\\mathbf{x_i})\\}_{l=1}^K)}{\\partial F_k(\\mathbf{x}_i) } \\right] \n",
    "=y_{ik} - p_{k}(\\mathbf{x}_i)$$\n",
    "\n",
    "You can take a look at the [derivation](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)\n",
    "if you're curious how to work it out yourself.\n",
    "Note that this formula has a nice intuition.\n",
    "When $y_{ik}=1$, if predicted probability $p_k(\\mathbf{x}_i)$ is terrible and close to 0, then pseudo residual will be positive, and the next boosting round will try to increase the predicted probability.\n",
    "Otherwise if the predicted probability is already good and close to 1, the pseudo residual will be close to 0 and the next boosting round won't change the predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting tree predicted values\n",
    "\n",
    "After training a regression tree to predict the pseudo residuals,  we need to adjust the predicted values in its terminal nodes to optimize the overall objective function.\n",
    "\n",
    "In the paper, he actually specifies finding the optimal value using a numerical optimization routine like line search.\n",
    "\n",
    "$$ v = \\text{argmin}_v \\sum_{i \\in t} J(y_{i}, F(\\mathbf{x}_i) + v) $$\n",
    "\n",
    "where $t$ is the set of samples falling into this terminal node.\n",
    "\n",
    "In the scikit-learn implementation of gradient boosting classification, the authors instead use a single Newton descent step to approximate the optimal predicted value for each terminal node.\n",
    "See code and comments for the function `_update_terminal_regions` in the scikit-learn gradient boosting module.\n",
    "The updated value is computed as \n",
    "\n",
    "$$ v = -\\frac{\\sum_{i \\in t} J'(F_k(\\mathbf{x}_i))}{\\sum_{i \\in t} J''(F_k(\\mathbf{x}_i))} $$\n",
    "\n",
    "We already found the first derivative or gradient of the objective, so we just need to calculate the second derivative or hessian.\n",
    "\n",
    "$$ J''(F_k(\\mathbf{x}_i)) = \n",
    "\\left[ \\frac{\\partial J(\\{ y_{il}, F_l(\\mathbf{x_i})\\}_{l=1}^K)}{\\partial ^2 F_k(\\mathbf{x}_i) } \\right]\n",
    "= p_k(\\mathbf{x}_i) (1 - p_k(\\mathbf{x}_i))\n",
    "$$\n",
    "\n",
    "We now have all the ingredients we need to implement gradient boosting multi-class classification from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class GBClassifier():\n",
    "    '''Gradient Boosting Classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int\n",
    "        number of boosting rounds\n",
    "        \n",
    "    learning_rate : float\n",
    "        learning rate hyperparameter\n",
    "        \n",
    "    max_depth : int\n",
    "        maximum tree depth\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_estimators, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators=n_estimators; \n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_depth=max_depth;\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Fit the GBM using the specified loss function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of size (number observations, number features)\n",
    "            design matrix\n",
    "            \n",
    "        y : ndarray of size (number observations,)\n",
    "            target labels in {0,1,...,k-1}\n",
    "        '''\n",
    "        \n",
    "        self.n_classes = pd.Series(y).nunique()\n",
    "        y_ohe = self._one_hot_encode_labels(y)\n",
    "\n",
    "        raw_predictions = np.zeros(shape=y_ohe.shape)\n",
    "        probabilities = self._softmax(raw_predictions)\n",
    "        self.boosters = []\n",
    "        for m in range(self.n_estimators):\n",
    "            class_trees = []\n",
    "            for k in range(self.n_classes):\n",
    "                negative_gradients = self._negative_gradients(y_ohe[:, k], probabilities[:, k])\n",
    "                hessians = self._hessians(probabilities[:, k])\n",
    "                tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "                tree.fit(X, negative_gradients);\n",
    "                self._update_terminal_nodes(tree, X, negative_gradients, hessians)\n",
    "                raw_predictions[:, k] += self.learning_rate * tree.predict(X)\n",
    "                probabilities = self._softmax(raw_predictions)\n",
    "                class_trees.append(tree)\n",
    "            self.boosters.append(class_trees)\n",
    "    \n",
    "    def _one_hot_encode_labels(self, y):\n",
    "        if isinstance(y, pd.Series): y = y.values\n",
    "        ohe = OneHotEncoder()\n",
    "        y_ohe = ohe.fit_transform(y.reshape(-1, 1)).toarray()\n",
    "        return y_ohe\n",
    "        \n",
    "    def _negative_gradients(self, y, p):\n",
    "        return y - p\n",
    "    \n",
    "    def _hessians(self, p): \n",
    "        return p * (1 - p)\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1)\n",
    "        \n",
    "    def _update_terminal_nodes(self, tree, X, negative_gradients, hessians):\n",
    "        '''Update the terminal node predicted values'''\n",
    "        # terminal node id's\n",
    "        leaf_nodes = np.nonzero(tree.tree_.children_left == -1)[0]\n",
    "        # compute leaf for each sample in ``X``.\n",
    "        leaf_node_for_each_sample = tree.apply(X)\n",
    "        for leaf in leaf_nodes:\n",
    "            samples_in_this_leaf = np.where(leaf_node_for_each_sample == leaf)[0]\n",
    "            negative_gradients_in_leaf = negative_gradients.take(samples_in_this_leaf, axis=0)\n",
    "            hessians_in_leaf = hessians.take(samples_in_this_leaf, axis=0)\n",
    "            val = np.sum(negative_gradients_in_leaf) / np.sum(hessians_in_leaf)\n",
    "            tree.tree_.value[leaf, 0, 0] = val\n",
    "          \n",
    "    def predict_proba(self, X):\n",
    "        '''Generate probability predictions for the given input data.'''\n",
    "        raw_predictions =  np.zeros(shape=(X.shape[0], self.n_classes))\n",
    "        for k in range(self.n_classes):\n",
    "            for booster in self.boosters:\n",
    "                raw_predictions[:, k] += booster[k].predict(X)\n",
    "        probabilities = self._softmax(raw_predictions)\n",
    "        return probabilities\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''Generate predicted labels (as 1-d array)'''\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "\n",
    "# dbunch = load_iris(as_frame=True)\n",
    "# df = dbunch['frame']\n",
    "# target = 'target'\n",
    "# features = dbunch['feature_names']\n",
    "# X = df[features]\n",
    "# y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_classification(n_samples=10000, \n",
    "                           n_classes=5, \n",
    "                           n_features=20,\n",
    "                           n_informative=10,\n",
    "                           random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.444"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_sklearn = GradientBoostingClassifier(n_estimators=10, max_depth=1)\n",
    "gb_sklearn.fit(X_train, y_train)\n",
    "accuracy_score(y_test, gb_sklearn.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4524"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_scratch = GBClassifier(n_estimators=10, max_depth=1)\n",
    "gb_scratch.fit(X_train, y_train)\n",
    "accuracy_score(y_test, gb_scratch.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
