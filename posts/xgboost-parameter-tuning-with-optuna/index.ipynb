{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: The Ultimate Guide to XGBoost Parameter Tuning\n",
    "categories:\n",
    "- python\n",
    "- tutorial\n",
    "- gradient boosting\n",
    "- xgboost\n",
    "date: '2023-12-26'\n",
    "description: My approach for efficiently tuning XGBoost parameters with optuna in python\n",
    "draft: false\n",
    "image: optuna_thumbnail.jpg \n",
    "image-alt: lunar halo\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh, the dark art of hyperparameter tuning.\n",
    "It's a key step in the machine learning workflow,\n",
    "and it's an activity that can easily be overlooked or be overkill.\n",
    "Therefore, dear reader, it is an art that requires the application of both skill and wisdom to realize its full potential while avoiding its perils.\n",
    "Today I'll show you my approach for hyperparameter tuning XGBoost, although the principles apply to any GBT framework.\n",
    "I'll give you some intuition for how to think about the key parameters in XGBoost,\n",
    "and I'll show you an efficient strategy for parameter tuning GBTs.\n",
    "I'll be using the optuna python library to tune parameters with bayesian optimization, but you can implement my strategy with whatever hyperparameter tuning utility you like.\n",
    "You can download a notebook with this tuning workflow from my [data science templates repository](https://github.com/mcb00/ds-templates).\n",
    "Finally we'll wrap up with the kind of cautionary tale data scientists tell their colleagues around the campfire about when all this fancy hyperparameter tuning can backfire catastrophically&mdash;ignore at your own peril."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lunar halo on a frosty night in Johnson City, TN](optuna_main.jpg \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Parameters\n",
    "\n",
    "Gradient boosting algorithms like XGBoost have two main types of hyperparameters: *tree parameters* which control the decision tree trained at each boosting round and *boosting parameters* which control the boosting procedure itself.\n",
    "Below I'll highlight my favorite parameters, but you can see the full list in the [documentation](https://xgboost.readthedocs.io/en/stable/parameter.html).\n",
    "\n",
    "### Tree Parameters\n",
    "In theory you can use any kind of model as a base learner in a gradient boosting algorithm, but for reasons we discussed before, [decision trees](/posts/consider-the-decision-tree/) \n",
    "are typically the best choice.\n",
    "In XGBoost, we can choose the tree construction algorithm, and we get three types of parameters to control its behavior: tree complexity parameters, sampling parameters, and regularization parameters.\n",
    "\n",
    "#### Tree construction algorithm\n",
    "The tree construction algorithm boils down to split finding, and\n",
    "different algorithms have different ways of generating candidate splits to consider.\n",
    "In XGBoost we have the parameter:\n",
    "\n",
    "* `tree_method` - select tree construction algorithm: `exact`, `hist`,  `approx`, or the horrifying default&mdash;`auto`&mdash;which outsources your choice of tree construction algo to XGBoost and which you should never ever use.  I've been burned by this hidden `tree_method=auto` default multiple times before learning my lesson. Why is the online model worse than the offline model? Why is this model suddenly taking so much longer to train? Avoid these debugging nightmares and set `tree_method` explicitly; the exact method tends to be slow and ironically less accurate, so I use either approx or hist.\n",
    "\n",
    "#### Tree complexity parameters\n",
    "Tree complexity just means how many leaf nodes the trees have, and therefore how expressive they can be.\n",
    "I use these two parameters:\n",
    "\n",
    "* `max_depth` - maximum number of split levels allowed. Reasonable values are usually from 3-12.\n",
    "* `min_child_weight` - minimum allowable sum of hessian values over data in a node. When using the default squared error objective, this is the minimum number of samples allowed in a leaf node (see [this explanation](https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm) of why that's true).\n",
    "For a squared error objective, values in [1, 200] usually work well.\n",
    "\n",
    "These two parameters oppose each other; increasing max depth allows for more expressive trees, while increasing  min child weight makes trees less expressive and therefore is a powerful way to counter overfitting.\n",
    "Note that `gamma` (a.k.a. `min_split_loss`) also limits node splitting, but I usually don't use it because `min_child_weight` seems to work well enough on its own.\n",
    "\n",
    "#### Sampling parameters\n",
    "XGBoost can randomly sample rows and columns to be used for training each tree;\n",
    "you might think of this as *bagging*.\n",
    "We have a few parameters:\n",
    "\n",
    "* `subsample` - proportion of rows to use in each tree. Setting this less than 1.0 results in stochastic gradient descent, because each tree is trained on only a subset of the entire training dataset. Any value in (0,1] is valid, but it seems like values in [0.7, 1] are usually the best.\n",
    "* `colsample_bytree`, `colsample_bylevel`, `colsample_bynode` - control the fraction of columns available to each tree, at each split level, or at each split, respectively. I usually use either by level or by node because I like the idea that trees might be forced to learn interactions by having different features available at each subsequent split.  Again, values in (0,1] are valid, but values in [0.5,1] usually seem to work best.\n",
    "\n",
    "#### Regularization parameters\n",
    "In XGBoost, regularization penalizes the actual values predicted by the individual trees, pushing values toward zero.\n",
    "I usually use:\n",
    "\n",
    "* `reg_lambda` - L2 regularization  of tree predicted values. Increasing this parameter decreases tree expressiveness and therefore counters overfitting. \n",
    "Valid values are in [0,$\\infty$), but good values typically fall in [0,10].\n",
    "\n",
    "There is also an L1 regularization parameter called `reg_alpha`; feel free to use it instead.\n",
    "It seems that using one or the other is usually sufficient.\n",
    "\n",
    "### Boosting Parameters and Early Stopping\n",
    "Trained gradient boosting models take the form:\n",
    "\n",
    "$$ F(\\mathbf{x}) = b + \\eta \\sum_{k=1}^{K} f_k(\\mathbf{x}) $$ \n",
    "\n",
    "where $b$ is the constant base predicted value, $f_k(\\cdot)$ is the base learner for round $k$, parameter $K$ is the number of boosting rounds, and parameter $\\eta$ is the learning rate.\n",
    "In XGBoost these parameters correspond with:\n",
    "\n",
    "* `num_boost_round` ($K$) - the number of boosting iterations\n",
    "* `learning_rate` ($\\eta$) - the scaling or \"shrinkage\" factor applied to the predicted value of each base learner. Valid values are in (0,1]; the default is 0.3. Fun fact: the $\\eta$ character is called \"eta\", and `learning_rate` is aliased to `eta` in xgboost, so you can use parameter `eta` instead of `learning_rate` if you like.\n",
    "\n",
    "These two parameters are very closely linked; the optimal value of one depends on the value of the other.\n",
    "To illustrate their relationship, we can train two different XGBoost models on the same training dataset, where one model has a lower learning rate than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| fig-cap: Hold-out validation score (RMSE) by boosting round for two XGBoost models differing only by learning rate.\n",
    "#| fig-alt: XGBoost objective curves for two models\n",
    "#| code-fold: true\n",
    "\n",
    "import xgboost as xgb \n",
    "from sklearn.datasets import make_regression \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X, y = make_regression(5000, random_state=0)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "eta1, eta2 = 0.3, 0.15\n",
    "reg1 = xgb.XGBRegressor(n_estimators=1000, learning_rate=eta1, early_stopping_rounds=25)\n",
    "reg1.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "        verbose=0);\n",
    "reg2 = xgb.XGBRegressor(n_estimators=1000, learning_rate=eta2, early_stopping_rounds=25)\n",
    "reg2.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)], \n",
    "        verbose=0);\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "best_round1, best_round2 = reg1.best_iteration, reg2.best_iteration\n",
    "obj1 = reg1.evals_result()['validation_1']['rmse']\n",
    "obj2 = reg2.evals_result()['validation_1']['rmse']\n",
    "best_obj1 = obj1[best_round1]\n",
    "best_obj2 = obj2[best_round1]\n",
    "\n",
    "plt.plot(reg1.evals_result()['validation_1']['rmse'], '-b', label=f'learning_rate={eta1}')\n",
    "plt.plot(reg2.evals_result()['validation_1']['rmse'], '-r', label=f'learning_rate={eta2}')\n",
    "ax.annotate(f'learning_rate={eta1}\\nbest_iteration={best_round1}', \n",
    "            xy=(best_round1, best_obj1), \n",
    "            xytext=(best_round1, best_obj1+20),\n",
    "            horizontalalignment='right',\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "ax.annotate(f'learning_rate={eta2}\\nbest_iteration={best_round2}', \n",
    "            xy=(best_round2, best_obj2), \n",
    "            xytext=(best_round2, best_obj2+20),\n",
    "            horizontalalignment='right',\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE') \n",
    "plt.xlabel('boosting round') \n",
    "plt.title('Validation Scores by Boosting Round');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows root mean squared error measured on a held-out validation dataset for two different XGBoost models: one with a higher learning rate and one with a lower learning rate.\n",
    "The figure demonstrates two key properties of the boosting parameters:\n",
    "\n",
    "1. While training a model with a given learning rate, the evaluation score (computed on a hold-out set) tends to improve with additional boosting rounds up to a certain point, but beyond that point it flattens out or even gets worse.\n",
    "1. All else constant, a smaller learning rate leads to a model with more boosting rounds and better  evaluation score.\n",
    "\n",
    "We can leverage the first property to make our tuning more efficient by using XGBoost's `early_stopping_rounds: int` argument, which terminates training after observing the specified number of boosting rounds [without sufficient improvement](https://github.com/dmlc/xgboost/pull/6942)\n",
    "to the evaluation metric.\n",
    "The models above were trained using `early_stopping_rounds=50`, which terminates training after 50 boosting rounds without improvement in RMSE on the validation data.\n",
    "For each model, the arrow indicates the boosting round with the best score.\n",
    "\n",
    "The figure also exemplifies the second property, where the model with lower learning rate attains a better validation score but requires more boosting rounds to trigger early stopping.\n",
    "Note that smaller and smaller learning rates will provide diminishing improvements to the validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## An Efficient Parameter Search Strategy for XGBoost\n",
    "Efficiency is the key to effective parameter tuning, because wasting less time means searching more  parameter values and finding better models in a given amount of time.\n",
    "But as we just saw, there is a tradeoff between accuracy and training time via the learning rate.\n",
    "Given infinite time and compute resources, we would just choose an arbitrarily tiny learning rate and search through tree parameter values while using early stopping to choose the number of boosting rounds.\n",
    "The problem is that tiny learning rates require tons of boosting rounds, which will make our ideal search prohibitively slow when confronted with the reality of finite time and resources.\n",
    "So what can we do?\n",
    "\n",
    "My approach is based on the claim that\n",
    "*good tree parameters at one learning rate are also good tree parameters at other learning rates*.\n",
    "The intuition is that given two models&mdash;one with good tree parameters and one with bad tree parameters&mdash;the model with good tree parameters will score better, regardless of the learning rate.\n",
    "Thus, tree parameters are \"independent\" of boosting parameters&mdash;See \n",
    "[this notebook](https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing) for justification of this claim.\n",
    "\n",
    "Independence between tree parameters and boosting parameters suggests a two-stage procedure where we first find optimal tree parameters, then we maximize performance by pushing boosting parameters to the extreme.\n",
    "The procedure is:\n",
    "\n",
    "1. **Tune tree parameters.** Fix the learning rate at a relatively high value (like 0.3ish) and enable early stopping so that each model trains within a few seconds. Use your favorite hyperparameter tuning technique to find the optimal tree parameters.\n",
    "1. **Tune boosting parameters.** Using these optimal tree parameter values, fix the learning rate as low as you want and train your model, using early stopping to identify the optimal number of boosting rounds.\n",
    "\n",
    "Why is this a good idea? \n",
    "Because by starting with a high learning rate and early stopping enabled, you can burn through hundreds of model training trials and find some really good tree parameters in a few minutes.\n",
    "Then, with the confidence that your tree parameters are actually quite good,  you can set a really low learning rate and boost a few thousand rounds to get a model with the best of both tree parameter and boosting parameter worlds.\n",
    "\n",
    "> You can check out [this notebook](https://colab.research.google.com/drive/1feyrtDphFizVI1VmctrNgpJzonsmWiHx?usp=sharing)\n",
    "where I justify this approach by running two parameter searches&mdash;one with high learning rate and one with low learning rate&mdash;showing that they recover the same optimal tree parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning XGBoost Parameters with Optuna\n",
    "\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/)\n",
    "is a model-agnostic python library for hyperparameter tuning.\n",
    "I like it because it has a flexible API that abstracts away the details of the search algorithm being used.\n",
    "That means you can use this one library to tune all kinds of different models, and you can easily switch the parameter sampling approach among grid search, random search, the very sensible default bayesian optimization, and more.\n",
    "Another massive benefit is that optuna provides a specific [XGBoost integration](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.XGBoostPruningCallback.html) \n",
    "which terminates training early on lousy parameter combinations.\n",
    "\n",
    "You can install optuna with anaconda, e.g.\n",
    "\n",
    "```.zsh\n",
    "$ conda install -c conda-forge optuna\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Tuning the Bluebook for Bulldozers Regression Model\n",
    "\n",
    "To illustrate the procedure, we'll tune the parameters for the regression model we built back in the [XGBoost for regression](/posts/xgboost-for-regression-in-python/) post.\n",
    "First we'll load up the bulldozer data and prepare the features and target just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p3/n6r4g5mj3m3dhls3j66tztmh0000gn/T/ipykernel_61627/2745696469.py:11: DtypeWarning: Columns (13,39,40,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../xgboost-for-regression-in-python/Train.csv', parse_dates=['saledate']);\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "#| output: false\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import optuna \n",
    "\n",
    "df = pd.read_csv('../xgboost-for-regression-in-python/Train.csv', parse_dates=['saledate']);\n",
    "\n",
    "def encode_string_features(df):\n",
    "    out_df = df.copy()\n",
    "    for feature, feature_type in df.dtypes.items():\n",
    "        if feature_type == 'object':\n",
    "            out_df[feature] = out_df[feature].astype('category')\n",
    "    return out_df\n",
    "\n",
    "df = encode_string_features(df)\n",
    "\n",
    "df['saledate_days_since_epoch'] = (\n",
    "    df['saledate'] - pd.Timestamp(year=1970, month=1, day=1)\n",
    "    ).dt.days\n",
    "\n",
    "df['logSalePrice'] = np.log1p(df['SalePrice'])\n",
    "\n",
    "\n",
    "features = [\n",
    "    'SalesID',\n",
    "    'MachineID',\n",
    "    'ModelID',\n",
    "    'datasource',\n",
    "    'auctioneerID',\n",
    "    'YearMade',\n",
    "    'MachineHoursCurrentMeter',\n",
    "    'UsageBand',\n",
    "    'fiModelDesc',\n",
    "    'fiBaseModel',\n",
    "    'fiSecondaryDesc',\n",
    "    'fiModelSeries',\n",
    "    'fiModelDescriptor',\n",
    "    'ProductSize',\n",
    "    'fiProductClassDesc',\n",
    "    'state',\n",
    "    'ProductGroup',\n",
    "    'ProductGroupDesc',\n",
    "    'Drive_System',\n",
    "    'Enclosure',\n",
    "    'Forks',\n",
    "    'Pad_Type',\n",
    "    'Ride_Control',\n",
    "    'Stick',\n",
    "    'Transmission',\n",
    "    'Turbocharged',\n",
    "    'Blade_Extension',\n",
    "    'Blade_Width',\n",
    "    'Enclosure_Type',\n",
    "    'Engine_Horsepower',\n",
    "    'Hydraulics',\n",
    "    'Pushblock',\n",
    "    'Ripper',\n",
    "    'Scarifier',\n",
    "    'Tip_Control',\n",
    "    'Tire_Size',\n",
    "    'Coupler',\n",
    "    'Coupler_System',\n",
    "    'Grouser_Tracks',\n",
    "    'Hydraulics_Flow',\n",
    "    'Track_Type',\n",
    "    'Undercarriage_Pad_Width',\n",
    "    'Stick_Length',\n",
    "    'Thumb',\n",
    "    'Pattern_Changer',\n",
    "    'Grouser_Type',\n",
    "    'Backhoe_Mounting',\n",
    "    'Blade_Type',\n",
    "    'Travel_Controls',\n",
    "    'Differential_Type',\n",
    "    'Steering_Controls',\n",
    "    'saledate_days_since_epoch'\n",
    " ]\n",
    "\n",
    "target = 'logSalePrice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this time, since we're going to slam our validation set over and over during hyperparameter search, we want to reserve an actual test set to check how the final model generalizes.\n",
    "We make four different `xgboost.DMatrix` datasets for this process: training, validation, training+validation, and test. \n",
    "Training and validation are for the parameter search, and training+validation and test are for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_valid = 12000\n",
    "n_test = 12000\n",
    "\n",
    "sorted_df = df.sort_values(by='saledate')\n",
    "train_df = sorted_df[:-(n_valid + n_test)] \n",
    "valid_df = sorted_df[-(n_valid + n_test):-n_test] \n",
    "test_df = sorted_df[-n_test:]\n",
    "\n",
    "dtrain = xgb.DMatrix(data=train_df[features], label=train_df[target], \n",
    "                     enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=valid_df[features], label=valid_df[target], \n",
    "                     enable_categorical=True)\n",
    "dtest = xgb.DMatrix(data=test_df[features], label=test_df[target], \n",
    "                    enable_categorical=True)\n",
    "dtrainvalid = xgb.DMatrix(data=pd.concat([train_df, valid_df])[features], \n",
    "                          label=pd.concat([train_df, valid_df])[target], \n",
    "                          enable_categorical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries: base parameters and scoring function\n",
    "\n",
    "We'll go ahead and set a couple of parameters that we usually want to keep fixed across all trials in a parameter search, including the XGBoost objective for training and the evaluation metric to be used for early stopping.\n",
    "We'll also want to implement a model scoring function that takes a trained model and a dataset and returns the score, in our case, RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'rmse'\n",
    "base_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': metric,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model: xgb.core.Booster, dmat: xgb.core.DMatrix) -> float:\n",
    "    y_true = dmat.get_label() \n",
    "    y_pred = model.predict(dmat) \n",
    "    return mean_squared_error(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Tune Tree Parameters with Optuna\n",
    "\n",
    "Next we need to choose a fixed learning rate and tune the tree parameters.\n",
    "We want a learning rate that allows us to train within a few seconds, so we need to time model training.\n",
    "Start with a high learning rate (like 0.8) and work down until you find a rate that takes a few seconds.\n",
    "Below I end up landing at 0.3, which takes about 4 seconds to train on my little laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5 seconds\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "\n",
    "params = {\n",
    "    'tree_method': 'approx',\n",
    "    'learning_rate': learning_rate\n",
    "}\n",
    "params.update(base_params)\n",
    "tic = time.time()\n",
    "model = xgb.train(params=params, dtrain=dtrain,\n",
    "                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                  num_boost_round=10000,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=0)\n",
    "print(f'{time.time() - tic:.1f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we implement our optuna objective, a function taking an optuna study [`Trial`](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html) \n",
    "object and returning the score we want to optimize.\n",
    "We use the `suggest_categorical`, `suggest_float`, and `suggest_int` methods of the `Trial` object to define the search space for each parameter.\n",
    "Note the use of the pruning callback function which we pass into the `callback` argument of the XGBoost `train` function; this is a must, since it allows optuna to terminate training on lousy models after a few boosting rounds.\n",
    "After training a model with the selected parameter values, we stash the optimal number of boosting rounds from early stopping into an optuna user attribute using the [`trial.user_attrs()`](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/003_attributes.html) \n",
    "method. \n",
    "Finally we return the score computed by our `model_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'tree_method': trial.suggest_categorical('tree_method', ['approx', 'hist']),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 250),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.1, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 25, log=True),\n",
    "        'learning_rate': learning_rate,\n",
    "    }\n",
    "    num_boost_round = 10000\n",
    "    params.update(base_params)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, f'valid-{metric}')\n",
    "    model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
    "                      evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                      early_stopping_rounds=50,\n",
    "                      verbose_eval=0,\n",
    "                      callbacks=[pruning_callback])\n",
    "    trial.set_user_attr('best_iteration', model.best_iteration)\n",
    "    return model.best_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new optuna study and search through 50 parameter combinations, you could just run these two lines.\n",
    "\n",
    "```python\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "```\n",
    "\n",
    "But, in practice, I prefer to run these potentially long running tasks for a pre-specified amount of clock time, rather than a specified number of trials&mdash;who knows how long 50 trials will take.\n",
    "I also want the results to be reproducible.\n",
    "So, to set the random seed and run the optimization for around 300 seconds (long enough to go make a nice cup of tea, stretch,  and come back), I do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-25 10:56:44,240] A new study created in memory with name: no-name-c02dbf06-f9df-4d94-aabe-3e4aa3483743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-25 10:56:54,971] Trial 0 finished with value: 0.24806856718849682 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 150, 'subsample': 0.24041677639819287, 'colsample_bynode': 0.2403950683025824, 'reg_lambda': 0.001800728515054226}. Best is trial 0 with value: 0.24806856718849682.\n",
      "[I 2023-12-25 10:57:01,291] Trial 1 finished with value: 0.23107522766919256 and parameters: {'tree_method': 'approx', 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.9729188669457949, 'colsample_bynode': 0.8491983767203796, 'reg_lambda': 0.008587261143813469}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 10:57:14,861] Trial 2 finished with value: 0.23580702157545438 and parameters: {'tree_method': 'hist', 'max_depth': 6, 'min_child_weight': 132, 'subsample': 0.48875051677790415, 'colsample_bynode': 0.36210622617823773, 'reg_lambda': 0.4907861801695248}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 10:57:25,068] Trial 3 finished with value: 0.23483754550892544 and parameters: {'tree_method': 'hist', 'max_depth': 6, 'min_child_weight': 115, 'subsample': 0.8066583652537123, 'colsample_bynode': 0.2797064039425238, 'reg_lambda': 0.18263003844477702}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 10:57:33,513] Trial 4 finished with value: 0.24529773036120728 and parameters: {'tree_method': 'approx', 'max_depth': 9, 'min_child_weight': 43, 'subsample': 0.1585464336867516, 'colsample_bynode': 0.9539969835279999, 'reg_lambda': 17.65191192764218}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 10:57:33,564] Trial 5 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:57:33,908] Trial 6 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:57:33,991] Trial 7 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:57:34,033] Trial 8 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:57:34,166] Trial 9 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:34,585] Trial 10 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:34,640] Trial 11 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:34,823] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:35,025] Trial 13 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:35,095] Trial 14 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:35,497] Trial 15 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:57:35,577] Trial 16 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:35,837] Trial 17 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:35,874] Trial 18 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:36,083] Trial 19 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:36,313] Trial 20 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:36,871] Trial 21 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:57:37,364] Trial 22 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:57:37,610] Trial 23 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:37,665] Trial 24 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:37,849] Trial 25 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:38,190] Trial 26 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:38,561] Trial 27 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:38,795] Trial 28 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:57:39,062] Trial 29 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,100] Trial 30 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,206] Trial 31 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,249] Trial 32 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,311] Trial 33 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,379] Trial 34 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,416] Trial 35 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,491] Trial 36 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:39,853] Trial 37 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:57:40,256] Trial 38 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:57:40,662] Trial 39 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:40,846] Trial 40 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:41,040] Trial 41 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:57:41,107] Trial 42 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:41,213] Trial 43 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:41,347] Trial 44 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:41,417] Trial 45 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:41,819] Trial 46 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:41,992] Trial 47 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:42,387] Trial 48 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:57:43,022] Trial 49 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:43,214] Trial 50 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:43,461] Trial 51 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:43,955] Trial 52 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:44,537] Trial 53 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:57:44,630] Trial 54 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:57:44,810] Trial 55 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 10:57:45,209] Trial 56 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:45,454] Trial 57 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:57:45,644] Trial 58 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:57:45,821] Trial 59 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:45,964] Trial 60 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:57:46,334] Trial 61 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:57:59,258] Trial 62 pruned. Trial was pruned at iteration 352.\n",
      "[I 2023-12-25 10:57:59,744] Trial 63 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:57:59,823] Trial 64 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:00,218] Trial 65 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:00,614] Trial 66 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:00,976] Trial 67 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:58:01,143] Trial 68 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:01,336] Trial 69 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:01,880] Trial 70 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:12,740] Trial 71 finished with value: 0.236310655413018 and parameters: {'tree_method': 'hist', 'max_depth': 10, 'min_child_weight': 134, 'subsample': 0.9611896852850221, 'colsample_bynode': 0.14256871567731344, 'reg_lambda': 0.12630589704850248}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 10:58:12,817] Trial 72 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:12,873] Trial 73 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:12,981] Trial 74 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:13,072] Trial 75 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:13,352] Trial 76 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:19,153] Trial 77 finished with value: 0.24076604474340896 and parameters: {'tree_method': 'hist', 'max_depth': 11, 'min_child_weight': 58, 'subsample': 0.8671268437047952, 'colsample_bynode': 0.14455518401838097, 'reg_lambda': 0.029490898325781917}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 10:58:19,403] Trial 78 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:19,695] Trial 79 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:19,808] Trial 80 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:20,046] Trial 81 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:20,194] Trial 82 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:20,302] Trial 83 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:20,405] Trial 84 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:20,465] Trial 85 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:20,630] Trial 86 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:20,801] Trial 87 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:20,887] Trial 88 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:21,102] Trial 89 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:21,289] Trial 90 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:21,404] Trial 91 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:21,655] Trial 92 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 10:58:21,731] Trial 93 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:21,833] Trial 94 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:21,950] Trial 95 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:22,073] Trial 96 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:22,719] Trial 97 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:23,029] Trial 98 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:23,211] Trial 99 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:23,427] Trial 100 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:23,742] Trial 101 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:24,003] Trial 102 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:24,281] Trial 103 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:58:24,509] Trial 104 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:24,836] Trial 105 pruned. Trial was pruned at iteration 12.\n",
      "[I 2023-12-25 10:58:24,873] Trial 106 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:25,155] Trial 107 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:58:25,369] Trial 108 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:25,538] Trial 109 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:25,630] Trial 110 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:25,890] Trial 111 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:25,948] Trial 112 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:26,232] Trial 113 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:58:26,443] Trial 114 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:26,659] Trial 115 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:26,966] Trial 116 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:27,192] Trial 117 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:27,459] Trial 118 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:27,597] Trial 119 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:27,833] Trial 120 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:27,897] Trial 121 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:28,136] Trial 122 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:28,202] Trial 123 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:28,294] Trial 124 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:28,374] Trial 125 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:28,438] Trial 126 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:28,527] Trial 127 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:28,748] Trial 128 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 10:58:28,794] Trial 129 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:28,851] Trial 130 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:29,118] Trial 131 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:29,372] Trial 132 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:29,638] Trial 133 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:29,708] Trial 134 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:29,904] Trial 135 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:30,028] Trial 136 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:30,300] Trial 137 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:30,573] Trial 138 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:30,979] Trial 139 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:31,284] Trial 140 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:31,360] Trial 141 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:31,460] Trial 142 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:31,774] Trial 143 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:58:31,906] Trial 144 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:31,956] Trial 145 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:32,134] Trial 146 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:32,278] Trial 147 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:32,623] Trial 148 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:32,833] Trial 149 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:33,183] Trial 150 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 10:58:33,511] Trial 151 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:33,763] Trial 152 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:33,968] Trial 153 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:34,206] Trial 154 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:34,398] Trial 155 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:34,457] Trial 156 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:34,714] Trial 157 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:34,901] Trial 158 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:35,155] Trial 159 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 10:58:35,391] Trial 160 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 10:58:35,668] Trial 161 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:35,884] Trial 162 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:36,099] Trial 163 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:36,140] Trial 164 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:36,373] Trial 165 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:36,423] Trial 166 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:36,597] Trial 167 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:36,875] Trial 168 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:36,946] Trial 169 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:37,124] Trial 170 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:37,440] Trial 171 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:37,687] Trial 172 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:37,874] Trial 173 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:38,116] Trial 174 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:38,305] Trial 175 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:38,392] Trial 176 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:38,667] Trial 177 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:38,861] Trial 178 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:39,037] Trial 179 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:39,224] Trial 180 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:39,438] Trial 181 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:39,615] Trial 182 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:39,731] Trial 183 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:39,769] Trial 184 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:39,868] Trial 185 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:40,028] Trial 186 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:40,168] Trial 187 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:40,242] Trial 188 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:40,585] Trial 189 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:58:40,782] Trial 190 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:41,052] Trial 191 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:41,316] Trial 192 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:41,558] Trial 193 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:41,628] Trial 194 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:41,874] Trial 195 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:42,094] Trial 196 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:42,141] Trial 197 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:42,183] Trial 198 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:42,430] Trial 199 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:42,488] Trial 200 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:42,709] Trial 201 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:42,974] Trial 202 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:43,282] Trial 203 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-25 10:58:43,614] Trial 204 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:43,858] Trial 205 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:43,948] Trial 206 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:44,237] Trial 207 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 10:58:44,500] Trial 208 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:44,729] Trial 209 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:45,009] Trial 210 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:58:45,277] Trial 211 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:45,506] Trial 212 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:45,545] Trial 213 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:45,589] Trial 214 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:45,833] Trial 215 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:46,038] Trial 216 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:46,279] Trial 217 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:46,544] Trial 218 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:46,790] Trial 219 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:47,033] Trial 220 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:47,128] Trial 221 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:47,374] Trial 222 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:47,667] Trial 223 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:58:47,713] Trial 224 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:47,924] Trial 225 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:48,252] Trial 226 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:48,577] Trial 227 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:48,937] Trial 228 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:49,255] Trial 229 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:49,314] Trial 230 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:49,555] Trial 231 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:49,791] Trial 232 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:50,017] Trial 233 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:50,301] Trial 234 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:50,353] Trial 235 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:50,559] Trial 236 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:50,722] Trial 237 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:50,822] Trial 238 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:51,005] Trial 239 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:51,368] Trial 240 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:51,619] Trial 241 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:51,911] Trial 242 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:58:52,117] Trial 243 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:52,353] Trial 244 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:52,603] Trial 245 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:52,653] Trial 246 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:53,016] Trial 247 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:58:53,358] Trial 248 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:53,527] Trial 249 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:53,915] Trial 250 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:58:54,098] Trial 251 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:54,280] Trial 252 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:54,322] Trial 253 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:54,494] Trial 254 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:54,674] Trial 255 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:54,866] Trial 256 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:54,970] Trial 257 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:55,094] Trial 258 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:55,280] Trial 259 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:58:55,320] Trial 260 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:55,499] Trial 261 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:55,771] Trial 262 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:58:56,001] Trial 263 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:56,207] Trial 264 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:56,535] Trial 265 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:56,582] Trial 266 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:56,940] Trial 267 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:57,209] Trial 268 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:57,628] Trial 269 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:57,772] Trial 270 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:58,155] Trial 271 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:58:58,505] Trial 272 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:58,570] Trial 273 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:58:58,761] Trial 274 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:58:59,099] Trial 275 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:59,334] Trial 276 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:58:59,532] Trial 277 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:00,024] Trial 278 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:59:00,355] Trial 279 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:00,634] Trial 280 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:00,749] Trial 281 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:01,065] Trial 282 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:01,293] Trial 283 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:01,481] Trial 284 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:05,229] Trial 285 finished with value: 0.23652893740008707 and parameters: {'tree_method': 'approx', 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.794056840409941, 'colsample_bynode': 0.34674329256574654, 'reg_lambda': 0.0626566027861816}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 10:59:05,288] Trial 286 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:05,611] Trial 287 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:05,895] Trial 288 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:06,034] Trial 289 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:06,134] Trial 290 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:06,382] Trial 291 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:59:06,718] Trial 292 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:06,808] Trial 293 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:06,944] Trial 294 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:07,113] Trial 295 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:07,187] Trial 296 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:07,381] Trial 297 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:07,537] Trial 298 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:07,592] Trial 299 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:07,834] Trial 300 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:08,121] Trial 301 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:08,285] Trial 302 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:08,424] Trial 303 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 10:59:08,611] Trial 304 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:08,960] Trial 305 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:09,106] Trial 306 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:09,286] Trial 307 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:09,432] Trial 308 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:09,714] Trial 309 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:10,107] Trial 310 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:10,252] Trial 311 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:10,386] Trial 312 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:10,576] Trial 313 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:10,853] Trial 314 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:11,071] Trial 315 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:11,332] Trial 316 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:11,576] Trial 317 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:11,813] Trial 318 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:11,885] Trial 319 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:12,072] Trial 320 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:12,437] Trial 321 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:59:12,562] Trial 322 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:12,904] Trial 323 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:13,246] Trial 324 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:13,321] Trial 325 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:13,603] Trial 326 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:13,818] Trial 327 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:13,967] Trial 328 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:14,101] Trial 329 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:14,432] Trial 330 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:14,684] Trial 331 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:14,896] Trial 332 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:15,097] Trial 333 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:15,313] Trial 334 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:15,362] Trial 335 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:15,624] Trial 336 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:15,913] Trial 337 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:16,052] Trial 338 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:16,397] Trial 339 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 10:59:16,709] Trial 340 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 10:59:16,895] Trial 341 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:16,942] Trial 342 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:17,288] Trial 343 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:17,703] Trial 344 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:17,977] Trial 345 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:18,212] Trial 346 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:18,430] Trial 347 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:18,752] Trial 348 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:18,990] Trial 349 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:19,179] Trial 350 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:19,321] Trial 351 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:19,442] Trial 352 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:19,643] Trial 353 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:19,818] Trial 354 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:19,910] Trial 355 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:19,952] Trial 356 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:20,289] Trial 357 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:20,615] Trial 358 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:20,903] Trial 359 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:59:21,265] Trial 360 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:21,542] Trial 361 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:21,836] Trial 362 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:22,140] Trial 363 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:22,479] Trial 364 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:22,609] Trial 365 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:22,950] Trial 366 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:23,158] Trial 367 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:23,486] Trial 368 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:23,614] Trial 369 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:23,941] Trial 370 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:24,291] Trial 371 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:24,542] Trial 372 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:24,720] Trial 373 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:24,926] Trial 374 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:25,026] Trial 375 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:25,116] Trial 376 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:25,332] Trial 377 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:25,574] Trial 378 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:25,805] Trial 379 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:26,134] Trial 380 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:26,294] Trial 381 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:26,532] Trial 382 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:26,917] Trial 383 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:27,282] Trial 384 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:27,521] Trial 385 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:27,564] Trial 386 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:27,817] Trial 387 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 10:59:28,060] Trial 388 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:28,313] Trial 389 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:28,656] Trial 390 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:28,913] Trial 391 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:28,963] Trial 392 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:29,384] Trial 393 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:59:29,532] Trial 394 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:29,605] Trial 395 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:29,849] Trial 396 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:30,045] Trial 397 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:30,228] Trial 398 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:30,439] Trial 399 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:30,722] Trial 400 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:30,866] Trial 401 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:30,989] Trial 402 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:31,269] Trial 403 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:31,439] Trial 404 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:31,685] Trial 405 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:59:32,060] Trial 406 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:32,227] Trial 407 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:32,278] Trial 408 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:32,495] Trial 409 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:32,753] Trial 410 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:33,075] Trial 411 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 10:59:33,328] Trial 412 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:33,697] Trial 413 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:33,865] Trial 414 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:33,910] Trial 415 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:34,234] Trial 416 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 10:59:34,560] Trial 417 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:34,609] Trial 418 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:34,725] Trial 419 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:35,009] Trial 420 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:35,390] Trial 421 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:59:35,463] Trial 422 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:35,604] Trial 423 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:35,821] Trial 424 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:36,129] Trial 425 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:36,349] Trial 426 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:36,694] Trial 427 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:36,896] Trial 428 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:37,021] Trial 429 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:37,205] Trial 430 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:37,362] Trial 431 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:37,416] Trial 432 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:37,596] Trial 433 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:37,897] Trial 434 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:37,948] Trial 435 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:38,049] Trial 436 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:38,239] Trial 437 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:38,401] Trial 438 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:38,542] Trial 439 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:38,886] Trial 440 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:39,205] Trial 441 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:39,313] Trial 442 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:39,649] Trial 443 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:39,790] Trial 444 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:39,854] Trial 445 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:40,122] Trial 446 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:40,450] Trial 447 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:40,660] Trial 448 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:40,818] Trial 449 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:41,004] Trial 450 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:41,226] Trial 451 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:41,334] Trial 452 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:41,620] Trial 453 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 10:59:41,928] Trial 454 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:41,997] Trial 455 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:42,242] Trial 456 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:42,614] Trial 457 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:42,806] Trial 458 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:43,079] Trial 459 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-25 10:59:43,305] Trial 460 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:43,529] Trial 461 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:43,854] Trial 462 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:43,950] Trial 463 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:44,124] Trial 464 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:44,370] Trial 465 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:59:44,525] Trial 466 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:44,570] Trial 467 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:44,847] Trial 468 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:44,995] Trial 469 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:45,089] Trial 470 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:45,336] Trial 471 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 10:59:45,566] Trial 472 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:45,813] Trial 473 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:46,013] Trial 474 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:46,063] Trial 475 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:46,114] Trial 476 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:46,416] Trial 477 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:46,540] Trial 478 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:46,589] Trial 479 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:46,636] Trial 480 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:46,773] Trial 481 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:47,022] Trial 482 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:47,068] Trial 483 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:47,279] Trial 484 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:47,381] Trial 485 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:47,578] Trial 486 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:47,894] Trial 487 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:48,173] Trial 488 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:48,451] Trial 489 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 10:59:48,573] Trial 490 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:48,666] Trial 491 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:48,962] Trial 492 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:49,013] Trial 493 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:49,089] Trial 494 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:49,429] Trial 495 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:49,505] Trial 496 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:49,615] Trial 497 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:49,731] Trial 498 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:49,895] Trial 499 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:49,966] Trial 500 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:50,163] Trial 501 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:50,216] Trial 502 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:50,567] Trial 503 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:50,898] Trial 504 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:51,193] Trial 505 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:51,645] Trial 506 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:51,804] Trial 507 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:52,069] Trial 508 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:52,374] Trial 509 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:52,645] Trial 510 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:52,876] Trial 511 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:53,100] Trial 512 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:53,462] Trial 513 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:53,624] Trial 514 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:53,874] Trial 515 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 10:59:54,062] Trial 516 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:54,393] Trial 517 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:54,675] Trial 518 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:54,807] Trial 519 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:54,910] Trial 520 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:55,121] Trial 521 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:55,580] Trial 522 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:55,945] Trial 523 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:56,217] Trial 524 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:59:56,310] Trial 525 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:56,588] Trial 526 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:56,912] Trial 527 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:57,117] Trial 528 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:57,449] Trial 529 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 10:59:57,537] Trial 530 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 10:59:57,769] Trial 531 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 10:59:57,958] Trial 532 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:58,451] Trial 533 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-25 10:59:58,865] Trial 534 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 10:59:59,284] Trial 535 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 10:59:59,494] Trial 536 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 10:59:59,914] Trial 537 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 11:00:00,017] Trial 538 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:00,460] Trial 539 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:00:00,926] Trial 540 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:00:01,289] Trial 541 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:01,440] Trial 542 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:01,820] Trial 543 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:02,223] Trial 544 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:02,322] Trial 545 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:02,558] Trial 546 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:02,788] Trial 547 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:03,015] Trial 548 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:03,419] Trial 549 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:03,760] Trial 550 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:04,235] Trial 551 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:04,415] Trial 552 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:04,615] Trial 553 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:04,841] Trial 554 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:05,198] Trial 555 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:05,447] Trial 556 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:05,742] Trial 557 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:06,024] Trial 558 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:06,267] Trial 559 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:06,470] Trial 560 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:06,606] Trial 561 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:06,909] Trial 562 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:07,307] Trial 563 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:07,697] Trial 564 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:07,948] Trial 565 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:08,166] Trial 566 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:08,265] Trial 567 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:08,470] Trial 568 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:08,689] Trial 569 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:09,020] Trial 570 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:09,518] Trial 571 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:00:09,591] Trial 572 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:09,824] Trial 573 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:10,027] Trial 574 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:10,210] Trial 575 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:10,348] Trial 576 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:10,570] Trial 577 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:10,935] Trial 578 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:11,174] Trial 579 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:11,406] Trial 580 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:11,591] Trial 581 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:11,978] Trial 582 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:00:12,273] Trial 583 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:12,348] Trial 584 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:12,713] Trial 585 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:13,080] Trial 586 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:17,629] Trial 587 finished with value: 0.2362401726263655 and parameters: {'tree_method': 'hist', 'max_depth': 6, 'min_child_weight': 6, 'subsample': 0.7818849992061668, 'colsample_bynode': 0.3425224130555876, 'reg_lambda': 0.018293235850862403}. Best is trial 1 with value: 0.23107522766919256.\n",
      "[I 2023-12-25 11:00:17,914] Trial 588 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:18,098] Trial 589 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:18,210] Trial 590 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:18,465] Trial 591 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:18,676] Trial 592 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:18,799] Trial 593 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:18,888] Trial 594 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:18,962] Trial 595 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:19,158] Trial 596 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:19,430] Trial 597 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:19,781] Trial 598 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:19,909] Trial 599 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:20,016] Trial 600 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:20,168] Trial 601 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:20,396] Trial 602 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:20,480] Trial 603 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:20,752] Trial 604 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:20,817] Trial 605 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:21,010] Trial 606 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:21,391] Trial 607 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:21,690] Trial 608 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:21,935] Trial 609 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:22,282] Trial 610 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:22,563] Trial 611 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:22,835] Trial 612 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:23,169] Trial 613 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:23,244] Trial 614 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:23,626] Trial 615 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:23,770] Trial 616 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:24,124] Trial 617 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:24,324] Trial 618 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:24,526] Trial 619 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:24,692] Trial 620 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:25,009] Trial 621 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:25,206] Trial 622 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:25,459] Trial 623 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:25,731] Trial 624 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:26,174] Trial 625 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:26,563] Trial 626 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:00:26,926] Trial 627 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:27,321] Trial 628 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:00:27,651] Trial 629 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:27,732] Trial 630 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:27,924] Trial 631 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:28,269] Trial 632 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:28,479] Trial 633 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:28,784] Trial 634 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:00:28,977] Trial 635 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:29,135] Trial 636 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:29,441] Trial 637 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:29,636] Trial 638 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:29,817] Trial 639 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:00:30,022] Trial 640 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:30,168] Trial 641 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:30,550] Trial 642 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:30,771] Trial 643 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:31,046] Trial 644 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:31,403] Trial 645 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:31,662] Trial 646 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:31,953] Trial 647 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:32,354] Trial 648 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:32,553] Trial 649 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:32,820] Trial 650 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:33,155] Trial 651 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:33,343] Trial 652 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:33,613] Trial 653 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:33,933] Trial 654 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:00:34,155] Trial 655 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:34,350] Trial 656 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:34,620] Trial 657 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:34,942] Trial 658 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:35,144] Trial 659 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:35,386] Trial 660 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:35,545] Trial 661 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:35,744] Trial 662 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:35,979] Trial 663 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:36,217] Trial 664 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:36,397] Trial 665 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:36,609] Trial 666 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:36,802] Trial 667 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:36,873] Trial 668 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:37,063] Trial 669 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:37,212] Trial 670 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:37,504] Trial 671 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:37,681] Trial 672 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:38,129] Trial 673 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:38,260] Trial 674 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:38,590] Trial 675 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:38,800] Trial 676 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:38,857] Trial 677 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:39,183] Trial 678 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:39,328] Trial 679 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:39,534] Trial 680 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:39,865] Trial 681 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:40,108] Trial 682 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:40,415] Trial 683 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:40,617] Trial 684 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:40,916] Trial 685 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:40,995] Trial 686 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:41,357] Trial 687 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:41,750] Trial 688 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:00:42,013] Trial 689 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:42,470] Trial 690 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:42,800] Trial 691 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:43,017] Trial 692 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:43,384] Trial 693 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:43,549] Trial 694 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:43,611] Trial 695 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:43,857] Trial 696 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:44,127] Trial 697 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:44,438] Trial 698 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:44,631] Trial 699 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:44,832] Trial 700 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:45,028] Trial 701 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:45,325] Trial 702 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:00:45,513] Trial 703 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:45,601] Trial 704 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:45,841] Trial 705 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:46,010] Trial 706 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:46,358] Trial 707 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:46,769] Trial 708 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:47,104] Trial 709 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:47,285] Trial 710 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:47,577] Trial 711 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:47,877] Trial 712 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:47,958] Trial 713 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:48,298] Trial 714 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:48,634] Trial 715 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:48,892] Trial 716 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:49,051] Trial 717 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:49,454] Trial 718 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:49,781] Trial 719 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:50,083] Trial 720 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:00:50,332] Trial 721 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:50,683] Trial 722 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:51,071] Trial 723 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:51,126] Trial 724 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:51,318] Trial 725 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:51,535] Trial 726 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:51,903] Trial 727 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:52,338] Trial 728 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:52,540] Trial 729 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:52,794] Trial 730 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:53,014] Trial 731 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:53,246] Trial 732 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:53,329] Trial 733 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:53,758] Trial 734 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:54,123] Trial 735 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:00:54,418] Trial 736 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:54,794] Trial 737 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:55,029] Trial 738 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:55,279] Trial 739 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:55,651] Trial 740 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:55,945] Trial 741 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:00:56,027] Trial 742 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:56,238] Trial 743 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:56,528] Trial 744 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:00:56,788] Trial 745 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:57,296] Trial 746 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:57,496] Trial 747 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:57,660] Trial 748 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:58,040] Trial 749 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:00:58,407] Trial 750 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:00:58,492] Trial 751 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:00:58,694] Trial 752 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:59,252] Trial 753 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:00:59,478] Trial 754 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:00:59,697] Trial 755 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:00,086] Trial 756 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:00,556] Trial 757 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:01:00,776] Trial 758 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:01,098] Trial 759 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:01,334] Trial 760 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:01,559] Trial 761 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:01,722] Trial 762 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:02,072] Trial 763 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:02,252] Trial 764 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:02,489] Trial 765 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:02,894] Trial 766 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:01:03,326] Trial 767 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:03,606] Trial 768 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:03,966] Trial 769 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:04,333] Trial 770 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:04,502] Trial 771 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:04,851] Trial 772 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:05,190] Trial 773 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:05,414] Trial 774 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:05,615] Trial 775 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:05,959] Trial 776 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:06,118] Trial 777 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:06,460] Trial 778 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:06,730] Trial 779 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:06,798] Trial 780 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:07,189] Trial 781 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:07,868] Trial 782 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-25 11:01:07,996] Trial 783 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:08,076] Trial 784 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:08,231] Trial 785 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:08,711] Trial 786 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:01:08,787] Trial 787 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:08,942] Trial 788 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:09,071] Trial 789 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:09,197] Trial 790 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:09,294] Trial 791 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:09,439] Trial 792 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:09,864] Trial 793 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:09,936] Trial 794 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:10,025] Trial 795 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:10,305] Trial 796 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:10,752] Trial 797 pruned. Trial was pruned at iteration 10.\n",
      "[I 2023-12-25 11:01:11,109] Trial 798 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:11,428] Trial 799 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:11,543] Trial 800 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:12,015] Trial 801 pruned. Trial was pruned at iteration 11.\n",
      "[I 2023-12-25 11:01:12,274] Trial 802 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:01:12,347] Trial 803 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:12,709] Trial 804 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:13,047] Trial 805 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:13,381] Trial 806 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:13,454] Trial 807 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:13,826] Trial 808 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:13,975] Trial 809 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:14,271] Trial 810 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:14,634] Trial 811 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:14,745] Trial 812 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:14,822] Trial 813 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:14,886] Trial 814 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:15,220] Trial 815 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:15,338] Trial 816 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:15,445] Trial 817 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:15,525] Trial 818 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:15,862] Trial 819 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:15,980] Trial 820 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:16,342] Trial 821 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:16,662] Trial 822 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:17,112] Trial 823 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:17,429] Trial 824 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:17,504] Trial 825 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:17,598] Trial 826 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:17,677] Trial 827 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:17,915] Trial 828 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:17,996] Trial 829 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:18,091] Trial 830 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:18,215] Trial 831 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:18,513] Trial 832 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:18,806] Trial 833 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:19,074] Trial 834 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:19,213] Trial 835 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:19,314] Trial 836 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:19,545] Trial 837 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:19,684] Trial 838 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:20,000] Trial 839 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:20,156] Trial 840 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:20,227] Trial 841 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:20,536] Trial 842 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:20,662] Trial 843 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:20,845] Trial 844 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:20,922] Trial 845 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:21,283] Trial 846 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:21,425] Trial 847 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:21,530] Trial 848 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:21,813] Trial 849 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:01:21,885] Trial 850 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:21,978] Trial 851 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:22,122] Trial 852 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:22,227] Trial 853 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:22,287] Trial 854 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:22,542] Trial 855 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:22,911] Trial 856 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:23,229] Trial 857 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:23,499] Trial 858 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:23,615] Trial 859 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:23,844] Trial 860 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:24,039] Trial 861 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:24,320] Trial 862 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:24,696] Trial 863 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:01:24,803] Trial 864 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:25,037] Trial 865 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:25,212] Trial 866 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:25,296] Trial 867 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:25,537] Trial 868 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:25,758] Trial 869 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:25,966] Trial 870 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:26,263] Trial 871 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:26,404] Trial 872 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:26,821] Trial 873 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:27,239] Trial 874 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:27,602] Trial 875 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:01:27,794] Trial 876 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:28,030] Trial 877 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:01:28,504] Trial 878 pruned. Trial was pruned at iteration 9.\n",
      "[I 2023-12-25 11:01:28,745] Trial 879 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:28,869] Trial 880 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:29,340] Trial 881 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:29,523] Trial 882 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:29,888] Trial 883 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:30,072] Trial 884 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:30,141] Trial 885 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:30,430] Trial 886 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:30,674] Trial 887 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:31,008] Trial 888 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:01:31,230] Trial 889 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:31,566] Trial 890 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:01:31,939] Trial 891 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:01:32,205] Trial 892 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:32,370] Trial 893 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:32,752] Trial 894 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:33,038] Trial 895 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:01:33,195] Trial 896 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:33,580] Trial 897 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:34,010] Trial 898 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:34,411] Trial 899 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:34,642] Trial 900 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:34,721] Trial 901 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:34,935] Trial 902 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:35,276] Trial 903 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:35,435] Trial 904 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:35,700] Trial 905 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:35,838] Trial 906 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:36,075] Trial 907 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:36,225] Trial 908 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:36,431] Trial 909 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:36,672] Trial 910 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:37,173] Trial 911 pruned. Trial was pruned at iteration 7.\n",
      "[I 2023-12-25 11:01:37,255] Trial 912 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:37,461] Trial 913 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:37,971] Trial 914 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:38,305] Trial 915 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:38,760] Trial 916 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:38,827] Trial 917 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:39,161] Trial 918 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:01:39,335] Trial 919 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:39,417] Trial 920 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:39,661] Trial 921 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:40,085] Trial 922 pruned. Trial was pruned at iteration 6.\n",
      "[I 2023-12-25 11:01:40,298] Trial 923 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:40,485] Trial 924 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:40,557] Trial 925 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:40,784] Trial 926 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:41,048] Trial 927 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:41,247] Trial 928 pruned. Trial was pruned at iteration 2.\n",
      "[I 2023-12-25 11:01:41,566] Trial 929 pruned. Trial was pruned at iteration 3.\n",
      "[I 2023-12-25 11:01:41,795] Trial 930 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:42,109] Trial 931 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:42,299] Trial 932 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:42,379] Trial 933 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:42,722] Trial 934 pruned. Trial was pruned at iteration 5.\n",
      "[I 2023-12-25 11:01:42,883] Trial 935 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:42,986] Trial 936 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:43,238] Trial 937 pruned. Trial was pruned at iteration 1.\n",
      "[I 2023-12-25 11:01:43,426] Trial 938 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:43,769] Trial 939 pruned. Trial was pruned at iteration 4.\n",
      "[I 2023-12-25 11:01:44,132] Trial 940 pruned. Trial was pruned at iteration 8.\n",
      "[I 2023-12-25 11:01:44,243] Trial 941 pruned. Trial was pruned at iteration 0.\n",
      "[I 2023-12-25 11:01:44,729] Trial 942 pruned. Trial was pruned at iteration 8.\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "tic = time.time()\n",
    "while time.time() - tic < 300:\n",
    "    study.optimize(objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 ==============================\n",
      "best score = 0.23107522766919256\n",
      "boosting params ---------------------------\n",
      "fixed learning rate: 0.3\n",
      "best boosting round: 23\n",
      "best tree params --------------------------\n",
      "tree_method : approx\n",
      "max_depth : 10\n",
      "min_child_weight : 6\n",
      "subsample : 0.9729188669457949\n",
      "colsample_bynode : 0.8491983767203796\n",
      "reg_lambda : 0.008587261143813469\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print('Stage 1 ==============================')\n",
    "print(f'best score = {study.best_trial.value}')\n",
    "print('boosting params ---------------------------')\n",
    "print(f'fixed learning rate: {learning_rate}')\n",
    "print(f'best boosting round: {study.best_trial.user_attrs[\"best_iteration\"]}')\n",
    "print('best tree params --------------------------')\n",
    "for k, v in study.best_trial.params.items():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide we want to tune the tree parameters a little more, we can just call `study.optimize(...)` again, adding as many trials as we want to the study.\n",
    "Once we're happy with the tree parameters, we can proceed to stage 2.\n",
    "\n",
    "## Stage 2: Intensify the Boosting Parameters\n",
    "\n",
    "Now we take the optimal tree parameters that we found in stage 1, and we train a new model with a fixed low learning rate; here I use 0.01, but you could go lower.\n",
    "The lower your learning rate, the better your performance (with diminishing returns) and the more boosting rounds you'll need to max out the evaluation metric on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "low_learning_rate = 0.01\n",
    "\n",
    "params = {}\n",
    "params.update(base_params)\n",
    "params.update(study.best_trial.params)\n",
    "params['learning_rate'] = low_learning_rate\n",
    "model_stage2 = xgb.train(params=params, dtrain=dtrain, \n",
    "                         num_boost_round=10000,\n",
    "                         evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                         early_stopping_rounds=50,\n",
    "                         verbose_eval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 2 ==============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score = 0.22172991931438446\n",
      "boosting params ---------------------------\n",
      "fixed learning rate: 0.01\n",
      "best boosting round: 1446\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print('Stage 2 ==============================')\n",
    "print(f'best score = {score_model(model_stage2, dvalid)}')\n",
    "print('boosting params ---------------------------')\n",
    "print(f'fixed learning rate: {params[\"learning_rate\"]}')\n",
    "print(f'best boosting round: {model_stage2.best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the Final Model\n",
    "\n",
    "Now we can train our final model on the combined training and validation datasets using the optimal tree parameters from stage 1 and the fixed learning rate and optimal boosting rounds from stage 2.\n",
    "Then we evaluate on the held out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "model_final = xgb.train(params=params, dtrain=dtrainvalid, \n",
    "                        num_boost_round=model_stage2.best_iteration,\n",
    "                        verbose_eval=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model ==========================\n",
      "test score = 0.21621863543987274\n",
      "parameters ---------------------------\n",
      "objective : reg:squarederror\n",
      "eval_metric : rmse\n",
      "tree_method : approx\n",
      "max_depth : 10\n",
      "min_child_weight : 6\n",
      "subsample : 0.9729188669457949\n",
      "colsample_bynode : 0.8491983767203796\n",
      "reg_lambda : 0.008587261143813469\n",
      "learning_rate : 0.01\n",
      "num_boost_round: 1446\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "print('Final Model ==========================')\n",
    "print(f'test score = {score_model(model_final, dtest)}')\n",
    "print('parameters ---------------------------')\n",
    "for k, v in params.items():\n",
    "    print(k, ':', v)\n",
    "print(f'num_boost_round: {model_stage2.best_iteration}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back in the [regression post](/posts/xgboost-for-regression-in-python/) \n",
    "we got an RMSE of about 0.231 just using default parameter values, which put us in 5th place on the [leaderboard for the Kagle dozers competition](https://www.kaggle.com/competitions/bluebook-for-bulldozers/leaderboard).\n",
    "Now with about 10 minutes of hyperparameter tuning, our RMSE is down to 0.216 which puts us in 1st place by a huge margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could possibly go wrong?\n",
    "\n",
    "Hyperparameter tuning can easily be overlooked in the move-fast-and-break-everything hustle of building an ML product, but it can also easily become overkill or even downright harmful, depending on the application.\n",
    "There are three key questions to ask:\n",
    "\n",
    "1. How much value is created by an incremental gain in model prediction accuracy?\n",
    "1. What is the cost of increasing model prediction accuracy?\n",
    "1. Is my model answering the right question?\n",
    "\n",
    "Sometimes a small gain in model prediction performance translates into millions of dollars of impact.\n",
    "The dream scenario is that you swoop in on some key model in your organization, markedly improve its accuracy with an easy afternoon of hyperparameter tuning, realize massive improvements in your org's KPIs, and get mad respect, bonuses, and promoted.\n",
    "But the reality is that often additional model accuracy doesn't really change business KPIs by very much.\n",
    "Try to figure out the actual value of improved model accuracy and proceed accordingly.\n",
    "\n",
    "Remember too that hyperparameter tuning has its costs, most obviously the developer time and compute resources for the search itself.\n",
    "It can also lead us to larger and deeper models which take longer to train, occupy larger memory footprints, and have higher prediction latency.\n",
    "\n",
    "Worst of all and quite counterintuitively, it's possible that improving a model's prediction accuracy can compromise overall business KPIs.\n",
    "I've seen this with my own eyes at work;\n",
    "offline testing shows that hyperparameter tuning significantly improves a model's prediction accuracy, but when the model goes into production, an AB test shows that the business KPIs are actually worse.\n",
    "What happened?\n",
    "In this case, the model's prediction was being used indirectly to infer the relationship between one of the features and the prediction target to inform automatic business decisions.\n",
    "Answering questions about how changing an input will affect an output requires causal reasoning, and [traditional ML models are not the right tool for the job](https://arxiv.org/abs/1608.00060).\n",
    "I'll have a lot more to say about that soon;\n",
    "let this story foreshadow an epic new epoch on Random Realizations...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "There it is, an efficient and ridiculously easy hyperparameter tuning strategy for XGBoost using optuna.\n",
    "If you found this helpful, if you have questions, or if you have your own preferred method for parameter search, let me know about it down in the comments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
