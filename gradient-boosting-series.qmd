---
title: "Gradient Boosting"
listing:
  contents:
    - posts/gradient-boosting-machine-from-scratch
    - posts/get-down-with-gradient-descent
    - posts/how-gradient-boosting-does-gradient-descent
    - posts/gradient-boosting-machine-with-any-loss-function
    - posts/consider-the-decision-tree
    - posts/decision-tree-from-scratch
    - posts/xgboost-explained
    - posts/xgboost-from-scratch
    - posts/xgboost-for-regression-in-python
    - posts/gradient-boosting-multi-class-classification-from-scratch
    
  sort: "date"
  type: default
  categories: true
  sort-ui: false
  filter-ui: false
  fields: [date, title, description, categories, image]
page-layout: full
title-block-banner: false
---

Ahh, gradient boosting.  In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. 
This series of posts strives to give a comprehensive understanding of gradient boosting by providing intuitive mathematical explanations, from-scratch implementations of key algorithms, and examples of how to apply modern gradient boosting libraries to solve practical data science problems.

I recommend reading through the series in order, since concepts tend to build on earlier ideas.