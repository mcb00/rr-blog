---
title: "Gradient Boosting"
listing:
  contents:
    - posts/02-gradient-boosting-machine-from-scratch
    - posts/03-get-down-with-gradient-descent
    - posts/04-how-gradient-boosting-does-gradient-descent
    - posts/06-gradient-boosting-machine-with-any-loss-function
    - posts/07-consider-the-decision-tree
    - posts/08-decision-tree-from-scratch
    - posts/09-how-to-understand-xgboost
    - posts/10-xgboost-from-scratch
  sort: "date"
  type: default
  categories: true
  sort-ui: false
  filter-ui: false
  fields: [date, title, description, categories, image]
page-layout: full
title-block-banner: false
---

Ahh, gradient boosting.  In addition to having a totally kickass name, this family of machine learning algorithms is currently among the best known approaches for prediction problems on structured data. 
This series of posts strives to give a comprehensive understanding of gradient boosting by providing intuitive mathematical explanations, from-scratch implementations of key algorithms, and examples of how to apply modern gradient boosting libraries to solve practical data science problems.

I recommend reading through the series in order, since concepts tend to build on earlier ideas.